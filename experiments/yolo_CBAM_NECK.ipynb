{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8153b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2938f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6998f56",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a602257",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'Unformed': 0, 'Burr': 1}\n",
    "CLASS_ID_TO_NAME = {0: 'Unformed', 1: 'Burr'}\n",
    "BOX_COLOR = {'Unformed':(200, 0, 0), 'Burr':(0, 0, 200)}\n",
    "TEXT_COLOR = (255, 255, 255)\n",
    "\n",
    "def save_model(model_state, model_name, save_dir=\"./trained_model\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))\n",
    "\n",
    "\n",
    "def visualize_bbox(image, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    x_center, y_center, w, h = bbox\n",
    "    x_min = int(x_center - w/2)\n",
    "    y_min = int(y_center - h/2)\n",
    "    x_max = int(x_center + w/2)\n",
    "    y_max = int(y_center + h/2)\n",
    "    \n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color[class_name], thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(image, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color[class_name], -1)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "#         print('category_id: ',category_id)\n",
    "        class_name = CLASS_ID_TO_NAME[category_id.item()]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957496f",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "395a1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PET_dataset():\n",
    "    def __init__(self,part,neck_dir,body_dir,phase, transformer=None, aug=None, aug_factor=0):\n",
    "        self.neck_dir=neck_dir\n",
    "        self.body_dir=body_dir\n",
    "        self.part=part\n",
    "        self.phase=phase\n",
    "        self.transformer=transformer\n",
    "        self.aug=aug\n",
    "        self.aug_factor=aug_factor\n",
    "        if(self.part==\"body\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.body_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.body_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        elif(self.part==\"neck\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.neck_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.neck_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        \n",
    "        self.auged_img_list, self.auged_label_list=self.make_aug_list(self.image_files, self.label_files)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if(self.aug==None):\n",
    "            filename, image = self.get_image(self.part, index)\n",
    "            bboxes, class_ids = self.get_label(self.part, index)\n",
    "\n",
    "            if(self.transformer):\n",
    "                transformed_data=self.transformer(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "                image = transformed_data['image']\n",
    "                bboxes = np.array(transformed_data['bboxes'])\n",
    "                class_ids = np.array(transformed_data['class_ids'])\n",
    "\n",
    "\n",
    "            target = {}\n",
    "    #         print(f'bboxes:{bboxes}\\nclass_ids:{class_ids}\\nlen_bboxes:{len(bboxes)}\\nlen_class_ids:{len(class_ids)}')\n",
    "    #         print(f'filename: {filename}')\n",
    "            target[\"boxes\"] = torch.Tensor(bboxes).float()\n",
    "            target[\"labels\"] = torch.Tensor(class_ids).long()\n",
    "\n",
    "            ###\n",
    "            bboxes=torch.Tensor(bboxes).float()\n",
    "            class_ids=torch.Tensor(class_ids).long()\n",
    "            target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "            ###\n",
    "        else:\n",
    "            image=self.auged_img_list[index][1]\n",
    "            target=self.auged_label_list[index]\n",
    "            filename=self.auged_img_list[index][0]\n",
    "        return image, target, filename\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        length=0\n",
    "        if(self.aug==None):\n",
    "            length=len(self.image_files)\n",
    "        else:\n",
    "            length=len(self.auged_img_list)\n",
    "        return length\n",
    "    \n",
    "    def make_aug_list(self,ori_image_list,ori_label_files):\n",
    "        aug_image_list=[]\n",
    "        aug_label_list=[]\n",
    "        \n",
    "        print(f\"start making augmented images-- augmented factor:{self.aug_factor}\")\n",
    "        for i in range(len(ori_image_list)):\n",
    "            filename, ori_image = self.get_image(self.part, i)\n",
    "            ori_bboxes, ori_class_ids = self.get_label(self.part, i)\n",
    "            for j in range(self.aug_factor):\n",
    "                auged_data=self.aug(image=ori_image, bboxes=ori_bboxes, class_ids=ori_class_ids)\n",
    "                image = auged_data['image']\n",
    "                bboxes = np.array(auged_data['bboxes'])\n",
    "                class_ids = np.array(auged_data['class_ids'])\n",
    "                \n",
    "                bboxes=torch.Tensor(bboxes).float()\n",
    "                class_ids=torch.Tensor(class_ids).long()\n",
    "                \n",
    "                aug_image_list.append((filename, image))\n",
    "                aug_label_list.append(np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1))\n",
    "        \n",
    "        print(f\"total length of augmented images: {len(aug_image_list)}\")\n",
    "        \n",
    "        return aug_image_list, aug_label_list\n",
    "        \n",
    "    \n",
    "    def get_image(self, part, index): # 이미지 불러오는 함수\n",
    "        filename = self.image_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body called!-> {self.part}\")\n",
    "            image_path = self.body_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck called!-> {self.part}\")\n",
    "            image_path = self.neck_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return filename, image\n",
    "    \n",
    "    def get_label(self, part, index): # label (box좌표, class_id) 불러오는 함수\n",
    "        label_filename=self.label_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body label called!-> {self.part}\")\n",
    "            label_path = self.body_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck label called!-> {self.part}\")\n",
    "            label_path = self.neck_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        with open(label_path, 'r') as file:\n",
    "            labels = file.readlines()\n",
    "        \n",
    "        class_ids=[]\n",
    "        bboxes=[]\n",
    "        for label in labels:\n",
    "            label=label.replace(\"\\n\", \"\")\n",
    "            obj=label.split(' ')[0]\n",
    "            coor=label.split(' ')[1:]\n",
    "            obj=int(obj)\n",
    "            coor=list(map(float, coor))\n",
    "            class_ids.append(obj)\n",
    "            bboxes.append(coor)\n",
    "            \n",
    "        return bboxes, class_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214f686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 448\n",
    "\n",
    "transformer = A.Compose([ \n",
    "        # bounding box의 변환, augmentation에서 albumentations는 Detection 학습을 할 때 굉장히 유용하다. \n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        # albumentations 라이브러리에서는 Normalization을 먼저 진행해 주고 tensor화를 진행해 주어야한다.\n",
    "    ],\n",
    "    # box 위치에 대한 transformation도 함께 진행된다. \n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "augmentator=A.Compose([\n",
    "#     A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.7),\n",
    "#     A.Sharpen(p=0.7),\n",
    "    A.BBoxSafeRandomCrop(p=0.6),\n",
    "    A.VerticalFlip (p=0.5),\n",
    "    A.HueSaturationValue(p=0.5),\n",
    "    A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), target_list, filename_list\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d997e857",
   "metadata": {},
   "source": [
    "# NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "# BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "\n",
    "NECK_PATH = '/home/host_data/PET_data_IP_AUG/aug_patched_Neck/'\n",
    "BODY_PATH = '/home/host_data/PET_data_image_patching/Body'\n",
    "# trainset_yes_aug=PET_dataset(part='neck',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=augmentator, aug_factor=5)\n",
    "trainset_no_aug=PET_dataset(part='neck',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='valid', transformer=transformer, aug=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "745d4296",
   "metadata": {},
   "source": [
    "len(trainset_no_aug)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08066fa2",
   "metadata": {},
   "source": [
    "@interact(index=(0, len(trainset_no_aug)-1))\n",
    "\n",
    "def show_sample(index=0):\n",
    "    image, target, filename = trainset_no_aug[index]\n",
    "    image=image.permute(1,2,0).numpy()\n",
    "    img_H, img_W, _ = image.shape\n",
    "    print(filename)\n",
    "    print(image.shape)\n",
    "#     print(image)\n",
    "\n",
    "#     bboxes = target['boxes']\n",
    "#     class_ids = target[\"labels\"]\n",
    "    \n",
    "    ###\n",
    "    bboxes = target[:, 0:4]\n",
    "    class_ids = target[:, 4]\n",
    "    ###\n",
    "    bboxes[:, [0,2]] *= img_W\n",
    "    bboxes[:, [1,3]] *= img_H\n",
    "\n",
    "    canvas = visualize(image, bboxes, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show_sample()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39a26f95",
   "metadata": {},
   "source": [
    "@interact(index=(0, len(trainset_yes_aug)-1))\n",
    "\n",
    "def show_sample(index=0):\n",
    "    image, target, filename = trainset_yes_aug[index]\n",
    "    image=image.permute(1,2,0).numpy()\n",
    "    img_H, img_W, _ = image.shape\n",
    "    print(filename)\n",
    "    print(image.shape)\n",
    "#     print(image)\n",
    "\n",
    "#     bboxes = target['boxes']\n",
    "#     class_ids = target[\"labels\"]\n",
    "    ###\n",
    "    bboxes = target[:, 0:4]\n",
    "    class_ids = target[:, 4]\n",
    "    ###\n",
    "    bboxes[:, [0,2]] *= img_W\n",
    "    bboxes[:, [1,3]] *= img_H\n",
    "    print(bboxes)\n",
    "\n",
    "    canvas = visualize(image, bboxes, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eba35b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83862362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18_cbam', 'resnet34_cbam', 'resnet50_cbam', 'resnet101_cbam',\n",
    "           'resnet152_cbam']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.ca = ChannelAttention(planes)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.ca = ChannelAttention(planes * 4)\n",
    "        self.sa = SpatialAttention()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.ca(out) * out\n",
    "        out = self.sa(out) * out\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=2048, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=12, kernel_size=2, padding=0, stride=2,bias=False),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.head(x)\n",
    "\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18_cbam(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        pretrained_state_dict = model_zoo.load_url(model_urls['resnet18'])\n",
    "        now_state_dict        = model.state_dict()\n",
    "        now_state_dict.update(pretrained_state_dict)\n",
    "        model.load_state_dict(now_state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34_cbam(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        pretrained_state_dict = model_zoo.load_url(model_urls['resnet34'])\n",
    "        now_state_dict        = model.state_dict()\n",
    "        now_state_dict.update(pretrained_state_dict)\n",
    "        model.load_state_dict(now_state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50_cbam(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        pretrained_state_dict = model_zoo.load_url(model_urls['resnet50'])\n",
    "        now_state_dict        = model.state_dict()\n",
    "        now_state_dict.update(pretrained_state_dict)\n",
    "        model.load_state_dict(now_state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101_cbam(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        pretrained_state_dict = model_zoo.load_url(model_urls['resnet101'])\n",
    "        now_state_dict        = model.state_dict()\n",
    "        now_state_dict.update(pretrained_state_dict)\n",
    "        model.load_state_dict(now_state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152_cbam(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        pretrained_state_dict = model_zoo.load_url(model_urls['resnet152'])\n",
    "        now_state_dict        = model.state_dict()\n",
    "        now_state_dict.update(pretrained_state_dict)\n",
    "        model.load_state_dict(now_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3dd80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (ca): ChannelAttention(\n",
       "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "        (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
       "        (fc): Sequential(\n",
       "          (0): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): ReLU()\n",
       "          (2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "      (sa): SpatialAttention(\n",
       "        (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(512, 12, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "    (7): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "# model = YOLO_SWIN(num_classes=NUM_CLASSES)\n",
    "model = resnet50_cbam(pretrained=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ecd862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]           4,096\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "           Conv2d-11        [-1, 256, 112, 112]          16,384\n",
      "      BatchNorm2d-12        [-1, 256, 112, 112]             512\n",
      "AdaptiveAvgPool2d-13            [-1, 256, 1, 1]               0\n",
      "           Conv2d-14             [-1, 16, 1, 1]           4,096\n",
      "             ReLU-15             [-1, 16, 1, 1]               0\n",
      "           Conv2d-16            [-1, 256, 1, 1]           4,096\n",
      "AdaptiveMaxPool2d-17            [-1, 256, 1, 1]               0\n",
      "           Conv2d-18             [-1, 16, 1, 1]           4,096\n",
      "             ReLU-19             [-1, 16, 1, 1]               0\n",
      "           Conv2d-20            [-1, 256, 1, 1]           4,096\n",
      "          Sigmoid-21            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-22            [-1, 256, 1, 1]               0\n",
      "           Conv2d-23          [-1, 1, 112, 112]              98\n",
      "          Sigmoid-24          [-1, 1, 112, 112]               0\n",
      " SpatialAttention-25          [-1, 1, 112, 112]               0\n",
      "           Conv2d-26        [-1, 256, 112, 112]          16,384\n",
      "      BatchNorm2d-27        [-1, 256, 112, 112]             512\n",
      "             ReLU-28        [-1, 256, 112, 112]               0\n",
      "       Bottleneck-29        [-1, 256, 112, 112]               0\n",
      "           Conv2d-30         [-1, 64, 112, 112]          16,384\n",
      "      BatchNorm2d-31         [-1, 64, 112, 112]             128\n",
      "             ReLU-32         [-1, 64, 112, 112]               0\n",
      "           Conv2d-33         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-34         [-1, 64, 112, 112]             128\n",
      "             ReLU-35         [-1, 64, 112, 112]               0\n",
      "           Conv2d-36        [-1, 256, 112, 112]          16,384\n",
      "      BatchNorm2d-37        [-1, 256, 112, 112]             512\n",
      "AdaptiveAvgPool2d-38            [-1, 256, 1, 1]               0\n",
      "           Conv2d-39             [-1, 16, 1, 1]           4,096\n",
      "             ReLU-40             [-1, 16, 1, 1]               0\n",
      "           Conv2d-41            [-1, 256, 1, 1]           4,096\n",
      "AdaptiveMaxPool2d-42            [-1, 256, 1, 1]               0\n",
      "           Conv2d-43             [-1, 16, 1, 1]           4,096\n",
      "             ReLU-44             [-1, 16, 1, 1]               0\n",
      "           Conv2d-45            [-1, 256, 1, 1]           4,096\n",
      "          Sigmoid-46            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-47            [-1, 256, 1, 1]               0\n",
      "           Conv2d-48          [-1, 1, 112, 112]              98\n",
      "          Sigmoid-49          [-1, 1, 112, 112]               0\n",
      " SpatialAttention-50          [-1, 1, 112, 112]               0\n",
      "             ReLU-51        [-1, 256, 112, 112]               0\n",
      "       Bottleneck-52        [-1, 256, 112, 112]               0\n",
      "           Conv2d-53         [-1, 64, 112, 112]          16,384\n",
      "      BatchNorm2d-54         [-1, 64, 112, 112]             128\n",
      "             ReLU-55         [-1, 64, 112, 112]               0\n",
      "           Conv2d-56         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-57         [-1, 64, 112, 112]             128\n",
      "             ReLU-58         [-1, 64, 112, 112]               0\n",
      "           Conv2d-59        [-1, 256, 112, 112]          16,384\n",
      "      BatchNorm2d-60        [-1, 256, 112, 112]             512\n",
      "AdaptiveAvgPool2d-61            [-1, 256, 1, 1]               0\n",
      "           Conv2d-62             [-1, 16, 1, 1]           4,096\n",
      "             ReLU-63             [-1, 16, 1, 1]               0\n",
      "           Conv2d-64            [-1, 256, 1, 1]           4,096\n",
      "AdaptiveMaxPool2d-65            [-1, 256, 1, 1]               0\n",
      "           Conv2d-66             [-1, 16, 1, 1]           4,096\n",
      "             ReLU-67             [-1, 16, 1, 1]               0\n",
      "           Conv2d-68            [-1, 256, 1, 1]           4,096\n",
      "          Sigmoid-69            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-70            [-1, 256, 1, 1]               0\n",
      "           Conv2d-71          [-1, 1, 112, 112]              98\n",
      "          Sigmoid-72          [-1, 1, 112, 112]               0\n",
      " SpatialAttention-73          [-1, 1, 112, 112]               0\n",
      "             ReLU-74        [-1, 256, 112, 112]               0\n",
      "       Bottleneck-75        [-1, 256, 112, 112]               0\n",
      "           Conv2d-76        [-1, 128, 112, 112]          32,768\n",
      "      BatchNorm2d-77        [-1, 128, 112, 112]             256\n",
      "             ReLU-78        [-1, 128, 112, 112]               0\n",
      "           Conv2d-79          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-80          [-1, 128, 56, 56]             256\n",
      "             ReLU-81          [-1, 128, 56, 56]               0\n",
      "           Conv2d-82          [-1, 512, 56, 56]          65,536\n",
      "      BatchNorm2d-83          [-1, 512, 56, 56]           1,024\n",
      "AdaptiveAvgPool2d-84            [-1, 512, 1, 1]               0\n",
      "           Conv2d-85             [-1, 32, 1, 1]          16,384\n",
      "             ReLU-86             [-1, 32, 1, 1]               0\n",
      "           Conv2d-87            [-1, 512, 1, 1]          16,384\n",
      "AdaptiveMaxPool2d-88            [-1, 512, 1, 1]               0\n",
      "           Conv2d-89             [-1, 32, 1, 1]          16,384\n",
      "             ReLU-90             [-1, 32, 1, 1]               0\n",
      "           Conv2d-91            [-1, 512, 1, 1]          16,384\n",
      "          Sigmoid-92            [-1, 512, 1, 1]               0\n",
      " ChannelAttention-93            [-1, 512, 1, 1]               0\n",
      "           Conv2d-94            [-1, 1, 56, 56]              98\n",
      "          Sigmoid-95            [-1, 1, 56, 56]               0\n",
      " SpatialAttention-96            [-1, 1, 56, 56]               0\n",
      "           Conv2d-97          [-1, 512, 56, 56]         131,072\n",
      "      BatchNorm2d-98          [-1, 512, 56, 56]           1,024\n",
      "             ReLU-99          [-1, 512, 56, 56]               0\n",
      "      Bottleneck-100          [-1, 512, 56, 56]               0\n",
      "          Conv2d-101          [-1, 128, 56, 56]          65,536\n",
      "     BatchNorm2d-102          [-1, 128, 56, 56]             256\n",
      "            ReLU-103          [-1, 128, 56, 56]               0\n",
      "          Conv2d-104          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-105          [-1, 128, 56, 56]             256\n",
      "            ReLU-106          [-1, 128, 56, 56]               0\n",
      "          Conv2d-107          [-1, 512, 56, 56]          65,536\n",
      "     BatchNorm2d-108          [-1, 512, 56, 56]           1,024\n",
      "AdaptiveAvgPool2d-109            [-1, 512, 1, 1]               0\n",
      "          Conv2d-110             [-1, 32, 1, 1]          16,384\n",
      "            ReLU-111             [-1, 32, 1, 1]               0\n",
      "          Conv2d-112            [-1, 512, 1, 1]          16,384\n",
      "AdaptiveMaxPool2d-113            [-1, 512, 1, 1]               0\n",
      "          Conv2d-114             [-1, 32, 1, 1]          16,384\n",
      "            ReLU-115             [-1, 32, 1, 1]               0\n",
      "          Conv2d-116            [-1, 512, 1, 1]          16,384\n",
      "         Sigmoid-117            [-1, 512, 1, 1]               0\n",
      "ChannelAttention-118            [-1, 512, 1, 1]               0\n",
      "          Conv2d-119            [-1, 1, 56, 56]              98\n",
      "         Sigmoid-120            [-1, 1, 56, 56]               0\n",
      "SpatialAttention-121            [-1, 1, 56, 56]               0\n",
      "            ReLU-122          [-1, 512, 56, 56]               0\n",
      "      Bottleneck-123          [-1, 512, 56, 56]               0\n",
      "          Conv2d-124          [-1, 128, 56, 56]          65,536\n",
      "     BatchNorm2d-125          [-1, 128, 56, 56]             256\n",
      "            ReLU-126          [-1, 128, 56, 56]               0\n",
      "          Conv2d-127          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-128          [-1, 128, 56, 56]             256\n",
      "            ReLU-129          [-1, 128, 56, 56]               0\n",
      "          Conv2d-130          [-1, 512, 56, 56]          65,536\n",
      "     BatchNorm2d-131          [-1, 512, 56, 56]           1,024\n",
      "AdaptiveAvgPool2d-132            [-1, 512, 1, 1]               0\n",
      "          Conv2d-133             [-1, 32, 1, 1]          16,384\n",
      "            ReLU-134             [-1, 32, 1, 1]               0\n",
      "          Conv2d-135            [-1, 512, 1, 1]          16,384\n",
      "AdaptiveMaxPool2d-136            [-1, 512, 1, 1]               0\n",
      "          Conv2d-137             [-1, 32, 1, 1]          16,384\n",
      "            ReLU-138             [-1, 32, 1, 1]               0\n",
      "          Conv2d-139            [-1, 512, 1, 1]          16,384\n",
      "         Sigmoid-140            [-1, 512, 1, 1]               0\n",
      "ChannelAttention-141            [-1, 512, 1, 1]               0\n",
      "          Conv2d-142            [-1, 1, 56, 56]              98\n",
      "         Sigmoid-143            [-1, 1, 56, 56]               0\n",
      "SpatialAttention-144            [-1, 1, 56, 56]               0\n",
      "            ReLU-145          [-1, 512, 56, 56]               0\n",
      "      Bottleneck-146          [-1, 512, 56, 56]               0\n",
      "          Conv2d-147          [-1, 128, 56, 56]          65,536\n",
      "     BatchNorm2d-148          [-1, 128, 56, 56]             256\n",
      "            ReLU-149          [-1, 128, 56, 56]               0\n",
      "          Conv2d-150          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-151          [-1, 128, 56, 56]             256\n",
      "            ReLU-152          [-1, 128, 56, 56]               0\n",
      "          Conv2d-153          [-1, 512, 56, 56]          65,536\n",
      "     BatchNorm2d-154          [-1, 512, 56, 56]           1,024\n",
      "AdaptiveAvgPool2d-155            [-1, 512, 1, 1]               0\n",
      "          Conv2d-156             [-1, 32, 1, 1]          16,384\n",
      "            ReLU-157             [-1, 32, 1, 1]               0\n",
      "          Conv2d-158            [-1, 512, 1, 1]          16,384\n",
      "AdaptiveMaxPool2d-159            [-1, 512, 1, 1]               0\n",
      "          Conv2d-160             [-1, 32, 1, 1]          16,384\n",
      "            ReLU-161             [-1, 32, 1, 1]               0\n",
      "          Conv2d-162            [-1, 512, 1, 1]          16,384\n",
      "         Sigmoid-163            [-1, 512, 1, 1]               0\n",
      "ChannelAttention-164            [-1, 512, 1, 1]               0\n",
      "          Conv2d-165            [-1, 1, 56, 56]              98\n",
      "         Sigmoid-166            [-1, 1, 56, 56]               0\n",
      "SpatialAttention-167            [-1, 1, 56, 56]               0\n",
      "            ReLU-168          [-1, 512, 56, 56]               0\n",
      "      Bottleneck-169          [-1, 512, 56, 56]               0\n",
      "          Conv2d-170          [-1, 256, 56, 56]         131,072\n",
      "     BatchNorm2d-171          [-1, 256, 56, 56]             512\n",
      "            ReLU-172          [-1, 256, 56, 56]               0\n",
      "          Conv2d-173          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-174          [-1, 256, 28, 28]             512\n",
      "            ReLU-175          [-1, 256, 28, 28]               0\n",
      "          Conv2d-176         [-1, 1024, 28, 28]         262,144\n",
      "     BatchNorm2d-177         [-1, 1024, 28, 28]           2,048\n",
      "AdaptiveAvgPool2d-178           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-179             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-180             [-1, 64, 1, 1]               0\n",
      "          Conv2d-181           [-1, 1024, 1, 1]          65,536\n",
      "AdaptiveMaxPool2d-182           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-183             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-184             [-1, 64, 1, 1]               0\n",
      "          Conv2d-185           [-1, 1024, 1, 1]          65,536\n",
      "         Sigmoid-186           [-1, 1024, 1, 1]               0\n",
      "ChannelAttention-187           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-188            [-1, 1, 28, 28]              98\n",
      "         Sigmoid-189            [-1, 1, 28, 28]               0\n",
      "SpatialAttention-190            [-1, 1, 28, 28]               0\n",
      "          Conv2d-191         [-1, 1024, 28, 28]         524,288\n",
      "     BatchNorm2d-192         [-1, 1024, 28, 28]           2,048\n",
      "            ReLU-193         [-1, 1024, 28, 28]               0\n",
      "      Bottleneck-194         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-195          [-1, 256, 28, 28]         262,144\n",
      "     BatchNorm2d-196          [-1, 256, 28, 28]             512\n",
      "            ReLU-197          [-1, 256, 28, 28]               0\n",
      "          Conv2d-198          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-199          [-1, 256, 28, 28]             512\n",
      "            ReLU-200          [-1, 256, 28, 28]               0\n",
      "          Conv2d-201         [-1, 1024, 28, 28]         262,144\n",
      "     BatchNorm2d-202         [-1, 1024, 28, 28]           2,048\n",
      "AdaptiveAvgPool2d-203           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-204             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-205             [-1, 64, 1, 1]               0\n",
      "          Conv2d-206           [-1, 1024, 1, 1]          65,536\n",
      "AdaptiveMaxPool2d-207           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-208             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-209             [-1, 64, 1, 1]               0\n",
      "          Conv2d-210           [-1, 1024, 1, 1]          65,536\n",
      "         Sigmoid-211           [-1, 1024, 1, 1]               0\n",
      "ChannelAttention-212           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-213            [-1, 1, 28, 28]              98\n",
      "         Sigmoid-214            [-1, 1, 28, 28]               0\n",
      "SpatialAttention-215            [-1, 1, 28, 28]               0\n",
      "            ReLU-216         [-1, 1024, 28, 28]               0\n",
      "      Bottleneck-217         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-218          [-1, 256, 28, 28]         262,144\n",
      "     BatchNorm2d-219          [-1, 256, 28, 28]             512\n",
      "            ReLU-220          [-1, 256, 28, 28]               0\n",
      "          Conv2d-221          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-222          [-1, 256, 28, 28]             512\n",
      "            ReLU-223          [-1, 256, 28, 28]               0\n",
      "          Conv2d-224         [-1, 1024, 28, 28]         262,144\n",
      "     BatchNorm2d-225         [-1, 1024, 28, 28]           2,048\n",
      "AdaptiveAvgPool2d-226           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-227             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-228             [-1, 64, 1, 1]               0\n",
      "          Conv2d-229           [-1, 1024, 1, 1]          65,536\n",
      "AdaptiveMaxPool2d-230           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-231             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-232             [-1, 64, 1, 1]               0\n",
      "          Conv2d-233           [-1, 1024, 1, 1]          65,536\n",
      "         Sigmoid-234           [-1, 1024, 1, 1]               0\n",
      "ChannelAttention-235           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-236            [-1, 1, 28, 28]              98\n",
      "         Sigmoid-237            [-1, 1, 28, 28]               0\n",
      "SpatialAttention-238            [-1, 1, 28, 28]               0\n",
      "            ReLU-239         [-1, 1024, 28, 28]               0\n",
      "      Bottleneck-240         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-241          [-1, 256, 28, 28]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 28, 28]             512\n",
      "            ReLU-243          [-1, 256, 28, 28]               0\n",
      "          Conv2d-244          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 28, 28]             512\n",
      "            ReLU-246          [-1, 256, 28, 28]               0\n",
      "          Conv2d-247         [-1, 1024, 28, 28]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 28, 28]           2,048\n",
      "AdaptiveAvgPool2d-249           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-250             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-251             [-1, 64, 1, 1]               0\n",
      "          Conv2d-252           [-1, 1024, 1, 1]          65,536\n",
      "AdaptiveMaxPool2d-253           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-254             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-255             [-1, 64, 1, 1]               0\n",
      "          Conv2d-256           [-1, 1024, 1, 1]          65,536\n",
      "         Sigmoid-257           [-1, 1024, 1, 1]               0\n",
      "ChannelAttention-258           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-259            [-1, 1, 28, 28]              98\n",
      "         Sigmoid-260            [-1, 1, 28, 28]               0\n",
      "SpatialAttention-261            [-1, 1, 28, 28]               0\n",
      "            ReLU-262         [-1, 1024, 28, 28]               0\n",
      "      Bottleneck-263         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-264          [-1, 256, 28, 28]         262,144\n",
      "     BatchNorm2d-265          [-1, 256, 28, 28]             512\n",
      "            ReLU-266          [-1, 256, 28, 28]               0\n",
      "          Conv2d-267          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-268          [-1, 256, 28, 28]             512\n",
      "            ReLU-269          [-1, 256, 28, 28]               0\n",
      "          Conv2d-270         [-1, 1024, 28, 28]         262,144\n",
      "     BatchNorm2d-271         [-1, 1024, 28, 28]           2,048\n",
      "AdaptiveAvgPool2d-272           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-273             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-274             [-1, 64, 1, 1]               0\n",
      "          Conv2d-275           [-1, 1024, 1, 1]          65,536\n",
      "AdaptiveMaxPool2d-276           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-277             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-278             [-1, 64, 1, 1]               0\n",
      "          Conv2d-279           [-1, 1024, 1, 1]          65,536\n",
      "         Sigmoid-280           [-1, 1024, 1, 1]               0\n",
      "ChannelAttention-281           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-282            [-1, 1, 28, 28]              98\n",
      "         Sigmoid-283            [-1, 1, 28, 28]               0\n",
      "SpatialAttention-284            [-1, 1, 28, 28]               0\n",
      "            ReLU-285         [-1, 1024, 28, 28]               0\n",
      "      Bottleneck-286         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-287          [-1, 256, 28, 28]         262,144\n",
      "     BatchNorm2d-288          [-1, 256, 28, 28]             512\n",
      "            ReLU-289          [-1, 256, 28, 28]               0\n",
      "          Conv2d-290          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-291          [-1, 256, 28, 28]             512\n",
      "            ReLU-292          [-1, 256, 28, 28]               0\n",
      "          Conv2d-293         [-1, 1024, 28, 28]         262,144\n",
      "     BatchNorm2d-294         [-1, 1024, 28, 28]           2,048\n",
      "AdaptiveAvgPool2d-295           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-296             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-297             [-1, 64, 1, 1]               0\n",
      "          Conv2d-298           [-1, 1024, 1, 1]          65,536\n",
      "AdaptiveMaxPool2d-299           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-300             [-1, 64, 1, 1]          65,536\n",
      "            ReLU-301             [-1, 64, 1, 1]               0\n",
      "          Conv2d-302           [-1, 1024, 1, 1]          65,536\n",
      "         Sigmoid-303           [-1, 1024, 1, 1]               0\n",
      "ChannelAttention-304           [-1, 1024, 1, 1]               0\n",
      "          Conv2d-305            [-1, 1, 28, 28]              98\n",
      "         Sigmoid-306            [-1, 1, 28, 28]               0\n",
      "SpatialAttention-307            [-1, 1, 28, 28]               0\n",
      "            ReLU-308         [-1, 1024, 28, 28]               0\n",
      "      Bottleneck-309         [-1, 1024, 28, 28]               0\n",
      "          Conv2d-310          [-1, 512, 28, 28]         524,288\n",
      "     BatchNorm2d-311          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-312          [-1, 512, 28, 28]               0\n",
      "          Conv2d-313          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-314          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-315          [-1, 512, 14, 14]               0\n",
      "          Conv2d-316         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-317         [-1, 2048, 14, 14]           4,096\n",
      "AdaptiveAvgPool2d-318           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-319            [-1, 128, 1, 1]         262,144\n",
      "            ReLU-320            [-1, 128, 1, 1]               0\n",
      "          Conv2d-321           [-1, 2048, 1, 1]         262,144\n",
      "AdaptiveMaxPool2d-322           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-323            [-1, 128, 1, 1]         262,144\n",
      "            ReLU-324            [-1, 128, 1, 1]               0\n",
      "          Conv2d-325           [-1, 2048, 1, 1]         262,144\n",
      "         Sigmoid-326           [-1, 2048, 1, 1]               0\n",
      "ChannelAttention-327           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-328            [-1, 1, 14, 14]              98\n",
      "         Sigmoid-329            [-1, 1, 14, 14]               0\n",
      "SpatialAttention-330            [-1, 1, 14, 14]               0\n",
      "          Conv2d-331         [-1, 2048, 14, 14]       2,097,152\n",
      "     BatchNorm2d-332         [-1, 2048, 14, 14]           4,096\n",
      "            ReLU-333         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-334         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-335          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-336          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-337          [-1, 512, 14, 14]               0\n",
      "          Conv2d-338          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-339          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-340          [-1, 512, 14, 14]               0\n",
      "          Conv2d-341         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-342         [-1, 2048, 14, 14]           4,096\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-344            [-1, 128, 1, 1]         262,144\n",
      "            ReLU-345            [-1, 128, 1, 1]               0\n",
      "          Conv2d-346           [-1, 2048, 1, 1]         262,144\n",
      "AdaptiveMaxPool2d-347           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-348            [-1, 128, 1, 1]         262,144\n",
      "            ReLU-349            [-1, 128, 1, 1]               0\n",
      "          Conv2d-350           [-1, 2048, 1, 1]         262,144\n",
      "         Sigmoid-351           [-1, 2048, 1, 1]               0\n",
      "ChannelAttention-352           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-353            [-1, 1, 14, 14]              98\n",
      "         Sigmoid-354            [-1, 1, 14, 14]               0\n",
      "SpatialAttention-355            [-1, 1, 14, 14]               0\n",
      "            ReLU-356         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-357         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-358          [-1, 512, 14, 14]       1,048,576\n",
      "     BatchNorm2d-359          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-360          [-1, 512, 14, 14]               0\n",
      "          Conv2d-361          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-362          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-363          [-1, 512, 14, 14]               0\n",
      "          Conv2d-364         [-1, 2048, 14, 14]       1,048,576\n",
      "     BatchNorm2d-365         [-1, 2048, 14, 14]           4,096\n",
      "AdaptiveAvgPool2d-366           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-367            [-1, 128, 1, 1]         262,144\n",
      "            ReLU-368            [-1, 128, 1, 1]               0\n",
      "          Conv2d-369           [-1, 2048, 1, 1]         262,144\n",
      "AdaptiveMaxPool2d-370           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-371            [-1, 128, 1, 1]         262,144\n",
      "            ReLU-372            [-1, 128, 1, 1]               0\n",
      "          Conv2d-373           [-1, 2048, 1, 1]         262,144\n",
      "         Sigmoid-374           [-1, 2048, 1, 1]               0\n",
      "ChannelAttention-375           [-1, 2048, 1, 1]               0\n",
      "          Conv2d-376            [-1, 1, 14, 14]              98\n",
      "         Sigmoid-377            [-1, 1, 14, 14]               0\n",
      "SpatialAttention-378            [-1, 1, 14, 14]               0\n",
      "            ReLU-379         [-1, 2048, 14, 14]               0\n",
      "      Bottleneck-380         [-1, 2048, 14, 14]               0\n",
      "          Conv2d-381         [-1, 1024, 14, 14]      18,874,368\n",
      "     BatchNorm2d-382         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-383         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-384          [-1, 512, 14, 14]       4,718,592\n",
      "     BatchNorm2d-385          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-386          [-1, 512, 14, 14]               0\n",
      "          Conv2d-387             [-1, 12, 7, 7]          24,576\n",
      "     BatchNorm2d-388             [-1, 12, 7, 7]              24\n",
      "            ReLU-389             [-1, 12, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 52,160,120\n",
      "Trainable params: 52,160,120\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 1155.03\n",
      "Params size (MB): 198.98\n",
      "Estimated Total Size (MB): 1356.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cf0424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 448, 448).to(device)\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df4b3a",
   "metadata": {},
   "source": [
    "## Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_LOSS():\n",
    "    def __init__(self, num_classes, device, lambda_coord=5., lambda_noobj=0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.grid_size = 7\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def __call__(self, predictions, targets):\n",
    "        self.batch_size, _, _, _ = predictions.shape\n",
    "        groundtruths = self.build_batch_target_grid(targets)\n",
    "        groundtruths = groundtruths.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iou1 = self.get_IoU(predictions[:, 1:5, ...], groundtruths[:, 1:5, ...])\n",
    "            iou2 = self.get_IoU(predictions[:, 6:10, ...], groundtruths[:, 1:5, ...])\n",
    "\n",
    "        ious = torch.stack([iou1, iou2], dim=1)\n",
    "        max_iou, best_box = ious.max(dim=1, keepdim=True)\n",
    "        max_iou = torch.cat([max_iou, max_iou], dim=1)\n",
    "        best_box = torch.cat([best_box.eq(0), best_box.eq(1)], dim=1)\n",
    "\n",
    "        predictions_ = predictions[:, :5*2, ...].reshape(self.batch_size, 2, 5, self.grid_size, self.grid_size)\n",
    "        obj_pred = predictions_[:, :, 0, ...]\n",
    "        xy_pred = predictions_[:, :, 1:3, ...]\n",
    "        wh_pred = predictions_[:, :, 3:5, ...]\n",
    "        cls_pred = predictions[:, 5*2:, ...]\n",
    "\n",
    "        groundtruths_ = groundtruths[:, :5, ...].reshape(self.batch_size, 1, 5, self.grid_size, self.grid_size)\n",
    "        obj_target = groundtruths_[:, :, 0, ...]\n",
    "        xy_target = groundtruths_[:, :, 1:3, ...]\n",
    "        wh_target= groundtruths_[:, :, 3:5, ...]\n",
    "        cls_target = groundtruths[:, 5:, ...]\n",
    "        \n",
    "        positive = obj_target * best_box\n",
    "\n",
    "        obj_loss = self.mse_loss(positive * obj_pred, positive * ious)\n",
    "        noobj_loss = self.mse_loss((1 - positive) * obj_pred, ious*0)\n",
    "        xy_loss = self.mse_loss(positive.unsqueeze(dim=2) * xy_pred, positive.unsqueeze(dim=2) * xy_target)\n",
    "        wh_loss = self.mse_loss(positive.unsqueeze(dim=2) * (wh_pred.sign() * (wh_pred.abs() + 1e-8).sqrt()),\n",
    "                           positive.unsqueeze(dim=2) * (wh_target + 1e-8).sqrt())\n",
    "        cls_loss = self.mse_loss(obj_target * cls_pred, cls_target)\n",
    "        \n",
    "        obj_loss /= self.batch_size\n",
    "        noobj_loss /= self.batch_size\n",
    "        bbox_loss = (xy_loss+wh_loss) / self.batch_size\n",
    "        cls_loss /= self.batch_size\n",
    "        \n",
    "        total_loss = obj_loss + self.lambda_noobj*noobj_loss + self.lambda_coord*bbox_loss + cls_loss\n",
    "        return total_loss, (obj_loss.item(), noobj_loss.item(), bbox_loss.item(), cls_loss.item())\n",
    "    \n",
    "    def build_target_grid(self, target):\n",
    "        target_grid = torch.zeros((1+4+self.num_classes, self.grid_size, self.grid_size), device=self.device)\n",
    "\n",
    "        for gt in target:\n",
    "            xc, yc, w, h, cls_id = gt\n",
    "            xn = (xc % (1/self.grid_size))\n",
    "            yn = (yc % (1/self.grid_size))\n",
    "            cls_id = int(cls_id)\n",
    "\n",
    "            i_grid = int(xc * self.grid_size)\n",
    "            j_grid = int(yc * self.grid_size)\n",
    "            target_grid[0, j_grid, i_grid] = 1\n",
    "            target_grid[1:5, j_grid, i_grid] = torch.Tensor([xn,yn,w,h])\n",
    "#             print(5+cls_id, j_grid, i_grid)\n",
    "            target_grid[5+cls_id, j_grid, i_grid] = 1\n",
    "\n",
    "        return target_grid\n",
    "    \n",
    "    def build_batch_target_grid(self, targets):\n",
    "        target_grid_batch = torch.stack([self.build_target_grid(target) for target in targets], dim=0)\n",
    "        return target_grid_batch\n",
    "    \n",
    "    def get_IoU(self, cbox1, cbox2):\n",
    "        box1 = self.xywh_to_xyxy(cbox1)\n",
    "        box2 = self.xywh_to_xyxy(cbox2)\n",
    "\n",
    "        x1 = torch.max(box1[:, 0, ...], box2[:, 0, ...])\n",
    "        y1 = torch.max(box1[:, 1, ...], box2[:, 1, ...])\n",
    "        x2 = torch.min(box1[:, 2, ...], box2[:, 2, ...])\n",
    "        y2 = torch.min(box1[:, 3, ...], box2[:, 3, ...])\n",
    "\n",
    "        intersection = (x2-x1).clamp(min=0) * (y2-y1).clamp(min=0)\n",
    "        union = abs(cbox1[:, 2, ...]*cbox1[:, 3, ...]) + \\\n",
    "                abs(cbox2[:, 2, ...]*cbox2[:, 3, ...]) - intersection\n",
    "\n",
    "        intersection[intersection.gt(0)] = intersection[intersection.gt(0)] / union[intersection.gt(0)]\n",
    "        return intersection\n",
    "    \n",
    "    def generate_xy_normed_grid(self):\n",
    "        y_offset, x_offset = torch.meshgrid(torch.arange(self.grid_size), torch.arange(self.grid_size))\n",
    "        xy_grid = torch.stack([x_offset, y_offset], dim=0)\n",
    "        xy_normed_grid = xy_grid / self.grid_size\n",
    "        return xy_normed_grid.to(self.device)\n",
    "\n",
    "    def xywh_to_xyxy(self, bboxes):\n",
    "        xy_normed_grid = self.generate_xy_normed_grid()\n",
    "        xcyc = bboxes[:,0:2,...] + xy_normed_grid.tile(self.batch_size, 1,1,1)\n",
    "        wh = bboxes[:,2:4,...]\n",
    "        x1y1 = xcyc - (wh/2)\n",
    "        x2y2 = xcyc + (wh/2)\n",
    "        return torch.cat([x1y1, x2y2], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbfb76",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "    \n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        running_loss = defaultdict(float)\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0].to(device)\n",
    "            targets = batch[1]\n",
    "            filenames = batch[2]\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"): # phase가 train 일때만 gradient 추적기능을 킨다.\n",
    "                predictions = model(images) #prediction shape=> B,12,7,7\n",
    "#             print(f\"predictions:{predictions}, \\ntargets: {targets}\\n\")\n",
    "            loss, (obj_loss, noobj_loss, bbox_loss, cls_loss) = criterion(predictions, targets)\n",
    "#             print(f\"loss:{loss}, obj_loss:{obj_loss}, noobj_loss:{noobj_loss}\\nbbox_loss:{bbox_loss}, cls_loss:{cls_loss}\\n--------------\\n\")\n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 현재 epoch단계에서 loss가 얼마인지 running loss 가출력\n",
    "                running_loss[\"total_loss\"] += loss.item()\n",
    "                running_loss[\"obj_loss\"] += obj_loss\n",
    "                running_loss[\"noobj_loss\"] += noobj_loss\n",
    "                running_loss[\"bbox_loss\"] += bbox_loss\n",
    "                running_loss[\"cls_loss\"] += cls_loss\n",
    "                \n",
    "                train_loss[\"total_loss\"] += loss.item()\n",
    "                train_loss[\"obj_loss\"] += obj_loss\n",
    "                train_loss[\"noobj_loss\"] += noobj_loss\n",
    "                train_loss[\"bbox_loss\"] += bbox_loss\n",
    "                train_loss[\"cls_loss\"] += cls_loss\n",
    "                \n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                    text = f\"<<<iteration:[{index}/{len(dataloaders[phase])}] - \"\n",
    "                    for k, v in running_loss.items():\n",
    "                        text += f\"{k}: {v/VERBOSE_FREQ:.4f}  \"\n",
    "                        running_loss[k] = 0.\n",
    "                    print(text)\n",
    "            else:\n",
    "                val_loss[\"total_loss\"] += loss.item()\n",
    "                val_loss[\"obj_loss\"] += obj_loss\n",
    "                val_loss[\"noobj_loss\"] += noobj_loss\n",
    "                val_loss[\"bbox_loss\"] += bbox_loss\n",
    "                val_loss[\"cls_loss\"] += cls_loss\n",
    "\n",
    "    for k in train_loss.keys():\n",
    "        train_loss[k] /= len(dataloaders[\"train\"])\n",
    "        val_loss[k] /= len(dataloaders[\"val\"])\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97049805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(part, NECK_PATH, BODY_PATH, batch_size=2, aug_factor=0):\n",
    "    IMAGE_SIZE = 448\n",
    "    transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    "    )\n",
    "    augmentator=A.Compose([\n",
    "    #     A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "    #     A.Sharpen(p=0.7),\n",
    "        A.BBoxSafeRandomCrop(p=0.6),\n",
    "        A.VerticalFlip (p=0.6),\n",
    "        A.HueSaturationValue(p=0.6),\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    "    )\n",
    "    \n",
    "    dataloaders = {}\n",
    "#     train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "#     train_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=augmentator, aug_factor=aug_factor)\n",
    "    train_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=None)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#     val_dataset = Detection_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
    "    val_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='valid', transformer=transformer, aug=None)\n",
    "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    print(f\"trainset:{len(train_dataset)} validset:{len(val_dataset)}\")\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c260f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making augmented images-- augmented factor:0\n",
      "total length of augmented images: 0\n",
      "start making augmented images-- augmented factor:0\n",
      "total length of augmented images: 0\n",
      "trainset:10500 validset:1800\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/content/drive/MyDrive/fastCamMedicalProj/DATASET/DATASET/Detection/\"\n",
    "# NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "# BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "NECK_PATH = '/home/host_data/PET_data_image_patching/patched_Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data_image_patching/Body'\n",
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 16\n",
    "VERBOSE_FREQ = 20\n",
    "LR=0.0001\n",
    "AUG_FACTOR=0\n",
    "PATCH_FACTOR=50\n",
    "BACKBONE=\"YOLO_RESNET_CBAM\"\n",
    "PART=\"neck\"\n",
    "num_epochs = 100\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "\n",
    "dataloaders = build_dataloader(part=PART,NECK_PATH=NECK_PATH,BODY_PATH=BODY_PATH,batch_size=BATCH_SIZE, aug_factor=AUG_FACTOR)\n",
    "model = resnet50_cbam(pretrained=True)\n",
    "model = model.to(device)\n",
    "criterion = YOLO_LOSS(num_classes=NUM_CLASSES, device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "817b20fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgomduribo\u001b[0m (\u001b[33murp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Plastic_Bottle_defect_detection/experiments/wandb/run-20231103_142641-q3h05unm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/q3h05unm' target=\"_blank\">dutiful-bird-3</a></strong> to <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/q3h05unm' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/q3h05unm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/q3h05unm?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f0b8c819220>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"yolo_cbam_neck_IMAGE_PATCH\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"architecture\": BACKBONE,\n",
    "    \"dataset\": PART,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"patch factor\":PATCH_FACTOR,\n",
    "    \"aug factor\":AUG_FACTOR,\n",
    "    \"pretrained\": \"yes\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b358951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3423.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[20/657] - total_loss: 12.0819  obj_loss: 0.1302  noobj_loss: 14.3798  bbox_loss: 0.7267  cls_loss: 1.1281  \n",
      "<<<iteration:[40/657] - total_loss: 4.7470  obj_loss: 0.0481  noobj_loss: 4.6425  bbox_loss: 0.2963  cls_loss: 0.8962  \n",
      "<<<iteration:[60/657] - total_loss: 4.6513  obj_loss: 0.0415  noobj_loss: 3.6433  bbox_loss: 0.3973  cls_loss: 0.8017  \n",
      "<<<iteration:[80/657] - total_loss: 3.2378  obj_loss: 0.0340  noobj_loss: 2.9291  bbox_loss: 0.1990  cls_loss: 0.7443  \n",
      "<<<iteration:[100/657] - total_loss: 2.9075  obj_loss: 0.0300  noobj_loss: 2.2805  bbox_loss: 0.2022  cls_loss: 0.7263  \n",
      "<<<iteration:[120/657] - total_loss: 2.4645  obj_loss: 0.0258  noobj_loss: 1.8256  bbox_loss: 0.1716  cls_loss: 0.6681  \n",
      "<<<iteration:[140/657] - total_loss: 2.2688  obj_loss: 0.0254  noobj_loss: 1.6617  bbox_loss: 0.1658  cls_loss: 0.5835  \n",
      "<<<iteration:[160/657] - total_loss: 1.9935  obj_loss: 0.0212  noobj_loss: 1.2392  bbox_loss: 0.1527  cls_loss: 0.5893  \n",
      "<<<iteration:[180/657] - total_loss: 1.8439  obj_loss: 0.0239  noobj_loss: 1.0538  bbox_loss: 0.1495  cls_loss: 0.5456  \n",
      "<<<iteration:[200/657] - total_loss: 1.6082  obj_loss: 0.0261  noobj_loss: 0.9265  bbox_loss: 0.1278  cls_loss: 0.4799  \n",
      "<<<iteration:[220/657] - total_loss: 1.5541  obj_loss: 0.0321  noobj_loss: 0.7196  bbox_loss: 0.1363  cls_loss: 0.4807  \n",
      "<<<iteration:[240/657] - total_loss: 1.5133  obj_loss: 0.0331  noobj_loss: 0.6419  bbox_loss: 0.1293  cls_loss: 0.5130  \n",
      "<<<iteration:[260/657] - total_loss: 1.4488  obj_loss: 0.0326  noobj_loss: 0.5968  bbox_loss: 0.1295  cls_loss: 0.4702  \n",
      "<<<iteration:[280/657] - total_loss: 1.3603  obj_loss: 0.0320  noobj_loss: 0.4674  bbox_loss: 0.1301  cls_loss: 0.4443  \n",
      "<<<iteration:[300/657] - total_loss: 1.3038  obj_loss: 0.0276  noobj_loss: 0.4317  bbox_loss: 0.1242  cls_loss: 0.4393  \n",
      "<<<iteration:[320/657] - total_loss: 1.2763  obj_loss: 0.0364  noobj_loss: 0.4097  bbox_loss: 0.1236  cls_loss: 0.4170  \n",
      "<<<iteration:[340/657] - total_loss: 1.1192  obj_loss: 0.0331  noobj_loss: 0.3875  bbox_loss: 0.1061  cls_loss: 0.3618  \n",
      "<<<iteration:[360/657] - total_loss: 1.1388  obj_loss: 0.0384  noobj_loss: 0.2704  bbox_loss: 0.1156  cls_loss: 0.3870  \n",
      "<<<iteration:[380/657] - total_loss: 1.0531  obj_loss: 0.0331  noobj_loss: 0.3169  bbox_loss: 0.1004  cls_loss: 0.3598  \n",
      "<<<iteration:[400/657] - total_loss: 1.0694  obj_loss: 0.0399  noobj_loss: 0.2541  bbox_loss: 0.1047  cls_loss: 0.3788  \n",
      "<<<iteration:[420/657] - total_loss: 1.0856  obj_loss: 0.0430  noobj_loss: 0.2365  bbox_loss: 0.1066  cls_loss: 0.3915  \n",
      "<<<iteration:[440/657] - total_loss: 0.9660  obj_loss: 0.0424  noobj_loss: 0.2149  bbox_loss: 0.0959  cls_loss: 0.3366  \n",
      "<<<iteration:[460/657] - total_loss: 0.9930  obj_loss: 0.0426  noobj_loss: 0.1604  bbox_loss: 0.1040  cls_loss: 0.3501  \n",
      "<<<iteration:[480/657] - total_loss: 1.0253  obj_loss: 0.0420  noobj_loss: 0.2123  bbox_loss: 0.0995  cls_loss: 0.3797  \n",
      "<<<iteration:[500/657] - total_loss: 0.9084  obj_loss: 0.0389  noobj_loss: 0.1737  bbox_loss: 0.0909  cls_loss: 0.3281  \n",
      "<<<iteration:[520/657] - total_loss: 0.8637  obj_loss: 0.0453  noobj_loss: 0.1691  bbox_loss: 0.0829  cls_loss: 0.3192  \n",
      "<<<iteration:[540/657] - total_loss: 0.9530  obj_loss: 0.0418  noobj_loss: 0.1527  bbox_loss: 0.0904  cls_loss: 0.3828  \n",
      "<<<iteration:[560/657] - total_loss: 0.9195  obj_loss: 0.0435  noobj_loss: 0.1133  bbox_loss: 0.0932  cls_loss: 0.3535  \n",
      "<<<iteration:[580/657] - total_loss: 0.8561  obj_loss: 0.0488  noobj_loss: 0.1545  bbox_loss: 0.0827  cls_loss: 0.3167  \n",
      "<<<iteration:[600/657] - total_loss: 0.8412  obj_loss: 0.0521  noobj_loss: 0.1193  bbox_loss: 0.0825  cls_loss: 0.3172  \n",
      "<<<iteration:[620/657] - total_loss: 0.8315  obj_loss: 0.0467  noobj_loss: 0.1180  bbox_loss: 0.0853  cls_loss: 0.2990  \n",
      "<<<iteration:[640/657] - total_loss: 0.8162  obj_loss: 0.0556  noobj_loss: 0.0905  bbox_loss: 0.0814  cls_loss: 0.3085  \n",
      "\n",
      "epoch:1/100 - Train Loss: 1.8992, Val Loss: 0.9083\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.8126  obj_loss: 0.0620  noobj_loss: 0.0971  bbox_loss: 0.0786  cls_loss: 0.3090  \n",
      "<<<iteration:[40/657] - total_loss: 0.7472  obj_loss: 0.0509  noobj_loss: 0.0804  bbox_loss: 0.0757  cls_loss: 0.2775  \n",
      "<<<iteration:[60/657] - total_loss: 0.7815  obj_loss: 0.0544  noobj_loss: 0.1099  bbox_loss: 0.0795  cls_loss: 0.2746  \n",
      "<<<iteration:[80/657] - total_loss: 0.7166  obj_loss: 0.0558  noobj_loss: 0.0738  bbox_loss: 0.0714  cls_loss: 0.2670  \n",
      "<<<iteration:[100/657] - total_loss: 0.7254  obj_loss: 0.0495  noobj_loss: 0.0908  bbox_loss: 0.0738  cls_loss: 0.2614  \n",
      "<<<iteration:[120/657] - total_loss: 0.7468  obj_loss: 0.0508  noobj_loss: 0.0887  bbox_loss: 0.0756  cls_loss: 0.2737  \n",
      "<<<iteration:[140/657] - total_loss: 0.7014  obj_loss: 0.0492  noobj_loss: 0.0580  bbox_loss: 0.0697  cls_loss: 0.2744  \n",
      "<<<iteration:[160/657] - total_loss: 0.7615  obj_loss: 0.0518  noobj_loss: 0.0626  bbox_loss: 0.0759  cls_loss: 0.2987  \n",
      "<<<iteration:[180/657] - total_loss: 0.7088  obj_loss: 0.0580  noobj_loss: 0.0637  bbox_loss: 0.0717  cls_loss: 0.2605  \n",
      "<<<iteration:[200/657] - total_loss: 0.6520  obj_loss: 0.0663  noobj_loss: 0.0539  bbox_loss: 0.0634  cls_loss: 0.2419  \n",
      "<<<iteration:[220/657] - total_loss: 0.6151  obj_loss: 0.0522  noobj_loss: 0.0479  bbox_loss: 0.0613  cls_loss: 0.2327  \n",
      "<<<iteration:[240/657] - total_loss: 0.6934  obj_loss: 0.0552  noobj_loss: 0.0600  bbox_loss: 0.0715  cls_loss: 0.2509  \n",
      "<<<iteration:[260/657] - total_loss: 0.6497  obj_loss: 0.0558  noobj_loss: 0.0592  bbox_loss: 0.0650  cls_loss: 0.2392  \n",
      "<<<iteration:[280/657] - total_loss: 0.6613  obj_loss: 0.0523  noobj_loss: 0.0494  bbox_loss: 0.0672  cls_loss: 0.2485  \n",
      "<<<iteration:[300/657] - total_loss: 0.6460  obj_loss: 0.0544  noobj_loss: 0.0500  bbox_loss: 0.0653  cls_loss: 0.2399  \n",
      "<<<iteration:[320/657] - total_loss: 0.6566  obj_loss: 0.0512  noobj_loss: 0.0422  bbox_loss: 0.0672  cls_loss: 0.2482  \n",
      "<<<iteration:[340/657] - total_loss: 0.6783  obj_loss: 0.0520  noobj_loss: 0.0563  bbox_loss: 0.0656  cls_loss: 0.2701  \n",
      "<<<iteration:[360/657] - total_loss: 0.6835  obj_loss: 0.0542  noobj_loss: 0.0611  bbox_loss: 0.0676  cls_loss: 0.2604  \n",
      "<<<iteration:[380/657] - total_loss: 0.6562  obj_loss: 0.0666  noobj_loss: 0.0384  bbox_loss: 0.0643  cls_loss: 0.2489  \n",
      "<<<iteration:[400/657] - total_loss: 0.6070  obj_loss: 0.0562  noobj_loss: 0.0415  bbox_loss: 0.0627  cls_loss: 0.2166  \n",
      "<<<iteration:[420/657] - total_loss: 0.6396  obj_loss: 0.0585  noobj_loss: 0.0342  bbox_loss: 0.0634  cls_loss: 0.2468  \n",
      "<<<iteration:[440/657] - total_loss: 0.6313  obj_loss: 0.0562  noobj_loss: 0.0336  bbox_loss: 0.0627  cls_loss: 0.2449  \n",
      "<<<iteration:[460/657] - total_loss: 0.6396  obj_loss: 0.0458  noobj_loss: 0.0466  bbox_loss: 0.0667  cls_loss: 0.2368  \n",
      "<<<iteration:[480/657] - total_loss: 0.6107  obj_loss: 0.0589  noobj_loss: 0.0486  bbox_loss: 0.0630  cls_loss: 0.2125  \n",
      "<<<iteration:[500/657] - total_loss: 0.6382  obj_loss: 0.0570  noobj_loss: 0.0431  bbox_loss: 0.0618  cls_loss: 0.2508  \n",
      "<<<iteration:[520/657] - total_loss: 0.5654  obj_loss: 0.0508  noobj_loss: 0.0317  bbox_loss: 0.0582  cls_loss: 0.2077  \n",
      "<<<iteration:[540/657] - total_loss: 0.6183  obj_loss: 0.0575  noobj_loss: 0.0268  bbox_loss: 0.0661  cls_loss: 0.2167  \n",
      "<<<iteration:[560/657] - total_loss: 0.6062  obj_loss: 0.0662  noobj_loss: 0.0322  bbox_loss: 0.0596  cls_loss: 0.2258  \n",
      "<<<iteration:[580/657] - total_loss: 0.5679  obj_loss: 0.0621  noobj_loss: 0.0359  bbox_loss: 0.0595  cls_loss: 0.1901  \n",
      "<<<iteration:[600/657] - total_loss: 0.5801  obj_loss: 0.0626  noobj_loss: 0.0327  bbox_loss: 0.0555  cls_loss: 0.2235  \n",
      "<<<iteration:[620/657] - total_loss: 0.5305  obj_loss: 0.0532  noobj_loss: 0.0262  bbox_loss: 0.0538  cls_loss: 0.1950  \n",
      "<<<iteration:[640/657] - total_loss: 0.5490  obj_loss: 0.0643  noobj_loss: 0.0233  bbox_loss: 0.0540  cls_loss: 0.2029  \n",
      "\n",
      "epoch:2/100 - Train Loss: 0.6627, Val Loss: 0.6595\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5846  obj_loss: 0.0660  noobj_loss: 0.0404  bbox_loss: 0.0582  cls_loss: 0.2075  \n",
      "<<<iteration:[40/657] - total_loss: 0.5582  obj_loss: 0.0590  noobj_loss: 0.0308  bbox_loss: 0.0583  cls_loss: 0.1925  \n",
      "<<<iteration:[60/657] - total_loss: 0.5759  obj_loss: 0.0642  noobj_loss: 0.0294  bbox_loss: 0.0613  cls_loss: 0.1906  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[80/657] - total_loss: 0.5282  obj_loss: 0.0578  noobj_loss: 0.0252  bbox_loss: 0.0545  cls_loss: 0.1854  \n",
      "<<<iteration:[100/657] - total_loss: 0.5469  obj_loss: 0.0667  noobj_loss: 0.0289  bbox_loss: 0.0560  cls_loss: 0.1859  \n",
      "<<<iteration:[120/657] - total_loss: 0.5488  obj_loss: 0.0581  noobj_loss: 0.0251  bbox_loss: 0.0564  cls_loss: 0.1963  \n",
      "<<<iteration:[140/657] - total_loss: 0.5846  obj_loss: 0.0657  noobj_loss: 0.0270  bbox_loss: 0.0605  cls_loss: 0.2029  \n",
      "<<<iteration:[160/657] - total_loss: 0.5039  obj_loss: 0.0657  noobj_loss: 0.0237  bbox_loss: 0.0482  cls_loss: 0.1856  \n",
      "<<<iteration:[180/657] - total_loss: 0.5313  obj_loss: 0.0557  noobj_loss: 0.0215  bbox_loss: 0.0565  cls_loss: 0.1823  \n",
      "<<<iteration:[200/657] - total_loss: 0.5280  obj_loss: 0.0583  noobj_loss: 0.0294  bbox_loss: 0.0518  cls_loss: 0.1959  \n",
      "<<<iteration:[220/657] - total_loss: 0.5344  obj_loss: 0.0602  noobj_loss: 0.0166  bbox_loss: 0.0522  cls_loss: 0.2048  \n",
      "<<<iteration:[240/657] - total_loss: 0.5280  obj_loss: 0.0562  noobj_loss: 0.0267  bbox_loss: 0.0543  cls_loss: 0.1869  \n",
      "<<<iteration:[260/657] - total_loss: 0.5285  obj_loss: 0.0556  noobj_loss: 0.0277  bbox_loss: 0.0548  cls_loss: 0.1850  \n",
      "<<<iteration:[280/657] - total_loss: 0.5059  obj_loss: 0.0625  noobj_loss: 0.0180  bbox_loss: 0.0530  cls_loss: 0.1693  \n",
      "<<<iteration:[300/657] - total_loss: 0.5197  obj_loss: 0.0542  noobj_loss: 0.0185  bbox_loss: 0.0533  cls_loss: 0.1900  \n",
      "<<<iteration:[320/657] - total_loss: 0.4710  obj_loss: 0.0725  noobj_loss: 0.0153  bbox_loss: 0.0462  cls_loss: 0.1598  \n",
      "<<<iteration:[340/657] - total_loss: 0.5297  obj_loss: 0.0548  noobj_loss: 0.0255  bbox_loss: 0.0581  cls_loss: 0.1717  \n",
      "<<<iteration:[360/657] - total_loss: 0.5511  obj_loss: 0.0577  noobj_loss: 0.0232  bbox_loss: 0.0606  cls_loss: 0.1787  \n",
      "<<<iteration:[380/657] - total_loss: 0.5308  obj_loss: 0.0544  noobj_loss: 0.0183  bbox_loss: 0.0547  cls_loss: 0.1939  \n",
      "<<<iteration:[400/657] - total_loss: 0.5296  obj_loss: 0.0666  noobj_loss: 0.0194  bbox_loss: 0.0586  cls_loss: 0.1603  \n",
      "<<<iteration:[420/657] - total_loss: 0.4885  obj_loss: 0.0683  noobj_loss: 0.0120  bbox_loss: 0.0532  cls_loss: 0.1484  \n",
      "<<<iteration:[440/657] - total_loss: 0.4845  obj_loss: 0.0589  noobj_loss: 0.0159  bbox_loss: 0.0494  cls_loss: 0.1708  \n",
      "<<<iteration:[460/657] - total_loss: 0.4915  obj_loss: 0.0608  noobj_loss: 0.0150  bbox_loss: 0.0524  cls_loss: 0.1614  \n",
      "<<<iteration:[480/657] - total_loss: 0.4708  obj_loss: 0.0666  noobj_loss: 0.0150  bbox_loss: 0.0499  cls_loss: 0.1472  \n",
      "<<<iteration:[500/657] - total_loss: 0.4533  obj_loss: 0.0663  noobj_loss: 0.0160  bbox_loss: 0.0464  cls_loss: 0.1472  \n",
      "<<<iteration:[520/657] - total_loss: 0.5257  obj_loss: 0.0619  noobj_loss: 0.0169  bbox_loss: 0.0534  cls_loss: 0.1882  \n",
      "<<<iteration:[540/657] - total_loss: 0.5094  obj_loss: 0.0624  noobj_loss: 0.0124  bbox_loss: 0.0517  cls_loss: 0.1820  \n",
      "<<<iteration:[560/657] - total_loss: 0.4849  obj_loss: 0.0664  noobj_loss: 0.0093  bbox_loss: 0.0503  cls_loss: 0.1623  \n",
      "<<<iteration:[580/657] - total_loss: 0.4905  obj_loss: 0.0673  noobj_loss: 0.0164  bbox_loss: 0.0530  cls_loss: 0.1501  \n",
      "<<<iteration:[600/657] - total_loss: 0.4673  obj_loss: 0.0626  noobj_loss: 0.0163  bbox_loss: 0.0500  cls_loss: 0.1467  \n",
      "<<<iteration:[620/657] - total_loss: 0.5470  obj_loss: 0.0660  noobj_loss: 0.0138  bbox_loss: 0.0601  cls_loss: 0.1734  \n",
      "<<<iteration:[640/657] - total_loss: 0.4822  obj_loss: 0.0637  noobj_loss: 0.0104  bbox_loss: 0.0523  cls_loss: 0.1517  \n",
      "\n",
      "epoch:3/100 - Train Loss: 0.5181, Val Loss: 0.5420\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4555  obj_loss: 0.0555  noobj_loss: 0.0149  bbox_loss: 0.0505  cls_loss: 0.1399  \n",
      "<<<iteration:[40/657] - total_loss: 0.4878  obj_loss: 0.0679  noobj_loss: 0.0121  bbox_loss: 0.0517  cls_loss: 0.1551  \n",
      "<<<iteration:[60/657] - total_loss: 0.4852  obj_loss: 0.0615  noobj_loss: 0.0131  bbox_loss: 0.0503  cls_loss: 0.1659  \n",
      "<<<iteration:[80/657] - total_loss: 0.4418  obj_loss: 0.0604  noobj_loss: 0.0198  bbox_loss: 0.0467  cls_loss: 0.1379  \n",
      "<<<iteration:[100/657] - total_loss: 0.4467  obj_loss: 0.0645  noobj_loss: 0.0101  bbox_loss: 0.0465  cls_loss: 0.1449  \n",
      "<<<iteration:[120/657] - total_loss: 0.4629  obj_loss: 0.0689  noobj_loss: 0.0151  bbox_loss: 0.0507  cls_loss: 0.1330  \n",
      "<<<iteration:[140/657] - total_loss: 0.4303  obj_loss: 0.0675  noobj_loss: 0.0175  bbox_loss: 0.0442  cls_loss: 0.1330  \n",
      "<<<iteration:[160/657] - total_loss: 0.4565  obj_loss: 0.0599  noobj_loss: 0.0115  bbox_loss: 0.0493  cls_loss: 0.1443  \n",
      "<<<iteration:[180/657] - total_loss: 0.4412  obj_loss: 0.0706  noobj_loss: 0.0102  bbox_loss: 0.0461  cls_loss: 0.1348  \n",
      "<<<iteration:[200/657] - total_loss: 0.4926  obj_loss: 0.0652  noobj_loss: 0.0135  bbox_loss: 0.0525  cls_loss: 0.1579  \n",
      "<<<iteration:[220/657] - total_loss: 0.4364  obj_loss: 0.0672  noobj_loss: 0.0141  bbox_loss: 0.0444  cls_loss: 0.1403  \n",
      "<<<iteration:[240/657] - total_loss: 0.4344  obj_loss: 0.0587  noobj_loss: 0.0131  bbox_loss: 0.0445  cls_loss: 0.1464  \n",
      "<<<iteration:[260/657] - total_loss: 0.4293  obj_loss: 0.0657  noobj_loss: 0.0111  bbox_loss: 0.0461  cls_loss: 0.1277  \n",
      "<<<iteration:[280/657] - total_loss: 0.4645  obj_loss: 0.0698  noobj_loss: 0.0094  bbox_loss: 0.0483  cls_loss: 0.1484  \n",
      "<<<iteration:[300/657] - total_loss: 0.4690  obj_loss: 0.0649  noobj_loss: 0.0146  bbox_loss: 0.0499  cls_loss: 0.1476  \n",
      "<<<iteration:[320/657] - total_loss: 0.4295  obj_loss: 0.0653  noobj_loss: 0.0100  bbox_loss: 0.0450  cls_loss: 0.1342  \n",
      "<<<iteration:[340/657] - total_loss: 0.4814  obj_loss: 0.0665  noobj_loss: 0.0082  bbox_loss: 0.0504  cls_loss: 0.1586  \n",
      "<<<iteration:[360/657] - total_loss: 0.4278  obj_loss: 0.0707  noobj_loss: 0.0121  bbox_loss: 0.0432  cls_loss: 0.1352  \n",
      "<<<iteration:[380/657] - total_loss: 0.4576  obj_loss: 0.0579  noobj_loss: 0.0175  bbox_loss: 0.0483  cls_loss: 0.1496  \n",
      "<<<iteration:[400/657] - total_loss: 0.4298  obj_loss: 0.0686  noobj_loss: 0.0097  bbox_loss: 0.0459  cls_loss: 0.1267  \n",
      "<<<iteration:[420/657] - total_loss: 0.4469  obj_loss: 0.0618  noobj_loss: 0.0110  bbox_loss: 0.0489  cls_loss: 0.1349  \n",
      "<<<iteration:[440/657] - total_loss: 0.4172  obj_loss: 0.0632  noobj_loss: 0.0113  bbox_loss: 0.0449  cls_loss: 0.1238  \n",
      "<<<iteration:[460/657] - total_loss: 0.4145  obj_loss: 0.0623  noobj_loss: 0.0076  bbox_loss: 0.0445  cls_loss: 0.1258  \n",
      "<<<iteration:[480/657] - total_loss: 0.4356  obj_loss: 0.0728  noobj_loss: 0.0130  bbox_loss: 0.0443  cls_loss: 0.1347  \n",
      "<<<iteration:[500/657] - total_loss: 0.4987  obj_loss: 0.0663  noobj_loss: 0.0109  bbox_loss: 0.0531  cls_loss: 0.1616  \n",
      "<<<iteration:[520/657] - total_loss: 0.4422  obj_loss: 0.0716  noobj_loss: 0.0104  bbox_loss: 0.0470  cls_loss: 0.1306  \n",
      "<<<iteration:[540/657] - total_loss: 0.4253  obj_loss: 0.0662  noobj_loss: 0.0070  bbox_loss: 0.0474  cls_loss: 0.1185  \n",
      "<<<iteration:[560/657] - total_loss: 0.4075  obj_loss: 0.0732  noobj_loss: 0.0079  bbox_loss: 0.0414  cls_loss: 0.1234  \n",
      "<<<iteration:[580/657] - total_loss: 0.4198  obj_loss: 0.0661  noobj_loss: 0.0066  bbox_loss: 0.0457  cls_loss: 0.1217  \n",
      "<<<iteration:[600/657] - total_loss: 0.4340  obj_loss: 0.0738  noobj_loss: 0.0115  bbox_loss: 0.0440  cls_loss: 0.1344  \n",
      "<<<iteration:[620/657] - total_loss: 0.4406  obj_loss: 0.0659  noobj_loss: 0.0045  bbox_loss: 0.0490  cls_loss: 0.1276  \n",
      "<<<iteration:[640/657] - total_loss: 0.4282  obj_loss: 0.0638  noobj_loss: 0.0098  bbox_loss: 0.0465  cls_loss: 0.1269  \n",
      "\n",
      "epoch:4/100 - Train Loss: 0.4442, Val Loss: 0.4600\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4514  obj_loss: 0.0696  noobj_loss: 0.0090  bbox_loss: 0.0482  cls_loss: 0.1363  \n",
      "<<<iteration:[40/657] - total_loss: 0.4161  obj_loss: 0.0585  noobj_loss: 0.0057  bbox_loss: 0.0459  cls_loss: 0.1253  \n",
      "<<<iteration:[60/657] - total_loss: 0.4481  obj_loss: 0.0663  noobj_loss: 0.0091  bbox_loss: 0.0490  cls_loss: 0.1324  \n",
      "<<<iteration:[80/657] - total_loss: 0.4257  obj_loss: 0.0703  noobj_loss: 0.0085  bbox_loss: 0.0463  cls_loss: 0.1194  \n",
      "<<<iteration:[100/657] - total_loss: 0.4044  obj_loss: 0.0800  noobj_loss: 0.0085  bbox_loss: 0.0416  cls_loss: 0.1120  \n",
      "<<<iteration:[120/657] - total_loss: 0.4339  obj_loss: 0.0750  noobj_loss: 0.0056  bbox_loss: 0.0466  cls_loss: 0.1230  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[140/657] - total_loss: 0.4164  obj_loss: 0.0712  noobj_loss: 0.0082  bbox_loss: 0.0441  cls_loss: 0.1206  \n",
      "<<<iteration:[160/657] - total_loss: 0.3999  obj_loss: 0.0736  noobj_loss: 0.0110  bbox_loss: 0.0428  cls_loss: 0.1068  \n",
      "<<<iteration:[180/657] - total_loss: 0.4230  obj_loss: 0.0657  noobj_loss: 0.0082  bbox_loss: 0.0433  cls_loss: 0.1366  \n",
      "<<<iteration:[200/657] - total_loss: 0.4202  obj_loss: 0.0766  noobj_loss: 0.0106  bbox_loss: 0.0428  cls_loss: 0.1244  \n",
      "<<<iteration:[220/657] - total_loss: 0.4246  obj_loss: 0.0636  noobj_loss: 0.0050  bbox_loss: 0.0447  cls_loss: 0.1350  \n",
      "<<<iteration:[240/657] - total_loss: 0.3925  obj_loss: 0.0656  noobj_loss: 0.0066  bbox_loss: 0.0411  cls_loss: 0.1182  \n",
      "<<<iteration:[260/657] - total_loss: 0.4229  obj_loss: 0.0751  noobj_loss: 0.0086  bbox_loss: 0.0425  cls_loss: 0.1308  \n",
      "<<<iteration:[280/657] - total_loss: 0.4067  obj_loss: 0.0668  noobj_loss: 0.0121  bbox_loss: 0.0445  cls_loss: 0.1111  \n",
      "<<<iteration:[300/657] - total_loss: 0.3933  obj_loss: 0.0665  noobj_loss: 0.0053  bbox_loss: 0.0420  cls_loss: 0.1142  \n",
      "<<<iteration:[320/657] - total_loss: 0.3837  obj_loss: 0.0674  noobj_loss: 0.0060  bbox_loss: 0.0398  cls_loss: 0.1144  \n",
      "<<<iteration:[340/657] - total_loss: 0.3617  obj_loss: 0.0621  noobj_loss: 0.0079  bbox_loss: 0.0407  cls_loss: 0.0920  \n",
      "<<<iteration:[360/657] - total_loss: 0.4192  obj_loss: 0.0636  noobj_loss: 0.0079  bbox_loss: 0.0497  cls_loss: 0.1030  \n",
      "<<<iteration:[380/657] - total_loss: 0.4531  obj_loss: 0.0640  noobj_loss: 0.0106  bbox_loss: 0.0500  cls_loss: 0.1336  \n",
      "<<<iteration:[400/657] - total_loss: 0.3702  obj_loss: 0.0673  noobj_loss: 0.0053  bbox_loss: 0.0396  cls_loss: 0.1024  \n",
      "<<<iteration:[420/657] - total_loss: 0.4216  obj_loss: 0.0731  noobj_loss: 0.0062  bbox_loss: 0.0474  cls_loss: 0.1082  \n",
      "<<<iteration:[440/657] - total_loss: 0.3777  obj_loss: 0.0622  noobj_loss: 0.0111  bbox_loss: 0.0406  cls_loss: 0.1069  \n",
      "<<<iteration:[460/657] - total_loss: 0.3950  obj_loss: 0.0745  noobj_loss: 0.0097  bbox_loss: 0.0409  cls_loss: 0.1109  \n",
      "<<<iteration:[480/657] - total_loss: 0.3830  obj_loss: 0.0694  noobj_loss: 0.0093  bbox_loss: 0.0447  cls_loss: 0.0853  \n",
      "<<<iteration:[500/657] - total_loss: 0.3962  obj_loss: 0.0750  noobj_loss: 0.0092  bbox_loss: 0.0438  cls_loss: 0.0978  \n",
      "<<<iteration:[520/657] - total_loss: 0.3851  obj_loss: 0.0734  noobj_loss: 0.0085  bbox_loss: 0.0409  cls_loss: 0.1029  \n",
      "<<<iteration:[540/657] - total_loss: 0.3889  obj_loss: 0.0591  noobj_loss: 0.0088  bbox_loss: 0.0445  cls_loss: 0.1029  \n",
      "<<<iteration:[560/657] - total_loss: 0.3617  obj_loss: 0.0614  noobj_loss: 0.0078  bbox_loss: 0.0389  cls_loss: 0.1017  \n",
      "<<<iteration:[580/657] - total_loss: 0.3981  obj_loss: 0.0694  noobj_loss: 0.0063  bbox_loss: 0.0447  cls_loss: 0.1022  \n",
      "<<<iteration:[600/657] - total_loss: 0.3720  obj_loss: 0.0568  noobj_loss: 0.0052  bbox_loss: 0.0416  cls_loss: 0.1048  \n",
      "<<<iteration:[620/657] - total_loss: 0.3870  obj_loss: 0.0715  noobj_loss: 0.0063  bbox_loss: 0.0408  cls_loss: 0.1084  \n",
      "<<<iteration:[640/657] - total_loss: 0.3981  obj_loss: 0.0708  noobj_loss: 0.0084  bbox_loss: 0.0440  cls_loss: 0.1033  \n",
      "\n",
      "epoch:5/100 - Train Loss: 0.4022, Val Loss: 0.4271\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4006  obj_loss: 0.0781  noobj_loss: 0.0054  bbox_loss: 0.0413  cls_loss: 0.1131  \n",
      "<<<iteration:[40/657] - total_loss: 0.3730  obj_loss: 0.0777  noobj_loss: 0.0067  bbox_loss: 0.0397  cls_loss: 0.0936  \n",
      "<<<iteration:[60/657] - total_loss: 0.3826  obj_loss: 0.0696  noobj_loss: 0.0045  bbox_loss: 0.0393  cls_loss: 0.1144  \n",
      "<<<iteration:[80/657] - total_loss: 0.4075  obj_loss: 0.0706  noobj_loss: 0.0088  bbox_loss: 0.0449  cls_loss: 0.1081  \n",
      "<<<iteration:[100/657] - total_loss: 0.3532  obj_loss: 0.0769  noobj_loss: 0.0043  bbox_loss: 0.0385  cls_loss: 0.0815  \n",
      "<<<iteration:[120/657] - total_loss: 0.3844  obj_loss: 0.0607  noobj_loss: 0.0095  bbox_loss: 0.0440  cls_loss: 0.0990  \n",
      "<<<iteration:[140/657] - total_loss: 0.3486  obj_loss: 0.0647  noobj_loss: 0.0065  bbox_loss: 0.0375  cls_loss: 0.0933  \n",
      "<<<iteration:[160/657] - total_loss: 0.3776  obj_loss: 0.0643  noobj_loss: 0.0065  bbox_loss: 0.0401  cls_loss: 0.1094  \n",
      "<<<iteration:[180/657] - total_loss: 0.3678  obj_loss: 0.0666  noobj_loss: 0.0093  bbox_loss: 0.0381  cls_loss: 0.1059  \n",
      "<<<iteration:[200/657] - total_loss: 0.3563  obj_loss: 0.0677  noobj_loss: 0.0045  bbox_loss: 0.0381  cls_loss: 0.0958  \n",
      "<<<iteration:[220/657] - total_loss: 0.3834  obj_loss: 0.0697  noobj_loss: 0.0065  bbox_loss: 0.0405  cls_loss: 0.1081  \n",
      "<<<iteration:[240/657] - total_loss: 0.3553  obj_loss: 0.0677  noobj_loss: 0.0037  bbox_loss: 0.0365  cls_loss: 0.1031  \n",
      "<<<iteration:[260/657] - total_loss: 0.3784  obj_loss: 0.0678  noobj_loss: 0.0108  bbox_loss: 0.0412  cls_loss: 0.0993  \n",
      "<<<iteration:[280/657] - total_loss: 0.3506  obj_loss: 0.0742  noobj_loss: 0.0044  bbox_loss: 0.0370  cls_loss: 0.0891  \n",
      "<<<iteration:[300/657] - total_loss: 0.4022  obj_loss: 0.0640  noobj_loss: 0.0066  bbox_loss: 0.0452  cls_loss: 0.1087  \n",
      "<<<iteration:[320/657] - total_loss: 0.3837  obj_loss: 0.0683  noobj_loss: 0.0044  bbox_loss: 0.0421  cls_loss: 0.1028  \n",
      "<<<iteration:[340/657] - total_loss: 0.3469  obj_loss: 0.0843  noobj_loss: 0.0041  bbox_loss: 0.0359  cls_loss: 0.0808  \n",
      "<<<iteration:[360/657] - total_loss: 0.3694  obj_loss: 0.0686  noobj_loss: 0.0045  bbox_loss: 0.0403  cls_loss: 0.0973  \n",
      "<<<iteration:[380/657] - total_loss: 0.3671  obj_loss: 0.0702  noobj_loss: 0.0085  bbox_loss: 0.0388  cls_loss: 0.0987  \n",
      "<<<iteration:[400/657] - total_loss: 0.3741  obj_loss: 0.0713  noobj_loss: 0.0064  bbox_loss: 0.0406  cls_loss: 0.0967  \n",
      "<<<iteration:[420/657] - total_loss: 0.3778  obj_loss: 0.0749  noobj_loss: 0.0066  bbox_loss: 0.0419  cls_loss: 0.0903  \n",
      "<<<iteration:[440/657] - total_loss: 0.3479  obj_loss: 0.0726  noobj_loss: 0.0066  bbox_loss: 0.0374  cls_loss: 0.0847  \n",
      "<<<iteration:[460/657] - total_loss: 0.3626  obj_loss: 0.0796  noobj_loss: 0.0052  bbox_loss: 0.0373  cls_loss: 0.0940  \n",
      "<<<iteration:[480/657] - total_loss: 0.3591  obj_loss: 0.0701  noobj_loss: 0.0067  bbox_loss: 0.0400  cls_loss: 0.0857  \n",
      "<<<iteration:[500/657] - total_loss: 0.3682  obj_loss: 0.0639  noobj_loss: 0.0042  bbox_loss: 0.0397  cls_loss: 0.1036  \n",
      "<<<iteration:[520/657] - total_loss: 0.3509  obj_loss: 0.0782  noobj_loss: 0.0044  bbox_loss: 0.0368  cls_loss: 0.0866  \n",
      "<<<iteration:[540/657] - total_loss: 0.3640  obj_loss: 0.0736  noobj_loss: 0.0069  bbox_loss: 0.0385  cls_loss: 0.0943  \n",
      "<<<iteration:[560/657] - total_loss: 0.3692  obj_loss: 0.0705  noobj_loss: 0.0041  bbox_loss: 0.0391  cls_loss: 0.1010  \n",
      "<<<iteration:[580/657] - total_loss: 0.3505  obj_loss: 0.0749  noobj_loss: 0.0059  bbox_loss: 0.0378  cls_loss: 0.0839  \n",
      "<<<iteration:[600/657] - total_loss: 0.3696  obj_loss: 0.0616  noobj_loss: 0.0057  bbox_loss: 0.0430  cls_loss: 0.0902  \n",
      "<<<iteration:[620/657] - total_loss: 0.3635  obj_loss: 0.0759  noobj_loss: 0.0074  bbox_loss: 0.0372  cls_loss: 0.0980  \n",
      "<<<iteration:[640/657] - total_loss: 0.3645  obj_loss: 0.0692  noobj_loss: 0.0052  bbox_loss: 0.0401  cls_loss: 0.0924  \n",
      "\n",
      "epoch:6/100 - Train Loss: 0.3689, Val Loss: 0.4106\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3774  obj_loss: 0.0751  noobj_loss: 0.0076  bbox_loss: 0.0414  cls_loss: 0.0914  \n",
      "<<<iteration:[40/657] - total_loss: 0.3506  obj_loss: 0.0716  noobj_loss: 0.0055  bbox_loss: 0.0383  cls_loss: 0.0845  \n",
      "<<<iteration:[60/657] - total_loss: 0.3955  obj_loss: 0.0704  noobj_loss: 0.0098  bbox_loss: 0.0445  cls_loss: 0.0977  \n",
      "<<<iteration:[80/657] - total_loss: 0.3428  obj_loss: 0.0713  noobj_loss: 0.0073  bbox_loss: 0.0387  cls_loss: 0.0745  \n",
      "<<<iteration:[100/657] - total_loss: 0.3471  obj_loss: 0.0766  noobj_loss: 0.0030  bbox_loss: 0.0371  cls_loss: 0.0836  \n",
      "<<<iteration:[120/657] - total_loss: 0.3504  obj_loss: 0.0773  noobj_loss: 0.0046  bbox_loss: 0.0349  cls_loss: 0.0964  \n",
      "<<<iteration:[140/657] - total_loss: 0.3582  obj_loss: 0.0729  noobj_loss: 0.0048  bbox_loss: 0.0392  cls_loss: 0.0867  \n",
      "<<<iteration:[160/657] - total_loss: 0.3472  obj_loss: 0.0756  noobj_loss: 0.0047  bbox_loss: 0.0372  cls_loss: 0.0832  \n",
      "<<<iteration:[180/657] - total_loss: 0.3551  obj_loss: 0.0740  noobj_loss: 0.0036  bbox_loss: 0.0385  cls_loss: 0.0870  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[200/657] - total_loss: 0.3413  obj_loss: 0.0729  noobj_loss: 0.0047  bbox_loss: 0.0380  cls_loss: 0.0762  \n",
      "<<<iteration:[220/657] - total_loss: 0.3352  obj_loss: 0.0600  noobj_loss: 0.0074  bbox_loss: 0.0373  cls_loss: 0.0848  \n",
      "<<<iteration:[240/657] - total_loss: 0.3630  obj_loss: 0.0778  noobj_loss: 0.0067  bbox_loss: 0.0380  cls_loss: 0.0919  \n",
      "<<<iteration:[260/657] - total_loss: 0.3407  obj_loss: 0.0785  noobj_loss: 0.0074  bbox_loss: 0.0364  cls_loss: 0.0766  \n",
      "<<<iteration:[280/657] - total_loss: 0.3430  obj_loss: 0.0749  noobj_loss: 0.0043  bbox_loss: 0.0359  cls_loss: 0.0863  \n",
      "<<<iteration:[300/657] - total_loss: 0.3225  obj_loss: 0.0748  noobj_loss: 0.0042  bbox_loss: 0.0347  cls_loss: 0.0719  \n",
      "<<<iteration:[320/657] - total_loss: 0.3433  obj_loss: 0.0661  noobj_loss: 0.0055  bbox_loss: 0.0375  cls_loss: 0.0870  \n",
      "<<<iteration:[340/657] - total_loss: 0.3369  obj_loss: 0.0684  noobj_loss: 0.0057  bbox_loss: 0.0360  cls_loss: 0.0856  \n",
      "<<<iteration:[360/657] - total_loss: 0.3829  obj_loss: 0.0679  noobj_loss: 0.0055  bbox_loss: 0.0418  cls_loss: 0.1035  \n",
      "<<<iteration:[380/657] - total_loss: 0.3372  obj_loss: 0.0718  noobj_loss: 0.0047  bbox_loss: 0.0361  cls_loss: 0.0828  \n",
      "<<<iteration:[400/657] - total_loss: 0.3303  obj_loss: 0.0776  noobj_loss: 0.0061  bbox_loss: 0.0345  cls_loss: 0.0773  \n",
      "<<<iteration:[420/657] - total_loss: 0.3648  obj_loss: 0.0667  noobj_loss: 0.0049  bbox_loss: 0.0409  cls_loss: 0.0914  \n",
      "<<<iteration:[440/657] - total_loss: 0.3418  obj_loss: 0.0707  noobj_loss: 0.0075  bbox_loss: 0.0369  cls_loss: 0.0830  \n",
      "<<<iteration:[460/657] - total_loss: 0.3556  obj_loss: 0.0719  noobj_loss: 0.0041  bbox_loss: 0.0372  cls_loss: 0.0958  \n",
      "<<<iteration:[480/657] - total_loss: 0.3377  obj_loss: 0.0660  noobj_loss: 0.0036  bbox_loss: 0.0369  cls_loss: 0.0855  \n",
      "<<<iteration:[500/657] - total_loss: 0.3711  obj_loss: 0.0793  noobj_loss: 0.0048  bbox_loss: 0.0376  cls_loss: 0.1013  \n",
      "<<<iteration:[520/657] - total_loss: 0.3674  obj_loss: 0.0734  noobj_loss: 0.0083  bbox_loss: 0.0404  cls_loss: 0.0881  \n",
      "<<<iteration:[540/657] - total_loss: 0.3311  obj_loss: 0.0668  noobj_loss: 0.0048  bbox_loss: 0.0383  cls_loss: 0.0705  \n",
      "<<<iteration:[560/657] - total_loss: 0.3516  obj_loss: 0.0691  noobj_loss: 0.0054  bbox_loss: 0.0377  cls_loss: 0.0913  \n",
      "<<<iteration:[580/657] - total_loss: 0.3330  obj_loss: 0.0818  noobj_loss: 0.0058  bbox_loss: 0.0345  cls_loss: 0.0757  \n",
      "<<<iteration:[600/657] - total_loss: 0.3433  obj_loss: 0.0734  noobj_loss: 0.0048  bbox_loss: 0.0374  cls_loss: 0.0805  \n",
      "<<<iteration:[620/657] - total_loss: 0.3376  obj_loss: 0.0714  noobj_loss: 0.0072  bbox_loss: 0.0372  cls_loss: 0.0768  \n",
      "<<<iteration:[640/657] - total_loss: 0.3790  obj_loss: 0.0793  noobj_loss: 0.0051  bbox_loss: 0.0410  cls_loss: 0.0919  \n",
      "\n",
      "epoch:7/100 - Train Loss: 0.3496, Val Loss: 0.3971\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3810  obj_loss: 0.0820  noobj_loss: 0.0060  bbox_loss: 0.0410  cls_loss: 0.0910  \n",
      "<<<iteration:[40/657] - total_loss: 0.3335  obj_loss: 0.0746  noobj_loss: 0.0044  bbox_loss: 0.0364  cls_loss: 0.0747  \n",
      "<<<iteration:[60/657] - total_loss: 0.3372  obj_loss: 0.0758  noobj_loss: 0.0049  bbox_loss: 0.0354  cls_loss: 0.0819  \n",
      "<<<iteration:[80/657] - total_loss: 0.3348  obj_loss: 0.0741  noobj_loss: 0.0048  bbox_loss: 0.0388  cls_loss: 0.0645  \n",
      "<<<iteration:[100/657] - total_loss: 0.3284  obj_loss: 0.0746  noobj_loss: 0.0059  bbox_loss: 0.0367  cls_loss: 0.0676  \n",
      "<<<iteration:[120/657] - total_loss: 0.3282  obj_loss: 0.0744  noobj_loss: 0.0023  bbox_loss: 0.0343  cls_loss: 0.0810  \n",
      "<<<iteration:[140/657] - total_loss: 0.3087  obj_loss: 0.0795  noobj_loss: 0.0047  bbox_loss: 0.0329  cls_loss: 0.0626  \n",
      "<<<iteration:[160/657] - total_loss: 0.3872  obj_loss: 0.0733  noobj_loss: 0.0111  bbox_loss: 0.0416  cls_loss: 0.1006  \n",
      "<<<iteration:[180/657] - total_loss: 0.3370  obj_loss: 0.0734  noobj_loss: 0.0035  bbox_loss: 0.0372  cls_loss: 0.0761  \n",
      "<<<iteration:[200/657] - total_loss: 0.3535  obj_loss: 0.0904  noobj_loss: 0.0029  bbox_loss: 0.0367  cls_loss: 0.0781  \n",
      "<<<iteration:[220/657] - total_loss: 0.3406  obj_loss: 0.0716  noobj_loss: 0.0070  bbox_loss: 0.0352  cls_loss: 0.0895  \n",
      "<<<iteration:[240/657] - total_loss: 0.3464  obj_loss: 0.0765  noobj_loss: 0.0053  bbox_loss: 0.0381  cls_loss: 0.0768  \n",
      "<<<iteration:[260/657] - total_loss: 0.3348  obj_loss: 0.0691  noobj_loss: 0.0042  bbox_loss: 0.0375  cls_loss: 0.0760  \n",
      "<<<iteration:[280/657] - total_loss: 0.3199  obj_loss: 0.0696  noobj_loss: 0.0052  bbox_loss: 0.0347  cls_loss: 0.0742  \n",
      "<<<iteration:[300/657] - total_loss: 0.3233  obj_loss: 0.0673  noobj_loss: 0.0057  bbox_loss: 0.0364  cls_loss: 0.0711  \n",
      "<<<iteration:[320/657] - total_loss: 0.3586  obj_loss: 0.0727  noobj_loss: 0.0044  bbox_loss: 0.0385  cls_loss: 0.0912  \n",
      "<<<iteration:[340/657] - total_loss: 0.3220  obj_loss: 0.0766  noobj_loss: 0.0047  bbox_loss: 0.0335  cls_loss: 0.0758  \n",
      "<<<iteration:[360/657] - total_loss: 0.3356  obj_loss: 0.0735  noobj_loss: 0.0052  bbox_loss: 0.0353  cls_loss: 0.0829  \n",
      "<<<iteration:[380/657] - total_loss: 0.3226  obj_loss: 0.0799  noobj_loss: 0.0035  bbox_loss: 0.0335  cls_loss: 0.0734  \n",
      "<<<iteration:[400/657] - total_loss: 0.3281  obj_loss: 0.0741  noobj_loss: 0.0052  bbox_loss: 0.0344  cls_loss: 0.0793  \n",
      "<<<iteration:[420/657] - total_loss: 0.3142  obj_loss: 0.0777  noobj_loss: 0.0043  bbox_loss: 0.0322  cls_loss: 0.0732  \n",
      "<<<iteration:[440/657] - total_loss: 0.3232  obj_loss: 0.0725  noobj_loss: 0.0058  bbox_loss: 0.0363  cls_loss: 0.0663  \n",
      "<<<iteration:[460/657] - total_loss: 0.3437  obj_loss: 0.0789  noobj_loss: 0.0066  bbox_loss: 0.0374  cls_loss: 0.0742  \n",
      "<<<iteration:[480/657] - total_loss: 0.3535  obj_loss: 0.0774  noobj_loss: 0.0029  bbox_loss: 0.0406  cls_loss: 0.0717  \n",
      "<<<iteration:[500/657] - total_loss: 0.3112  obj_loss: 0.0691  noobj_loss: 0.0030  bbox_loss: 0.0338  cls_loss: 0.0717  \n",
      "<<<iteration:[520/657] - total_loss: 0.3435  obj_loss: 0.0827  noobj_loss: 0.0058  bbox_loss: 0.0346  cls_loss: 0.0850  \n",
      "<<<iteration:[540/657] - total_loss: 0.3085  obj_loss: 0.0790  noobj_loss: 0.0040  bbox_loss: 0.0328  cls_loss: 0.0636  \n",
      "<<<iteration:[560/657] - total_loss: 0.3215  obj_loss: 0.0837  noobj_loss: 0.0036  bbox_loss: 0.0331  cls_loss: 0.0706  \n",
      "<<<iteration:[580/657] - total_loss: 0.3214  obj_loss: 0.0806  noobj_loss: 0.0053  bbox_loss: 0.0333  cls_loss: 0.0719  \n",
      "<<<iteration:[600/657] - total_loss: 0.3594  obj_loss: 0.0750  noobj_loss: 0.0048  bbox_loss: 0.0403  cls_loss: 0.0804  \n",
      "<<<iteration:[620/657] - total_loss: 0.3560  obj_loss: 0.0846  noobj_loss: 0.0041  bbox_loss: 0.0381  cls_loss: 0.0790  \n",
      "<<<iteration:[640/657] - total_loss: 0.3272  obj_loss: 0.0778  noobj_loss: 0.0036  bbox_loss: 0.0366  cls_loss: 0.0648  \n",
      "\n",
      "epoch:8/100 - Train Loss: 0.3350, Val Loss: 0.3780\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3482  obj_loss: 0.0765  noobj_loss: 0.0060  bbox_loss: 0.0358  cls_loss: 0.0895  \n",
      "<<<iteration:[40/657] - total_loss: 0.3461  obj_loss: 0.0862  noobj_loss: 0.0055  bbox_loss: 0.0374  cls_loss: 0.0701  \n",
      "<<<iteration:[60/657] - total_loss: 0.3335  obj_loss: 0.0737  noobj_loss: 0.0069  bbox_loss: 0.0355  cls_loss: 0.0790  \n",
      "<<<iteration:[80/657] - total_loss: 0.3186  obj_loss: 0.0832  noobj_loss: 0.0038  bbox_loss: 0.0336  cls_loss: 0.0656  \n",
      "<<<iteration:[100/657] - total_loss: 0.3302  obj_loss: 0.0827  noobj_loss: 0.0068  bbox_loss: 0.0359  cls_loss: 0.0646  \n",
      "<<<iteration:[120/657] - total_loss: 0.3289  obj_loss: 0.0830  noobj_loss: 0.0062  bbox_loss: 0.0338  cls_loss: 0.0736  \n",
      "<<<iteration:[140/657] - total_loss: 0.3138  obj_loss: 0.0741  noobj_loss: 0.0047  bbox_loss: 0.0346  cls_loss: 0.0641  \n",
      "<<<iteration:[160/657] - total_loss: 0.3274  obj_loss: 0.0774  noobj_loss: 0.0058  bbox_loss: 0.0347  cls_loss: 0.0735  \n",
      "<<<iteration:[180/657] - total_loss: 0.3198  obj_loss: 0.0792  noobj_loss: 0.0067  bbox_loss: 0.0345  cls_loss: 0.0650  \n",
      "<<<iteration:[200/657] - total_loss: 0.3275  obj_loss: 0.0829  noobj_loss: 0.0047  bbox_loss: 0.0346  cls_loss: 0.0695  \n",
      "<<<iteration:[220/657] - total_loss: 0.3204  obj_loss: 0.0816  noobj_loss: 0.0035  bbox_loss: 0.0345  cls_loss: 0.0643  \n",
      "<<<iteration:[240/657] - total_loss: 0.3497  obj_loss: 0.0839  noobj_loss: 0.0041  bbox_loss: 0.0371  cls_loss: 0.0780  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[260/657] - total_loss: 0.3334  obj_loss: 0.0798  noobj_loss: 0.0078  bbox_loss: 0.0356  cls_loss: 0.0717  \n",
      "<<<iteration:[280/657] - total_loss: 0.3140  obj_loss: 0.0790  noobj_loss: 0.0058  bbox_loss: 0.0329  cls_loss: 0.0675  \n",
      "<<<iteration:[300/657] - total_loss: 0.2903  obj_loss: 0.0639  noobj_loss: 0.0052  bbox_loss: 0.0298  cls_loss: 0.0747  \n",
      "<<<iteration:[320/657] - total_loss: 0.3064  obj_loss: 0.0699  noobj_loss: 0.0043  bbox_loss: 0.0338  cls_loss: 0.0655  \n",
      "<<<iteration:[340/657] - total_loss: 0.3121  obj_loss: 0.0754  noobj_loss: 0.0040  bbox_loss: 0.0349  cls_loss: 0.0604  \n",
      "<<<iteration:[360/657] - total_loss: 0.3256  obj_loss: 0.0770  noobj_loss: 0.0038  bbox_loss: 0.0348  cls_loss: 0.0726  \n",
      "<<<iteration:[380/657] - total_loss: 0.2970  obj_loss: 0.0784  noobj_loss: 0.0029  bbox_loss: 0.0302  cls_loss: 0.0659  \n",
      "<<<iteration:[400/657] - total_loss: 0.3203  obj_loss: 0.0791  noobj_loss: 0.0073  bbox_loss: 0.0339  cls_loss: 0.0682  \n",
      "<<<iteration:[420/657] - total_loss: 0.2977  obj_loss: 0.0804  noobj_loss: 0.0047  bbox_loss: 0.0303  cls_loss: 0.0633  \n",
      "<<<iteration:[440/657] - total_loss: 0.3498  obj_loss: 0.0802  noobj_loss: 0.0074  bbox_loss: 0.0378  cls_loss: 0.0767  \n",
      "<<<iteration:[460/657] - total_loss: 0.3243  obj_loss: 0.0823  noobj_loss: 0.0031  bbox_loss: 0.0335  cls_loss: 0.0728  \n",
      "<<<iteration:[480/657] - total_loss: 0.3253  obj_loss: 0.0769  noobj_loss: 0.0049  bbox_loss: 0.0359  cls_loss: 0.0663  \n",
      "<<<iteration:[500/657] - total_loss: 0.3187  obj_loss: 0.0873  noobj_loss: 0.0026  bbox_loss: 0.0311  cls_loss: 0.0745  \n",
      "<<<iteration:[520/657] - total_loss: 0.3049  obj_loss: 0.0786  noobj_loss: 0.0054  bbox_loss: 0.0328  cls_loss: 0.0595  \n",
      "<<<iteration:[540/657] - total_loss: 0.3431  obj_loss: 0.0751  noobj_loss: 0.0051  bbox_loss: 0.0361  cls_loss: 0.0849  \n",
      "<<<iteration:[560/657] - total_loss: 0.3080  obj_loss: 0.0823  noobj_loss: 0.0048  bbox_loss: 0.0329  cls_loss: 0.0588  \n",
      "<<<iteration:[580/657] - total_loss: 0.3160  obj_loss: 0.0768  noobj_loss: 0.0075  bbox_loss: 0.0331  cls_loss: 0.0700  \n",
      "<<<iteration:[600/657] - total_loss: 0.2923  obj_loss: 0.0720  noobj_loss: 0.0034  bbox_loss: 0.0310  cls_loss: 0.0638  \n",
      "<<<iteration:[620/657] - total_loss: 0.3112  obj_loss: 0.0696  noobj_loss: 0.0043  bbox_loss: 0.0337  cls_loss: 0.0710  \n",
      "<<<iteration:[640/657] - total_loss: 0.3137  obj_loss: 0.0688  noobj_loss: 0.0040  bbox_loss: 0.0365  cls_loss: 0.0602  \n",
      "\n",
      "epoch:9/100 - Train Loss: 0.3209, Val Loss: 0.3596\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3240  obj_loss: 0.0837  noobj_loss: 0.0049  bbox_loss: 0.0328  cls_loss: 0.0738  \n",
      "<<<iteration:[40/657] - total_loss: 0.3390  obj_loss: 0.0893  noobj_loss: 0.0090  bbox_loss: 0.0330  cls_loss: 0.0801  \n",
      "<<<iteration:[60/657] - total_loss: 0.3048  obj_loss: 0.0752  noobj_loss: 0.0060  bbox_loss: 0.0330  cls_loss: 0.0617  \n",
      "<<<iteration:[80/657] - total_loss: 0.3123  obj_loss: 0.0788  noobj_loss: 0.0028  bbox_loss: 0.0321  cls_loss: 0.0716  \n",
      "<<<iteration:[100/657] - total_loss: 0.2953  obj_loss: 0.0781  noobj_loss: 0.0071  bbox_loss: 0.0313  cls_loss: 0.0574  \n",
      "<<<iteration:[120/657] - total_loss: 0.3328  obj_loss: 0.0830  noobj_loss: 0.0056  bbox_loss: 0.0341  cls_loss: 0.0764  \n",
      "<<<iteration:[140/657] - total_loss: 0.3090  obj_loss: 0.0777  noobj_loss: 0.0067  bbox_loss: 0.0332  cls_loss: 0.0619  \n",
      "<<<iteration:[160/657] - total_loss: 0.3397  obj_loss: 0.0852  noobj_loss: 0.0067  bbox_loss: 0.0369  cls_loss: 0.0666  \n",
      "<<<iteration:[180/657] - total_loss: 0.3073  obj_loss: 0.0796  noobj_loss: 0.0051  bbox_loss: 0.0316  cls_loss: 0.0673  \n",
      "<<<iteration:[200/657] - total_loss: 0.3301  obj_loss: 0.0839  noobj_loss: 0.0082  bbox_loss: 0.0326  cls_loss: 0.0788  \n",
      "<<<iteration:[220/657] - total_loss: 0.3290  obj_loss: 0.0903  noobj_loss: 0.0049  bbox_loss: 0.0343  cls_loss: 0.0649  \n",
      "<<<iteration:[240/657] - total_loss: 0.2718  obj_loss: 0.0692  noobj_loss: 0.0049  bbox_loss: 0.0284  cls_loss: 0.0581  \n",
      "<<<iteration:[260/657] - total_loss: 0.3233  obj_loss: 0.0841  noobj_loss: 0.0094  bbox_loss: 0.0353  cls_loss: 0.0582  \n",
      "<<<iteration:[280/657] - total_loss: 0.3310  obj_loss: 0.0763  noobj_loss: 0.0046  bbox_loss: 0.0369  cls_loss: 0.0679  \n",
      "<<<iteration:[300/657] - total_loss: 0.3137  obj_loss: 0.0879  noobj_loss: 0.0059  bbox_loss: 0.0313  cls_loss: 0.0662  \n",
      "<<<iteration:[320/657] - total_loss: 0.2876  obj_loss: 0.0695  noobj_loss: 0.0065  bbox_loss: 0.0307  cls_loss: 0.0613  \n",
      "<<<iteration:[340/657] - total_loss: 0.2880  obj_loss: 0.0731  noobj_loss: 0.0057  bbox_loss: 0.0317  cls_loss: 0.0533  \n",
      "<<<iteration:[360/657] - total_loss: 0.3215  obj_loss: 0.0716  noobj_loss: 0.0076  bbox_loss: 0.0339  cls_loss: 0.0766  \n",
      "<<<iteration:[380/657] - total_loss: 0.3074  obj_loss: 0.0852  noobj_loss: 0.0046  bbox_loss: 0.0310  cls_loss: 0.0648  \n",
      "<<<iteration:[400/657] - total_loss: 0.3216  obj_loss: 0.0887  noobj_loss: 0.0061  bbox_loss: 0.0317  cls_loss: 0.0714  \n",
      "<<<iteration:[420/657] - total_loss: 0.2943  obj_loss: 0.0765  noobj_loss: 0.0046  bbox_loss: 0.0306  cls_loss: 0.0623  \n",
      "<<<iteration:[440/657] - total_loss: 0.2948  obj_loss: 0.0827  noobj_loss: 0.0079  bbox_loss: 0.0310  cls_loss: 0.0533  \n",
      "<<<iteration:[460/657] - total_loss: 0.3223  obj_loss: 0.0866  noobj_loss: 0.0047  bbox_loss: 0.0336  cls_loss: 0.0655  \n",
      "<<<iteration:[480/657] - total_loss: 0.3163  obj_loss: 0.0846  noobj_loss: 0.0070  bbox_loss: 0.0338  cls_loss: 0.0593  \n",
      "<<<iteration:[500/657] - total_loss: 0.2918  obj_loss: 0.0634  noobj_loss: 0.0086  bbox_loss: 0.0312  cls_loss: 0.0680  \n",
      "<<<iteration:[520/657] - total_loss: 0.2840  obj_loss: 0.0886  noobj_loss: 0.0044  bbox_loss: 0.0280  cls_loss: 0.0530  \n",
      "<<<iteration:[540/657] - total_loss: 0.3088  obj_loss: 0.0834  noobj_loss: 0.0033  bbox_loss: 0.0310  cls_loss: 0.0685  \n",
      "<<<iteration:[560/657] - total_loss: 0.3006  obj_loss: 0.0846  noobj_loss: 0.0051  bbox_loss: 0.0309  cls_loss: 0.0589  \n",
      "<<<iteration:[580/657] - total_loss: 0.3135  obj_loss: 0.0845  noobj_loss: 0.0044  bbox_loss: 0.0329  cls_loss: 0.0621  \n",
      "<<<iteration:[600/657] - total_loss: 0.3383  obj_loss: 0.0869  noobj_loss: 0.0086  bbox_loss: 0.0350  cls_loss: 0.0719  \n",
      "<<<iteration:[620/657] - total_loss: 0.3140  obj_loss: 0.0916  noobj_loss: 0.0073  bbox_loss: 0.0314  cls_loss: 0.0619  \n",
      "<<<iteration:[640/657] - total_loss: 0.2995  obj_loss: 0.0825  noobj_loss: 0.0051  bbox_loss: 0.0309  cls_loss: 0.0599  \n",
      "\n",
      "epoch:10/100 - Train Loss: 0.3111, Val Loss: 0.3402\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3018  obj_loss: 0.0941  noobj_loss: 0.0041  bbox_loss: 0.0304  cls_loss: 0.0534  \n",
      "<<<iteration:[40/657] - total_loss: 0.2887  obj_loss: 0.0820  noobj_loss: 0.0054  bbox_loss: 0.0301  cls_loss: 0.0535  \n",
      "<<<iteration:[60/657] - total_loss: 0.3069  obj_loss: 0.0883  noobj_loss: 0.0069  bbox_loss: 0.0307  cls_loss: 0.0619  \n",
      "<<<iteration:[80/657] - total_loss: 0.3045  obj_loss: 0.0785  noobj_loss: 0.0109  bbox_loss: 0.0322  cls_loss: 0.0597  \n",
      "<<<iteration:[100/657] - total_loss: 0.3265  obj_loss: 0.0880  noobj_loss: 0.0081  bbox_loss: 0.0344  cls_loss: 0.0627  \n",
      "<<<iteration:[120/657] - total_loss: 0.3089  obj_loss: 0.0836  noobj_loss: 0.0060  bbox_loss: 0.0316  cls_loss: 0.0640  \n",
      "<<<iteration:[140/657] - total_loss: 0.3172  obj_loss: 0.0941  noobj_loss: 0.0079  bbox_loss: 0.0303  cls_loss: 0.0678  \n",
      "<<<iteration:[160/657] - total_loss: 0.3016  obj_loss: 0.1025  noobj_loss: 0.0073  bbox_loss: 0.0294  cls_loss: 0.0484  \n",
      "<<<iteration:[180/657] - total_loss: 0.2981  obj_loss: 0.0811  noobj_loss: 0.0074  bbox_loss: 0.0305  cls_loss: 0.0605  \n",
      "<<<iteration:[200/657] - total_loss: 0.3012  obj_loss: 0.0775  noobj_loss: 0.0061  bbox_loss: 0.0315  cls_loss: 0.0631  \n",
      "<<<iteration:[220/657] - total_loss: 0.3384  obj_loss: 0.1001  noobj_loss: 0.0110  bbox_loss: 0.0336  cls_loss: 0.0645  \n",
      "<<<iteration:[240/657] - total_loss: 0.2920  obj_loss: 0.0776  noobj_loss: 0.0054  bbox_loss: 0.0296  cls_loss: 0.0635  \n",
      "<<<iteration:[260/657] - total_loss: 0.2926  obj_loss: 0.0774  noobj_loss: 0.0045  bbox_loss: 0.0307  cls_loss: 0.0595  \n",
      "<<<iteration:[280/657] - total_loss: 0.3189  obj_loss: 0.0983  noobj_loss: 0.0072  bbox_loss: 0.0303  cls_loss: 0.0657  \n",
      "<<<iteration:[300/657] - total_loss: 0.3185  obj_loss: 0.0914  noobj_loss: 0.0069  bbox_loss: 0.0334  cls_loss: 0.0566  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[320/657] - total_loss: 0.3048  obj_loss: 0.0804  noobj_loss: 0.0047  bbox_loss: 0.0322  cls_loss: 0.0609  \n",
      "<<<iteration:[340/657] - total_loss: 0.3213  obj_loss: 0.0815  noobj_loss: 0.0075  bbox_loss: 0.0316  cls_loss: 0.0781  \n",
      "<<<iteration:[360/657] - total_loss: 0.2993  obj_loss: 0.0834  noobj_loss: 0.0066  bbox_loss: 0.0316  cls_loss: 0.0547  \n",
      "<<<iteration:[380/657] - total_loss: 0.3205  obj_loss: 0.0881  noobj_loss: 0.0075  bbox_loss: 0.0319  cls_loss: 0.0690  \n",
      "<<<iteration:[400/657] - total_loss: 0.3009  obj_loss: 0.0877  noobj_loss: 0.0074  bbox_loss: 0.0310  cls_loss: 0.0547  \n",
      "<<<iteration:[420/657] - total_loss: 0.3020  obj_loss: 0.0831  noobj_loss: 0.0077  bbox_loss: 0.0303  cls_loss: 0.0637  \n",
      "<<<iteration:[440/657] - total_loss: 0.3250  obj_loss: 0.0968  noobj_loss: 0.0098  bbox_loss: 0.0306  cls_loss: 0.0705  \n",
      "<<<iteration:[460/657] - total_loss: 0.3065  obj_loss: 0.0883  noobj_loss: 0.0065  bbox_loss: 0.0298  cls_loss: 0.0656  \n",
      "<<<iteration:[480/657] - total_loss: 0.3201  obj_loss: 0.0959  noobj_loss: 0.0088  bbox_loss: 0.0325  cls_loss: 0.0574  \n",
      "<<<iteration:[500/657] - total_loss: 0.3045  obj_loss: 0.0837  noobj_loss: 0.0070  bbox_loss: 0.0331  cls_loss: 0.0519  \n",
      "<<<iteration:[520/657] - total_loss: 0.3313  obj_loss: 0.0943  noobj_loss: 0.0059  bbox_loss: 0.0336  cls_loss: 0.0659  \n",
      "<<<iteration:[540/657] - total_loss: 0.3043  obj_loss: 0.0810  noobj_loss: 0.0052  bbox_loss: 0.0328  cls_loss: 0.0567  \n",
      "<<<iteration:[560/657] - total_loss: 0.3006  obj_loss: 0.0878  noobj_loss: 0.0057  bbox_loss: 0.0314  cls_loss: 0.0529  \n",
      "<<<iteration:[580/657] - total_loss: 0.3023  obj_loss: 0.0916  noobj_loss: 0.0054  bbox_loss: 0.0292  cls_loss: 0.0623  \n",
      "<<<iteration:[600/657] - total_loss: 0.2844  obj_loss: 0.0907  noobj_loss: 0.0086  bbox_loss: 0.0280  cls_loss: 0.0496  \n",
      "<<<iteration:[620/657] - total_loss: 0.3020  obj_loss: 0.0917  noobj_loss: 0.0054  bbox_loss: 0.0301  cls_loss: 0.0574  \n",
      "<<<iteration:[640/657] - total_loss: 0.3113  obj_loss: 0.0837  noobj_loss: 0.0094  bbox_loss: 0.0337  cls_loss: 0.0543  \n",
      "\n",
      "epoch:11/100 - Train Loss: 0.3075, Val Loss: 0.3376\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2937  obj_loss: 0.0915  noobj_loss: 0.0096  bbox_loss: 0.0288  cls_loss: 0.0532  \n",
      "<<<iteration:[40/657] - total_loss: 0.3049  obj_loss: 0.0953  noobj_loss: 0.0080  bbox_loss: 0.0287  cls_loss: 0.0620  \n",
      "<<<iteration:[60/657] - total_loss: 0.2996  obj_loss: 0.0768  noobj_loss: 0.0069  bbox_loss: 0.0338  cls_loss: 0.0504  \n",
      "<<<iteration:[80/657] - total_loss: 0.2780  obj_loss: 0.0852  noobj_loss: 0.0080  bbox_loss: 0.0284  cls_loss: 0.0465  \n",
      "<<<iteration:[100/657] - total_loss: 0.2938  obj_loss: 0.0970  noobj_loss: 0.0075  bbox_loss: 0.0283  cls_loss: 0.0515  \n",
      "<<<iteration:[120/657] - total_loss: 0.2866  obj_loss: 0.0820  noobj_loss: 0.0069  bbox_loss: 0.0298  cls_loss: 0.0521  \n",
      "<<<iteration:[140/657] - total_loss: 0.2825  obj_loss: 0.1018  noobj_loss: 0.0049  bbox_loss: 0.0258  cls_loss: 0.0494  \n",
      "<<<iteration:[160/657] - total_loss: 0.3098  obj_loss: 0.0812  noobj_loss: 0.0058  bbox_loss: 0.0340  cls_loss: 0.0555  \n",
      "<<<iteration:[180/657] - total_loss: 0.2948  obj_loss: 0.0947  noobj_loss: 0.0058  bbox_loss: 0.0289  cls_loss: 0.0525  \n",
      "<<<iteration:[200/657] - total_loss: 0.3249  obj_loss: 0.0912  noobj_loss: 0.0070  bbox_loss: 0.0334  cls_loss: 0.0632  \n",
      "<<<iteration:[220/657] - total_loss: 0.2904  obj_loss: 0.0863  noobj_loss: 0.0098  bbox_loss: 0.0302  cls_loss: 0.0480  \n",
      "<<<iteration:[240/657] - total_loss: 0.3153  obj_loss: 0.0972  noobj_loss: 0.0081  bbox_loss: 0.0321  cls_loss: 0.0537  \n",
      "<<<iteration:[260/657] - total_loss: 0.3082  obj_loss: 0.0979  noobj_loss: 0.0060  bbox_loss: 0.0293  cls_loss: 0.0608  \n",
      "<<<iteration:[280/657] - total_loss: 0.3059  obj_loss: 0.0935  noobj_loss: 0.0094  bbox_loss: 0.0294  cls_loss: 0.0605  \n",
      "<<<iteration:[300/657] - total_loss: 0.2890  obj_loss: 0.0920  noobj_loss: 0.0071  bbox_loss: 0.0282  cls_loss: 0.0525  \n",
      "<<<iteration:[320/657] - total_loss: 0.2854  obj_loss: 0.0825  noobj_loss: 0.0081  bbox_loss: 0.0306  cls_loss: 0.0456  \n",
      "<<<iteration:[340/657] - total_loss: 0.2771  obj_loss: 0.0937  noobj_loss: 0.0056  bbox_loss: 0.0269  cls_loss: 0.0460  \n",
      "<<<iteration:[360/657] - total_loss: 0.3081  obj_loss: 0.0852  noobj_loss: 0.0114  bbox_loss: 0.0311  cls_loss: 0.0616  \n",
      "<<<iteration:[380/657] - total_loss: 0.3120  obj_loss: 0.0909  noobj_loss: 0.0053  bbox_loss: 0.0315  cls_loss: 0.0607  \n",
      "<<<iteration:[400/657] - total_loss: 0.2945  obj_loss: 0.0890  noobj_loss: 0.0057  bbox_loss: 0.0321  cls_loss: 0.0423  \n",
      "<<<iteration:[420/657] - total_loss: 0.3104  obj_loss: 0.0898  noobj_loss: 0.0071  bbox_loss: 0.0320  cls_loss: 0.0572  \n",
      "<<<iteration:[440/657] - total_loss: 0.3073  obj_loss: 0.0878  noobj_loss: 0.0100  bbox_loss: 0.0305  cls_loss: 0.0619  \n",
      "<<<iteration:[460/657] - total_loss: 0.3122  obj_loss: 0.0869  noobj_loss: 0.0103  bbox_loss: 0.0315  cls_loss: 0.0629  \n",
      "<<<iteration:[480/657] - total_loss: 0.3149  obj_loss: 0.0994  noobj_loss: 0.0093  bbox_loss: 0.0310  cls_loss: 0.0557  \n",
      "<<<iteration:[500/657] - total_loss: 0.3141  obj_loss: 0.0840  noobj_loss: 0.0113  bbox_loss: 0.0344  cls_loss: 0.0525  \n",
      "<<<iteration:[520/657] - total_loss: 0.3080  obj_loss: 0.0964  noobj_loss: 0.0076  bbox_loss: 0.0306  cls_loss: 0.0546  \n",
      "<<<iteration:[540/657] - total_loss: 0.2913  obj_loss: 0.0849  noobj_loss: 0.0106  bbox_loss: 0.0284  cls_loss: 0.0590  \n",
      "<<<iteration:[560/657] - total_loss: 0.2809  obj_loss: 0.0813  noobj_loss: 0.0118  bbox_loss: 0.0291  cls_loss: 0.0485  \n",
      "<<<iteration:[580/657] - total_loss: 0.2871  obj_loss: 0.0945  noobj_loss: 0.0064  bbox_loss: 0.0265  cls_loss: 0.0566  \n",
      "<<<iteration:[600/657] - total_loss: 0.2938  obj_loss: 0.0910  noobj_loss: 0.0076  bbox_loss: 0.0289  cls_loss: 0.0545  \n",
      "<<<iteration:[620/657] - total_loss: 0.3086  obj_loss: 0.0920  noobj_loss: 0.0088  bbox_loss: 0.0291  cls_loss: 0.0667  \n",
      "<<<iteration:[640/657] - total_loss: 0.2812  obj_loss: 0.0890  noobj_loss: 0.0051  bbox_loss: 0.0285  cls_loss: 0.0471  \n",
      "\n",
      "epoch:12/100 - Train Loss: 0.2994, Val Loss: 0.3553\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3093  obj_loss: 0.0972  noobj_loss: 0.0105  bbox_loss: 0.0307  cls_loss: 0.0533  \n",
      "<<<iteration:[40/657] - total_loss: 0.2885  obj_loss: 0.0964  noobj_loss: 0.0085  bbox_loss: 0.0286  cls_loss: 0.0447  \n",
      "<<<iteration:[60/657] - total_loss: 0.2819  obj_loss: 0.0809  noobj_loss: 0.0091  bbox_loss: 0.0286  cls_loss: 0.0534  \n",
      "<<<iteration:[80/657] - total_loss: 0.2931  obj_loss: 0.0975  noobj_loss: 0.0092  bbox_loss: 0.0265  cls_loss: 0.0584  \n",
      "<<<iteration:[100/657] - total_loss: 0.3028  obj_loss: 0.0891  noobj_loss: 0.0078  bbox_loss: 0.0329  cls_loss: 0.0454  \n",
      "<<<iteration:[120/657] - total_loss: 0.2934  obj_loss: 0.0966  noobj_loss: 0.0099  bbox_loss: 0.0276  cls_loss: 0.0538  \n",
      "<<<iteration:[140/657] - total_loss: 0.2897  obj_loss: 0.0930  noobj_loss: 0.0093  bbox_loss: 0.0283  cls_loss: 0.0506  \n",
      "<<<iteration:[160/657] - total_loss: 0.2652  obj_loss: 0.0773  noobj_loss: 0.0111  bbox_loss: 0.0258  cls_loss: 0.0532  \n",
      "<<<iteration:[180/657] - total_loss: 0.3090  obj_loss: 0.0991  noobj_loss: 0.0136  bbox_loss: 0.0291  cls_loss: 0.0577  \n",
      "<<<iteration:[200/657] - total_loss: 0.2675  obj_loss: 0.0921  noobj_loss: 0.0122  bbox_loss: 0.0249  cls_loss: 0.0447  \n",
      "<<<iteration:[220/657] - total_loss: 0.2924  obj_loss: 0.0868  noobj_loss: 0.0143  bbox_loss: 0.0275  cls_loss: 0.0611  \n",
      "<<<iteration:[240/657] - total_loss: 0.2850  obj_loss: 0.1006  noobj_loss: 0.0091  bbox_loss: 0.0265  cls_loss: 0.0472  \n",
      "<<<iteration:[260/657] - total_loss: 0.2865  obj_loss: 0.1086  noobj_loss: 0.0075  bbox_loss: 0.0248  cls_loss: 0.0501  \n",
      "<<<iteration:[280/657] - total_loss: 0.2714  obj_loss: 0.0977  noobj_loss: 0.0134  bbox_loss: 0.0253  cls_loss: 0.0407  \n",
      "<<<iteration:[300/657] - total_loss: 0.2988  obj_loss: 0.0959  noobj_loss: 0.0070  bbox_loss: 0.0296  cls_loss: 0.0513  \n",
      "<<<iteration:[320/657] - total_loss: 0.2848  obj_loss: 0.0983  noobj_loss: 0.0082  bbox_loss: 0.0272  cls_loss: 0.0462  \n",
      "<<<iteration:[340/657] - total_loss: 0.2794  obj_loss: 0.0934  noobj_loss: 0.0074  bbox_loss: 0.0279  cls_loss: 0.0426  \n",
      "<<<iteration:[360/657] - total_loss: 0.2557  obj_loss: 0.0961  noobj_loss: 0.0097  bbox_loss: 0.0235  cls_loss: 0.0373  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[380/657] - total_loss: 0.2924  obj_loss: 0.0890  noobj_loss: 0.0166  bbox_loss: 0.0296  cls_loss: 0.0473  \n",
      "<<<iteration:[400/657] - total_loss: 0.2806  obj_loss: 0.0999  noobj_loss: 0.0111  bbox_loss: 0.0248  cls_loss: 0.0514  \n",
      "<<<iteration:[420/657] - total_loss: 0.3073  obj_loss: 0.1088  noobj_loss: 0.0130  bbox_loss: 0.0277  cls_loss: 0.0532  \n",
      "<<<iteration:[440/657] - total_loss: 0.2967  obj_loss: 0.0944  noobj_loss: 0.0119  bbox_loss: 0.0296  cls_loss: 0.0482  \n",
      "<<<iteration:[460/657] - total_loss: 0.2806  obj_loss: 0.0869  noobj_loss: 0.0092  bbox_loss: 0.0283  cls_loss: 0.0478  \n",
      "<<<iteration:[480/657] - total_loss: 0.2923  obj_loss: 0.0849  noobj_loss: 0.0128  bbox_loss: 0.0301  cls_loss: 0.0506  \n",
      "<<<iteration:[500/657] - total_loss: 0.3056  obj_loss: 0.0851  noobj_loss: 0.0125  bbox_loss: 0.0323  cls_loss: 0.0526  \n",
      "<<<iteration:[520/657] - total_loss: 0.2765  obj_loss: 0.0895  noobj_loss: 0.0124  bbox_loss: 0.0279  cls_loss: 0.0415  \n",
      "<<<iteration:[540/657] - total_loss: 0.2858  obj_loss: 0.0887  noobj_loss: 0.0090  bbox_loss: 0.0290  cls_loss: 0.0474  \n",
      "<<<iteration:[560/657] - total_loss: 0.2973  obj_loss: 0.0882  noobj_loss: 0.0119  bbox_loss: 0.0299  cls_loss: 0.0539  \n",
      "<<<iteration:[580/657] - total_loss: 0.3062  obj_loss: 0.0951  noobj_loss: 0.0133  bbox_loss: 0.0295  cls_loss: 0.0568  \n",
      "<<<iteration:[600/657] - total_loss: 0.2854  obj_loss: 0.0922  noobj_loss: 0.0098  bbox_loss: 0.0284  cls_loss: 0.0465  \n",
      "<<<iteration:[620/657] - total_loss: 0.3289  obj_loss: 0.1079  noobj_loss: 0.0129  bbox_loss: 0.0307  cls_loss: 0.0613  \n",
      "<<<iteration:[640/657] - total_loss: 0.3031  obj_loss: 0.0993  noobj_loss: 0.0107  bbox_loss: 0.0288  cls_loss: 0.0546  \n",
      "\n",
      "epoch:13/100 - Train Loss: 0.2908, Val Loss: 0.3347\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2937  obj_loss: 0.1033  noobj_loss: 0.0142  bbox_loss: 0.0276  cls_loss: 0.0455  \n",
      "<<<iteration:[40/657] - total_loss: 0.2711  obj_loss: 0.0945  noobj_loss: 0.0105  bbox_loss: 0.0261  cls_loss: 0.0409  \n",
      "<<<iteration:[60/657] - total_loss: 0.3050  obj_loss: 0.1022  noobj_loss: 0.0103  bbox_loss: 0.0289  cls_loss: 0.0531  \n",
      "<<<iteration:[80/657] - total_loss: 0.2673  obj_loss: 0.0909  noobj_loss: 0.0094  bbox_loss: 0.0256  cls_loss: 0.0436  \n",
      "<<<iteration:[100/657] - total_loss: 0.3099  obj_loss: 0.1042  noobj_loss: 0.0132  bbox_loss: 0.0294  cls_loss: 0.0519  \n",
      "<<<iteration:[120/657] - total_loss: 0.2928  obj_loss: 0.1050  noobj_loss: 0.0121  bbox_loss: 0.0271  cls_loss: 0.0464  \n",
      "<<<iteration:[140/657] - total_loss: 0.2752  obj_loss: 0.1058  noobj_loss: 0.0141  bbox_loss: 0.0251  cls_loss: 0.0371  \n",
      "<<<iteration:[160/657] - total_loss: 0.3025  obj_loss: 0.1008  noobj_loss: 0.0139  bbox_loss: 0.0293  cls_loss: 0.0482  \n",
      "<<<iteration:[180/657] - total_loss: 0.2820  obj_loss: 0.0994  noobj_loss: 0.0127  bbox_loss: 0.0251  cls_loss: 0.0508  \n",
      "<<<iteration:[200/657] - total_loss: 0.2879  obj_loss: 0.0892  noobj_loss: 0.0106  bbox_loss: 0.0304  cls_loss: 0.0414  \n",
      "<<<iteration:[220/657] - total_loss: 0.2850  obj_loss: 0.0903  noobj_loss: 0.0131  bbox_loss: 0.0279  cls_loss: 0.0484  \n",
      "<<<iteration:[240/657] - total_loss: 0.2754  obj_loss: 0.0960  noobj_loss: 0.0088  bbox_loss: 0.0267  cls_loss: 0.0413  \n",
      "<<<iteration:[260/657] - total_loss: 0.2949  obj_loss: 0.0947  noobj_loss: 0.0149  bbox_loss: 0.0277  cls_loss: 0.0543  \n",
      "<<<iteration:[280/657] - total_loss: 0.3066  obj_loss: 0.0961  noobj_loss: 0.0153  bbox_loss: 0.0323  cls_loss: 0.0412  \n",
      "<<<iteration:[300/657] - total_loss: 0.2880  obj_loss: 0.0922  noobj_loss: 0.0153  bbox_loss: 0.0280  cls_loss: 0.0480  \n",
      "<<<iteration:[320/657] - total_loss: 0.2817  obj_loss: 0.0818  noobj_loss: 0.0122  bbox_loss: 0.0271  cls_loss: 0.0585  \n",
      "<<<iteration:[340/657] - total_loss: 0.2764  obj_loss: 0.0924  noobj_loss: 0.0115  bbox_loss: 0.0252  cls_loss: 0.0522  \n",
      "<<<iteration:[360/657] - total_loss: 0.2976  obj_loss: 0.1141  noobj_loss: 0.0166  bbox_loss: 0.0261  cls_loss: 0.0448  \n",
      "<<<iteration:[380/657] - total_loss: 0.2896  obj_loss: 0.1073  noobj_loss: 0.0070  bbox_loss: 0.0263  cls_loss: 0.0471  \n",
      "<<<iteration:[400/657] - total_loss: 0.2960  obj_loss: 0.0985  noobj_loss: 0.0095  bbox_loss: 0.0287  cls_loss: 0.0490  \n",
      "<<<iteration:[420/657] - total_loss: 0.2727  obj_loss: 0.1038  noobj_loss: 0.0097  bbox_loss: 0.0240  cls_loss: 0.0440  \n",
      "<<<iteration:[440/657] - total_loss: 0.2643  obj_loss: 0.0907  noobj_loss: 0.0101  bbox_loss: 0.0247  cls_loss: 0.0449  \n",
      "<<<iteration:[460/657] - total_loss: 0.2850  obj_loss: 0.0930  noobj_loss: 0.0127  bbox_loss: 0.0263  cls_loss: 0.0543  \n",
      "<<<iteration:[480/657] - total_loss: 0.2972  obj_loss: 0.0966  noobj_loss: 0.0092  bbox_loss: 0.0289  cls_loss: 0.0513  \n",
      "<<<iteration:[500/657] - total_loss: 0.3058  obj_loss: 0.0926  noobj_loss: 0.0172  bbox_loss: 0.0298  cls_loss: 0.0557  \n",
      "<<<iteration:[520/657] - total_loss: 0.2812  obj_loss: 0.1062  noobj_loss: 0.0082  bbox_loss: 0.0252  cls_loss: 0.0449  \n",
      "<<<iteration:[540/657] - total_loss: 0.2766  obj_loss: 0.0990  noobj_loss: 0.0121  bbox_loss: 0.0247  cls_loss: 0.0482  \n",
      "<<<iteration:[560/657] - total_loss: 0.2853  obj_loss: 0.1119  noobj_loss: 0.0125  bbox_loss: 0.0246  cls_loss: 0.0439  \n",
      "<<<iteration:[580/657] - total_loss: 0.2948  obj_loss: 0.1015  noobj_loss: 0.0142  bbox_loss: 0.0286  cls_loss: 0.0432  \n",
      "<<<iteration:[600/657] - total_loss: 0.3048  obj_loss: 0.1002  noobj_loss: 0.0136  bbox_loss: 0.0298  cls_loss: 0.0490  \n",
      "<<<iteration:[620/657] - total_loss: 0.2791  obj_loss: 0.0956  noobj_loss: 0.0124  bbox_loss: 0.0257  cls_loss: 0.0489  \n",
      "<<<iteration:[640/657] - total_loss: 0.2664  obj_loss: 0.1013  noobj_loss: 0.0118  bbox_loss: 0.0230  cls_loss: 0.0444  \n",
      "\n",
      "epoch:14/100 - Train Loss: 0.2868, Val Loss: 0.3245\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2980  obj_loss: 0.1062  noobj_loss: 0.0131  bbox_loss: 0.0279  cls_loss: 0.0457  \n",
      "<<<iteration:[40/657] - total_loss: 0.2944  obj_loss: 0.0999  noobj_loss: 0.0121  bbox_loss: 0.0274  cls_loss: 0.0515  \n",
      "<<<iteration:[60/657] - total_loss: 0.2699  obj_loss: 0.0998  noobj_loss: 0.0122  bbox_loss: 0.0240  cls_loss: 0.0439  \n",
      "<<<iteration:[80/657] - total_loss: 0.2825  obj_loss: 0.0908  noobj_loss: 0.0154  bbox_loss: 0.0278  cls_loss: 0.0449  \n",
      "<<<iteration:[100/657] - total_loss: 0.2942  obj_loss: 0.1010  noobj_loss: 0.0163  bbox_loss: 0.0274  cls_loss: 0.0479  \n",
      "<<<iteration:[120/657] - total_loss: 0.2625  obj_loss: 0.1015  noobj_loss: 0.0131  bbox_loss: 0.0230  cls_loss: 0.0397  \n",
      "<<<iteration:[140/657] - total_loss: 0.2860  obj_loss: 0.1076  noobj_loss: 0.0122  bbox_loss: 0.0266  cls_loss: 0.0393  \n",
      "<<<iteration:[160/657] - total_loss: 0.2981  obj_loss: 0.0950  noobj_loss: 0.0205  bbox_loss: 0.0293  cls_loss: 0.0462  \n",
      "<<<iteration:[180/657] - total_loss: 0.2870  obj_loss: 0.1123  noobj_loss: 0.0116  bbox_loss: 0.0254  cls_loss: 0.0417  \n",
      "<<<iteration:[200/657] - total_loss: 0.2610  obj_loss: 0.0900  noobj_loss: 0.0106  bbox_loss: 0.0254  cls_loss: 0.0387  \n",
      "<<<iteration:[220/657] - total_loss: 0.2957  obj_loss: 0.1082  noobj_loss: 0.0137  bbox_loss: 0.0265  cls_loss: 0.0482  \n",
      "<<<iteration:[240/657] - total_loss: 0.2619  obj_loss: 0.0904  noobj_loss: 0.0142  bbox_loss: 0.0249  cls_loss: 0.0401  \n",
      "<<<iteration:[260/657] - total_loss: 0.2872  obj_loss: 0.1028  noobj_loss: 0.0159  bbox_loss: 0.0257  cls_loss: 0.0478  \n",
      "<<<iteration:[280/657] - total_loss: 0.3005  obj_loss: 0.1193  noobj_loss: 0.0134  bbox_loss: 0.0257  cls_loss: 0.0459  \n",
      "<<<iteration:[300/657] - total_loss: 0.3032  obj_loss: 0.1089  noobj_loss: 0.0181  bbox_loss: 0.0276  cls_loss: 0.0473  \n",
      "<<<iteration:[320/657] - total_loss: 0.2841  obj_loss: 0.1126  noobj_loss: 0.0126  bbox_loss: 0.0233  cls_loss: 0.0488  \n",
      "<<<iteration:[340/657] - total_loss: 0.2845  obj_loss: 0.1076  noobj_loss: 0.0211  bbox_loss: 0.0247  cls_loss: 0.0430  \n",
      "<<<iteration:[360/657] - total_loss: 0.2752  obj_loss: 0.0985  noobj_loss: 0.0184  bbox_loss: 0.0228  cls_loss: 0.0536  \n",
      "<<<iteration:[380/657] - total_loss: 0.2794  obj_loss: 0.1061  noobj_loss: 0.0169  bbox_loss: 0.0241  cls_loss: 0.0443  \n",
      "<<<iteration:[400/657] - total_loss: 0.2857  obj_loss: 0.1080  noobj_loss: 0.0110  bbox_loss: 0.0255  cls_loss: 0.0444  \n",
      "<<<iteration:[420/657] - total_loss: 0.2827  obj_loss: 0.1140  noobj_loss: 0.0176  bbox_loss: 0.0234  cls_loss: 0.0431  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[440/657] - total_loss: 0.2817  obj_loss: 0.0888  noobj_loss: 0.0199  bbox_loss: 0.0285  cls_loss: 0.0405  \n",
      "<<<iteration:[460/657] - total_loss: 0.2839  obj_loss: 0.1053  noobj_loss: 0.0154  bbox_loss: 0.0246  cls_loss: 0.0478  \n",
      "<<<iteration:[480/657] - total_loss: 0.2786  obj_loss: 0.1061  noobj_loss: 0.0172  bbox_loss: 0.0239  cls_loss: 0.0444  \n",
      "<<<iteration:[500/657] - total_loss: 0.2870  obj_loss: 0.1037  noobj_loss: 0.0131  bbox_loss: 0.0249  cls_loss: 0.0522  \n",
      "<<<iteration:[520/657] - total_loss: 0.2915  obj_loss: 0.0954  noobj_loss: 0.0176  bbox_loss: 0.0275  cls_loss: 0.0498  \n",
      "<<<iteration:[540/657] - total_loss: 0.2994  obj_loss: 0.1087  noobj_loss: 0.0121  bbox_loss: 0.0266  cls_loss: 0.0516  \n",
      "<<<iteration:[560/657] - total_loss: 0.2682  obj_loss: 0.0990  noobj_loss: 0.0186  bbox_loss: 0.0250  cls_loss: 0.0350  \n",
      "<<<iteration:[580/657] - total_loss: 0.2843  obj_loss: 0.0973  noobj_loss: 0.0154  bbox_loss: 0.0273  cls_loss: 0.0426  \n",
      "<<<iteration:[600/657] - total_loss: 0.2832  obj_loss: 0.1042  noobj_loss: 0.0126  bbox_loss: 0.0251  cls_loss: 0.0472  \n",
      "<<<iteration:[620/657] - total_loss: 0.2761  obj_loss: 0.1145  noobj_loss: 0.0145  bbox_loss: 0.0217  cls_loss: 0.0456  \n",
      "<<<iteration:[640/657] - total_loss: 0.2742  obj_loss: 0.1135  noobj_loss: 0.0150  bbox_loss: 0.0225  cls_loss: 0.0406  \n",
      "\n",
      "epoch:15/100 - Train Loss: 0.2835, Val Loss: 0.3149\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2889  obj_loss: 0.1120  noobj_loss: 0.0181  bbox_loss: 0.0267  cls_loss: 0.0345  \n",
      "<<<iteration:[40/657] - total_loss: 0.2999  obj_loss: 0.1060  noobj_loss: 0.0199  bbox_loss: 0.0276  cls_loss: 0.0460  \n",
      "<<<iteration:[60/657] - total_loss: 0.2857  obj_loss: 0.1070  noobj_loss: 0.0166  bbox_loss: 0.0261  cls_loss: 0.0401  \n",
      "<<<iteration:[80/657] - total_loss: 0.2886  obj_loss: 0.1067  noobj_loss: 0.0185  bbox_loss: 0.0255  cls_loss: 0.0451  \n",
      "<<<iteration:[100/657] - total_loss: 0.2632  obj_loss: 0.1042  noobj_loss: 0.0156  bbox_loss: 0.0221  cls_loss: 0.0409  \n",
      "<<<iteration:[120/657] - total_loss: 0.2654  obj_loss: 0.1007  noobj_loss: 0.0158  bbox_loss: 0.0232  cls_loss: 0.0406  \n",
      "<<<iteration:[140/657] - total_loss: 0.3068  obj_loss: 0.1186  noobj_loss: 0.0188  bbox_loss: 0.0259  cls_loss: 0.0491  \n",
      "<<<iteration:[160/657] - total_loss: 0.2753  obj_loss: 0.0995  noobj_loss: 0.0221  bbox_loss: 0.0254  cls_loss: 0.0376  \n",
      "<<<iteration:[180/657] - total_loss: 0.2850  obj_loss: 0.1030  noobj_loss: 0.0155  bbox_loss: 0.0263  cls_loss: 0.0430  \n",
      "<<<iteration:[200/657] - total_loss: 0.2807  obj_loss: 0.1060  noobj_loss: 0.0203  bbox_loss: 0.0248  cls_loss: 0.0406  \n",
      "<<<iteration:[220/657] - total_loss: 0.2640  obj_loss: 0.0953  noobj_loss: 0.0169  bbox_loss: 0.0246  cls_loss: 0.0375  \n",
      "<<<iteration:[240/657] - total_loss: 0.2792  obj_loss: 0.1096  noobj_loss: 0.0157  bbox_loss: 0.0225  cls_loss: 0.0495  \n",
      "<<<iteration:[260/657] - total_loss: 0.2836  obj_loss: 0.1015  noobj_loss: 0.0159  bbox_loss: 0.0250  cls_loss: 0.0489  \n",
      "<<<iteration:[280/657] - total_loss: 0.2664  obj_loss: 0.0973  noobj_loss: 0.0172  bbox_loss: 0.0236  cls_loss: 0.0422  \n",
      "<<<iteration:[300/657] - total_loss: 0.2817  obj_loss: 0.1019  noobj_loss: 0.0182  bbox_loss: 0.0256  cls_loss: 0.0426  \n",
      "<<<iteration:[320/657] - total_loss: 0.2875  obj_loss: 0.1058  noobj_loss: 0.0172  bbox_loss: 0.0258  cls_loss: 0.0443  \n",
      "<<<iteration:[340/657] - total_loss: 0.2950  obj_loss: 0.1101  noobj_loss: 0.0179  bbox_loss: 0.0267  cls_loss: 0.0425  \n",
      "<<<iteration:[360/657] - total_loss: 0.2680  obj_loss: 0.1063  noobj_loss: 0.0197  bbox_loss: 0.0226  cls_loss: 0.0391  \n",
      "<<<iteration:[380/657] - total_loss: 0.2752  obj_loss: 0.1074  noobj_loss: 0.0200  bbox_loss: 0.0234  cls_loss: 0.0407  \n",
      "<<<iteration:[400/657] - total_loss: 0.2858  obj_loss: 0.1034  noobj_loss: 0.0194  bbox_loss: 0.0253  cls_loss: 0.0462  \n",
      "<<<iteration:[420/657] - total_loss: 0.2860  obj_loss: 0.1084  noobj_loss: 0.0192  bbox_loss: 0.0240  cls_loss: 0.0481  \n",
      "<<<iteration:[440/657] - total_loss: 0.2751  obj_loss: 0.1106  noobj_loss: 0.0179  bbox_loss: 0.0228  cls_loss: 0.0413  \n",
      "<<<iteration:[460/657] - total_loss: 0.2680  obj_loss: 0.1041  noobj_loss: 0.0156  bbox_loss: 0.0232  cls_loss: 0.0403  \n",
      "<<<iteration:[480/657] - total_loss: 0.2711  obj_loss: 0.0945  noobj_loss: 0.0208  bbox_loss: 0.0250  cls_loss: 0.0413  \n",
      "<<<iteration:[500/657] - total_loss: 0.2812  obj_loss: 0.1050  noobj_loss: 0.0220  bbox_loss: 0.0250  cls_loss: 0.0405  \n",
      "<<<iteration:[520/657] - total_loss: 0.2894  obj_loss: 0.1134  noobj_loss: 0.0233  bbox_loss: 0.0253  cls_loss: 0.0381  \n",
      "<<<iteration:[540/657] - total_loss: 0.2823  obj_loss: 0.1159  noobj_loss: 0.0200  bbox_loss: 0.0238  cls_loss: 0.0376  \n",
      "<<<iteration:[560/657] - total_loss: 0.2607  obj_loss: 0.0958  noobj_loss: 0.0183  bbox_loss: 0.0236  cls_loss: 0.0376  \n",
      "<<<iteration:[580/657] - total_loss: 0.2912  obj_loss: 0.1188  noobj_loss: 0.0151  bbox_loss: 0.0245  cls_loss: 0.0424  \n",
      "<<<iteration:[600/657] - total_loss: 0.2829  obj_loss: 0.1028  noobj_loss: 0.0196  bbox_loss: 0.0240  cls_loss: 0.0504  \n",
      "<<<iteration:[620/657] - total_loss: 0.2752  obj_loss: 0.1068  noobj_loss: 0.0189  bbox_loss: 0.0241  cls_loss: 0.0385  \n",
      "<<<iteration:[640/657] - total_loss: 0.2556  obj_loss: 0.1016  noobj_loss: 0.0147  bbox_loss: 0.0225  cls_loss: 0.0341  \n",
      "\n",
      "epoch:16/100 - Train Loss: 0.2791, Val Loss: 0.3204\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2777  obj_loss: 0.1093  noobj_loss: 0.0232  bbox_loss: 0.0228  cls_loss: 0.0428  \n",
      "<<<iteration:[40/657] - total_loss: 0.2797  obj_loss: 0.0998  noobj_loss: 0.0171  bbox_loss: 0.0252  cls_loss: 0.0454  \n",
      "<<<iteration:[60/657] - total_loss: 0.2784  obj_loss: 0.1046  noobj_loss: 0.0188  bbox_loss: 0.0244  cls_loss: 0.0426  \n",
      "<<<iteration:[80/657] - total_loss: 0.2683  obj_loss: 0.1023  noobj_loss: 0.0182  bbox_loss: 0.0231  cls_loss: 0.0412  \n",
      "<<<iteration:[100/657] - total_loss: 0.2687  obj_loss: 0.1003  noobj_loss: 0.0213  bbox_loss: 0.0229  cls_loss: 0.0435  \n",
      "<<<iteration:[120/657] - total_loss: 0.2673  obj_loss: 0.1119  noobj_loss: 0.0140  bbox_loss: 0.0222  cls_loss: 0.0373  \n",
      "<<<iteration:[140/657] - total_loss: 0.2783  obj_loss: 0.1035  noobj_loss: 0.0205  bbox_loss: 0.0242  cls_loss: 0.0436  \n",
      "<<<iteration:[160/657] - total_loss: 0.2857  obj_loss: 0.1177  noobj_loss: 0.0177  bbox_loss: 0.0217  cls_loss: 0.0504  \n",
      "<<<iteration:[180/657] - total_loss: 0.2807  obj_loss: 0.0974  noobj_loss: 0.0160  bbox_loss: 0.0268  cls_loss: 0.0414  \n",
      "<<<iteration:[200/657] - total_loss: 0.2794  obj_loss: 0.1119  noobj_loss: 0.0157  bbox_loss: 0.0225  cls_loss: 0.0472  \n",
      "<<<iteration:[220/657] - total_loss: 0.2732  obj_loss: 0.1069  noobj_loss: 0.0175  bbox_loss: 0.0239  cls_loss: 0.0382  \n",
      "<<<iteration:[240/657] - total_loss: 0.2775  obj_loss: 0.1084  noobj_loss: 0.0246  bbox_loss: 0.0223  cls_loss: 0.0450  \n",
      "<<<iteration:[260/657] - total_loss: 0.2926  obj_loss: 0.1188  noobj_loss: 0.0175  bbox_loss: 0.0244  cls_loss: 0.0432  \n",
      "<<<iteration:[280/657] - total_loss: 0.2791  obj_loss: 0.1139  noobj_loss: 0.0223  bbox_loss: 0.0235  cls_loss: 0.0367  \n",
      "<<<iteration:[300/657] - total_loss: 0.2964  obj_loss: 0.1068  noobj_loss: 0.0289  bbox_loss: 0.0263  cls_loss: 0.0435  \n",
      "<<<iteration:[320/657] - total_loss: 0.2697  obj_loss: 0.1030  noobj_loss: 0.0218  bbox_loss: 0.0229  cls_loss: 0.0411  \n",
      "<<<iteration:[340/657] - total_loss: 0.2854  obj_loss: 0.0953  noobj_loss: 0.0218  bbox_loss: 0.0272  cls_loss: 0.0431  \n",
      "<<<iteration:[360/657] - total_loss: 0.2760  obj_loss: 0.1124  noobj_loss: 0.0196  bbox_loss: 0.0230  cls_loss: 0.0390  \n",
      "<<<iteration:[380/657] - total_loss: 0.2694  obj_loss: 0.1054  noobj_loss: 0.0218  bbox_loss: 0.0226  cls_loss: 0.0403  \n",
      "<<<iteration:[400/657] - total_loss: 0.2713  obj_loss: 0.1116  noobj_loss: 0.0271  bbox_loss: 0.0217  cls_loss: 0.0375  \n",
      "<<<iteration:[420/657] - total_loss: 0.2945  obj_loss: 0.1078  noobj_loss: 0.0277  bbox_loss: 0.0265  cls_loss: 0.0406  \n",
      "<<<iteration:[440/657] - total_loss: 0.2660  obj_loss: 0.1074  noobj_loss: 0.0210  bbox_loss: 0.0218  cls_loss: 0.0390  \n",
      "<<<iteration:[460/657] - total_loss: 0.2752  obj_loss: 0.1037  noobj_loss: 0.0251  bbox_loss: 0.0227  cls_loss: 0.0454  \n",
      "<<<iteration:[480/657] - total_loss: 0.2835  obj_loss: 0.1182  noobj_loss: 0.0218  bbox_loss: 0.0223  cls_loss: 0.0428  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[500/657] - total_loss: 0.2818  obj_loss: 0.1023  noobj_loss: 0.0200  bbox_loss: 0.0254  cls_loss: 0.0424  \n",
      "<<<iteration:[520/657] - total_loss: 0.2780  obj_loss: 0.1105  noobj_loss: 0.0174  bbox_loss: 0.0231  cls_loss: 0.0433  \n",
      "<<<iteration:[540/657] - total_loss: 0.2704  obj_loss: 0.0982  noobj_loss: 0.0248  bbox_loss: 0.0240  cls_loss: 0.0397  \n",
      "<<<iteration:[560/657] - total_loss: 0.2634  obj_loss: 0.0950  noobj_loss: 0.0195  bbox_loss: 0.0241  cls_loss: 0.0381  \n",
      "<<<iteration:[580/657] - total_loss: 0.2810  obj_loss: 0.1275  noobj_loss: 0.0208  bbox_loss: 0.0218  cls_loss: 0.0339  \n",
      "<<<iteration:[600/657] - total_loss: 0.2631  obj_loss: 0.1142  noobj_loss: 0.0263  bbox_loss: 0.0201  cls_loss: 0.0353  \n",
      "<<<iteration:[620/657] - total_loss: 0.2405  obj_loss: 0.1028  noobj_loss: 0.0242  bbox_loss: 0.0186  cls_loss: 0.0327  \n",
      "<<<iteration:[640/657] - total_loss: 0.2714  obj_loss: 0.1060  noobj_loss: 0.0227  bbox_loss: 0.0234  cls_loss: 0.0372  \n",
      "\n",
      "epoch:17/100 - Train Loss: 0.2756, Val Loss: 0.3070\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2838  obj_loss: 0.1076  noobj_loss: 0.0272  bbox_loss: 0.0244  cls_loss: 0.0405  \n",
      "<<<iteration:[40/657] - total_loss: 0.2600  obj_loss: 0.1062  noobj_loss: 0.0275  bbox_loss: 0.0212  cls_loss: 0.0340  \n",
      "<<<iteration:[60/657] - total_loss: 0.2680  obj_loss: 0.1143  noobj_loss: 0.0247  bbox_loss: 0.0205  cls_loss: 0.0389  \n",
      "<<<iteration:[80/657] - total_loss: 0.2523  obj_loss: 0.0976  noobj_loss: 0.0203  bbox_loss: 0.0217  cls_loss: 0.0362  \n",
      "<<<iteration:[100/657] - total_loss: 0.2712  obj_loss: 0.1101  noobj_loss: 0.0255  bbox_loss: 0.0221  cls_loss: 0.0380  \n",
      "<<<iteration:[120/657] - total_loss: 0.2832  obj_loss: 0.1049  noobj_loss: 0.0288  bbox_loss: 0.0246  cls_loss: 0.0410  \n",
      "<<<iteration:[140/657] - total_loss: 0.2765  obj_loss: 0.1106  noobj_loss: 0.0271  bbox_loss: 0.0220  cls_loss: 0.0421  \n",
      "<<<iteration:[160/657] - total_loss: 0.2618  obj_loss: 0.1019  noobj_loss: 0.0273  bbox_loss: 0.0215  cls_loss: 0.0389  \n",
      "<<<iteration:[180/657] - total_loss: 0.2780  obj_loss: 0.1228  noobj_loss: 0.0265  bbox_loss: 0.0207  cls_loss: 0.0384  \n",
      "<<<iteration:[200/657] - total_loss: 0.2890  obj_loss: 0.1075  noobj_loss: 0.0293  bbox_loss: 0.0250  cls_loss: 0.0419  \n",
      "<<<iteration:[220/657] - total_loss: 0.2879  obj_loss: 0.1203  noobj_loss: 0.0244  bbox_loss: 0.0227  cls_loss: 0.0418  \n",
      "<<<iteration:[240/657] - total_loss: 0.2890  obj_loss: 0.1201  noobj_loss: 0.0297  bbox_loss: 0.0231  cls_loss: 0.0384  \n",
      "<<<iteration:[260/657] - total_loss: 0.2724  obj_loss: 0.1070  noobj_loss: 0.0263  bbox_loss: 0.0238  cls_loss: 0.0331  \n",
      "<<<iteration:[280/657] - total_loss: 0.2731  obj_loss: 0.1204  noobj_loss: 0.0233  bbox_loss: 0.0213  cls_loss: 0.0347  \n",
      "<<<iteration:[300/657] - total_loss: 0.2739  obj_loss: 0.1057  noobj_loss: 0.0294  bbox_loss: 0.0227  cls_loss: 0.0401  \n",
      "<<<iteration:[320/657] - total_loss: 0.2611  obj_loss: 0.1099  noobj_loss: 0.0216  bbox_loss: 0.0208  cls_loss: 0.0363  \n",
      "<<<iteration:[340/657] - total_loss: 0.2700  obj_loss: 0.1148  noobj_loss: 0.0210  bbox_loss: 0.0216  cls_loss: 0.0365  \n",
      "<<<iteration:[360/657] - total_loss: 0.2805  obj_loss: 0.1046  noobj_loss: 0.0253  bbox_loss: 0.0230  cls_loss: 0.0483  \n",
      "<<<iteration:[380/657] - total_loss: 0.2725  obj_loss: 0.1115  noobj_loss: 0.0313  bbox_loss: 0.0219  cls_loss: 0.0358  \n",
      "<<<iteration:[400/657] - total_loss: 0.2925  obj_loss: 0.1194  noobj_loss: 0.0234  bbox_loss: 0.0241  cls_loss: 0.0409  \n",
      "<<<iteration:[420/657] - total_loss: 0.2717  obj_loss: 0.1036  noobj_loss: 0.0324  bbox_loss: 0.0227  cls_loss: 0.0386  \n",
      "<<<iteration:[440/657] - total_loss: 0.2842  obj_loss: 0.1169  noobj_loss: 0.0240  bbox_loss: 0.0225  cls_loss: 0.0427  \n",
      "<<<iteration:[460/657] - total_loss: 0.2729  obj_loss: 0.1088  noobj_loss: 0.0201  bbox_loss: 0.0230  cls_loss: 0.0390  \n",
      "<<<iteration:[480/657] - total_loss: 0.2899  obj_loss: 0.1002  noobj_loss: 0.0271  bbox_loss: 0.0268  cls_loss: 0.0422  \n",
      "<<<iteration:[500/657] - total_loss: 0.2876  obj_loss: 0.1003  noobj_loss: 0.0308  bbox_loss: 0.0253  cls_loss: 0.0456  \n",
      "<<<iteration:[520/657] - total_loss: 0.2684  obj_loss: 0.1091  noobj_loss: 0.0215  bbox_loss: 0.0221  cls_loss: 0.0379  \n",
      "<<<iteration:[540/657] - total_loss: 0.2866  obj_loss: 0.1218  noobj_loss: 0.0230  bbox_loss: 0.0217  cls_loss: 0.0446  \n",
      "<<<iteration:[560/657] - total_loss: 0.2896  obj_loss: 0.1210  noobj_loss: 0.0215  bbox_loss: 0.0251  cls_loss: 0.0325  \n",
      "<<<iteration:[580/657] - total_loss: 0.2783  obj_loss: 0.1073  noobj_loss: 0.0232  bbox_loss: 0.0241  cls_loss: 0.0388  \n",
      "<<<iteration:[600/657] - total_loss: 0.2809  obj_loss: 0.1140  noobj_loss: 0.0212  bbox_loss: 0.0228  cls_loss: 0.0421  \n",
      "<<<iteration:[620/657] - total_loss: 0.2599  obj_loss: 0.0982  noobj_loss: 0.0256  bbox_loss: 0.0224  cls_loss: 0.0369  \n",
      "<<<iteration:[640/657] - total_loss: 0.2594  obj_loss: 0.1130  noobj_loss: 0.0217  bbox_loss: 0.0199  cls_loss: 0.0360  \n",
      "\n",
      "epoch:18/100 - Train Loss: 0.2752, Val Loss: 0.3145\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2785  obj_loss: 0.1198  noobj_loss: 0.0254  bbox_loss: 0.0214  cls_loss: 0.0389  \n",
      "<<<iteration:[40/657] - total_loss: 0.2754  obj_loss: 0.1056  noobj_loss: 0.0236  bbox_loss: 0.0228  cls_loss: 0.0440  \n",
      "<<<iteration:[60/657] - total_loss: 0.2716  obj_loss: 0.1094  noobj_loss: 0.0257  bbox_loss: 0.0218  cls_loss: 0.0406  \n",
      "<<<iteration:[80/657] - total_loss: 0.2768  obj_loss: 0.1304  noobj_loss: 0.0229  bbox_loss: 0.0190  cls_loss: 0.0403  \n",
      "<<<iteration:[100/657] - total_loss: 0.2834  obj_loss: 0.1193  noobj_loss: 0.0249  bbox_loss: 0.0226  cls_loss: 0.0388  \n",
      "<<<iteration:[120/657] - total_loss: 0.2771  obj_loss: 0.1138  noobj_loss: 0.0234  bbox_loss: 0.0232  cls_loss: 0.0356  \n",
      "<<<iteration:[140/657] - total_loss: 0.2720  obj_loss: 0.1138  noobj_loss: 0.0291  bbox_loss: 0.0216  cls_loss: 0.0357  \n",
      "<<<iteration:[160/657] - total_loss: 0.2875  obj_loss: 0.1198  noobj_loss: 0.0296  bbox_loss: 0.0216  cls_loss: 0.0450  \n",
      "<<<iteration:[180/657] - total_loss: 0.2845  obj_loss: 0.1251  noobj_loss: 0.0225  bbox_loss: 0.0220  cls_loss: 0.0384  \n",
      "<<<iteration:[200/657] - total_loss: 0.2767  obj_loss: 0.0991  noobj_loss: 0.0334  bbox_loss: 0.0246  cls_loss: 0.0379  \n",
      "<<<iteration:[220/657] - total_loss: 0.2806  obj_loss: 0.1139  noobj_loss: 0.0319  bbox_loss: 0.0222  cls_loss: 0.0399  \n",
      "<<<iteration:[240/657] - total_loss: 0.2765  obj_loss: 0.1021  noobj_loss: 0.0292  bbox_loss: 0.0248  cls_loss: 0.0357  \n",
      "<<<iteration:[260/657] - total_loss: 0.2695  obj_loss: 0.1187  noobj_loss: 0.0273  bbox_loss: 0.0209  cls_loss: 0.0327  \n",
      "<<<iteration:[280/657] - total_loss: 0.2575  obj_loss: 0.0937  noobj_loss: 0.0319  bbox_loss: 0.0227  cls_loss: 0.0343  \n",
      "<<<iteration:[300/657] - total_loss: 0.2606  obj_loss: 0.1156  noobj_loss: 0.0270  bbox_loss: 0.0195  cls_loss: 0.0341  \n",
      "<<<iteration:[320/657] - total_loss: 0.2792  obj_loss: 0.1097  noobj_loss: 0.0298  bbox_loss: 0.0226  cls_loss: 0.0418  \n",
      "<<<iteration:[340/657] - total_loss: 0.2653  obj_loss: 0.1060  noobj_loss: 0.0302  bbox_loss: 0.0223  cls_loss: 0.0329  \n",
      "<<<iteration:[360/657] - total_loss: 0.2760  obj_loss: 0.1080  noobj_loss: 0.0276  bbox_loss: 0.0233  cls_loss: 0.0377  \n",
      "<<<iteration:[380/657] - total_loss: 0.2653  obj_loss: 0.1086  noobj_loss: 0.0311  bbox_loss: 0.0209  cls_loss: 0.0367  \n",
      "<<<iteration:[400/657] - total_loss: 0.2859  obj_loss: 0.1070  noobj_loss: 0.0299  bbox_loss: 0.0250  cls_loss: 0.0389  \n",
      "<<<iteration:[420/657] - total_loss: 0.2723  obj_loss: 0.1167  noobj_loss: 0.0334  bbox_loss: 0.0212  cls_loss: 0.0329  \n",
      "<<<iteration:[440/657] - total_loss: 0.2508  obj_loss: 0.1122  noobj_loss: 0.0332  bbox_loss: 0.0186  cls_loss: 0.0289  \n",
      "<<<iteration:[460/657] - total_loss: 0.2579  obj_loss: 0.1066  noobj_loss: 0.0280  bbox_loss: 0.0199  cls_loss: 0.0380  \n",
      "<<<iteration:[480/657] - total_loss: 0.2671  obj_loss: 0.1255  noobj_loss: 0.0269  bbox_loss: 0.0187  cls_loss: 0.0349  \n",
      "<<<iteration:[500/657] - total_loss: 0.2646  obj_loss: 0.1029  noobj_loss: 0.0374  bbox_loss: 0.0215  cls_loss: 0.0354  \n",
      "<<<iteration:[520/657] - total_loss: 0.2653  obj_loss: 0.1128  noobj_loss: 0.0290  bbox_loss: 0.0209  cls_loss: 0.0337  \n",
      "<<<iteration:[540/657] - total_loss: 0.2649  obj_loss: 0.1192  noobj_loss: 0.0322  bbox_loss: 0.0188  cls_loss: 0.0357  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[560/657] - total_loss: 0.2713  obj_loss: 0.1141  noobj_loss: 0.0322  bbox_loss: 0.0205  cls_loss: 0.0386  \n",
      "<<<iteration:[580/657] - total_loss: 0.2712  obj_loss: 0.1195  noobj_loss: 0.0306  bbox_loss: 0.0204  cls_loss: 0.0346  \n",
      "<<<iteration:[600/657] - total_loss: 0.2844  obj_loss: 0.1144  noobj_loss: 0.0383  bbox_loss: 0.0228  cls_loss: 0.0369  \n",
      "<<<iteration:[620/657] - total_loss: 0.2641  obj_loss: 0.1109  noobj_loss: 0.0308  bbox_loss: 0.0204  cls_loss: 0.0360  \n",
      "<<<iteration:[640/657] - total_loss: 0.3037  obj_loss: 0.1092  noobj_loss: 0.0304  bbox_loss: 0.0264  cls_loss: 0.0475  \n",
      "\n",
      "epoch:19/100 - Train Loss: 0.2733, Val Loss: 0.3082\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2803  obj_loss: 0.1297  noobj_loss: 0.0259  bbox_loss: 0.0187  cls_loss: 0.0442  \n",
      "<<<iteration:[40/657] - total_loss: 0.2703  obj_loss: 0.1261  noobj_loss: 0.0316  bbox_loss: 0.0196  cls_loss: 0.0305  \n",
      "<<<iteration:[60/657] - total_loss: 0.2701  obj_loss: 0.1035  noobj_loss: 0.0273  bbox_loss: 0.0233  cls_loss: 0.0367  \n",
      "<<<iteration:[80/657] - total_loss: 0.2560  obj_loss: 0.1118  noobj_loss: 0.0356  bbox_loss: 0.0190  cls_loss: 0.0313  \n",
      "<<<iteration:[100/657] - total_loss: 0.2705  obj_loss: 0.1232  noobj_loss: 0.0325  bbox_loss: 0.0190  cls_loss: 0.0357  \n",
      "<<<iteration:[120/657] - total_loss: 0.2639  obj_loss: 0.1020  noobj_loss: 0.0300  bbox_loss: 0.0218  cls_loss: 0.0378  \n",
      "<<<iteration:[140/657] - total_loss: 0.2717  obj_loss: 0.1236  noobj_loss: 0.0308  bbox_loss: 0.0195  cls_loss: 0.0349  \n",
      "<<<iteration:[160/657] - total_loss: 0.2545  obj_loss: 0.0982  noobj_loss: 0.0315  bbox_loss: 0.0212  cls_loss: 0.0348  \n",
      "<<<iteration:[180/657] - total_loss: 0.2734  obj_loss: 0.1108  noobj_loss: 0.0311  bbox_loss: 0.0226  cls_loss: 0.0342  \n",
      "<<<iteration:[200/657] - total_loss: 0.2740  obj_loss: 0.1089  noobj_loss: 0.0386  bbox_loss: 0.0222  cls_loss: 0.0347  \n",
      "<<<iteration:[220/657] - total_loss: 0.2798  obj_loss: 0.1220  noobj_loss: 0.0325  bbox_loss: 0.0204  cls_loss: 0.0396  \n",
      "<<<iteration:[240/657] - total_loss: 0.2852  obj_loss: 0.1198  noobj_loss: 0.0298  bbox_loss: 0.0210  cls_loss: 0.0457  \n",
      "<<<iteration:[260/657] - total_loss: 0.2544  obj_loss: 0.1209  noobj_loss: 0.0300  bbox_loss: 0.0177  cls_loss: 0.0300  \n",
      "<<<iteration:[280/657] - total_loss: 0.2635  obj_loss: 0.1175  noobj_loss: 0.0283  bbox_loss: 0.0195  cls_loss: 0.0344  \n",
      "<<<iteration:[300/657] - total_loss: 0.2733  obj_loss: 0.1024  noobj_loss: 0.0393  bbox_loss: 0.0223  cls_loss: 0.0400  \n",
      "<<<iteration:[320/657] - total_loss: 0.2796  obj_loss: 0.1314  noobj_loss: 0.0305  bbox_loss: 0.0187  cls_loss: 0.0393  \n",
      "<<<iteration:[340/657] - total_loss: 0.2934  obj_loss: 0.1227  noobj_loss: 0.0263  bbox_loss: 0.0235  cls_loss: 0.0401  \n",
      "<<<iteration:[360/657] - total_loss: 0.2630  obj_loss: 0.1162  noobj_loss: 0.0268  bbox_loss: 0.0202  cls_loss: 0.0326  \n",
      "<<<iteration:[380/657] - total_loss: 0.2632  obj_loss: 0.1080  noobj_loss: 0.0338  bbox_loss: 0.0210  cls_loss: 0.0331  \n",
      "<<<iteration:[400/657] - total_loss: 0.2678  obj_loss: 0.1124  noobj_loss: 0.0274  bbox_loss: 0.0216  cls_loss: 0.0338  \n",
      "<<<iteration:[420/657] - total_loss: 0.2578  obj_loss: 0.1038  noobj_loss: 0.0289  bbox_loss: 0.0211  cls_loss: 0.0338  \n",
      "<<<iteration:[440/657] - total_loss: 0.2615  obj_loss: 0.1155  noobj_loss: 0.0294  bbox_loss: 0.0197  cls_loss: 0.0328  \n",
      "<<<iteration:[460/657] - total_loss: 0.2615  obj_loss: 0.1101  noobj_loss: 0.0320  bbox_loss: 0.0211  cls_loss: 0.0300  \n",
      "<<<iteration:[480/657] - total_loss: 0.2719  obj_loss: 0.1061  noobj_loss: 0.0349  bbox_loss: 0.0209  cls_loss: 0.0436  \n",
      "<<<iteration:[500/657] - total_loss: 0.2727  obj_loss: 0.1098  noobj_loss: 0.0300  bbox_loss: 0.0214  cls_loss: 0.0409  \n",
      "<<<iteration:[520/657] - total_loss: 0.2599  obj_loss: 0.1139  noobj_loss: 0.0300  bbox_loss: 0.0195  cls_loss: 0.0336  \n",
      "<<<iteration:[540/657] - total_loss: 0.2713  obj_loss: 0.1267  noobj_loss: 0.0334  bbox_loss: 0.0184  cls_loss: 0.0357  \n",
      "<<<iteration:[560/657] - total_loss: 0.2631  obj_loss: 0.1052  noobj_loss: 0.0336  bbox_loss: 0.0207  cls_loss: 0.0378  \n",
      "<<<iteration:[580/657] - total_loss: 0.2701  obj_loss: 0.1031  noobj_loss: 0.0404  bbox_loss: 0.0215  cls_loss: 0.0391  \n",
      "<<<iteration:[600/657] - total_loss: 0.2851  obj_loss: 0.1250  noobj_loss: 0.0403  bbox_loss: 0.0214  cls_loss: 0.0331  \n",
      "<<<iteration:[620/657] - total_loss: 0.2756  obj_loss: 0.1211  noobj_loss: 0.0315  bbox_loss: 0.0193  cls_loss: 0.0424  \n",
      "<<<iteration:[640/657] - total_loss: 0.2485  obj_loss: 0.1161  noobj_loss: 0.0314  bbox_loss: 0.0182  cls_loss: 0.0255  \n",
      "\n",
      "epoch:20/100 - Train Loss: 0.2729, Val Loss: 0.3655\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2979  obj_loss: 0.1121  noobj_loss: 0.0395  bbox_loss: 0.0234  cls_loss: 0.0490  \n",
      "<<<iteration:[40/657] - total_loss: 0.2785  obj_loss: 0.1132  noobj_loss: 0.0389  bbox_loss: 0.0217  cls_loss: 0.0374  \n",
      "<<<iteration:[60/657] - total_loss: 0.2612  obj_loss: 0.1029  noobj_loss: 0.0454  bbox_loss: 0.0192  cls_loss: 0.0395  \n",
      "<<<iteration:[80/657] - total_loss: 0.2902  obj_loss: 0.1269  noobj_loss: 0.0399  bbox_loss: 0.0219  cls_loss: 0.0340  \n",
      "<<<iteration:[100/657] - total_loss: 0.2587  obj_loss: 0.1029  noobj_loss: 0.0321  bbox_loss: 0.0212  cls_loss: 0.0335  \n",
      "<<<iteration:[120/657] - total_loss: 0.2619  obj_loss: 0.1038  noobj_loss: 0.0371  bbox_loss: 0.0207  cls_loss: 0.0361  \n",
      "<<<iteration:[140/657] - total_loss: 0.2774  obj_loss: 0.1128  noobj_loss: 0.0384  bbox_loss: 0.0206  cls_loss: 0.0426  \n",
      "<<<iteration:[160/657] - total_loss: 0.2719  obj_loss: 0.1137  noobj_loss: 0.0400  bbox_loss: 0.0198  cls_loss: 0.0392  \n",
      "<<<iteration:[180/657] - total_loss: 0.2752  obj_loss: 0.1164  noobj_loss: 0.0373  bbox_loss: 0.0209  cls_loss: 0.0356  \n",
      "<<<iteration:[200/657] - total_loss: 0.2728  obj_loss: 0.1145  noobj_loss: 0.0290  bbox_loss: 0.0221  cls_loss: 0.0332  \n",
      "<<<iteration:[220/657] - total_loss: 0.2594  obj_loss: 0.1153  noobj_loss: 0.0342  bbox_loss: 0.0193  cls_loss: 0.0305  \n",
      "<<<iteration:[240/657] - total_loss: 0.2662  obj_loss: 0.1075  noobj_loss: 0.0282  bbox_loss: 0.0214  cls_loss: 0.0374  \n",
      "<<<iteration:[260/657] - total_loss: 0.2578  obj_loss: 0.1108  noobj_loss: 0.0358  bbox_loss: 0.0199  cls_loss: 0.0294  \n",
      "<<<iteration:[280/657] - total_loss: 0.2849  obj_loss: 0.1157  noobj_loss: 0.0385  bbox_loss: 0.0233  cls_loss: 0.0335  \n",
      "<<<iteration:[300/657] - total_loss: 0.2627  obj_loss: 0.1150  noobj_loss: 0.0373  bbox_loss: 0.0194  cls_loss: 0.0318  \n",
      "<<<iteration:[320/657] - total_loss: 0.2583  obj_loss: 0.1029  noobj_loss: 0.0428  bbox_loss: 0.0197  cls_loss: 0.0355  \n",
      "<<<iteration:[340/657] - total_loss: 0.2795  obj_loss: 0.1279  noobj_loss: 0.0352  bbox_loss: 0.0208  cls_loss: 0.0298  \n",
      "<<<iteration:[360/657] - total_loss: 0.2613  obj_loss: 0.1225  noobj_loss: 0.0337  bbox_loss: 0.0183  cls_loss: 0.0305  \n",
      "<<<iteration:[380/657] - total_loss: 0.2886  obj_loss: 0.1216  noobj_loss: 0.0429  bbox_loss: 0.0220  cls_loss: 0.0353  \n",
      "<<<iteration:[400/657] - total_loss: 0.2608  obj_loss: 0.1133  noobj_loss: 0.0345  bbox_loss: 0.0191  cls_loss: 0.0349  \n",
      "<<<iteration:[420/657] - total_loss: 0.2881  obj_loss: 0.1225  noobj_loss: 0.0323  bbox_loss: 0.0222  cls_loss: 0.0386  \n",
      "<<<iteration:[440/657] - total_loss: 0.2599  obj_loss: 0.1064  noobj_loss: 0.0343  bbox_loss: 0.0196  cls_loss: 0.0385  \n",
      "<<<iteration:[460/657] - total_loss: 0.2622  obj_loss: 0.1216  noobj_loss: 0.0325  bbox_loss: 0.0181  cls_loss: 0.0340  \n",
      "<<<iteration:[480/657] - total_loss: 0.2663  obj_loss: 0.1159  noobj_loss: 0.0333  bbox_loss: 0.0201  cls_loss: 0.0333  \n",
      "<<<iteration:[500/657] - total_loss: 0.2667  obj_loss: 0.1136  noobj_loss: 0.0408  bbox_loss: 0.0202  cls_loss: 0.0318  \n",
      "<<<iteration:[520/657] - total_loss: 0.2742  obj_loss: 0.1127  noobj_loss: 0.0410  bbox_loss: 0.0210  cls_loss: 0.0362  \n",
      "<<<iteration:[540/657] - total_loss: 0.2811  obj_loss: 0.1232  noobj_loss: 0.0443  bbox_loss: 0.0203  cls_loss: 0.0343  \n",
      "<<<iteration:[560/657] - total_loss: 0.2632  obj_loss: 0.1139  noobj_loss: 0.0277  bbox_loss: 0.0195  cls_loss: 0.0378  \n",
      "<<<iteration:[580/657] - total_loss: 0.2611  obj_loss: 0.1084  noobj_loss: 0.0295  bbox_loss: 0.0217  cls_loss: 0.0293  \n",
      "<<<iteration:[600/657] - total_loss: 0.2547  obj_loss: 0.1030  noobj_loss: 0.0383  bbox_loss: 0.0204  cls_loss: 0.0307  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[620/657] - total_loss: 0.2616  obj_loss: 0.1144  noobj_loss: 0.0327  bbox_loss: 0.0193  cls_loss: 0.0345  \n",
      "<<<iteration:[640/657] - total_loss: 0.2753  obj_loss: 0.1074  noobj_loss: 0.0314  bbox_loss: 0.0228  cls_loss: 0.0384  \n",
      "\n",
      "epoch:21/100 - Train Loss: 0.2697, Val Loss: 0.3007\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2869  obj_loss: 0.1298  noobj_loss: 0.0339  bbox_loss: 0.0212  cls_loss: 0.0342  \n",
      "<<<iteration:[40/657] - total_loss: 0.2586  obj_loss: 0.1188  noobj_loss: 0.0367  bbox_loss: 0.0179  cls_loss: 0.0320  \n",
      "<<<iteration:[60/657] - total_loss: 0.2584  obj_loss: 0.1016  noobj_loss: 0.0345  bbox_loss: 0.0219  cls_loss: 0.0303  \n",
      "<<<iteration:[80/657] - total_loss: 0.2488  obj_loss: 0.1159  noobj_loss: 0.0323  bbox_loss: 0.0177  cls_loss: 0.0282  \n",
      "<<<iteration:[100/657] - total_loss: 0.2559  obj_loss: 0.1095  noobj_loss: 0.0377  bbox_loss: 0.0186  cls_loss: 0.0343  \n",
      "<<<iteration:[120/657] - total_loss: 0.2708  obj_loss: 0.1220  noobj_loss: 0.0350  bbox_loss: 0.0196  cls_loss: 0.0332  \n",
      "<<<iteration:[140/657] - total_loss: 0.2581  obj_loss: 0.1070  noobj_loss: 0.0358  bbox_loss: 0.0199  cls_loss: 0.0338  \n",
      "<<<iteration:[160/657] - total_loss: 0.2711  obj_loss: 0.1192  noobj_loss: 0.0350  bbox_loss: 0.0200  cls_loss: 0.0344  \n",
      "<<<iteration:[180/657] - total_loss: 0.2674  obj_loss: 0.1150  noobj_loss: 0.0316  bbox_loss: 0.0204  cls_loss: 0.0345  \n",
      "<<<iteration:[200/657] - total_loss: 0.2516  obj_loss: 0.1152  noobj_loss: 0.0329  bbox_loss: 0.0180  cls_loss: 0.0301  \n",
      "<<<iteration:[220/657] - total_loss: 0.2788  obj_loss: 0.1211  noobj_loss: 0.0323  bbox_loss: 0.0219  cls_loss: 0.0320  \n",
      "<<<iteration:[240/657] - total_loss: 0.2753  obj_loss: 0.1110  noobj_loss: 0.0457  bbox_loss: 0.0208  cls_loss: 0.0376  \n",
      "<<<iteration:[260/657] - total_loss: 0.2717  obj_loss: 0.1109  noobj_loss: 0.0391  bbox_loss: 0.0214  cls_loss: 0.0340  \n",
      "<<<iteration:[280/657] - total_loss: 0.2659  obj_loss: 0.1115  noobj_loss: 0.0278  bbox_loss: 0.0217  cls_loss: 0.0320  \n",
      "<<<iteration:[300/657] - total_loss: 0.2714  obj_loss: 0.1134  noobj_loss: 0.0397  bbox_loss: 0.0225  cls_loss: 0.0255  \n",
      "<<<iteration:[320/657] - total_loss: 0.2804  obj_loss: 0.1240  noobj_loss: 0.0407  bbox_loss: 0.0205  cls_loss: 0.0334  \n",
      "<<<iteration:[340/657] - total_loss: 0.2609  obj_loss: 0.1307  noobj_loss: 0.0359  bbox_loss: 0.0159  cls_loss: 0.0328  \n",
      "<<<iteration:[360/657] - total_loss: 0.2647  obj_loss: 0.1239  noobj_loss: 0.0345  bbox_loss: 0.0184  cls_loss: 0.0315  \n",
      "<<<iteration:[380/657] - total_loss: 0.2683  obj_loss: 0.1224  noobj_loss: 0.0388  bbox_loss: 0.0190  cls_loss: 0.0314  \n",
      "<<<iteration:[400/657] - total_loss: 0.2583  obj_loss: 0.1076  noobj_loss: 0.0327  bbox_loss: 0.0200  cls_loss: 0.0342  \n",
      "<<<iteration:[420/657] - total_loss: 0.2485  obj_loss: 0.1107  noobj_loss: 0.0397  bbox_loss: 0.0181  cls_loss: 0.0275  \n",
      "<<<iteration:[440/657] - total_loss: 0.2473  obj_loss: 0.1069  noobj_loss: 0.0375  bbox_loss: 0.0177  cls_loss: 0.0334  \n",
      "<<<iteration:[460/657] - total_loss: 0.2547  obj_loss: 0.1095  noobj_loss: 0.0329  bbox_loss: 0.0197  cls_loss: 0.0305  \n",
      "<<<iteration:[480/657] - total_loss: 0.2733  obj_loss: 0.1233  noobj_loss: 0.0384  bbox_loss: 0.0184  cls_loss: 0.0387  \n",
      "<<<iteration:[500/657] - total_loss: 0.2618  obj_loss: 0.1332  noobj_loss: 0.0345  bbox_loss: 0.0161  cls_loss: 0.0310  \n",
      "<<<iteration:[520/657] - total_loss: 0.2663  obj_loss: 0.1145  noobj_loss: 0.0372  bbox_loss: 0.0190  cls_loss: 0.0383  \n",
      "<<<iteration:[540/657] - total_loss: 0.2655  obj_loss: 0.1121  noobj_loss: 0.0414  bbox_loss: 0.0196  cls_loss: 0.0347  \n",
      "<<<iteration:[560/657] - total_loss: 0.2678  obj_loss: 0.1139  noobj_loss: 0.0415  bbox_loss: 0.0190  cls_loss: 0.0382  \n",
      "<<<iteration:[580/657] - total_loss: 0.2605  obj_loss: 0.1321  noobj_loss: 0.0327  bbox_loss: 0.0174  cls_loss: 0.0249  \n",
      "<<<iteration:[600/657] - total_loss: 0.2644  obj_loss: 0.1115  noobj_loss: 0.0365  bbox_loss: 0.0215  cls_loss: 0.0272  \n",
      "<<<iteration:[620/657] - total_loss: 0.2782  obj_loss: 0.1053  noobj_loss: 0.0482  bbox_loss: 0.0220  cls_loss: 0.0387  \n",
      "<<<iteration:[640/657] - total_loss: 0.2473  obj_loss: 0.1047  noobj_loss: 0.0335  bbox_loss: 0.0180  cls_loss: 0.0361  \n",
      "\n",
      "epoch:22/100 - Train Loss: 0.2639, Val Loss: 0.3006\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2606  obj_loss: 0.1236  noobj_loss: 0.0358  bbox_loss: 0.0189  cls_loss: 0.0246  \n",
      "<<<iteration:[40/657] - total_loss: 0.2604  obj_loss: 0.1222  noobj_loss: 0.0375  bbox_loss: 0.0176  cls_loss: 0.0315  \n",
      "<<<iteration:[60/657] - total_loss: 0.2550  obj_loss: 0.1198  noobj_loss: 0.0350  bbox_loss: 0.0178  cls_loss: 0.0290  \n",
      "<<<iteration:[80/657] - total_loss: 0.2669  obj_loss: 0.1219  noobj_loss: 0.0372  bbox_loss: 0.0188  cls_loss: 0.0325  \n",
      "<<<iteration:[100/657] - total_loss: 0.2702  obj_loss: 0.1250  noobj_loss: 0.0456  bbox_loss: 0.0189  cls_loss: 0.0280  \n",
      "<<<iteration:[120/657] - total_loss: 0.2650  obj_loss: 0.1180  noobj_loss: 0.0380  bbox_loss: 0.0193  cls_loss: 0.0314  \n",
      "<<<iteration:[140/657] - total_loss: 0.2609  obj_loss: 0.1216  noobj_loss: 0.0408  bbox_loss: 0.0180  cls_loss: 0.0288  \n",
      "<<<iteration:[160/657] - total_loss: 0.2720  obj_loss: 0.1083  noobj_loss: 0.0390  bbox_loss: 0.0226  cls_loss: 0.0314  \n",
      "<<<iteration:[180/657] - total_loss: 0.2577  obj_loss: 0.1166  noobj_loss: 0.0366  bbox_loss: 0.0180  cls_loss: 0.0328  \n",
      "<<<iteration:[200/657] - total_loss: 0.2551  obj_loss: 0.1016  noobj_loss: 0.0390  bbox_loss: 0.0197  cls_loss: 0.0357  \n",
      "<<<iteration:[220/657] - total_loss: 0.2630  obj_loss: 0.1036  noobj_loss: 0.0391  bbox_loss: 0.0200  cls_loss: 0.0399  \n",
      "<<<iteration:[240/657] - total_loss: 0.2610  obj_loss: 0.1151  noobj_loss: 0.0445  bbox_loss: 0.0186  cls_loss: 0.0308  \n",
      "<<<iteration:[260/657] - total_loss: 0.2519  obj_loss: 0.1109  noobj_loss: 0.0368  bbox_loss: 0.0188  cls_loss: 0.0288  \n",
      "<<<iteration:[280/657] - total_loss: 0.2656  obj_loss: 0.1281  noobj_loss: 0.0328  bbox_loss: 0.0179  cls_loss: 0.0315  \n",
      "<<<iteration:[300/657] - total_loss: 0.2383  obj_loss: 0.1058  noobj_loss: 0.0381  bbox_loss: 0.0173  cls_loss: 0.0270  \n",
      "<<<iteration:[320/657] - total_loss: 0.2698  obj_loss: 0.1182  noobj_loss: 0.0379  bbox_loss: 0.0190  cls_loss: 0.0375  \n",
      "<<<iteration:[340/657] - total_loss: 0.2507  obj_loss: 0.1033  noobj_loss: 0.0406  bbox_loss: 0.0185  cls_loss: 0.0348  \n",
      "<<<iteration:[360/657] - total_loss: 0.2471  obj_loss: 0.1129  noobj_loss: 0.0288  bbox_loss: 0.0181  cls_loss: 0.0294  \n",
      "<<<iteration:[380/657] - total_loss: 0.2714  obj_loss: 0.1127  noobj_loss: 0.0417  bbox_loss: 0.0215  cls_loss: 0.0306  \n",
      "<<<iteration:[400/657] - total_loss: 0.2438  obj_loss: 0.1177  noobj_loss: 0.0376  bbox_loss: 0.0154  cls_loss: 0.0303  \n",
      "<<<iteration:[420/657] - total_loss: 0.2801  obj_loss: 0.1079  noobj_loss: 0.0456  bbox_loss: 0.0222  cls_loss: 0.0382  \n",
      "<<<iteration:[440/657] - total_loss: 0.2555  obj_loss: 0.1285  noobj_loss: 0.0374  bbox_loss: 0.0163  cls_loss: 0.0267  \n",
      "<<<iteration:[460/657] - total_loss: 0.2561  obj_loss: 0.1150  noobj_loss: 0.0434  bbox_loss: 0.0179  cls_loss: 0.0298  \n",
      "<<<iteration:[480/657] - total_loss: 0.2632  obj_loss: 0.1211  noobj_loss: 0.0422  bbox_loss: 0.0177  cls_loss: 0.0324  \n",
      "<<<iteration:[500/657] - total_loss: 0.2618  obj_loss: 0.1302  noobj_loss: 0.0334  bbox_loss: 0.0175  cls_loss: 0.0274  \n",
      "<<<iteration:[520/657] - total_loss: 0.2557  obj_loss: 0.1145  noobj_loss: 0.0415  bbox_loss: 0.0165  cls_loss: 0.0378  \n",
      "<<<iteration:[540/657] - total_loss: 0.2732  obj_loss: 0.1227  noobj_loss: 0.0458  bbox_loss: 0.0200  cls_loss: 0.0274  \n",
      "<<<iteration:[560/657] - total_loss: 0.2531  obj_loss: 0.1128  noobj_loss: 0.0431  bbox_loss: 0.0179  cls_loss: 0.0292  \n",
      "<<<iteration:[580/657] - total_loss: 0.2744  obj_loss: 0.1246  noobj_loss: 0.0368  bbox_loss: 0.0201  cls_loss: 0.0307  \n",
      "<<<iteration:[600/657] - total_loss: 0.2661  obj_loss: 0.1112  noobj_loss: 0.0432  bbox_loss: 0.0206  cls_loss: 0.0301  \n",
      "<<<iteration:[620/657] - total_loss: 0.2628  obj_loss: 0.1211  noobj_loss: 0.0428  bbox_loss: 0.0181  cls_loss: 0.0299  \n",
      "<<<iteration:[640/657] - total_loss: 0.2652  obj_loss: 0.1244  noobj_loss: 0.0324  bbox_loss: 0.0178  cls_loss: 0.0355  \n",
      "\n",
      "epoch:23/100 - Train Loss: 0.2609, Val Loss: 0.2961\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2751  obj_loss: 0.1332  noobj_loss: 0.0331  bbox_loss: 0.0192  cls_loss: 0.0291  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[40/657] - total_loss: 0.2614  obj_loss: 0.1182  noobj_loss: 0.0419  bbox_loss: 0.0176  cls_loss: 0.0342  \n",
      "<<<iteration:[60/657] - total_loss: 0.2557  obj_loss: 0.1116  noobj_loss: 0.0417  bbox_loss: 0.0192  cls_loss: 0.0272  \n",
      "<<<iteration:[80/657] - total_loss: 0.2549  obj_loss: 0.1210  noobj_loss: 0.0388  bbox_loss: 0.0172  cls_loss: 0.0287  \n",
      "<<<iteration:[100/657] - total_loss: 0.2676  obj_loss: 0.1134  noobj_loss: 0.0358  bbox_loss: 0.0196  cls_loss: 0.0380  \n",
      "<<<iteration:[120/657] - total_loss: 0.2551  obj_loss: 0.1147  noobj_loss: 0.0418  bbox_loss: 0.0175  cls_loss: 0.0322  \n",
      "<<<iteration:[140/657] - total_loss: 0.2516  obj_loss: 0.1106  noobj_loss: 0.0420  bbox_loss: 0.0179  cls_loss: 0.0307  \n",
      "<<<iteration:[160/657] - total_loss: 0.2534  obj_loss: 0.1167  noobj_loss: 0.0414  bbox_loss: 0.0176  cls_loss: 0.0280  \n",
      "<<<iteration:[180/657] - total_loss: 0.2685  obj_loss: 0.1268  noobj_loss: 0.0449  bbox_loss: 0.0183  cls_loss: 0.0278  \n",
      "<<<iteration:[200/657] - total_loss: 0.2726  obj_loss: 0.1151  noobj_loss: 0.0419  bbox_loss: 0.0203  cls_loss: 0.0352  \n",
      "<<<iteration:[220/657] - total_loss: 0.2692  obj_loss: 0.1103  noobj_loss: 0.0481  bbox_loss: 0.0202  cls_loss: 0.0340  \n",
      "<<<iteration:[240/657] - total_loss: 0.2544  obj_loss: 0.1256  noobj_loss: 0.0472  bbox_loss: 0.0159  cls_loss: 0.0256  \n",
      "<<<iteration:[260/657] - total_loss: 0.2565  obj_loss: 0.1092  noobj_loss: 0.0427  bbox_loss: 0.0187  cls_loss: 0.0324  \n",
      "<<<iteration:[280/657] - total_loss: 0.2601  obj_loss: 0.1165  noobj_loss: 0.0453  bbox_loss: 0.0181  cls_loss: 0.0306  \n",
      "<<<iteration:[300/657] - total_loss: 0.2447  obj_loss: 0.1090  noobj_loss: 0.0475  bbox_loss: 0.0164  cls_loss: 0.0300  \n",
      "<<<iteration:[320/657] - total_loss: 0.2532  obj_loss: 0.1129  noobj_loss: 0.0429  bbox_loss: 0.0178  cls_loss: 0.0297  \n",
      "<<<iteration:[340/657] - total_loss: 0.2532  obj_loss: 0.1076  noobj_loss: 0.0470  bbox_loss: 0.0192  cls_loss: 0.0263  \n",
      "<<<iteration:[360/657] - total_loss: 0.2537  obj_loss: 0.1154  noobj_loss: 0.0382  bbox_loss: 0.0179  cls_loss: 0.0300  \n",
      "<<<iteration:[380/657] - total_loss: 0.2606  obj_loss: 0.1110  noobj_loss: 0.0414  bbox_loss: 0.0196  cls_loss: 0.0310  \n",
      "<<<iteration:[400/657] - total_loss: 0.2461  obj_loss: 0.1041  noobj_loss: 0.0351  bbox_loss: 0.0185  cls_loss: 0.0321  \n",
      "<<<iteration:[420/657] - total_loss: 0.2715  obj_loss: 0.1253  noobj_loss: 0.0377  bbox_loss: 0.0192  cls_loss: 0.0313  \n",
      "<<<iteration:[440/657] - total_loss: 0.2657  obj_loss: 0.1138  noobj_loss: 0.0432  bbox_loss: 0.0191  cls_loss: 0.0350  \n",
      "<<<iteration:[460/657] - total_loss: 0.2529  obj_loss: 0.1163  noobj_loss: 0.0401  bbox_loss: 0.0173  cls_loss: 0.0300  \n",
      "<<<iteration:[480/657] - total_loss: 0.2650  obj_loss: 0.1121  noobj_loss: 0.0377  bbox_loss: 0.0210  cls_loss: 0.0293  \n",
      "<<<iteration:[500/657] - total_loss: 0.2666  obj_loss: 0.1167  noobj_loss: 0.0446  bbox_loss: 0.0201  cls_loss: 0.0273  \n",
      "<<<iteration:[520/657] - total_loss: 0.2471  obj_loss: 0.1095  noobj_loss: 0.0364  bbox_loss: 0.0173  cls_loss: 0.0330  \n",
      "<<<iteration:[540/657] - total_loss: 0.2537  obj_loss: 0.1153  noobj_loss: 0.0446  bbox_loss: 0.0173  cls_loss: 0.0297  \n",
      "<<<iteration:[560/657] - total_loss: 0.2612  obj_loss: 0.1168  noobj_loss: 0.0473  bbox_loss: 0.0185  cls_loss: 0.0285  \n",
      "<<<iteration:[580/657] - total_loss: 0.2513  obj_loss: 0.1097  noobj_loss: 0.0439  bbox_loss: 0.0177  cls_loss: 0.0313  \n",
      "<<<iteration:[600/657] - total_loss: 0.2497  obj_loss: 0.1214  noobj_loss: 0.0484  bbox_loss: 0.0161  cls_loss: 0.0236  \n",
      "<<<iteration:[620/657] - total_loss: 0.2396  obj_loss: 0.1261  noobj_loss: 0.0396  bbox_loss: 0.0147  cls_loss: 0.0204  \n",
      "<<<iteration:[640/657] - total_loss: 0.2407  obj_loss: 0.1131  noobj_loss: 0.0417  bbox_loss: 0.0162  cls_loss: 0.0260  \n",
      "\n",
      "epoch:24/100 - Train Loss: 0.2567, Val Loss: 0.2939\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2573  obj_loss: 0.1048  noobj_loss: 0.0480  bbox_loss: 0.0194  cls_loss: 0.0314  \n",
      "<<<iteration:[40/657] - total_loss: 0.2719  obj_loss: 0.1362  noobj_loss: 0.0376  bbox_loss: 0.0168  cls_loss: 0.0329  \n",
      "<<<iteration:[60/657] - total_loss: 0.2537  obj_loss: 0.1084  noobj_loss: 0.0405  bbox_loss: 0.0193  cls_loss: 0.0284  \n",
      "<<<iteration:[80/657] - total_loss: 0.2466  obj_loss: 0.1114  noobj_loss: 0.0425  bbox_loss: 0.0171  cls_loss: 0.0284  \n",
      "<<<iteration:[100/657] - total_loss: 0.2700  obj_loss: 0.1145  noobj_loss: 0.0410  bbox_loss: 0.0205  cls_loss: 0.0325  \n",
      "<<<iteration:[120/657] - total_loss: 0.2646  obj_loss: 0.1185  noobj_loss: 0.0405  bbox_loss: 0.0180  cls_loss: 0.0358  \n",
      "<<<iteration:[140/657] - total_loss: 0.2571  obj_loss: 0.1158  noobj_loss: 0.0535  bbox_loss: 0.0175  cls_loss: 0.0269  \n",
      "<<<iteration:[160/657] - total_loss: 0.2533  obj_loss: 0.1142  noobj_loss: 0.0451  bbox_loss: 0.0171  cls_loss: 0.0310  \n",
      "<<<iteration:[180/657] - total_loss: 0.2649  obj_loss: 0.1166  noobj_loss: 0.0492  bbox_loss: 0.0196  cls_loss: 0.0256  \n",
      "<<<iteration:[200/657] - total_loss: 0.2440  obj_loss: 0.1055  noobj_loss: 0.0381  bbox_loss: 0.0181  cls_loss: 0.0290  \n",
      "<<<iteration:[220/657] - total_loss: 0.2486  obj_loss: 0.1213  noobj_loss: 0.0447  bbox_loss: 0.0157  cls_loss: 0.0263  \n",
      "<<<iteration:[240/657] - total_loss: 0.2503  obj_loss: 0.1145  noobj_loss: 0.0439  bbox_loss: 0.0168  cls_loss: 0.0298  \n",
      "<<<iteration:[260/657] - total_loss: 0.2430  obj_loss: 0.1101  noobj_loss: 0.0467  bbox_loss: 0.0173  cls_loss: 0.0230  \n",
      "<<<iteration:[280/657] - total_loss: 0.2523  obj_loss: 0.1137  noobj_loss: 0.0449  bbox_loss: 0.0167  cls_loss: 0.0329  \n",
      "<<<iteration:[300/657] - total_loss: 0.2578  obj_loss: 0.1176  noobj_loss: 0.0460  bbox_loss: 0.0176  cls_loss: 0.0293  \n",
      "<<<iteration:[320/657] - total_loss: 0.2407  obj_loss: 0.1213  noobj_loss: 0.0425  bbox_loss: 0.0144  cls_loss: 0.0264  \n",
      "<<<iteration:[340/657] - total_loss: 0.2560  obj_loss: 0.1201  noobj_loss: 0.0430  bbox_loss: 0.0173  cls_loss: 0.0281  \n",
      "<<<iteration:[360/657] - total_loss: 0.2430  obj_loss: 0.1055  noobj_loss: 0.0476  bbox_loss: 0.0175  cls_loss: 0.0262  \n",
      "<<<iteration:[380/657] - total_loss: 0.2539  obj_loss: 0.1154  noobj_loss: 0.0392  bbox_loss: 0.0184  cls_loss: 0.0269  \n",
      "<<<iteration:[400/657] - total_loss: 0.2452  obj_loss: 0.1065  noobj_loss: 0.0422  bbox_loss: 0.0177  cls_loss: 0.0288  \n",
      "<<<iteration:[420/657] - total_loss: 0.2461  obj_loss: 0.1114  noobj_loss: 0.0412  bbox_loss: 0.0173  cls_loss: 0.0275  \n",
      "<<<iteration:[440/657] - total_loss: 0.2438  obj_loss: 0.1095  noobj_loss: 0.0367  bbox_loss: 0.0167  cls_loss: 0.0327  \n",
      "<<<iteration:[460/657] - total_loss: 0.2452  obj_loss: 0.1198  noobj_loss: 0.0453  bbox_loss: 0.0161  cls_loss: 0.0222  \n",
      "<<<iteration:[480/657] - total_loss: 0.2405  obj_loss: 0.1182  noobj_loss: 0.0429  bbox_loss: 0.0152  cls_loss: 0.0249  \n",
      "<<<iteration:[500/657] - total_loss: 0.2674  obj_loss: 0.1158  noobj_loss: 0.0444  bbox_loss: 0.0189  cls_loss: 0.0349  \n",
      "<<<iteration:[520/657] - total_loss: 0.2543  obj_loss: 0.1105  noobj_loss: 0.0436  bbox_loss: 0.0180  cls_loss: 0.0319  \n",
      "<<<iteration:[540/657] - total_loss: 0.2461  obj_loss: 0.1160  noobj_loss: 0.0408  bbox_loss: 0.0168  cls_loss: 0.0260  \n",
      "<<<iteration:[560/657] - total_loss: 0.2500  obj_loss: 0.1234  noobj_loss: 0.0441  bbox_loss: 0.0149  cls_loss: 0.0302  \n",
      "<<<iteration:[580/657] - total_loss: 0.2528  obj_loss: 0.1237  noobj_loss: 0.0394  bbox_loss: 0.0168  cls_loss: 0.0254  \n",
      "<<<iteration:[600/657] - total_loss: 0.2472  obj_loss: 0.1165  noobj_loss: 0.0427  bbox_loss: 0.0173  cls_loss: 0.0230  \n",
      "<<<iteration:[620/657] - total_loss: 0.2422  obj_loss: 0.1163  noobj_loss: 0.0449  bbox_loss: 0.0158  cls_loss: 0.0242  \n",
      "<<<iteration:[640/657] - total_loss: 0.2500  obj_loss: 0.1204  noobj_loss: 0.0446  bbox_loss: 0.0160  cls_loss: 0.0275  \n",
      "\n",
      "epoch:25/100 - Train Loss: 0.2516, Val Loss: 0.2969\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2550  obj_loss: 0.1220  noobj_loss: 0.0483  bbox_loss: 0.0156  cls_loss: 0.0309  \n",
      "<<<iteration:[40/657] - total_loss: 0.2541  obj_loss: 0.1253  noobj_loss: 0.0462  bbox_loss: 0.0160  cls_loss: 0.0258  \n",
      "<<<iteration:[60/657] - total_loss: 0.2483  obj_loss: 0.1135  noobj_loss: 0.0472  bbox_loss: 0.0165  cls_loss: 0.0285  \n",
      "<<<iteration:[80/657] - total_loss: 0.2318  obj_loss: 0.1045  noobj_loss: 0.0476  bbox_loss: 0.0158  cls_loss: 0.0244  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[100/657] - total_loss: 0.2410  obj_loss: 0.1107  noobj_loss: 0.0467  bbox_loss: 0.0166  cls_loss: 0.0237  \n",
      "<<<iteration:[120/657] - total_loss: 0.2581  obj_loss: 0.1350  noobj_loss: 0.0426  bbox_loss: 0.0144  cls_loss: 0.0298  \n",
      "<<<iteration:[140/657] - total_loss: 0.2546  obj_loss: 0.1251  noobj_loss: 0.0423  bbox_loss: 0.0154  cls_loss: 0.0312  \n",
      "<<<iteration:[160/657] - total_loss: 0.2322  obj_loss: 0.1185  noobj_loss: 0.0445  bbox_loss: 0.0144  cls_loss: 0.0197  \n",
      "<<<iteration:[180/657] - total_loss: 0.2447  obj_loss: 0.1079  noobj_loss: 0.0440  bbox_loss: 0.0177  cls_loss: 0.0263  \n",
      "<<<iteration:[200/657] - total_loss: 0.2441  obj_loss: 0.1055  noobj_loss: 0.0546  bbox_loss: 0.0167  cls_loss: 0.0281  \n",
      "<<<iteration:[220/657] - total_loss: 0.2714  obj_loss: 0.1227  noobj_loss: 0.0483  bbox_loss: 0.0188  cls_loss: 0.0306  \n",
      "<<<iteration:[240/657] - total_loss: 0.2522  obj_loss: 0.1170  noobj_loss: 0.0393  bbox_loss: 0.0171  cls_loss: 0.0300  \n",
      "<<<iteration:[260/657] - total_loss: 0.2424  obj_loss: 0.1219  noobj_loss: 0.0398  bbox_loss: 0.0150  cls_loss: 0.0254  \n",
      "<<<iteration:[280/657] - total_loss: 0.2466  obj_loss: 0.1031  noobj_loss: 0.0538  bbox_loss: 0.0177  cls_loss: 0.0281  \n",
      "<<<iteration:[300/657] - total_loss: 0.2388  obj_loss: 0.1098  noobj_loss: 0.0461  bbox_loss: 0.0163  cls_loss: 0.0246  \n",
      "<<<iteration:[320/657] - total_loss: 0.2498  obj_loss: 0.1090  noobj_loss: 0.0449  bbox_loss: 0.0179  cls_loss: 0.0289  \n",
      "<<<iteration:[340/657] - total_loss: 0.2609  obj_loss: 0.1198  noobj_loss: 0.0398  bbox_loss: 0.0175  cls_loss: 0.0336  \n",
      "<<<iteration:[360/657] - total_loss: 0.2435  obj_loss: 0.1119  noobj_loss: 0.0499  bbox_loss: 0.0164  cls_loss: 0.0250  \n",
      "<<<iteration:[380/657] - total_loss: 0.2444  obj_loss: 0.1147  noobj_loss: 0.0467  bbox_loss: 0.0159  cls_loss: 0.0270  \n",
      "<<<iteration:[400/657] - total_loss: 0.2385  obj_loss: 0.1034  noobj_loss: 0.0484  bbox_loss: 0.0170  cls_loss: 0.0260  \n",
      "<<<iteration:[420/657] - total_loss: 0.2665  obj_loss: 0.1189  noobj_loss: 0.0465  bbox_loss: 0.0190  cls_loss: 0.0294  \n",
      "<<<iteration:[440/657] - total_loss: 0.2567  obj_loss: 0.1151  noobj_loss: 0.0496  bbox_loss: 0.0168  cls_loss: 0.0327  \n",
      "<<<iteration:[460/657] - total_loss: 0.2579  obj_loss: 0.1193  noobj_loss: 0.0529  bbox_loss: 0.0167  cls_loss: 0.0285  \n",
      "<<<iteration:[480/657] - total_loss: 0.2606  obj_loss: 0.1225  noobj_loss: 0.0417  bbox_loss: 0.0185  cls_loss: 0.0245  \n",
      "<<<iteration:[500/657] - total_loss: 0.2323  obj_loss: 0.1046  noobj_loss: 0.0474  bbox_loss: 0.0151  cls_loss: 0.0284  \n",
      "<<<iteration:[520/657] - total_loss: 0.2553  obj_loss: 0.1140  noobj_loss: 0.0448  bbox_loss: 0.0181  cls_loss: 0.0282  \n",
      "<<<iteration:[540/657] - total_loss: 0.2751  obj_loss: 0.1291  noobj_loss: 0.0499  bbox_loss: 0.0180  cls_loss: 0.0309  \n",
      "<<<iteration:[560/657] - total_loss: 0.2519  obj_loss: 0.1140  noobj_loss: 0.0411  bbox_loss: 0.0168  cls_loss: 0.0332  \n",
      "<<<iteration:[580/657] - total_loss: 0.2559  obj_loss: 0.1143  noobj_loss: 0.0558  bbox_loss: 0.0165  cls_loss: 0.0311  \n",
      "<<<iteration:[600/657] - total_loss: 0.2580  obj_loss: 0.1262  noobj_loss: 0.0521  bbox_loss: 0.0166  cls_loss: 0.0226  \n",
      "<<<iteration:[620/657] - total_loss: 0.2439  obj_loss: 0.1100  noobj_loss: 0.0490  bbox_loss: 0.0168  cls_loss: 0.0253  \n",
      "<<<iteration:[640/657] - total_loss: 0.2526  obj_loss: 0.1262  noobj_loss: 0.0525  bbox_loss: 0.0152  cls_loss: 0.0241  \n",
      "\n",
      "epoch:26/100 - Train Loss: 0.2504, Val Loss: 0.2927\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2755  obj_loss: 0.1361  noobj_loss: 0.0556  bbox_loss: 0.0163  cls_loss: 0.0302  \n",
      "<<<iteration:[40/657] - total_loss: 0.2611  obj_loss: 0.1147  noobj_loss: 0.0487  bbox_loss: 0.0191  cls_loss: 0.0266  \n",
      "<<<iteration:[60/657] - total_loss: 0.2519  obj_loss: 0.1142  noobj_loss: 0.0553  bbox_loss: 0.0177  cls_loss: 0.0214  \n",
      "<<<iteration:[80/657] - total_loss: 0.2426  obj_loss: 0.1157  noobj_loss: 0.0491  bbox_loss: 0.0149  cls_loss: 0.0277  \n",
      "<<<iteration:[100/657] - total_loss: 0.2527  obj_loss: 0.1210  noobj_loss: 0.0447  bbox_loss: 0.0165  cls_loss: 0.0267  \n",
      "<<<iteration:[120/657] - total_loss: 0.2438  obj_loss: 0.1124  noobj_loss: 0.0514  bbox_loss: 0.0162  cls_loss: 0.0249  \n",
      "<<<iteration:[140/657] - total_loss: 0.2399  obj_loss: 0.1107  noobj_loss: 0.0495  bbox_loss: 0.0153  cls_loss: 0.0278  \n",
      "<<<iteration:[160/657] - total_loss: 0.2503  obj_loss: 0.1118  noobj_loss: 0.0435  bbox_loss: 0.0175  cls_loss: 0.0294  \n",
      "<<<iteration:[180/657] - total_loss: 0.2489  obj_loss: 0.1133  noobj_loss: 0.0465  bbox_loss: 0.0167  cls_loss: 0.0286  \n",
      "<<<iteration:[200/657] - total_loss: 0.2430  obj_loss: 0.1046  noobj_loss: 0.0516  bbox_loss: 0.0171  cls_loss: 0.0272  \n",
      "<<<iteration:[220/657] - total_loss: 0.2392  obj_loss: 0.1118  noobj_loss: 0.0503  bbox_loss: 0.0153  cls_loss: 0.0255  \n",
      "<<<iteration:[240/657] - total_loss: 0.2534  obj_loss: 0.1159  noobj_loss: 0.0470  bbox_loss: 0.0172  cls_loss: 0.0279  \n",
      "<<<iteration:[260/657] - total_loss: 0.2433  obj_loss: 0.1103  noobj_loss: 0.0477  bbox_loss: 0.0162  cls_loss: 0.0279  \n",
      "<<<iteration:[280/657] - total_loss: 0.2481  obj_loss: 0.1153  noobj_loss: 0.0448  bbox_loss: 0.0172  cls_loss: 0.0244  \n",
      "<<<iteration:[300/657] - total_loss: 0.2471  obj_loss: 0.1104  noobj_loss: 0.0470  bbox_loss: 0.0171  cls_loss: 0.0277  \n",
      "<<<iteration:[320/657] - total_loss: 0.2438  obj_loss: 0.1014  noobj_loss: 0.0498  bbox_loss: 0.0180  cls_loss: 0.0277  \n",
      "<<<iteration:[340/657] - total_loss: 0.2592  obj_loss: 0.1217  noobj_loss: 0.0399  bbox_loss: 0.0172  cls_loss: 0.0318  \n",
      "<<<iteration:[360/657] - total_loss: 0.2443  obj_loss: 0.1143  noobj_loss: 0.0451  bbox_loss: 0.0165  cls_loss: 0.0251  \n",
      "<<<iteration:[380/657] - total_loss: 0.2439  obj_loss: 0.1148  noobj_loss: 0.0472  bbox_loss: 0.0160  cls_loss: 0.0252  \n",
      "<<<iteration:[400/657] - total_loss: 0.2487  obj_loss: 0.1051  noobj_loss: 0.0518  bbox_loss: 0.0175  cls_loss: 0.0302  \n",
      "<<<iteration:[420/657] - total_loss: 0.2473  obj_loss: 0.1204  noobj_loss: 0.0450  bbox_loss: 0.0158  cls_loss: 0.0252  \n",
      "<<<iteration:[440/657] - total_loss: 0.2439  obj_loss: 0.1156  noobj_loss: 0.0509  bbox_loss: 0.0151  cls_loss: 0.0275  \n",
      "<<<iteration:[460/657] - total_loss: 0.2264  obj_loss: 0.1058  noobj_loss: 0.0539  bbox_loss: 0.0144  cls_loss: 0.0218  \n",
      "<<<iteration:[480/657] - total_loss: 0.2521  obj_loss: 0.1239  noobj_loss: 0.0446  bbox_loss: 0.0157  cls_loss: 0.0277  \n",
      "<<<iteration:[500/657] - total_loss: 0.2305  obj_loss: 0.1132  noobj_loss: 0.0418  bbox_loss: 0.0147  cls_loss: 0.0230  \n",
      "<<<iteration:[520/657] - total_loss: 0.2382  obj_loss: 0.1046  noobj_loss: 0.0486  bbox_loss: 0.0173  cls_loss: 0.0229  \n",
      "<<<iteration:[540/657] - total_loss: 0.2535  obj_loss: 0.1255  noobj_loss: 0.0441  bbox_loss: 0.0156  cls_loss: 0.0280  \n",
      "<<<iteration:[560/657] - total_loss: 0.2497  obj_loss: 0.1203  noobj_loss: 0.0477  bbox_loss: 0.0163  cls_loss: 0.0238  \n",
      "<<<iteration:[580/657] - total_loss: 0.2614  obj_loss: 0.1190  noobj_loss: 0.0517  bbox_loss: 0.0171  cls_loss: 0.0312  \n",
      "<<<iteration:[600/657] - total_loss: 0.2446  obj_loss: 0.1014  noobj_loss: 0.0533  bbox_loss: 0.0173  cls_loss: 0.0300  \n",
      "<<<iteration:[620/657] - total_loss: 0.2424  obj_loss: 0.1165  noobj_loss: 0.0468  bbox_loss: 0.0157  cls_loss: 0.0242  \n",
      "<<<iteration:[640/657] - total_loss: 0.2491  obj_loss: 0.1208  noobj_loss: 0.0529  bbox_loss: 0.0152  cls_loss: 0.0260  \n",
      "\n",
      "epoch:27/100 - Train Loss: 0.2475, Val Loss: 0.2869\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2701  obj_loss: 0.1202  noobj_loss: 0.0516  bbox_loss: 0.0197  cls_loss: 0.0259  \n",
      "<<<iteration:[40/657] - total_loss: 0.2434  obj_loss: 0.1127  noobj_loss: 0.0508  bbox_loss: 0.0160  cls_loss: 0.0256  \n",
      "<<<iteration:[60/657] - total_loss: 0.2423  obj_loss: 0.1022  noobj_loss: 0.0429  bbox_loss: 0.0182  cls_loss: 0.0275  \n",
      "<<<iteration:[80/657] - total_loss: 0.2282  obj_loss: 0.1106  noobj_loss: 0.0440  bbox_loss: 0.0145  cls_loss: 0.0229  \n",
      "<<<iteration:[100/657] - total_loss: 0.2563  obj_loss: 0.1166  noobj_loss: 0.0511  bbox_loss: 0.0169  cls_loss: 0.0296  \n",
      "<<<iteration:[120/657] - total_loss: 0.2404  obj_loss: 0.1138  noobj_loss: 0.0493  bbox_loss: 0.0155  cls_loss: 0.0245  \n",
      "<<<iteration:[140/657] - total_loss: 0.2547  obj_loss: 0.1107  noobj_loss: 0.0545  bbox_loss: 0.0175  cls_loss: 0.0292  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[160/657] - total_loss: 0.2564  obj_loss: 0.1197  noobj_loss: 0.0505  bbox_loss: 0.0160  cls_loss: 0.0313  \n",
      "<<<iteration:[180/657] - total_loss: 0.2425  obj_loss: 0.1105  noobj_loss: 0.0514  bbox_loss: 0.0151  cls_loss: 0.0306  \n",
      "<<<iteration:[200/657] - total_loss: 0.2605  obj_loss: 0.1312  noobj_loss: 0.0461  bbox_loss: 0.0164  cls_loss: 0.0242  \n",
      "<<<iteration:[220/657] - total_loss: 0.2433  obj_loss: 0.1049  noobj_loss: 0.0494  bbox_loss: 0.0174  cls_loss: 0.0268  \n",
      "<<<iteration:[240/657] - total_loss: 0.2442  obj_loss: 0.1204  noobj_loss: 0.0438  bbox_loss: 0.0148  cls_loss: 0.0280  \n",
      "<<<iteration:[260/657] - total_loss: 0.2444  obj_loss: 0.1080  noobj_loss: 0.0504  bbox_loss: 0.0170  cls_loss: 0.0263  \n",
      "<<<iteration:[280/657] - total_loss: 0.2564  obj_loss: 0.1195  noobj_loss: 0.0576  bbox_loss: 0.0167  cls_loss: 0.0245  \n",
      "<<<iteration:[300/657] - total_loss: 0.2498  obj_loss: 0.1171  noobj_loss: 0.0493  bbox_loss: 0.0155  cls_loss: 0.0308  \n",
      "<<<iteration:[320/657] - total_loss: 0.2546  obj_loss: 0.1272  noobj_loss: 0.0504  bbox_loss: 0.0153  cls_loss: 0.0255  \n",
      "<<<iteration:[340/657] - total_loss: 0.2463  obj_loss: 0.1240  noobj_loss: 0.0505  bbox_loss: 0.0139  cls_loss: 0.0276  \n",
      "<<<iteration:[360/657] - total_loss: 0.2352  obj_loss: 0.1093  noobj_loss: 0.0523  bbox_loss: 0.0153  cls_loss: 0.0230  \n",
      "<<<iteration:[380/657] - total_loss: 0.2672  obj_loss: 0.1298  noobj_loss: 0.0548  bbox_loss: 0.0169  cls_loss: 0.0256  \n",
      "<<<iteration:[400/657] - total_loss: 0.2534  obj_loss: 0.1297  noobj_loss: 0.0512  bbox_loss: 0.0146  cls_loss: 0.0253  \n",
      "<<<iteration:[420/657] - total_loss: 0.2561  obj_loss: 0.1201  noobj_loss: 0.0453  bbox_loss: 0.0168  cls_loss: 0.0294  \n",
      "<<<iteration:[440/657] - total_loss: 0.2347  obj_loss: 0.1041  noobj_loss: 0.0585  bbox_loss: 0.0157  cls_loss: 0.0229  \n",
      "<<<iteration:[460/657] - total_loss: 0.2478  obj_loss: 0.1225  noobj_loss: 0.0469  bbox_loss: 0.0157  cls_loss: 0.0231  \n",
      "<<<iteration:[480/657] - total_loss: 0.2421  obj_loss: 0.1173  noobj_loss: 0.0564  bbox_loss: 0.0150  cls_loss: 0.0217  \n",
      "<<<iteration:[500/657] - total_loss: 0.2513  obj_loss: 0.1090  noobj_loss: 0.0561  bbox_loss: 0.0177  cls_loss: 0.0260  \n",
      "<<<iteration:[520/657] - total_loss: 0.2306  obj_loss: 0.1044  noobj_loss: 0.0464  bbox_loss: 0.0157  cls_loss: 0.0242  \n",
      "<<<iteration:[540/657] - total_loss: 0.2333  obj_loss: 0.1109  noobj_loss: 0.0501  bbox_loss: 0.0144  cls_loss: 0.0256  \n",
      "<<<iteration:[560/657] - total_loss: 0.2461  obj_loss: 0.1074  noobj_loss: 0.0463  bbox_loss: 0.0169  cls_loss: 0.0310  \n",
      "<<<iteration:[580/657] - total_loss: 0.2356  obj_loss: 0.1191  noobj_loss: 0.0515  bbox_loss: 0.0138  cls_loss: 0.0219  \n",
      "<<<iteration:[600/657] - total_loss: 0.2457  obj_loss: 0.1190  noobj_loss: 0.0555  bbox_loss: 0.0151  cls_loss: 0.0236  \n",
      "<<<iteration:[620/657] - total_loss: 0.2604  obj_loss: 0.1231  noobj_loss: 0.0508  bbox_loss: 0.0166  cls_loss: 0.0291  \n",
      "<<<iteration:[640/657] - total_loss: 0.2341  obj_loss: 0.1040  noobj_loss: 0.0503  bbox_loss: 0.0160  cls_loss: 0.0248  \n",
      "\n",
      "epoch:28/100 - Train Loss: 0.2476, Val Loss: 0.2975\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2599  obj_loss: 0.1256  noobj_loss: 0.0557  bbox_loss: 0.0161  cls_loss: 0.0260  \n",
      "<<<iteration:[40/657] - total_loss: 0.2426  obj_loss: 0.1196  noobj_loss: 0.0515  bbox_loss: 0.0140  cls_loss: 0.0270  \n",
      "<<<iteration:[60/657] - total_loss: 0.2350  obj_loss: 0.1078  noobj_loss: 0.0456  bbox_loss: 0.0158  cls_loss: 0.0253  \n",
      "<<<iteration:[80/657] - total_loss: 0.2456  obj_loss: 0.1011  noobj_loss: 0.0518  bbox_loss: 0.0181  cls_loss: 0.0280  \n",
      "<<<iteration:[100/657] - total_loss: 0.2495  obj_loss: 0.1248  noobj_loss: 0.0530  bbox_loss: 0.0149  cls_loss: 0.0239  \n",
      "<<<iteration:[120/657] - total_loss: 0.2544  obj_loss: 0.1103  noobj_loss: 0.0497  bbox_loss: 0.0176  cls_loss: 0.0313  \n",
      "<<<iteration:[140/657] - total_loss: 0.2343  obj_loss: 0.1088  noobj_loss: 0.0507  bbox_loss: 0.0156  cls_loss: 0.0222  \n",
      "<<<iteration:[160/657] - total_loss: 0.2432  obj_loss: 0.1136  noobj_loss: 0.0554  bbox_loss: 0.0149  cls_loss: 0.0273  \n",
      "<<<iteration:[180/657] - total_loss: 0.2487  obj_loss: 0.1112  noobj_loss: 0.0527  bbox_loss: 0.0172  cls_loss: 0.0250  \n",
      "<<<iteration:[200/657] - total_loss: 0.2403  obj_loss: 0.1275  noobj_loss: 0.0480  bbox_loss: 0.0124  cls_loss: 0.0268  \n",
      "<<<iteration:[220/657] - total_loss: 0.2213  obj_loss: 0.1140  noobj_loss: 0.0478  bbox_loss: 0.0124  cls_loss: 0.0217  \n",
      "<<<iteration:[240/657] - total_loss: 0.2350  obj_loss: 0.1081  noobj_loss: 0.0534  bbox_loss: 0.0154  cls_loss: 0.0234  \n",
      "<<<iteration:[260/657] - total_loss: 0.2555  obj_loss: 0.1261  noobj_loss: 0.0494  bbox_loss: 0.0155  cls_loss: 0.0269  \n",
      "<<<iteration:[280/657] - total_loss: 0.2438  obj_loss: 0.1132  noobj_loss: 0.0470  bbox_loss: 0.0162  cls_loss: 0.0259  \n",
      "<<<iteration:[300/657] - total_loss: 0.2713  obj_loss: 0.1129  noobj_loss: 0.0560  bbox_loss: 0.0195  cls_loss: 0.0331  \n",
      "<<<iteration:[320/657] - total_loss: 0.2542  obj_loss: 0.1154  noobj_loss: 0.0564  bbox_loss: 0.0163  cls_loss: 0.0291  \n",
      "<<<iteration:[340/657] - total_loss: 0.2455  obj_loss: 0.1207  noobj_loss: 0.0530  bbox_loss: 0.0143  cls_loss: 0.0268  \n",
      "<<<iteration:[360/657] - total_loss: 0.2420  obj_loss: 0.1190  noobj_loss: 0.0465  bbox_loss: 0.0152  cls_loss: 0.0239  \n",
      "<<<iteration:[380/657] - total_loss: 0.2483  obj_loss: 0.1063  noobj_loss: 0.0504  bbox_loss: 0.0185  cls_loss: 0.0243  \n",
      "<<<iteration:[400/657] - total_loss: 0.2499  obj_loss: 0.1235  noobj_loss: 0.0537  bbox_loss: 0.0139  cls_loss: 0.0302  \n",
      "<<<iteration:[420/657] - total_loss: 0.2377  obj_loss: 0.1184  noobj_loss: 0.0554  bbox_loss: 0.0135  cls_loss: 0.0241  \n",
      "<<<iteration:[440/657] - total_loss: 0.2601  obj_loss: 0.1173  noobj_loss: 0.0556  bbox_loss: 0.0167  cls_loss: 0.0316  \n",
      "<<<iteration:[460/657] - total_loss: 0.2298  obj_loss: 0.0986  noobj_loss: 0.0496  bbox_loss: 0.0166  cls_loss: 0.0237  \n",
      "<<<iteration:[480/657] - total_loss: 0.2476  obj_loss: 0.1228  noobj_loss: 0.0501  bbox_loss: 0.0148  cls_loss: 0.0259  \n",
      "<<<iteration:[500/657] - total_loss: 0.2286  obj_loss: 0.1027  noobj_loss: 0.0479  bbox_loss: 0.0158  cls_loss: 0.0231  \n",
      "<<<iteration:[520/657] - total_loss: 0.2362  obj_loss: 0.1023  noobj_loss: 0.0532  bbox_loss: 0.0152  cls_loss: 0.0311  \n",
      "<<<iteration:[540/657] - total_loss: 0.2309  obj_loss: 0.1153  noobj_loss: 0.0481  bbox_loss: 0.0139  cls_loss: 0.0222  \n",
      "<<<iteration:[560/657] - total_loss: 0.2419  obj_loss: 0.1123  noobj_loss: 0.0484  bbox_loss: 0.0162  cls_loss: 0.0242  \n",
      "<<<iteration:[580/657] - total_loss: 0.2448  obj_loss: 0.1135  noobj_loss: 0.0576  bbox_loss: 0.0158  cls_loss: 0.0235  \n",
      "<<<iteration:[600/657] - total_loss: 0.2382  obj_loss: 0.1174  noobj_loss: 0.0481  bbox_loss: 0.0141  cls_loss: 0.0261  \n",
      "<<<iteration:[620/657] - total_loss: 0.2338  obj_loss: 0.0972  noobj_loss: 0.0507  bbox_loss: 0.0178  cls_loss: 0.0221  \n",
      "<<<iteration:[640/657] - total_loss: 0.2442  obj_loss: 0.1141  noobj_loss: 0.0577  bbox_loss: 0.0158  cls_loss: 0.0221  \n",
      "\n",
      "epoch:29/100 - Train Loss: 0.2437, Val Loss: 0.2847\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2466  obj_loss: 0.1202  noobj_loss: 0.0485  bbox_loss: 0.0161  cls_loss: 0.0215  \n",
      "<<<iteration:[40/657] - total_loss: 0.2462  obj_loss: 0.1180  noobj_loss: 0.0548  bbox_loss: 0.0155  cls_loss: 0.0233  \n",
      "<<<iteration:[60/657] - total_loss: 0.2442  obj_loss: 0.1212  noobj_loss: 0.0551  bbox_loss: 0.0142  cls_loss: 0.0243  \n",
      "<<<iteration:[80/657] - total_loss: 0.2334  obj_loss: 0.1133  noobj_loss: 0.0557  bbox_loss: 0.0134  cls_loss: 0.0252  \n",
      "<<<iteration:[100/657] - total_loss: 0.2503  obj_loss: 0.1138  noobj_loss: 0.0621  bbox_loss: 0.0156  cls_loss: 0.0273  \n",
      "<<<iteration:[120/657] - total_loss: 0.2514  obj_loss: 0.1165  noobj_loss: 0.0522  bbox_loss: 0.0166  cls_loss: 0.0261  \n",
      "<<<iteration:[140/657] - total_loss: 0.2283  obj_loss: 0.1053  noobj_loss: 0.0577  bbox_loss: 0.0143  cls_loss: 0.0227  \n",
      "<<<iteration:[160/657] - total_loss: 0.2489  obj_loss: 0.1162  noobj_loss: 0.0436  bbox_loss: 0.0172  cls_loss: 0.0247  \n",
      "<<<iteration:[180/657] - total_loss: 0.2459  obj_loss: 0.1274  noobj_loss: 0.0470  bbox_loss: 0.0128  cls_loss: 0.0309  \n",
      "<<<iteration:[200/657] - total_loss: 0.2209  obj_loss: 0.1171  noobj_loss: 0.0466  bbox_loss: 0.0125  cls_loss: 0.0181  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[220/657] - total_loss: 0.2329  obj_loss: 0.1101  noobj_loss: 0.0547  bbox_loss: 0.0144  cls_loss: 0.0235  \n",
      "<<<iteration:[240/657] - total_loss: 0.2455  obj_loss: 0.1133  noobj_loss: 0.0488  bbox_loss: 0.0162  cls_loss: 0.0266  \n",
      "<<<iteration:[260/657] - total_loss: 0.2575  obj_loss: 0.1167  noobj_loss: 0.0543  bbox_loss: 0.0174  cls_loss: 0.0264  \n",
      "<<<iteration:[280/657] - total_loss: 0.2547  obj_loss: 0.1079  noobj_loss: 0.0565  bbox_loss: 0.0182  cls_loss: 0.0274  \n",
      "<<<iteration:[300/657] - total_loss: 0.2400  obj_loss: 0.1073  noobj_loss: 0.0501  bbox_loss: 0.0162  cls_loss: 0.0268  \n",
      "<<<iteration:[320/657] - total_loss: 0.2515  obj_loss: 0.1217  noobj_loss: 0.0543  bbox_loss: 0.0149  cls_loss: 0.0281  \n",
      "<<<iteration:[340/657] - total_loss: 0.2355  obj_loss: 0.1038  noobj_loss: 0.0568  bbox_loss: 0.0161  cls_loss: 0.0230  \n",
      "<<<iteration:[360/657] - total_loss: 0.2466  obj_loss: 0.1158  noobj_loss: 0.0496  bbox_loss: 0.0164  cls_loss: 0.0241  \n",
      "<<<iteration:[380/657] - total_loss: 0.2378  obj_loss: 0.1075  noobj_loss: 0.0549  bbox_loss: 0.0158  cls_loss: 0.0237  \n",
      "<<<iteration:[400/657] - total_loss: 0.2340  obj_loss: 0.1165  noobj_loss: 0.0544  bbox_loss: 0.0139  cls_loss: 0.0208  \n",
      "<<<iteration:[420/657] - total_loss: 0.2437  obj_loss: 0.1142  noobj_loss: 0.0479  bbox_loss: 0.0155  cls_loss: 0.0282  \n",
      "<<<iteration:[440/657] - total_loss: 0.2473  obj_loss: 0.1167  noobj_loss: 0.0524  bbox_loss: 0.0171  cls_loss: 0.0191  \n",
      "<<<iteration:[460/657] - total_loss: 0.2376  obj_loss: 0.1064  noobj_loss: 0.0468  bbox_loss: 0.0165  cls_loss: 0.0252  \n",
      "<<<iteration:[480/657] - total_loss: 0.2435  obj_loss: 0.1142  noobj_loss: 0.0586  bbox_loss: 0.0147  cls_loss: 0.0265  \n",
      "<<<iteration:[500/657] - total_loss: 0.2540  obj_loss: 0.1207  noobj_loss: 0.0508  bbox_loss: 0.0168  cls_loss: 0.0238  \n",
      "<<<iteration:[520/657] - total_loss: 0.2402  obj_loss: 0.1164  noobj_loss: 0.0536  bbox_loss: 0.0147  cls_loss: 0.0237  \n",
      "<<<iteration:[540/657] - total_loss: 0.2510  obj_loss: 0.1238  noobj_loss: 0.0549  bbox_loss: 0.0153  cls_loss: 0.0233  \n",
      "<<<iteration:[560/657] - total_loss: 0.2431  obj_loss: 0.1152  noobj_loss: 0.0516  bbox_loss: 0.0157  cls_loss: 0.0238  \n",
      "<<<iteration:[580/657] - total_loss: 0.2354  obj_loss: 0.1097  noobj_loss: 0.0561  bbox_loss: 0.0141  cls_loss: 0.0274  \n",
      "<<<iteration:[600/657] - total_loss: 0.2294  obj_loss: 0.1040  noobj_loss: 0.0485  bbox_loss: 0.0149  cls_loss: 0.0267  \n",
      "<<<iteration:[620/657] - total_loss: 0.2382  obj_loss: 0.1075  noobj_loss: 0.0535  bbox_loss: 0.0155  cls_loss: 0.0264  \n",
      "<<<iteration:[640/657] - total_loss: 0.2385  obj_loss: 0.1187  noobj_loss: 0.0537  bbox_loss: 0.0133  cls_loss: 0.0262  \n",
      "\n",
      "epoch:30/100 - Train Loss: 0.2420, Val Loss: 0.2816\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2684  obj_loss: 0.1308  noobj_loss: 0.0549  bbox_loss: 0.0173  cls_loss: 0.0235  \n",
      "<<<iteration:[40/657] - total_loss: 0.2464  obj_loss: 0.1135  noobj_loss: 0.0601  bbox_loss: 0.0148  cls_loss: 0.0288  \n",
      "<<<iteration:[60/657] - total_loss: 0.2393  obj_loss: 0.1109  noobj_loss: 0.0450  bbox_loss: 0.0163  cls_loss: 0.0245  \n",
      "<<<iteration:[80/657] - total_loss: 0.2265  obj_loss: 0.1061  noobj_loss: 0.0579  bbox_loss: 0.0143  cls_loss: 0.0198  \n",
      "<<<iteration:[100/657] - total_loss: 0.2338  obj_loss: 0.1075  noobj_loss: 0.0537  bbox_loss: 0.0159  cls_loss: 0.0201  \n",
      "<<<iteration:[120/657] - total_loss: 0.2433  obj_loss: 0.1048  noobj_loss: 0.0580  bbox_loss: 0.0160  cls_loss: 0.0293  \n",
      "<<<iteration:[140/657] - total_loss: 0.2468  obj_loss: 0.1188  noobj_loss: 0.0510  bbox_loss: 0.0150  cls_loss: 0.0273  \n",
      "<<<iteration:[160/657] - total_loss: 0.2320  obj_loss: 0.1093  noobj_loss: 0.0499  bbox_loss: 0.0146  cls_loss: 0.0248  \n",
      "<<<iteration:[180/657] - total_loss: 0.2564  obj_loss: 0.1070  noobj_loss: 0.0635  bbox_loss: 0.0176  cls_loss: 0.0298  \n",
      "<<<iteration:[200/657] - total_loss: 0.2402  obj_loss: 0.1120  noobj_loss: 0.0476  bbox_loss: 0.0158  cls_loss: 0.0253  \n",
      "<<<iteration:[220/657] - total_loss: 0.2345  obj_loss: 0.1125  noobj_loss: 0.0536  bbox_loss: 0.0141  cls_loss: 0.0245  \n",
      "<<<iteration:[240/657] - total_loss: 0.2338  obj_loss: 0.1159  noobj_loss: 0.0512  bbox_loss: 0.0135  cls_loss: 0.0246  \n",
      "<<<iteration:[260/657] - total_loss: 0.2448  obj_loss: 0.1167  noobj_loss: 0.0593  bbox_loss: 0.0147  cls_loss: 0.0252  \n",
      "<<<iteration:[280/657] - total_loss: 0.2345  obj_loss: 0.0993  noobj_loss: 0.0579  bbox_loss: 0.0162  cls_loss: 0.0253  \n",
      "<<<iteration:[300/657] - total_loss: 0.2478  obj_loss: 0.0967  noobj_loss: 0.0623  bbox_loss: 0.0191  cls_loss: 0.0245  \n",
      "<<<iteration:[320/657] - total_loss: 0.2349  obj_loss: 0.1168  noobj_loss: 0.0502  bbox_loss: 0.0144  cls_loss: 0.0210  \n",
      "<<<iteration:[340/657] - total_loss: 0.2320  obj_loss: 0.1149  noobj_loss: 0.0503  bbox_loss: 0.0142  cls_loss: 0.0210  \n",
      "<<<iteration:[360/657] - total_loss: 0.2273  obj_loss: 0.1117  noobj_loss: 0.0544  bbox_loss: 0.0134  cls_loss: 0.0213  \n",
      "<<<iteration:[380/657] - total_loss: 0.2208  obj_loss: 0.1012  noobj_loss: 0.0533  bbox_loss: 0.0139  cls_loss: 0.0232  \n",
      "<<<iteration:[400/657] - total_loss: 0.2482  obj_loss: 0.1194  noobj_loss: 0.0528  bbox_loss: 0.0161  cls_loss: 0.0220  \n",
      "<<<iteration:[420/657] - total_loss: 0.2540  obj_loss: 0.1237  noobj_loss: 0.0585  bbox_loss: 0.0158  cls_loss: 0.0223  \n",
      "<<<iteration:[440/657] - total_loss: 0.2413  obj_loss: 0.1202  noobj_loss: 0.0528  bbox_loss: 0.0146  cls_loss: 0.0218  \n",
      "<<<iteration:[460/657] - total_loss: 0.2404  obj_loss: 0.1147  noobj_loss: 0.0576  bbox_loss: 0.0144  cls_loss: 0.0250  \n",
      "<<<iteration:[480/657] - total_loss: 0.2445  obj_loss: 0.1176  noobj_loss: 0.0523  bbox_loss: 0.0144  cls_loss: 0.0288  \n",
      "<<<iteration:[500/657] - total_loss: 0.2353  obj_loss: 0.1183  noobj_loss: 0.0563  bbox_loss: 0.0137  cls_loss: 0.0205  \n",
      "<<<iteration:[520/657] - total_loss: 0.2558  obj_loss: 0.1130  noobj_loss: 0.0571  bbox_loss: 0.0173  cls_loss: 0.0277  \n",
      "<<<iteration:[540/657] - total_loss: 0.2496  obj_loss: 0.1182  noobj_loss: 0.0571  bbox_loss: 0.0162  cls_loss: 0.0220  \n",
      "<<<iteration:[560/657] - total_loss: 0.2252  obj_loss: 0.1055  noobj_loss: 0.0531  bbox_loss: 0.0144  cls_loss: 0.0212  \n",
      "<<<iteration:[580/657] - total_loss: 0.2330  obj_loss: 0.1033  noobj_loss: 0.0564  bbox_loss: 0.0150  cls_loss: 0.0268  \n",
      "<<<iteration:[600/657] - total_loss: 0.2351  obj_loss: 0.1098  noobj_loss: 0.0548  bbox_loss: 0.0151  cls_loss: 0.0226  \n",
      "<<<iteration:[620/657] - total_loss: 0.2308  obj_loss: 0.1146  noobj_loss: 0.0526  bbox_loss: 0.0129  cls_loss: 0.0252  \n",
      "<<<iteration:[640/657] - total_loss: 0.2340  obj_loss: 0.1058  noobj_loss: 0.0602  bbox_loss: 0.0152  cls_loss: 0.0221  \n",
      "\n",
      "epoch:31/100 - Train Loss: 0.2392, Val Loss: 0.2787\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2356  obj_loss: 0.1044  noobj_loss: 0.0543  bbox_loss: 0.0152  cls_loss: 0.0281  \n",
      "<<<iteration:[40/657] - total_loss: 0.2385  obj_loss: 0.1068  noobj_loss: 0.0547  bbox_loss: 0.0162  cls_loss: 0.0234  \n",
      "<<<iteration:[60/657] - total_loss: 0.2498  obj_loss: 0.1227  noobj_loss: 0.0526  bbox_loss: 0.0154  cls_loss: 0.0237  \n",
      "<<<iteration:[80/657] - total_loss: 0.2374  obj_loss: 0.1111  noobj_loss: 0.0556  bbox_loss: 0.0148  cls_loss: 0.0244  \n",
      "<<<iteration:[100/657] - total_loss: 0.2464  obj_loss: 0.1078  noobj_loss: 0.0520  bbox_loss: 0.0176  cls_loss: 0.0245  \n",
      "<<<iteration:[120/657] - total_loss: 0.2328  obj_loss: 0.1136  noobj_loss: 0.0565  bbox_loss: 0.0139  cls_loss: 0.0212  \n",
      "<<<iteration:[140/657] - total_loss: 0.2230  obj_loss: 0.1067  noobj_loss: 0.0575  bbox_loss: 0.0134  cls_loss: 0.0207  \n",
      "<<<iteration:[160/657] - total_loss: 0.2412  obj_loss: 0.1108  noobj_loss: 0.0524  bbox_loss: 0.0158  cls_loss: 0.0250  \n",
      "<<<iteration:[180/657] - total_loss: 0.2433  obj_loss: 0.1221  noobj_loss: 0.0591  bbox_loss: 0.0139  cls_loss: 0.0222  \n",
      "<<<iteration:[200/657] - total_loss: 0.2378  obj_loss: 0.1148  noobj_loss: 0.0548  bbox_loss: 0.0136  cls_loss: 0.0278  \n",
      "<<<iteration:[220/657] - total_loss: 0.2367  obj_loss: 0.1053  noobj_loss: 0.0550  bbox_loss: 0.0152  cls_loss: 0.0278  \n",
      "<<<iteration:[240/657] - total_loss: 0.2343  obj_loss: 0.1052  noobj_loss: 0.0585  bbox_loss: 0.0151  cls_loss: 0.0242  \n",
      "<<<iteration:[260/657] - total_loss: 0.2363  obj_loss: 0.1099  noobj_loss: 0.0596  bbox_loss: 0.0152  cls_loss: 0.0208  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[280/657] - total_loss: 0.2231  obj_loss: 0.1137  noobj_loss: 0.0467  bbox_loss: 0.0131  cls_loss: 0.0204  \n",
      "<<<iteration:[300/657] - total_loss: 0.2371  obj_loss: 0.1134  noobj_loss: 0.0542  bbox_loss: 0.0143  cls_loss: 0.0253  \n",
      "<<<iteration:[320/657] - total_loss: 0.2543  obj_loss: 0.1357  noobj_loss: 0.0509  bbox_loss: 0.0135  cls_loss: 0.0258  \n",
      "<<<iteration:[340/657] - total_loss: 0.2442  obj_loss: 0.1218  noobj_loss: 0.0498  bbox_loss: 0.0149  cls_loss: 0.0229  \n",
      "<<<iteration:[360/657] - total_loss: 0.2453  obj_loss: 0.1165  noobj_loss: 0.0569  bbox_loss: 0.0155  cls_loss: 0.0229  \n",
      "<<<iteration:[380/657] - total_loss: 0.2156  obj_loss: 0.0970  noobj_loss: 0.0581  bbox_loss: 0.0136  cls_loss: 0.0218  \n",
      "<<<iteration:[400/657] - total_loss: 0.2302  obj_loss: 0.1150  noobj_loss: 0.0535  bbox_loss: 0.0141  cls_loss: 0.0179  \n",
      "<<<iteration:[420/657] - total_loss: 0.2264  obj_loss: 0.1165  noobj_loss: 0.0490  bbox_loss: 0.0124  cls_loss: 0.0235  \n",
      "<<<iteration:[440/657] - total_loss: 0.2555  obj_loss: 0.1213  noobj_loss: 0.0569  bbox_loss: 0.0168  cls_loss: 0.0218  \n",
      "<<<iteration:[460/657] - total_loss: 0.2258  obj_loss: 0.1053  noobj_loss: 0.0565  bbox_loss: 0.0140  cls_loss: 0.0225  \n",
      "<<<iteration:[480/657] - total_loss: 0.2247  obj_loss: 0.1132  noobj_loss: 0.0594  bbox_loss: 0.0124  cls_loss: 0.0197  \n",
      "<<<iteration:[500/657] - total_loss: 0.2395  obj_loss: 0.1121  noobj_loss: 0.0544  bbox_loss: 0.0157  cls_loss: 0.0218  \n",
      "<<<iteration:[520/657] - total_loss: 0.2518  obj_loss: 0.1357  noobj_loss: 0.0563  bbox_loss: 0.0130  cls_loss: 0.0232  \n",
      "<<<iteration:[540/657] - total_loss: 0.2209  obj_loss: 0.1009  noobj_loss: 0.0557  bbox_loss: 0.0142  cls_loss: 0.0212  \n",
      "<<<iteration:[560/657] - total_loss: 0.2174  obj_loss: 0.1045  noobj_loss: 0.0554  bbox_loss: 0.0129  cls_loss: 0.0204  \n",
      "<<<iteration:[580/657] - total_loss: 0.2613  obj_loss: 0.1202  noobj_loss: 0.0563  bbox_loss: 0.0172  cls_loss: 0.0271  \n",
      "<<<iteration:[600/657] - total_loss: 0.2394  obj_loss: 0.1158  noobj_loss: 0.0617  bbox_loss: 0.0137  cls_loss: 0.0242  \n",
      "<<<iteration:[620/657] - total_loss: 0.2208  obj_loss: 0.0972  noobj_loss: 0.0572  bbox_loss: 0.0146  cls_loss: 0.0221  \n",
      "<<<iteration:[640/657] - total_loss: 0.2479  obj_loss: 0.1174  noobj_loss: 0.0569  bbox_loss: 0.0163  cls_loss: 0.0204  \n",
      "\n",
      "epoch:32/100 - Train Loss: 0.2364, Val Loss: 0.2777\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2433  obj_loss: 0.1111  noobj_loss: 0.0565  bbox_loss: 0.0160  cls_loss: 0.0237  \n",
      "<<<iteration:[40/657] - total_loss: 0.2224  obj_loss: 0.1101  noobj_loss: 0.0557  bbox_loss: 0.0124  cls_loss: 0.0224  \n",
      "<<<iteration:[60/657] - total_loss: 0.2420  obj_loss: 0.1199  noobj_loss: 0.0693  bbox_loss: 0.0126  cls_loss: 0.0246  \n",
      "<<<iteration:[80/657] - total_loss: 0.2298  obj_loss: 0.1032  noobj_loss: 0.0562  bbox_loss: 0.0150  cls_loss: 0.0236  \n",
      "<<<iteration:[100/657] - total_loss: 0.2222  obj_loss: 0.1084  noobj_loss: 0.0527  bbox_loss: 0.0135  cls_loss: 0.0199  \n",
      "<<<iteration:[120/657] - total_loss: 0.2398  obj_loss: 0.1092  noobj_loss: 0.0559  bbox_loss: 0.0154  cls_loss: 0.0257  \n",
      "<<<iteration:[140/657] - total_loss: 0.2271  obj_loss: 0.1072  noobj_loss: 0.0507  bbox_loss: 0.0154  cls_loss: 0.0176  \n",
      "<<<iteration:[160/657] - total_loss: 0.2366  obj_loss: 0.1064  noobj_loss: 0.0534  bbox_loss: 0.0156  cls_loss: 0.0255  \n",
      "<<<iteration:[180/657] - total_loss: 0.2479  obj_loss: 0.1259  noobj_loss: 0.0515  bbox_loss: 0.0149  cls_loss: 0.0219  \n",
      "<<<iteration:[200/657] - total_loss: 0.2389  obj_loss: 0.1088  noobj_loss: 0.0606  bbox_loss: 0.0145  cls_loss: 0.0271  \n",
      "<<<iteration:[220/657] - total_loss: 0.2335  obj_loss: 0.1061  noobj_loss: 0.0539  bbox_loss: 0.0154  cls_loss: 0.0234  \n",
      "<<<iteration:[240/657] - total_loss: 0.2811  obj_loss: 0.1197  noobj_loss: 0.0777  bbox_loss: 0.0196  cls_loss: 0.0245  \n",
      "<<<iteration:[260/657] - total_loss: 0.2309  obj_loss: 0.1044  noobj_loss: 0.0587  bbox_loss: 0.0145  cls_loss: 0.0249  \n",
      "<<<iteration:[280/657] - total_loss: 0.2349  obj_loss: 0.1019  noobj_loss: 0.0483  bbox_loss: 0.0165  cls_loss: 0.0262  \n",
      "<<<iteration:[300/657] - total_loss: 0.2364  obj_loss: 0.1138  noobj_loss: 0.0549  bbox_loss: 0.0146  cls_loss: 0.0219  \n",
      "<<<iteration:[320/657] - total_loss: 0.2228  obj_loss: 0.1062  noobj_loss: 0.0510  bbox_loss: 0.0138  cls_loss: 0.0219  \n",
      "<<<iteration:[340/657] - total_loss: 0.2491  obj_loss: 0.1207  noobj_loss: 0.0573  bbox_loss: 0.0154  cls_loss: 0.0227  \n",
      "<<<iteration:[360/657] - total_loss: 0.2260  obj_loss: 0.1078  noobj_loss: 0.0574  bbox_loss: 0.0138  cls_loss: 0.0203  \n",
      "<<<iteration:[380/657] - total_loss: 0.2375  obj_loss: 0.1141  noobj_loss: 0.0533  bbox_loss: 0.0151  cls_loss: 0.0213  \n",
      "<<<iteration:[400/657] - total_loss: 0.2326  obj_loss: 0.1147  noobj_loss: 0.0578  bbox_loss: 0.0132  cls_loss: 0.0229  \n",
      "<<<iteration:[420/657] - total_loss: 0.2208  obj_loss: 0.1052  noobj_loss: 0.0508  bbox_loss: 0.0140  cls_loss: 0.0202  \n",
      "<<<iteration:[440/657] - total_loss: 0.2240  obj_loss: 0.1095  noobj_loss: 0.0514  bbox_loss: 0.0139  cls_loss: 0.0193  \n",
      "<<<iteration:[460/657] - total_loss: 0.2322  obj_loss: 0.1130  noobj_loss: 0.0525  bbox_loss: 0.0145  cls_loss: 0.0206  \n",
      "<<<iteration:[480/657] - total_loss: 0.2337  obj_loss: 0.1209  noobj_loss: 0.0609  bbox_loss: 0.0126  cls_loss: 0.0192  \n",
      "<<<iteration:[500/657] - total_loss: 0.2142  obj_loss: 0.1007  noobj_loss: 0.0592  bbox_loss: 0.0134  cls_loss: 0.0171  \n",
      "<<<iteration:[520/657] - total_loss: 0.2265  obj_loss: 0.1003  noobj_loss: 0.0550  bbox_loss: 0.0144  cls_loss: 0.0267  \n",
      "<<<iteration:[540/657] - total_loss: 0.2367  obj_loss: 0.1175  noobj_loss: 0.0519  bbox_loss: 0.0140  cls_loss: 0.0235  \n",
      "<<<iteration:[560/657] - total_loss: 0.2304  obj_loss: 0.1147  noobj_loss: 0.0588  bbox_loss: 0.0125  cls_loss: 0.0238  \n",
      "<<<iteration:[580/657] - total_loss: 0.2248  obj_loss: 0.1119  noobj_loss: 0.0533  bbox_loss: 0.0129  cls_loss: 0.0219  \n",
      "<<<iteration:[600/657] - total_loss: 0.2260  obj_loss: 0.1071  noobj_loss: 0.0592  bbox_loss: 0.0135  cls_loss: 0.0217  \n",
      "<<<iteration:[620/657] - total_loss: 0.2377  obj_loss: 0.1105  noobj_loss: 0.0606  bbox_loss: 0.0147  cls_loss: 0.0236  \n",
      "<<<iteration:[640/657] - total_loss: 0.2306  obj_loss: 0.1063  noobj_loss: 0.0544  bbox_loss: 0.0148  cls_loss: 0.0233  \n",
      "\n",
      "epoch:33/100 - Train Loss: 0.2334, Val Loss: 0.2793\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2324  obj_loss: 0.1117  noobj_loss: 0.0575  bbox_loss: 0.0142  cls_loss: 0.0212  \n",
      "<<<iteration:[40/657] - total_loss: 0.2318  obj_loss: 0.1084  noobj_loss: 0.0534  bbox_loss: 0.0150  cls_loss: 0.0216  \n",
      "<<<iteration:[60/657] - total_loss: 0.2305  obj_loss: 0.1233  noobj_loss: 0.0533  bbox_loss: 0.0116  cls_loss: 0.0225  \n",
      "<<<iteration:[80/657] - total_loss: 0.2377  obj_loss: 0.1220  noobj_loss: 0.0615  bbox_loss: 0.0129  cls_loss: 0.0203  \n",
      "<<<iteration:[100/657] - total_loss: 0.2236  obj_loss: 0.1199  noobj_loss: 0.0568  bbox_loss: 0.0115  cls_loss: 0.0181  \n",
      "<<<iteration:[120/657] - total_loss: 0.2378  obj_loss: 0.1137  noobj_loss: 0.0606  bbox_loss: 0.0137  cls_loss: 0.0251  \n",
      "<<<iteration:[140/657] - total_loss: 0.2280  obj_loss: 0.1122  noobj_loss: 0.0620  bbox_loss: 0.0132  cls_loss: 0.0186  \n",
      "<<<iteration:[160/657] - total_loss: 0.2216  obj_loss: 0.1117  noobj_loss: 0.0545  bbox_loss: 0.0128  cls_loss: 0.0184  \n",
      "<<<iteration:[180/657] - total_loss: 0.2527  obj_loss: 0.1211  noobj_loss: 0.0555  bbox_loss: 0.0165  cls_loss: 0.0211  \n",
      "<<<iteration:[200/657] - total_loss: 0.2311  obj_loss: 0.1061  noobj_loss: 0.0552  bbox_loss: 0.0151  cls_loss: 0.0221  \n",
      "<<<iteration:[220/657] - total_loss: 0.2208  obj_loss: 0.1022  noobj_loss: 0.0568  bbox_loss: 0.0142  cls_loss: 0.0191  \n",
      "<<<iteration:[240/657] - total_loss: 0.2126  obj_loss: 0.0977  noobj_loss: 0.0622  bbox_loss: 0.0133  cls_loss: 0.0173  \n",
      "<<<iteration:[260/657] - total_loss: 0.2348  obj_loss: 0.1080  noobj_loss: 0.0643  bbox_loss: 0.0151  cls_loss: 0.0193  \n",
      "<<<iteration:[280/657] - total_loss: 0.2404  obj_loss: 0.1089  noobj_loss: 0.0539  bbox_loss: 0.0164  cls_loss: 0.0224  \n",
      "<<<iteration:[300/657] - total_loss: 0.2256  obj_loss: 0.1088  noobj_loss: 0.0585  bbox_loss: 0.0139  cls_loss: 0.0179  \n",
      "<<<iteration:[320/657] - total_loss: 0.2359  obj_loss: 0.1008  noobj_loss: 0.0601  bbox_loss: 0.0158  cls_loss: 0.0259  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[340/657] - total_loss: 0.2354  obj_loss: 0.1152  noobj_loss: 0.0542  bbox_loss: 0.0144  cls_loss: 0.0212  \n",
      "<<<iteration:[360/657] - total_loss: 0.2349  obj_loss: 0.1068  noobj_loss: 0.0560  bbox_loss: 0.0156  cls_loss: 0.0222  \n",
      "<<<iteration:[380/657] - total_loss: 0.2385  obj_loss: 0.1099  noobj_loss: 0.0517  bbox_loss: 0.0144  cls_loss: 0.0309  \n",
      "<<<iteration:[400/657] - total_loss: 0.2300  obj_loss: 0.1024  noobj_loss: 0.0624  bbox_loss: 0.0145  cls_loss: 0.0236  \n",
      "<<<iteration:[420/657] - total_loss: 0.2214  obj_loss: 0.1094  noobj_loss: 0.0548  bbox_loss: 0.0129  cls_loss: 0.0202  \n",
      "<<<iteration:[440/657] - total_loss: 0.2136  obj_loss: 0.0958  noobj_loss: 0.0577  bbox_loss: 0.0134  cls_loss: 0.0219  \n",
      "<<<iteration:[460/657] - total_loss: 0.2365  obj_loss: 0.1158  noobj_loss: 0.0583  bbox_loss: 0.0137  cls_loss: 0.0233  \n",
      "<<<iteration:[480/657] - total_loss: 0.2335  obj_loss: 0.1184  noobj_loss: 0.0618  bbox_loss: 0.0130  cls_loss: 0.0193  \n",
      "<<<iteration:[500/657] - total_loss: 0.2437  obj_loss: 0.1035  noobj_loss: 0.0627  bbox_loss: 0.0170  cls_loss: 0.0239  \n",
      "<<<iteration:[520/657] - total_loss: 0.2231  obj_loss: 0.1053  noobj_loss: 0.0555  bbox_loss: 0.0140  cls_loss: 0.0201  \n",
      "<<<iteration:[540/657] - total_loss: 0.2284  obj_loss: 0.1045  noobj_loss: 0.0601  bbox_loss: 0.0139  cls_loss: 0.0242  \n",
      "<<<iteration:[560/657] - total_loss: 0.2254  obj_loss: 0.1141  noobj_loss: 0.0581  bbox_loss: 0.0129  cls_loss: 0.0178  \n",
      "<<<iteration:[580/657] - total_loss: 0.2389  obj_loss: 0.1115  noobj_loss: 0.0578  bbox_loss: 0.0152  cls_loss: 0.0226  \n",
      "<<<iteration:[600/657] - total_loss: 0.2107  obj_loss: 0.1010  noobj_loss: 0.0547  bbox_loss: 0.0124  cls_loss: 0.0206  \n",
      "<<<iteration:[620/657] - total_loss: 0.2306  obj_loss: 0.1165  noobj_loss: 0.0586  bbox_loss: 0.0126  cls_loss: 0.0218  \n",
      "<<<iteration:[640/657] - total_loss: 0.2296  obj_loss: 0.1101  noobj_loss: 0.0545  bbox_loss: 0.0138  cls_loss: 0.0232  \n",
      "\n",
      "epoch:34/100 - Train Loss: 0.2299, Val Loss: 0.2704\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2383  obj_loss: 0.1191  noobj_loss: 0.0565  bbox_loss: 0.0138  cls_loss: 0.0219  \n",
      "<<<iteration:[40/657] - total_loss: 0.2326  obj_loss: 0.1191  noobj_loss: 0.0548  bbox_loss: 0.0129  cls_loss: 0.0217  \n",
      "<<<iteration:[60/657] - total_loss: 0.2214  obj_loss: 0.1100  noobj_loss: 0.0575  bbox_loss: 0.0126  cls_loss: 0.0194  \n",
      "<<<iteration:[80/657] - total_loss: 0.2254  obj_loss: 0.1091  noobj_loss: 0.0583  bbox_loss: 0.0130  cls_loss: 0.0223  \n",
      "<<<iteration:[100/657] - total_loss: 0.2319  obj_loss: 0.1009  noobj_loss: 0.0591  bbox_loss: 0.0156  cls_loss: 0.0234  \n",
      "<<<iteration:[120/657] - total_loss: 0.2198  obj_loss: 0.1033  noobj_loss: 0.0592  bbox_loss: 0.0134  cls_loss: 0.0196  \n",
      "<<<iteration:[140/657] - total_loss: 0.2318  obj_loss: 0.1091  noobj_loss: 0.0591  bbox_loss: 0.0139  cls_loss: 0.0235  \n",
      "<<<iteration:[160/657] - total_loss: 0.2408  obj_loss: 0.1164  noobj_loss: 0.0581  bbox_loss: 0.0151  cls_loss: 0.0199  \n",
      "<<<iteration:[180/657] - total_loss: 0.2302  obj_loss: 0.1035  noobj_loss: 0.0696  bbox_loss: 0.0145  cls_loss: 0.0193  \n",
      "<<<iteration:[200/657] - total_loss: 0.2367  obj_loss: 0.1162  noobj_loss: 0.0551  bbox_loss: 0.0147  cls_loss: 0.0196  \n",
      "<<<iteration:[220/657] - total_loss: 0.2116  obj_loss: 0.0975  noobj_loss: 0.0582  bbox_loss: 0.0134  cls_loss: 0.0183  \n",
      "<<<iteration:[240/657] - total_loss: 0.2402  obj_loss: 0.1138  noobj_loss: 0.0565  bbox_loss: 0.0157  cls_loss: 0.0198  \n",
      "<<<iteration:[260/657] - total_loss: 0.2447  obj_loss: 0.1315  noobj_loss: 0.0635  bbox_loss: 0.0122  cls_loss: 0.0205  \n",
      "<<<iteration:[280/657] - total_loss: 0.2168  obj_loss: 0.0993  noobj_loss: 0.0553  bbox_loss: 0.0142  cls_loss: 0.0190  \n",
      "<<<iteration:[300/657] - total_loss: 0.2286  obj_loss: 0.1118  noobj_loss: 0.0458  bbox_loss: 0.0141  cls_loss: 0.0234  \n",
      "<<<iteration:[320/657] - total_loss: 0.2284  obj_loss: 0.1098  noobj_loss: 0.0604  bbox_loss: 0.0137  cls_loss: 0.0200  \n",
      "<<<iteration:[340/657] - total_loss: 0.2299  obj_loss: 0.1083  noobj_loss: 0.0628  bbox_loss: 0.0140  cls_loss: 0.0200  \n",
      "<<<iteration:[360/657] - total_loss: 0.2381  obj_loss: 0.1143  noobj_loss: 0.0575  bbox_loss: 0.0150  cls_loss: 0.0202  \n",
      "<<<iteration:[380/657] - total_loss: 0.2370  obj_loss: 0.1102  noobj_loss: 0.0603  bbox_loss: 0.0146  cls_loss: 0.0238  \n",
      "<<<iteration:[400/657] - total_loss: 0.2344  obj_loss: 0.1017  noobj_loss: 0.0550  bbox_loss: 0.0168  cls_loss: 0.0212  \n",
      "<<<iteration:[420/657] - total_loss: 0.2188  obj_loss: 0.1059  noobj_loss: 0.0605  bbox_loss: 0.0122  cls_loss: 0.0214  \n",
      "<<<iteration:[440/657] - total_loss: 0.2295  obj_loss: 0.1147  noobj_loss: 0.0495  bbox_loss: 0.0138  cls_loss: 0.0211  \n",
      "<<<iteration:[460/657] - total_loss: 0.2283  obj_loss: 0.0988  noobj_loss: 0.0531  bbox_loss: 0.0158  cls_loss: 0.0240  \n",
      "<<<iteration:[480/657] - total_loss: 0.2214  obj_loss: 0.1023  noobj_loss: 0.0615  bbox_loss: 0.0135  cls_loss: 0.0206  \n",
      "<<<iteration:[500/657] - total_loss: 0.2212  obj_loss: 0.1134  noobj_loss: 0.0586  bbox_loss: 0.0122  cls_loss: 0.0175  \n",
      "<<<iteration:[520/657] - total_loss: 0.2435  obj_loss: 0.1159  noobj_loss: 0.0544  bbox_loss: 0.0153  cls_loss: 0.0241  \n",
      "<<<iteration:[540/657] - total_loss: 0.2227  obj_loss: 0.0980  noobj_loss: 0.0576  bbox_loss: 0.0155  cls_loss: 0.0186  \n",
      "<<<iteration:[560/657] - total_loss: 0.2204  obj_loss: 0.1071  noobj_loss: 0.0576  bbox_loss: 0.0120  cls_loss: 0.0244  \n",
      "<<<iteration:[580/657] - total_loss: 0.2412  obj_loss: 0.1109  noobj_loss: 0.0578  bbox_loss: 0.0151  cls_loss: 0.0259  \n",
      "<<<iteration:[600/657] - total_loss: 0.2184  obj_loss: 0.1089  noobj_loss: 0.0534  bbox_loss: 0.0125  cls_loss: 0.0203  \n",
      "<<<iteration:[620/657] - total_loss: 0.2097  obj_loss: 0.0984  noobj_loss: 0.0528  bbox_loss: 0.0132  cls_loss: 0.0189  \n",
      "<<<iteration:[640/657] - total_loss: 0.2264  obj_loss: 0.1035  noobj_loss: 0.0596  bbox_loss: 0.0143  cls_loss: 0.0215  \n",
      "\n",
      "epoch:35/100 - Train Loss: 0.2286, Val Loss: 0.2719\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2349  obj_loss: 0.1127  noobj_loss: 0.0569  bbox_loss: 0.0136  cls_loss: 0.0260  \n",
      "<<<iteration:[40/657] - total_loss: 0.2382  obj_loss: 0.1112  noobj_loss: 0.0589  bbox_loss: 0.0156  cls_loss: 0.0197  \n",
      "<<<iteration:[60/657] - total_loss: 0.2198  obj_loss: 0.1067  noobj_loss: 0.0584  bbox_loss: 0.0125  cls_loss: 0.0211  \n",
      "<<<iteration:[80/657] - total_loss: 0.2196  obj_loss: 0.1012  noobj_loss: 0.0572  bbox_loss: 0.0140  cls_loss: 0.0199  \n",
      "<<<iteration:[100/657] - total_loss: 0.2163  obj_loss: 0.1042  noobj_loss: 0.0567  bbox_loss: 0.0133  cls_loss: 0.0170  \n",
      "<<<iteration:[120/657] - total_loss: 0.2221  obj_loss: 0.1063  noobj_loss: 0.0606  bbox_loss: 0.0137  cls_loss: 0.0172  \n",
      "<<<iteration:[140/657] - total_loss: 0.2302  obj_loss: 0.1144  noobj_loss: 0.0586  bbox_loss: 0.0133  cls_loss: 0.0203  \n",
      "<<<iteration:[160/657] - total_loss: 0.2332  obj_loss: 0.1035  noobj_loss: 0.0563  bbox_loss: 0.0154  cls_loss: 0.0244  \n",
      "<<<iteration:[180/657] - total_loss: 0.2200  obj_loss: 0.0995  noobj_loss: 0.0617  bbox_loss: 0.0135  cls_loss: 0.0219  \n",
      "<<<iteration:[200/657] - total_loss: 0.2227  obj_loss: 0.1126  noobj_loss: 0.0542  bbox_loss: 0.0123  cls_loss: 0.0215  \n",
      "<<<iteration:[220/657] - total_loss: 0.2211  obj_loss: 0.1034  noobj_loss: 0.0601  bbox_loss: 0.0130  cls_loss: 0.0228  \n",
      "<<<iteration:[240/657] - total_loss: 0.2384  obj_loss: 0.1207  noobj_loss: 0.0532  bbox_loss: 0.0137  cls_loss: 0.0227  \n",
      "<<<iteration:[260/657] - total_loss: 0.2110  obj_loss: 0.1023  noobj_loss: 0.0641  bbox_loss: 0.0118  cls_loss: 0.0174  \n",
      "<<<iteration:[280/657] - total_loss: 0.2286  obj_loss: 0.1151  noobj_loss: 0.0606  bbox_loss: 0.0126  cls_loss: 0.0203  \n",
      "<<<iteration:[300/657] - total_loss: 0.2247  obj_loss: 0.1104  noobj_loss: 0.0562  bbox_loss: 0.0133  cls_loss: 0.0198  \n",
      "<<<iteration:[320/657] - total_loss: 0.2150  obj_loss: 0.1045  noobj_loss: 0.0566  bbox_loss: 0.0124  cls_loss: 0.0200  \n",
      "<<<iteration:[340/657] - total_loss: 0.2346  obj_loss: 0.1157  noobj_loss: 0.0550  bbox_loss: 0.0140  cls_loss: 0.0214  \n",
      "<<<iteration:[360/657] - total_loss: 0.2181  obj_loss: 0.1077  noobj_loss: 0.0587  bbox_loss: 0.0128  cls_loss: 0.0170  \n",
      "<<<iteration:[380/657] - total_loss: 0.2311  obj_loss: 0.1212  noobj_loss: 0.0549  bbox_loss: 0.0126  cls_loss: 0.0194  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[400/657] - total_loss: 0.2119  obj_loss: 0.1095  noobj_loss: 0.0536  bbox_loss: 0.0114  cls_loss: 0.0183  \n",
      "<<<iteration:[420/657] - total_loss: 0.2302  obj_loss: 0.1001  noobj_loss: 0.0543  bbox_loss: 0.0168  cls_loss: 0.0188  \n",
      "<<<iteration:[440/657] - total_loss: 0.2408  obj_loss: 0.1269  noobj_loss: 0.0601  bbox_loss: 0.0120  cls_loss: 0.0238  \n",
      "<<<iteration:[460/657] - total_loss: 0.2332  obj_loss: 0.1101  noobj_loss: 0.0606  bbox_loss: 0.0145  cls_loss: 0.0204  \n",
      "<<<iteration:[480/657] - total_loss: 0.2195  obj_loss: 0.1030  noobj_loss: 0.0568  bbox_loss: 0.0133  cls_loss: 0.0214  \n",
      "<<<iteration:[500/657] - total_loss: 0.2371  obj_loss: 0.1045  noobj_loss: 0.0637  bbox_loss: 0.0155  cls_loss: 0.0232  \n",
      "<<<iteration:[520/657] - total_loss: 0.2435  obj_loss: 0.1138  noobj_loss: 0.0590  bbox_loss: 0.0154  cls_loss: 0.0234  \n",
      "<<<iteration:[540/657] - total_loss: 0.2254  obj_loss: 0.1188  noobj_loss: 0.0545  bbox_loss: 0.0120  cls_loss: 0.0191  \n",
      "<<<iteration:[560/657] - total_loss: 0.2289  obj_loss: 0.1042  noobj_loss: 0.0593  bbox_loss: 0.0153  cls_loss: 0.0188  \n",
      "<<<iteration:[580/657] - total_loss: 0.2220  obj_loss: 0.1045  noobj_loss: 0.0664  bbox_loss: 0.0128  cls_loss: 0.0203  \n",
      "<<<iteration:[600/657] - total_loss: 0.2340  obj_loss: 0.1111  noobj_loss: 0.0614  bbox_loss: 0.0133  cls_loss: 0.0256  \n",
      "<<<iteration:[620/657] - total_loss: 0.2356  obj_loss: 0.1061  noobj_loss: 0.0622  bbox_loss: 0.0154  cls_loss: 0.0213  \n",
      "<<<iteration:[640/657] - total_loss: 0.2261  obj_loss: 0.1072  noobj_loss: 0.0544  bbox_loss: 0.0143  cls_loss: 0.0202  \n",
      "\n",
      "epoch:36/100 - Train Loss: 0.2270, Val Loss: 0.2721\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2504  obj_loss: 0.1153  noobj_loss: 0.0609  bbox_loss: 0.0164  cls_loss: 0.0227  \n",
      "<<<iteration:[40/657] - total_loss: 0.2276  obj_loss: 0.1078  noobj_loss: 0.0573  bbox_loss: 0.0138  cls_loss: 0.0221  \n",
      "<<<iteration:[60/657] - total_loss: 0.2305  obj_loss: 0.1154  noobj_loss: 0.0600  bbox_loss: 0.0125  cls_loss: 0.0227  \n",
      "<<<iteration:[80/657] - total_loss: 0.2404  obj_loss: 0.1210  noobj_loss: 0.0670  bbox_loss: 0.0129  cls_loss: 0.0213  \n",
      "<<<iteration:[100/657] - total_loss: 0.2221  obj_loss: 0.1087  noobj_loss: 0.0656  bbox_loss: 0.0119  cls_loss: 0.0213  \n",
      "<<<iteration:[120/657] - total_loss: 0.2309  obj_loss: 0.1012  noobj_loss: 0.0716  bbox_loss: 0.0152  cls_loss: 0.0182  \n",
      "<<<iteration:[140/657] - total_loss: 0.2108  obj_loss: 0.0956  noobj_loss: 0.0558  bbox_loss: 0.0136  cls_loss: 0.0194  \n",
      "<<<iteration:[160/657] - total_loss: 0.2326  obj_loss: 0.1086  noobj_loss: 0.0561  bbox_loss: 0.0145  cls_loss: 0.0235  \n",
      "<<<iteration:[180/657] - total_loss: 0.2273  obj_loss: 0.1156  noobj_loss: 0.0604  bbox_loss: 0.0127  cls_loss: 0.0178  \n",
      "<<<iteration:[200/657] - total_loss: 0.2198  obj_loss: 0.1135  noobj_loss: 0.0654  bbox_loss: 0.0109  cls_loss: 0.0191  \n",
      "<<<iteration:[220/657] - total_loss: 0.2097  obj_loss: 0.1026  noobj_loss: 0.0576  bbox_loss: 0.0111  cls_loss: 0.0226  \n",
      "<<<iteration:[240/657] - total_loss: 0.2348  obj_loss: 0.1160  noobj_loss: 0.0621  bbox_loss: 0.0140  cls_loss: 0.0175  \n",
      "<<<iteration:[260/657] - total_loss: 0.2380  obj_loss: 0.1155  noobj_loss: 0.0589  bbox_loss: 0.0144  cls_loss: 0.0213  \n",
      "<<<iteration:[280/657] - total_loss: 0.2176  obj_loss: 0.1041  noobj_loss: 0.0622  bbox_loss: 0.0123  cls_loss: 0.0207  \n",
      "<<<iteration:[300/657] - total_loss: 0.2210  obj_loss: 0.1093  noobj_loss: 0.0573  bbox_loss: 0.0127  cls_loss: 0.0194  \n",
      "<<<iteration:[320/657] - total_loss: 0.2228  obj_loss: 0.1091  noobj_loss: 0.0552  bbox_loss: 0.0131  cls_loss: 0.0207  \n",
      "<<<iteration:[340/657] - total_loss: 0.2259  obj_loss: 0.1166  noobj_loss: 0.0562  bbox_loss: 0.0124  cls_loss: 0.0190  \n",
      "<<<iteration:[360/657] - total_loss: 0.2234  obj_loss: 0.1122  noobj_loss: 0.0557  bbox_loss: 0.0131  cls_loss: 0.0179  \n",
      "<<<iteration:[380/657] - total_loss: 0.2304  obj_loss: 0.1041  noobj_loss: 0.0628  bbox_loss: 0.0142  cls_loss: 0.0240  \n",
      "<<<iteration:[400/657] - total_loss: 0.2138  obj_loss: 0.1115  noobj_loss: 0.0566  bbox_loss: 0.0110  cls_loss: 0.0190  \n",
      "<<<iteration:[420/657] - total_loss: 0.2195  obj_loss: 0.1023  noobj_loss: 0.0595  bbox_loss: 0.0138  cls_loss: 0.0184  \n",
      "<<<iteration:[440/657] - total_loss: 0.2180  obj_loss: 0.1097  noobj_loss: 0.0548  bbox_loss: 0.0125  cls_loss: 0.0185  \n",
      "<<<iteration:[460/657] - total_loss: 0.2178  obj_loss: 0.1065  noobj_loss: 0.0596  bbox_loss: 0.0120  cls_loss: 0.0215  \n",
      "<<<iteration:[480/657] - total_loss: 0.2233  obj_loss: 0.1125  noobj_loss: 0.0538  bbox_loss: 0.0128  cls_loss: 0.0197  \n",
      "<<<iteration:[500/657] - total_loss: 0.2104  obj_loss: 0.0960  noobj_loss: 0.0615  bbox_loss: 0.0136  cls_loss: 0.0155  \n",
      "<<<iteration:[520/657] - total_loss: 0.2211  obj_loss: 0.1061  noobj_loss: 0.0564  bbox_loss: 0.0136  cls_loss: 0.0188  \n",
      "<<<iteration:[540/657] - total_loss: 0.2100  obj_loss: 0.1047  noobj_loss: 0.0584  bbox_loss: 0.0123  cls_loss: 0.0146  \n",
      "<<<iteration:[560/657] - total_loss: 0.2177  obj_loss: 0.1100  noobj_loss: 0.0555  bbox_loss: 0.0121  cls_loss: 0.0196  \n",
      "<<<iteration:[580/657] - total_loss: 0.2293  obj_loss: 0.1073  noobj_loss: 0.0671  bbox_loss: 0.0131  cls_loss: 0.0232  \n",
      "<<<iteration:[600/657] - total_loss: 0.2049  obj_loss: 0.0933  noobj_loss: 0.0557  bbox_loss: 0.0123  cls_loss: 0.0225  \n",
      "<<<iteration:[620/657] - total_loss: 0.2289  obj_loss: 0.1091  noobj_loss: 0.0621  bbox_loss: 0.0136  cls_loss: 0.0210  \n",
      "<<<iteration:[640/657] - total_loss: 0.2254  obj_loss: 0.1053  noobj_loss: 0.0515  bbox_loss: 0.0142  cls_loss: 0.0235  \n",
      "\n",
      "epoch:37/100 - Train Loss: 0.2232, Val Loss: 0.2702\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2379  obj_loss: 0.1163  noobj_loss: 0.0559  bbox_loss: 0.0144  cls_loss: 0.0214  \n",
      "<<<iteration:[40/657] - total_loss: 0.2318  obj_loss: 0.1061  noobj_loss: 0.0552  bbox_loss: 0.0157  cls_loss: 0.0195  \n",
      "<<<iteration:[60/657] - total_loss: 0.2205  obj_loss: 0.1076  noobj_loss: 0.0527  bbox_loss: 0.0133  cls_loss: 0.0200  \n",
      "<<<iteration:[80/657] - total_loss: 0.2191  obj_loss: 0.1028  noobj_loss: 0.0551  bbox_loss: 0.0140  cls_loss: 0.0190  \n",
      "<<<iteration:[100/657] - total_loss: 0.2103  obj_loss: 0.1048  noobj_loss: 0.0552  bbox_loss: 0.0119  cls_loss: 0.0183  \n",
      "<<<iteration:[120/657] - total_loss: 0.2220  obj_loss: 0.1102  noobj_loss: 0.0565  bbox_loss: 0.0131  cls_loss: 0.0182  \n",
      "<<<iteration:[140/657] - total_loss: 0.2304  obj_loss: 0.1066  noobj_loss: 0.0559  bbox_loss: 0.0145  cls_loss: 0.0236  \n",
      "<<<iteration:[160/657] - total_loss: 0.2058  obj_loss: 0.1025  noobj_loss: 0.0553  bbox_loss: 0.0112  cls_loss: 0.0195  \n",
      "<<<iteration:[180/657] - total_loss: 0.2210  obj_loss: 0.1106  noobj_loss: 0.0647  bbox_loss: 0.0118  cls_loss: 0.0188  \n",
      "<<<iteration:[200/657] - total_loss: 0.2112  obj_loss: 0.1011  noobj_loss: 0.0575  bbox_loss: 0.0126  cls_loss: 0.0181  \n",
      "<<<iteration:[220/657] - total_loss: 0.2405  obj_loss: 0.1168  noobj_loss: 0.0620  bbox_loss: 0.0139  cls_loss: 0.0234  \n",
      "<<<iteration:[240/657] - total_loss: 0.2307  obj_loss: 0.1041  noobj_loss: 0.0801  bbox_loss: 0.0137  cls_loss: 0.0179  \n",
      "<<<iteration:[260/657] - total_loss: 0.2268  obj_loss: 0.1144  noobj_loss: 0.0563  bbox_loss: 0.0133  cls_loss: 0.0180  \n",
      "<<<iteration:[280/657] - total_loss: 0.2204  obj_loss: 0.1154  noobj_loss: 0.0609  bbox_loss: 0.0112  cls_loss: 0.0187  \n",
      "<<<iteration:[300/657] - total_loss: 0.2109  obj_loss: 0.1072  noobj_loss: 0.0604  bbox_loss: 0.0111  cls_loss: 0.0181  \n",
      "<<<iteration:[320/657] - total_loss: 0.2118  obj_loss: 0.0949  noobj_loss: 0.0533  bbox_loss: 0.0139  cls_loss: 0.0207  \n",
      "<<<iteration:[340/657] - total_loss: 0.2192  obj_loss: 0.1134  noobj_loss: 0.0645  bbox_loss: 0.0117  cls_loss: 0.0152  \n",
      "<<<iteration:[360/657] - total_loss: 0.2202  obj_loss: 0.1038  noobj_loss: 0.0603  bbox_loss: 0.0133  cls_loss: 0.0200  \n",
      "<<<iteration:[380/657] - total_loss: 0.2257  obj_loss: 0.1098  noobj_loss: 0.0599  bbox_loss: 0.0122  cls_loss: 0.0252  \n",
      "<<<iteration:[400/657] - total_loss: 0.2240  obj_loss: 0.1161  noobj_loss: 0.0621  bbox_loss: 0.0112  cls_loss: 0.0210  \n",
      "<<<iteration:[420/657] - total_loss: 0.2389  obj_loss: 0.1165  noobj_loss: 0.0660  bbox_loss: 0.0132  cls_loss: 0.0236  \n",
      "<<<iteration:[440/657] - total_loss: 0.2144  obj_loss: 0.1041  noobj_loss: 0.0598  bbox_loss: 0.0123  cls_loss: 0.0188  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[460/657] - total_loss: 0.2001  obj_loss: 0.0988  noobj_loss: 0.0541  bbox_loss: 0.0113  cls_loss: 0.0180  \n",
      "<<<iteration:[480/657] - total_loss: 0.2315  obj_loss: 0.1005  noobj_loss: 0.0649  bbox_loss: 0.0154  cls_loss: 0.0213  \n",
      "<<<iteration:[500/657] - total_loss: 0.2335  obj_loss: 0.1175  noobj_loss: 0.0572  bbox_loss: 0.0136  cls_loss: 0.0195  \n",
      "<<<iteration:[520/657] - total_loss: 0.1998  obj_loss: 0.0963  noobj_loss: 0.0523  bbox_loss: 0.0117  cls_loss: 0.0188  \n",
      "<<<iteration:[540/657] - total_loss: 0.2219  obj_loss: 0.1070  noobj_loss: 0.0652  bbox_loss: 0.0130  cls_loss: 0.0173  \n",
      "<<<iteration:[560/657] - total_loss: 0.2233  obj_loss: 0.1042  noobj_loss: 0.0569  bbox_loss: 0.0140  cls_loss: 0.0208  \n",
      "<<<iteration:[580/657] - total_loss: 0.2078  obj_loss: 0.1017  noobj_loss: 0.0567  bbox_loss: 0.0118  cls_loss: 0.0188  \n",
      "<<<iteration:[600/657] - total_loss: 0.2231  obj_loss: 0.1068  noobj_loss: 0.0591  bbox_loss: 0.0139  cls_loss: 0.0172  \n",
      "<<<iteration:[620/657] - total_loss: 0.2080  obj_loss: 0.1001  noobj_loss: 0.0589  bbox_loss: 0.0115  cls_loss: 0.0212  \n",
      "<<<iteration:[640/657] - total_loss: 0.2236  obj_loss: 0.1022  noobj_loss: 0.0618  bbox_loss: 0.0141  cls_loss: 0.0200  \n",
      "\n",
      "epoch:38/100 - Train Loss: 0.2202, Val Loss: 0.2680\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2384  obj_loss: 0.1045  noobj_loss: 0.0638  bbox_loss: 0.0161  cls_loss: 0.0215  \n",
      "<<<iteration:[40/657] - total_loss: 0.2212  obj_loss: 0.1093  noobj_loss: 0.0630  bbox_loss: 0.0122  cls_loss: 0.0194  \n",
      "<<<iteration:[60/657] - total_loss: 0.2146  obj_loss: 0.1046  noobj_loss: 0.0568  bbox_loss: 0.0123  cls_loss: 0.0203  \n",
      "<<<iteration:[80/657] - total_loss: 0.2216  obj_loss: 0.1086  noobj_loss: 0.0580  bbox_loss: 0.0130  cls_loss: 0.0189  \n",
      "<<<iteration:[100/657] - total_loss: 0.2224  obj_loss: 0.1089  noobj_loss: 0.0622  bbox_loss: 0.0132  cls_loss: 0.0165  \n",
      "<<<iteration:[120/657] - total_loss: 0.2127  obj_loss: 0.1103  noobj_loss: 0.0600  bbox_loss: 0.0114  cls_loss: 0.0154  \n",
      "<<<iteration:[140/657] - total_loss: 0.2120  obj_loss: 0.1019  noobj_loss: 0.0564  bbox_loss: 0.0125  cls_loss: 0.0194  \n",
      "<<<iteration:[160/657] - total_loss: 0.2136  obj_loss: 0.0945  noobj_loss: 0.0608  bbox_loss: 0.0137  cls_loss: 0.0203  \n",
      "<<<iteration:[180/657] - total_loss: 0.2215  obj_loss: 0.1127  noobj_loss: 0.0625  bbox_loss: 0.0121  cls_loss: 0.0169  \n",
      "<<<iteration:[200/657] - total_loss: 0.2180  obj_loss: 0.1068  noobj_loss: 0.0561  bbox_loss: 0.0132  cls_loss: 0.0171  \n",
      "<<<iteration:[220/657] - total_loss: 0.2222  obj_loss: 0.1027  noobj_loss: 0.0586  bbox_loss: 0.0133  cls_loss: 0.0238  \n",
      "<<<iteration:[240/657] - total_loss: 0.2267  obj_loss: 0.1061  noobj_loss: 0.0563  bbox_loss: 0.0142  cls_loss: 0.0216  \n",
      "<<<iteration:[260/657] - total_loss: 0.2185  obj_loss: 0.1052  noobj_loss: 0.0615  bbox_loss: 0.0127  cls_loss: 0.0190  \n",
      "<<<iteration:[280/657] - total_loss: 0.2093  obj_loss: 0.0992  noobj_loss: 0.0556  bbox_loss: 0.0125  cls_loss: 0.0199  \n",
      "<<<iteration:[300/657] - total_loss: 0.2211  obj_loss: 0.1061  noobj_loss: 0.0604  bbox_loss: 0.0133  cls_loss: 0.0182  \n",
      "<<<iteration:[320/657] - total_loss: 0.2292  obj_loss: 0.1125  noobj_loss: 0.0587  bbox_loss: 0.0138  cls_loss: 0.0183  \n",
      "<<<iteration:[340/657] - total_loss: 0.2174  obj_loss: 0.1048  noobj_loss: 0.0579  bbox_loss: 0.0131  cls_loss: 0.0179  \n",
      "<<<iteration:[360/657] - total_loss: 0.2243  obj_loss: 0.1169  noobj_loss: 0.0668  bbox_loss: 0.0109  cls_loss: 0.0193  \n",
      "<<<iteration:[380/657] - total_loss: 0.2091  obj_loss: 0.1015  noobj_loss: 0.0593  bbox_loss: 0.0119  cls_loss: 0.0182  \n",
      "<<<iteration:[400/657] - total_loss: 0.2233  obj_loss: 0.1082  noobj_loss: 0.0588  bbox_loss: 0.0137  cls_loss: 0.0172  \n",
      "<<<iteration:[420/657] - total_loss: 0.2270  obj_loss: 0.1063  noobj_loss: 0.0650  bbox_loss: 0.0129  cls_loss: 0.0238  \n",
      "<<<iteration:[440/657] - total_loss: 0.2201  obj_loss: 0.1014  noobj_loss: 0.0544  bbox_loss: 0.0143  cls_loss: 0.0201  \n",
      "<<<iteration:[460/657] - total_loss: 0.2167  obj_loss: 0.1049  noobj_loss: 0.0590  bbox_loss: 0.0127  cls_loss: 0.0188  \n",
      "<<<iteration:[480/657] - total_loss: 0.2250  obj_loss: 0.1096  noobj_loss: 0.0602  bbox_loss: 0.0126  cls_loss: 0.0222  \n",
      "<<<iteration:[500/657] - total_loss: 0.2227  obj_loss: 0.1081  noobj_loss: 0.0563  bbox_loss: 0.0131  cls_loss: 0.0210  \n",
      "<<<iteration:[520/657] - total_loss: 0.2092  obj_loss: 0.1028  noobj_loss: 0.0626  bbox_loss: 0.0112  cls_loss: 0.0189  \n",
      "<<<iteration:[540/657] - total_loss: 0.2162  obj_loss: 0.1042  noobj_loss: 0.0586  bbox_loss: 0.0134  cls_loss: 0.0160  \n",
      "<<<iteration:[560/657] - total_loss: 0.2326  obj_loss: 0.1149  noobj_loss: 0.0645  bbox_loss: 0.0129  cls_loss: 0.0208  \n",
      "<<<iteration:[580/657] - total_loss: 0.2107  obj_loss: 0.1015  noobj_loss: 0.0582  bbox_loss: 0.0126  cls_loss: 0.0171  \n",
      "<<<iteration:[600/657] - total_loss: 0.2137  obj_loss: 0.1093  noobj_loss: 0.0585  bbox_loss: 0.0107  cls_loss: 0.0216  \n",
      "<<<iteration:[620/657] - total_loss: 0.2257  obj_loss: 0.1164  noobj_loss: 0.0647  bbox_loss: 0.0117  cls_loss: 0.0187  \n",
      "<<<iteration:[640/657] - total_loss: 0.2157  obj_loss: 0.0990  noobj_loss: 0.0579  bbox_loss: 0.0145  cls_loss: 0.0151  \n",
      "\n",
      "epoch:39/100 - Train Loss: 0.2196, Val Loss: 0.2635\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2364  obj_loss: 0.1039  noobj_loss: 0.0649  bbox_loss: 0.0158  cls_loss: 0.0213  \n",
      "<<<iteration:[40/657] - total_loss: 0.2245  obj_loss: 0.1135  noobj_loss: 0.0609  bbox_loss: 0.0121  cls_loss: 0.0199  \n",
      "<<<iteration:[60/657] - total_loss: 0.2238  obj_loss: 0.1008  noobj_loss: 0.0630  bbox_loss: 0.0146  cls_loss: 0.0187  \n",
      "<<<iteration:[80/657] - total_loss: 0.2130  obj_loss: 0.0978  noobj_loss: 0.0517  bbox_loss: 0.0140  cls_loss: 0.0194  \n",
      "<<<iteration:[100/657] - total_loss: 0.2333  obj_loss: 0.1112  noobj_loss: 0.0649  bbox_loss: 0.0135  cls_loss: 0.0222  \n",
      "<<<iteration:[120/657] - total_loss: 0.2104  obj_loss: 0.0971  noobj_loss: 0.0618  bbox_loss: 0.0127  cls_loss: 0.0187  \n",
      "<<<iteration:[140/657] - total_loss: 0.2167  obj_loss: 0.1044  noobj_loss: 0.0579  bbox_loss: 0.0126  cls_loss: 0.0204  \n",
      "<<<iteration:[160/657] - total_loss: 0.2092  obj_loss: 0.1020  noobj_loss: 0.0534  bbox_loss: 0.0126  cls_loss: 0.0175  \n",
      "<<<iteration:[180/657] - total_loss: 0.1986  obj_loss: 0.0955  noobj_loss: 0.0595  bbox_loss: 0.0106  cls_loss: 0.0201  \n",
      "<<<iteration:[200/657] - total_loss: 0.2087  obj_loss: 0.1019  noobj_loss: 0.0590  bbox_loss: 0.0120  cls_loss: 0.0171  \n",
      "<<<iteration:[220/657] - total_loss: 0.2022  obj_loss: 0.1043  noobj_loss: 0.0539  bbox_loss: 0.0106  cls_loss: 0.0178  \n",
      "<<<iteration:[240/657] - total_loss: 0.2169  obj_loss: 0.1113  noobj_loss: 0.0587  bbox_loss: 0.0115  cls_loss: 0.0185  \n",
      "<<<iteration:[260/657] - total_loss: 0.2289  obj_loss: 0.1111  noobj_loss: 0.0582  bbox_loss: 0.0138  cls_loss: 0.0198  \n",
      "<<<iteration:[280/657] - total_loss: 0.2201  obj_loss: 0.1088  noobj_loss: 0.0582  bbox_loss: 0.0124  cls_loss: 0.0202  \n",
      "<<<iteration:[300/657] - total_loss: 0.2202  obj_loss: 0.1070  noobj_loss: 0.0562  bbox_loss: 0.0128  cls_loss: 0.0212  \n",
      "<<<iteration:[320/657] - total_loss: 0.2075  obj_loss: 0.1048  noobj_loss: 0.0645  bbox_loss: 0.0107  cls_loss: 0.0169  \n",
      "<<<iteration:[340/657] - total_loss: 0.2181  obj_loss: 0.1109  noobj_loss: 0.0556  bbox_loss: 0.0121  cls_loss: 0.0190  \n",
      "<<<iteration:[360/657] - total_loss: 0.2264  obj_loss: 0.1031  noobj_loss: 0.0721  bbox_loss: 0.0129  cls_loss: 0.0227  \n",
      "<<<iteration:[380/657] - total_loss: 0.2167  obj_loss: 0.1077  noobj_loss: 0.0617  bbox_loss: 0.0119  cls_loss: 0.0184  \n",
      "<<<iteration:[400/657] - total_loss: 0.2024  obj_loss: 0.0928  noobj_loss: 0.0555  bbox_loss: 0.0127  cls_loss: 0.0183  \n",
      "<<<iteration:[420/657] - total_loss: 0.2244  obj_loss: 0.1132  noobj_loss: 0.0626  bbox_loss: 0.0123  cls_loss: 0.0184  \n",
      "<<<iteration:[440/657] - total_loss: 0.2214  obj_loss: 0.1077  noobj_loss: 0.0646  bbox_loss: 0.0128  cls_loss: 0.0172  \n",
      "<<<iteration:[460/657] - total_loss: 0.2195  obj_loss: 0.1012  noobj_loss: 0.0640  bbox_loss: 0.0137  cls_loss: 0.0175  \n",
      "<<<iteration:[480/657] - total_loss: 0.2117  obj_loss: 0.1092  noobj_loss: 0.0622  bbox_loss: 0.0108  cls_loss: 0.0176  \n",
      "<<<iteration:[500/657] - total_loss: 0.2221  obj_loss: 0.1046  noobj_loss: 0.0565  bbox_loss: 0.0127  cls_loss: 0.0258  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[520/657] - total_loss: 0.2175  obj_loss: 0.1042  noobj_loss: 0.0616  bbox_loss: 0.0126  cls_loss: 0.0193  \n",
      "<<<iteration:[540/657] - total_loss: 0.2170  obj_loss: 0.1009  noobj_loss: 0.0630  bbox_loss: 0.0129  cls_loss: 0.0199  \n",
      "<<<iteration:[560/657] - total_loss: 0.2291  obj_loss: 0.1203  noobj_loss: 0.0548  bbox_loss: 0.0116  cls_loss: 0.0233  \n",
      "<<<iteration:[580/657] - total_loss: 0.2193  obj_loss: 0.1065  noobj_loss: 0.0606  bbox_loss: 0.0127  cls_loss: 0.0188  \n",
      "<<<iteration:[600/657] - total_loss: 0.2280  obj_loss: 0.1074  noobj_loss: 0.0623  bbox_loss: 0.0143  cls_loss: 0.0180  \n",
      "<<<iteration:[620/657] - total_loss: 0.2146  obj_loss: 0.0955  noobj_loss: 0.0670  bbox_loss: 0.0137  cls_loss: 0.0173  \n",
      "<<<iteration:[640/657] - total_loss: 0.1988  obj_loss: 0.0979  noobj_loss: 0.0635  bbox_loss: 0.0103  cls_loss: 0.0176  \n",
      "\n",
      "epoch:40/100 - Train Loss: 0.2174, Val Loss: 0.2617\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2278  obj_loss: 0.1141  noobj_loss: 0.0660  bbox_loss: 0.0122  cls_loss: 0.0195  \n",
      "<<<iteration:[40/657] - total_loss: 0.2288  obj_loss: 0.1123  noobj_loss: 0.0578  bbox_loss: 0.0135  cls_loss: 0.0203  \n",
      "<<<iteration:[60/657] - total_loss: 0.2144  obj_loss: 0.1027  noobj_loss: 0.0653  bbox_loss: 0.0123  cls_loss: 0.0176  \n",
      "<<<iteration:[80/657] - total_loss: 0.2120  obj_loss: 0.1069  noobj_loss: 0.0558  bbox_loss: 0.0113  cls_loss: 0.0207  \n",
      "<<<iteration:[100/657] - total_loss: 0.2260  obj_loss: 0.1104  noobj_loss: 0.0666  bbox_loss: 0.0128  cls_loss: 0.0181  \n",
      "<<<iteration:[120/657] - total_loss: 0.2250  obj_loss: 0.1124  noobj_loss: 0.0569  bbox_loss: 0.0134  cls_loss: 0.0174  \n",
      "<<<iteration:[140/657] - total_loss: 0.2176  obj_loss: 0.1058  noobj_loss: 0.0597  bbox_loss: 0.0127  cls_loss: 0.0182  \n",
      "<<<iteration:[160/657] - total_loss: 0.1978  obj_loss: 0.0975  noobj_loss: 0.0620  bbox_loss: 0.0104  cls_loss: 0.0173  \n",
      "<<<iteration:[180/657] - total_loss: 0.2154  obj_loss: 0.1081  noobj_loss: 0.0599  bbox_loss: 0.0122  cls_loss: 0.0166  \n",
      "<<<iteration:[200/657] - total_loss: 0.2117  obj_loss: 0.1016  noobj_loss: 0.0629  bbox_loss: 0.0123  cls_loss: 0.0169  \n",
      "<<<iteration:[220/657] - total_loss: 0.2146  obj_loss: 0.1100  noobj_loss: 0.0598  bbox_loss: 0.0107  cls_loss: 0.0212  \n",
      "<<<iteration:[240/657] - total_loss: 0.2243  obj_loss: 0.1161  noobj_loss: 0.0512  bbox_loss: 0.0131  cls_loss: 0.0170  \n",
      "<<<iteration:[260/657] - total_loss: 0.2018  obj_loss: 0.0993  noobj_loss: 0.0538  bbox_loss: 0.0118  cls_loss: 0.0164  \n",
      "<<<iteration:[280/657] - total_loss: 0.2071  obj_loss: 0.0942  noobj_loss: 0.0626  bbox_loss: 0.0127  cls_loss: 0.0182  \n",
      "<<<iteration:[300/657] - total_loss: 0.2090  obj_loss: 0.1002  noobj_loss: 0.0518  bbox_loss: 0.0127  cls_loss: 0.0195  \n",
      "<<<iteration:[320/657] - total_loss: 0.2079  obj_loss: 0.1009  noobj_loss: 0.0608  bbox_loss: 0.0120  cls_loss: 0.0169  \n",
      "<<<iteration:[340/657] - total_loss: 0.2271  obj_loss: 0.1094  noobj_loss: 0.0598  bbox_loss: 0.0133  cls_loss: 0.0211  \n",
      "<<<iteration:[360/657] - total_loss: 0.2020  obj_loss: 0.0958  noobj_loss: 0.0567  bbox_loss: 0.0118  cls_loss: 0.0188  \n",
      "<<<iteration:[380/657] - total_loss: 0.2354  obj_loss: 0.1179  noobj_loss: 0.0620  bbox_loss: 0.0138  cls_loss: 0.0177  \n",
      "<<<iteration:[400/657] - total_loss: 0.2214  obj_loss: 0.1066  noobj_loss: 0.0591  bbox_loss: 0.0125  cls_loss: 0.0229  \n",
      "<<<iteration:[420/657] - total_loss: 0.2021  obj_loss: 0.0870  noobj_loss: 0.0630  bbox_loss: 0.0129  cls_loss: 0.0193  \n",
      "<<<iteration:[440/657] - total_loss: 0.2162  obj_loss: 0.1046  noobj_loss: 0.0616  bbox_loss: 0.0124  cls_loss: 0.0190  \n",
      "<<<iteration:[460/657] - total_loss: 0.2152  obj_loss: 0.1057  noobj_loss: 0.0563  bbox_loss: 0.0133  cls_loss: 0.0150  \n",
      "<<<iteration:[480/657] - total_loss: 0.2151  obj_loss: 0.1093  noobj_loss: 0.0686  bbox_loss: 0.0108  cls_loss: 0.0174  \n",
      "<<<iteration:[500/657] - total_loss: 0.2213  obj_loss: 0.1121  noobj_loss: 0.0556  bbox_loss: 0.0129  cls_loss: 0.0170  \n",
      "<<<iteration:[520/657] - total_loss: 0.2120  obj_loss: 0.1101  noobj_loss: 0.0576  bbox_loss: 0.0113  cls_loss: 0.0165  \n",
      "<<<iteration:[540/657] - total_loss: 0.2175  obj_loss: 0.1007  noobj_loss: 0.0610  bbox_loss: 0.0136  cls_loss: 0.0182  \n",
      "<<<iteration:[560/657] - total_loss: 0.2182  obj_loss: 0.1013  noobj_loss: 0.0628  bbox_loss: 0.0129  cls_loss: 0.0209  \n",
      "<<<iteration:[580/657] - total_loss: 0.2154  obj_loss: 0.1078  noobj_loss: 0.0603  bbox_loss: 0.0119  cls_loss: 0.0180  \n",
      "<<<iteration:[600/657] - total_loss: 0.1977  obj_loss: 0.0962  noobj_loss: 0.0555  bbox_loss: 0.0117  cls_loss: 0.0150  \n",
      "<<<iteration:[620/657] - total_loss: 0.2123  obj_loss: 0.1006  noobj_loss: 0.0607  bbox_loss: 0.0130  cls_loss: 0.0162  \n",
      "<<<iteration:[640/657] - total_loss: 0.2260  obj_loss: 0.1094  noobj_loss: 0.0575  bbox_loss: 0.0134  cls_loss: 0.0208  \n",
      "\n",
      "epoch:41/100 - Train Loss: 0.2157, Val Loss: 0.2568\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2012  obj_loss: 0.0994  noobj_loss: 0.0520  bbox_loss: 0.0123  cls_loss: 0.0143  \n",
      "<<<iteration:[40/657] - total_loss: 0.2195  obj_loss: 0.1077  noobj_loss: 0.0679  bbox_loss: 0.0121  cls_loss: 0.0173  \n",
      "<<<iteration:[60/657] - total_loss: 0.2196  obj_loss: 0.1127  noobj_loss: 0.0651  bbox_loss: 0.0112  cls_loss: 0.0187  \n",
      "<<<iteration:[80/657] - total_loss: 0.2294  obj_loss: 0.1146  noobj_loss: 0.0586  bbox_loss: 0.0134  cls_loss: 0.0185  \n",
      "<<<iteration:[100/657] - total_loss: 0.2172  obj_loss: 0.1057  noobj_loss: 0.0649  bbox_loss: 0.0119  cls_loss: 0.0194  \n",
      "<<<iteration:[120/657] - total_loss: 0.2257  obj_loss: 0.1031  noobj_loss: 0.0676  bbox_loss: 0.0140  cls_loss: 0.0185  \n",
      "<<<iteration:[140/657] - total_loss: 0.2013  obj_loss: 0.1014  noobj_loss: 0.0585  bbox_loss: 0.0107  cls_loss: 0.0173  \n",
      "<<<iteration:[160/657] - total_loss: 0.2144  obj_loss: 0.1074  noobj_loss: 0.0598  bbox_loss: 0.0122  cls_loss: 0.0164  \n",
      "<<<iteration:[180/657] - total_loss: 0.2145  obj_loss: 0.0954  noobj_loss: 0.0594  bbox_loss: 0.0137  cls_loss: 0.0208  \n",
      "<<<iteration:[200/657] - total_loss: 0.2157  obj_loss: 0.1096  noobj_loss: 0.0534  bbox_loss: 0.0121  cls_loss: 0.0191  \n",
      "<<<iteration:[220/657] - total_loss: 0.2221  obj_loss: 0.1122  noobj_loss: 0.0554  bbox_loss: 0.0130  cls_loss: 0.0171  \n",
      "<<<iteration:[240/657] - total_loss: 0.2183  obj_loss: 0.1072  noobj_loss: 0.0576  bbox_loss: 0.0121  cls_loss: 0.0217  \n",
      "<<<iteration:[260/657] - total_loss: 0.2164  obj_loss: 0.1005  noobj_loss: 0.0586  bbox_loss: 0.0135  cls_loss: 0.0189  \n",
      "<<<iteration:[280/657] - total_loss: 0.1940  obj_loss: 0.0949  noobj_loss: 0.0578  bbox_loss: 0.0107  cls_loss: 0.0168  \n",
      "<<<iteration:[300/657] - total_loss: 0.2255  obj_loss: 0.1031  noobj_loss: 0.0631  bbox_loss: 0.0147  cls_loss: 0.0173  \n",
      "<<<iteration:[320/657] - total_loss: 0.2064  obj_loss: 0.0987  noobj_loss: 0.0611  bbox_loss: 0.0124  cls_loss: 0.0153  \n",
      "<<<iteration:[340/657] - total_loss: 0.2093  obj_loss: 0.1028  noobj_loss: 0.0694  bbox_loss: 0.0111  cls_loss: 0.0162  \n",
      "<<<iteration:[360/657] - total_loss: 0.2115  obj_loss: 0.1049  noobj_loss: 0.0656  bbox_loss: 0.0106  cls_loss: 0.0205  \n",
      "<<<iteration:[380/657] - total_loss: 0.2059  obj_loss: 0.1078  noobj_loss: 0.0569  bbox_loss: 0.0102  cls_loss: 0.0186  \n",
      "<<<iteration:[400/657] - total_loss: 0.2084  obj_loss: 0.0975  noobj_loss: 0.0584  bbox_loss: 0.0127  cls_loss: 0.0180  \n",
      "<<<iteration:[420/657] - total_loss: 0.2059  obj_loss: 0.0928  noobj_loss: 0.0607  bbox_loss: 0.0128  cls_loss: 0.0187  \n",
      "<<<iteration:[440/657] - total_loss: 0.1946  obj_loss: 0.0873  noobj_loss: 0.0600  bbox_loss: 0.0122  cls_loss: 0.0163  \n",
      "<<<iteration:[460/657] - total_loss: 0.2204  obj_loss: 0.1114  noobj_loss: 0.0610  bbox_loss: 0.0123  cls_loss: 0.0171  \n",
      "<<<iteration:[480/657] - total_loss: 0.2092  obj_loss: 0.1088  noobj_loss: 0.0618  bbox_loss: 0.0107  cls_loss: 0.0159  \n",
      "<<<iteration:[500/657] - total_loss: 0.2035  obj_loss: 0.1005  noobj_loss: 0.0620  bbox_loss: 0.0113  cls_loss: 0.0155  \n",
      "<<<iteration:[520/657] - total_loss: 0.2126  obj_loss: 0.0947  noobj_loss: 0.0618  bbox_loss: 0.0139  cls_loss: 0.0176  \n",
      "<<<iteration:[540/657] - total_loss: 0.2229  obj_loss: 0.1098  noobj_loss: 0.0603  bbox_loss: 0.0129  cls_loss: 0.0186  \n",
      "<<<iteration:[560/657] - total_loss: 0.2142  obj_loss: 0.1057  noobj_loss: 0.0577  bbox_loss: 0.0126  cls_loss: 0.0169  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[580/657] - total_loss: 0.2086  obj_loss: 0.0918  noobj_loss: 0.0561  bbox_loss: 0.0138  cls_loss: 0.0195  \n",
      "<<<iteration:[600/657] - total_loss: 0.2157  obj_loss: 0.1080  noobj_loss: 0.0594  bbox_loss: 0.0122  cls_loss: 0.0171  \n",
      "<<<iteration:[620/657] - total_loss: 0.2166  obj_loss: 0.1001  noobj_loss: 0.0662  bbox_loss: 0.0128  cls_loss: 0.0194  \n",
      "<<<iteration:[640/657] - total_loss: 0.2100  obj_loss: 0.0978  noobj_loss: 0.0505  bbox_loss: 0.0137  cls_loss: 0.0181  \n",
      "\n",
      "epoch:42/100 - Train Loss: 0.2124, Val Loss: 0.2575\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2285  obj_loss: 0.1041  noobj_loss: 0.0563  bbox_loss: 0.0156  cls_loss: 0.0182  \n",
      "<<<iteration:[40/657] - total_loss: 0.2347  obj_loss: 0.1070  noobj_loss: 0.0541  bbox_loss: 0.0162  cls_loss: 0.0198  \n",
      "<<<iteration:[60/657] - total_loss: 0.2087  obj_loss: 0.1074  noobj_loss: 0.0586  bbox_loss: 0.0111  cls_loss: 0.0163  \n",
      "<<<iteration:[80/657] - total_loss: 0.2092  obj_loss: 0.1064  noobj_loss: 0.0608  bbox_loss: 0.0116  cls_loss: 0.0147  \n",
      "<<<iteration:[100/657] - total_loss: 0.2157  obj_loss: 0.1107  noobj_loss: 0.0574  bbox_loss: 0.0121  cls_loss: 0.0158  \n",
      "<<<iteration:[120/657] - total_loss: 0.2084  obj_loss: 0.0965  noobj_loss: 0.0587  bbox_loss: 0.0127  cls_loss: 0.0192  \n",
      "<<<iteration:[140/657] - total_loss: 0.2303  obj_loss: 0.1048  noobj_loss: 0.0617  bbox_loss: 0.0153  cls_loss: 0.0181  \n",
      "<<<iteration:[160/657] - total_loss: 0.2261  obj_loss: 0.1116  noobj_loss: 0.0720  bbox_loss: 0.0122  cls_loss: 0.0174  \n",
      "<<<iteration:[180/657] - total_loss: 0.2013  obj_loss: 0.0989  noobj_loss: 0.0604  bbox_loss: 0.0116  cls_loss: 0.0144  \n",
      "<<<iteration:[200/657] - total_loss: 0.2110  obj_loss: 0.1051  noobj_loss: 0.0538  bbox_loss: 0.0113  cls_loss: 0.0224  \n",
      "<<<iteration:[220/657] - total_loss: 0.2171  obj_loss: 0.1067  noobj_loss: 0.0679  bbox_loss: 0.0115  cls_loss: 0.0191  \n",
      "<<<iteration:[240/657] - total_loss: 0.2304  obj_loss: 0.1086  noobj_loss: 0.0646  bbox_loss: 0.0143  cls_loss: 0.0180  \n",
      "<<<iteration:[260/657] - total_loss: 0.2223  obj_loss: 0.1126  noobj_loss: 0.0623  bbox_loss: 0.0124  cls_loss: 0.0167  \n",
      "<<<iteration:[280/657] - total_loss: 0.1996  obj_loss: 0.1013  noobj_loss: 0.0597  bbox_loss: 0.0108  cls_loss: 0.0145  \n",
      "<<<iteration:[300/657] - total_loss: 0.2057  obj_loss: 0.1031  noobj_loss: 0.0580  bbox_loss: 0.0110  cls_loss: 0.0184  \n",
      "<<<iteration:[320/657] - total_loss: 0.2252  obj_loss: 0.1032  noobj_loss: 0.0676  bbox_loss: 0.0139  cls_loss: 0.0189  \n",
      "<<<iteration:[340/657] - total_loss: 0.2169  obj_loss: 0.1024  noobj_loss: 0.0556  bbox_loss: 0.0137  cls_loss: 0.0184  \n",
      "<<<iteration:[360/657] - total_loss: 0.2064  obj_loss: 0.1031  noobj_loss: 0.0608  bbox_loss: 0.0111  cls_loss: 0.0174  \n",
      "<<<iteration:[380/657] - total_loss: 0.2127  obj_loss: 0.1089  noobj_loss: 0.0602  bbox_loss: 0.0111  cls_loss: 0.0179  \n",
      "<<<iteration:[400/657] - total_loss: 0.2138  obj_loss: 0.1076  noobj_loss: 0.0642  bbox_loss: 0.0112  cls_loss: 0.0179  \n",
      "<<<iteration:[420/657] - total_loss: 0.2227  obj_loss: 0.1122  noobj_loss: 0.0578  bbox_loss: 0.0122  cls_loss: 0.0206  \n",
      "<<<iteration:[440/657] - total_loss: 0.2047  obj_loss: 0.1001  noobj_loss: 0.0615  bbox_loss: 0.0114  cls_loss: 0.0166  \n",
      "<<<iteration:[460/657] - total_loss: 0.2257  obj_loss: 0.1105  noobj_loss: 0.0635  bbox_loss: 0.0127  cls_loss: 0.0197  \n",
      "<<<iteration:[480/657] - total_loss: 0.2058  obj_loss: 0.1054  noobj_loss: 0.0628  bbox_loss: 0.0106  cls_loss: 0.0159  \n",
      "<<<iteration:[500/657] - total_loss: 0.2172  obj_loss: 0.1035  noobj_loss: 0.0609  bbox_loss: 0.0129  cls_loss: 0.0185  \n",
      "<<<iteration:[520/657] - total_loss: 0.2173  obj_loss: 0.1060  noobj_loss: 0.0588  bbox_loss: 0.0126  cls_loss: 0.0188  \n",
      "<<<iteration:[540/657] - total_loss: 0.2019  obj_loss: 0.0970  noobj_loss: 0.0541  bbox_loss: 0.0119  cls_loss: 0.0185  \n",
      "<<<iteration:[560/657] - total_loss: 0.2097  obj_loss: 0.1004  noobj_loss: 0.0582  bbox_loss: 0.0123  cls_loss: 0.0189  \n",
      "<<<iteration:[580/657] - total_loss: 0.1990  obj_loss: 0.0980  noobj_loss: 0.0609  bbox_loss: 0.0109  cls_loss: 0.0158  \n",
      "<<<iteration:[600/657] - total_loss: 0.1949  obj_loss: 0.0938  noobj_loss: 0.0594  bbox_loss: 0.0113  cls_loss: 0.0150  \n",
      "<<<iteration:[620/657] - total_loss: 0.2125  obj_loss: 0.0951  noobj_loss: 0.0586  bbox_loss: 0.0148  cls_loss: 0.0141  \n",
      "<<<iteration:[640/657] - total_loss: 0.2005  obj_loss: 0.0944  noobj_loss: 0.0610  bbox_loss: 0.0119  cls_loss: 0.0160  \n",
      "\n",
      "epoch:43/100 - Train Loss: 0.2134, Val Loss: 0.2544\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2310  obj_loss: 0.1111  noobj_loss: 0.0604  bbox_loss: 0.0144  cls_loss: 0.0175  \n",
      "<<<iteration:[40/657] - total_loss: 0.2129  obj_loss: 0.1051  noobj_loss: 0.0591  bbox_loss: 0.0124  cls_loss: 0.0160  \n",
      "<<<iteration:[60/657] - total_loss: 0.1982  obj_loss: 0.0972  noobj_loss: 0.0585  bbox_loss: 0.0116  cls_loss: 0.0136  \n",
      "<<<iteration:[80/657] - total_loss: 0.2060  obj_loss: 0.0936  noobj_loss: 0.0560  bbox_loss: 0.0134  cls_loss: 0.0172  \n",
      "<<<iteration:[100/657] - total_loss: 0.2040  obj_loss: 0.1076  noobj_loss: 0.0587  bbox_loss: 0.0109  cls_loss: 0.0127  \n",
      "<<<iteration:[120/657] - total_loss: 0.1992  obj_loss: 0.0951  noobj_loss: 0.0599  bbox_loss: 0.0112  cls_loss: 0.0182  \n",
      "<<<iteration:[140/657] - total_loss: 0.2228  obj_loss: 0.1060  noobj_loss: 0.0630  bbox_loss: 0.0137  cls_loss: 0.0168  \n",
      "<<<iteration:[160/657] - total_loss: 0.2070  obj_loss: 0.0994  noobj_loss: 0.0552  bbox_loss: 0.0130  cls_loss: 0.0152  \n",
      "<<<iteration:[180/657] - total_loss: 0.2064  obj_loss: 0.0956  noobj_loss: 0.0604  bbox_loss: 0.0127  cls_loss: 0.0172  \n",
      "<<<iteration:[200/657] - total_loss: 0.2224  obj_loss: 0.1089  noobj_loss: 0.0542  bbox_loss: 0.0137  cls_loss: 0.0179  \n",
      "<<<iteration:[220/657] - total_loss: 0.1996  obj_loss: 0.0983  noobj_loss: 0.0638  bbox_loss: 0.0109  cls_loss: 0.0147  \n",
      "<<<iteration:[240/657] - total_loss: 0.2237  obj_loss: 0.1056  noobj_loss: 0.0620  bbox_loss: 0.0131  cls_loss: 0.0215  \n",
      "<<<iteration:[260/657] - total_loss: 0.2111  obj_loss: 0.1137  noobj_loss: 0.0528  bbox_loss: 0.0109  cls_loss: 0.0167  \n",
      "<<<iteration:[280/657] - total_loss: 0.2133  obj_loss: 0.1036  noobj_loss: 0.0669  bbox_loss: 0.0118  cls_loss: 0.0175  \n",
      "<<<iteration:[300/657] - total_loss: 0.1945  obj_loss: 0.1002  noobj_loss: 0.0566  bbox_loss: 0.0099  cls_loss: 0.0166  \n",
      "<<<iteration:[320/657] - total_loss: 0.2101  obj_loss: 0.1048  noobj_loss: 0.0622  bbox_loss: 0.0113  cls_loss: 0.0176  \n",
      "<<<iteration:[340/657] - total_loss: 0.2065  obj_loss: 0.1047  noobj_loss: 0.0673  bbox_loss: 0.0104  cls_loss: 0.0163  \n",
      "<<<iteration:[360/657] - total_loss: 0.2089  obj_loss: 0.1029  noobj_loss: 0.0581  bbox_loss: 0.0121  cls_loss: 0.0166  \n",
      "<<<iteration:[380/657] - total_loss: 0.2026  obj_loss: 0.0923  noobj_loss: 0.0661  bbox_loss: 0.0116  cls_loss: 0.0192  \n",
      "<<<iteration:[400/657] - total_loss: 0.2078  obj_loss: 0.1002  noobj_loss: 0.0585  bbox_loss: 0.0126  cls_loss: 0.0151  \n",
      "<<<iteration:[420/657] - total_loss: 0.1958  obj_loss: 0.0891  noobj_loss: 0.0583  bbox_loss: 0.0118  cls_loss: 0.0185  \n",
      "<<<iteration:[440/657] - total_loss: 0.2121  obj_loss: 0.0916  noobj_loss: 0.0584  bbox_loss: 0.0137  cls_loss: 0.0228  \n",
      "<<<iteration:[460/657] - total_loss: 0.2147  obj_loss: 0.1024  noobj_loss: 0.0673  bbox_loss: 0.0117  cls_loss: 0.0200  \n",
      "<<<iteration:[480/657] - total_loss: 0.2220  obj_loss: 0.1057  noobj_loss: 0.0717  bbox_loss: 0.0120  cls_loss: 0.0203  \n",
      "<<<iteration:[500/657] - total_loss: 0.2131  obj_loss: 0.1120  noobj_loss: 0.0626  bbox_loss: 0.0112  cls_loss: 0.0140  \n",
      "<<<iteration:[520/657] - total_loss: 0.2060  obj_loss: 0.1062  noobj_loss: 0.0552  bbox_loss: 0.0107  cls_loss: 0.0185  \n",
      "<<<iteration:[540/657] - total_loss: 0.2259  obj_loss: 0.1145  noobj_loss: 0.0580  bbox_loss: 0.0126  cls_loss: 0.0195  \n",
      "<<<iteration:[560/657] - total_loss: 0.2034  obj_loss: 0.1001  noobj_loss: 0.0591  bbox_loss: 0.0115  cls_loss: 0.0163  \n",
      "<<<iteration:[580/657] - total_loss: 0.2180  obj_loss: 0.1075  noobj_loss: 0.0663  bbox_loss: 0.0124  cls_loss: 0.0152  \n",
      "<<<iteration:[600/657] - total_loss: 0.2048  obj_loss: 0.0982  noobj_loss: 0.0615  bbox_loss: 0.0121  cls_loss: 0.0153  \n",
      "<<<iteration:[620/657] - total_loss: 0.2275  obj_loss: 0.1053  noobj_loss: 0.0650  bbox_loss: 0.0145  cls_loss: 0.0172  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[640/657] - total_loss: 0.2018  obj_loss: 0.0981  noobj_loss: 0.0608  bbox_loss: 0.0115  cls_loss: 0.0157  \n",
      "\n",
      "epoch:44/100 - Train Loss: 0.2102, Val Loss: 0.2510\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2420  obj_loss: 0.1081  noobj_loss: 0.0638  bbox_loss: 0.0162  cls_loss: 0.0212  \n",
      "<<<iteration:[40/657] - total_loss: 0.2086  obj_loss: 0.1068  noobj_loss: 0.0571  bbox_loss: 0.0112  cls_loss: 0.0171  \n",
      "<<<iteration:[60/657] - total_loss: 0.1949  obj_loss: 0.0989  noobj_loss: 0.0658  bbox_loss: 0.0096  cls_loss: 0.0150  \n",
      "<<<iteration:[80/657] - total_loss: 0.2043  obj_loss: 0.0974  noobj_loss: 0.0602  bbox_loss: 0.0125  cls_loss: 0.0142  \n",
      "<<<iteration:[100/657] - total_loss: 0.2043  obj_loss: 0.1022  noobj_loss: 0.0582  bbox_loss: 0.0118  cls_loss: 0.0142  \n",
      "<<<iteration:[120/657] - total_loss: 0.2222  obj_loss: 0.1064  noobj_loss: 0.0587  bbox_loss: 0.0135  cls_loss: 0.0189  \n",
      "<<<iteration:[140/657] - total_loss: 0.2240  obj_loss: 0.1100  noobj_loss: 0.0605  bbox_loss: 0.0138  cls_loss: 0.0146  \n",
      "<<<iteration:[160/657] - total_loss: 0.2128  obj_loss: 0.1032  noobj_loss: 0.0549  bbox_loss: 0.0131  cls_loss: 0.0166  \n",
      "<<<iteration:[180/657] - total_loss: 0.1983  obj_loss: 0.1049  noobj_loss: 0.0558  bbox_loss: 0.0097  cls_loss: 0.0170  \n",
      "<<<iteration:[200/657] - total_loss: 0.2032  obj_loss: 0.1046  noobj_loss: 0.0605  bbox_loss: 0.0105  cls_loss: 0.0158  \n",
      "<<<iteration:[220/657] - total_loss: 0.2011  obj_loss: 0.1054  noobj_loss: 0.0591  bbox_loss: 0.0100  cls_loss: 0.0163  \n",
      "<<<iteration:[240/657] - total_loss: 0.2012  obj_loss: 0.1011  noobj_loss: 0.0619  bbox_loss: 0.0106  cls_loss: 0.0160  \n",
      "<<<iteration:[260/657] - total_loss: 0.2089  obj_loss: 0.0900  noobj_loss: 0.0620  bbox_loss: 0.0134  cls_loss: 0.0210  \n",
      "<<<iteration:[280/657] - total_loss: 0.2025  obj_loss: 0.1077  noobj_loss: 0.0582  bbox_loss: 0.0099  cls_loss: 0.0164  \n",
      "<<<iteration:[300/657] - total_loss: 0.2153  obj_loss: 0.1041  noobj_loss: 0.0601  bbox_loss: 0.0132  cls_loss: 0.0153  \n",
      "<<<iteration:[320/657] - total_loss: 0.1921  obj_loss: 0.0963  noobj_loss: 0.0601  bbox_loss: 0.0099  cls_loss: 0.0164  \n",
      "<<<iteration:[340/657] - total_loss: 0.2148  obj_loss: 0.1041  noobj_loss: 0.0608  bbox_loss: 0.0130  cls_loss: 0.0154  \n",
      "<<<iteration:[360/657] - total_loss: 0.2067  obj_loss: 0.0918  noobj_loss: 0.0590  bbox_loss: 0.0133  cls_loss: 0.0190  \n",
      "<<<iteration:[380/657] - total_loss: 0.1903  obj_loss: 0.0936  noobj_loss: 0.0583  bbox_loss: 0.0103  cls_loss: 0.0159  \n",
      "<<<iteration:[400/657] - total_loss: 0.2169  obj_loss: 0.0943  noobj_loss: 0.0607  bbox_loss: 0.0144  cls_loss: 0.0202  \n",
      "<<<iteration:[420/657] - total_loss: 0.2008  obj_loss: 0.1032  noobj_loss: 0.0592  bbox_loss: 0.0107  cls_loss: 0.0143  \n",
      "<<<iteration:[440/657] - total_loss: 0.2158  obj_loss: 0.0993  noobj_loss: 0.0554  bbox_loss: 0.0144  cls_loss: 0.0167  \n",
      "<<<iteration:[460/657] - total_loss: 0.1996  obj_loss: 0.0960  noobj_loss: 0.0605  bbox_loss: 0.0117  cls_loss: 0.0150  \n",
      "<<<iteration:[480/657] - total_loss: 0.2024  obj_loss: 0.1024  noobj_loss: 0.0630  bbox_loss: 0.0106  cls_loss: 0.0153  \n",
      "<<<iteration:[500/657] - total_loss: 0.2059  obj_loss: 0.1042  noobj_loss: 0.0591  bbox_loss: 0.0113  cls_loss: 0.0159  \n",
      "<<<iteration:[520/657] - total_loss: 0.2259  obj_loss: 0.1160  noobj_loss: 0.0635  bbox_loss: 0.0120  cls_loss: 0.0184  \n",
      "<<<iteration:[540/657] - total_loss: 0.1965  obj_loss: 0.1002  noobj_loss: 0.0596  bbox_loss: 0.0100  cls_loss: 0.0167  \n",
      "<<<iteration:[560/657] - total_loss: 0.2207  obj_loss: 0.1066  noobj_loss: 0.0645  bbox_loss: 0.0128  cls_loss: 0.0181  \n",
      "<<<iteration:[580/657] - total_loss: 0.1931  obj_loss: 0.0893  noobj_loss: 0.0641  bbox_loss: 0.0113  cls_loss: 0.0151  \n",
      "<<<iteration:[600/657] - total_loss: 0.2136  obj_loss: 0.1016  noobj_loss: 0.0573  bbox_loss: 0.0130  cls_loss: 0.0181  \n",
      "<<<iteration:[620/657] - total_loss: 0.1981  obj_loss: 0.0979  noobj_loss: 0.0596  bbox_loss: 0.0114  cls_loss: 0.0133  \n",
      "<<<iteration:[640/657] - total_loss: 0.2278  obj_loss: 0.1065  noobj_loss: 0.0711  bbox_loss: 0.0134  cls_loss: 0.0186  \n",
      "\n",
      "epoch:45/100 - Train Loss: 0.2078, Val Loss: 0.2503\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2393  obj_loss: 0.1116  noobj_loss: 0.0718  bbox_loss: 0.0148  cls_loss: 0.0180  \n",
      "<<<iteration:[40/657] - total_loss: 0.2148  obj_loss: 0.1065  noobj_loss: 0.0601  bbox_loss: 0.0115  cls_loss: 0.0209  \n",
      "<<<iteration:[60/657] - total_loss: 0.2003  obj_loss: 0.1028  noobj_loss: 0.0513  bbox_loss: 0.0114  cls_loss: 0.0149  \n",
      "<<<iteration:[80/657] - total_loss: 0.2282  obj_loss: 0.0984  noobj_loss: 0.0668  bbox_loss: 0.0151  cls_loss: 0.0211  \n",
      "<<<iteration:[100/657] - total_loss: 0.1991  obj_loss: 0.0989  noobj_loss: 0.0642  bbox_loss: 0.0103  cls_loss: 0.0164  \n",
      "<<<iteration:[120/657] - total_loss: 0.1892  obj_loss: 0.0907  noobj_loss: 0.0520  bbox_loss: 0.0111  cls_loss: 0.0169  \n",
      "<<<iteration:[140/657] - total_loss: 0.2076  obj_loss: 0.1055  noobj_loss: 0.0637  bbox_loss: 0.0116  cls_loss: 0.0122  \n",
      "<<<iteration:[160/657] - total_loss: 0.2064  obj_loss: 0.1023  noobj_loss: 0.0536  bbox_loss: 0.0121  cls_loss: 0.0169  \n",
      "<<<iteration:[180/657] - total_loss: 0.2086  obj_loss: 0.0991  noobj_loss: 0.0610  bbox_loss: 0.0119  cls_loss: 0.0197  \n",
      "<<<iteration:[200/657] - total_loss: 0.2123  obj_loss: 0.1082  noobj_loss: 0.0672  bbox_loss: 0.0109  cls_loss: 0.0160  \n",
      "<<<iteration:[220/657] - total_loss: 0.2025  obj_loss: 0.1012  noobj_loss: 0.0664  bbox_loss: 0.0106  cls_loss: 0.0149  \n",
      "<<<iteration:[240/657] - total_loss: 0.1928  obj_loss: 0.0946  noobj_loss: 0.0627  bbox_loss: 0.0102  cls_loss: 0.0156  \n",
      "<<<iteration:[260/657] - total_loss: 0.2009  obj_loss: 0.1017  noobj_loss: 0.0603  bbox_loss: 0.0106  cls_loss: 0.0159  \n",
      "<<<iteration:[280/657] - total_loss: 0.2103  obj_loss: 0.1022  noobj_loss: 0.0663  bbox_loss: 0.0116  cls_loss: 0.0171  \n",
      "<<<iteration:[300/657] - total_loss: 0.2076  obj_loss: 0.1092  noobj_loss: 0.0638  bbox_loss: 0.0105  cls_loss: 0.0138  \n",
      "<<<iteration:[320/657] - total_loss: 0.1923  obj_loss: 0.0956  noobj_loss: 0.0545  bbox_loss: 0.0106  cls_loss: 0.0164  \n",
      "<<<iteration:[340/657] - total_loss: 0.2061  obj_loss: 0.1022  noobj_loss: 0.0634  bbox_loss: 0.0117  cls_loss: 0.0138  \n",
      "<<<iteration:[360/657] - total_loss: 0.2153  obj_loss: 0.1062  noobj_loss: 0.0632  bbox_loss: 0.0113  cls_loss: 0.0208  \n",
      "<<<iteration:[380/657] - total_loss: 0.2122  obj_loss: 0.1070  noobj_loss: 0.0641  bbox_loss: 0.0113  cls_loss: 0.0168  \n",
      "<<<iteration:[400/657] - total_loss: 0.1960  obj_loss: 0.0937  noobj_loss: 0.0553  bbox_loss: 0.0115  cls_loss: 0.0169  \n",
      "<<<iteration:[420/657] - total_loss: 0.2255  obj_loss: 0.1146  noobj_loss: 0.0599  bbox_loss: 0.0120  cls_loss: 0.0208  \n",
      "<<<iteration:[440/657] - total_loss: 0.1926  obj_loss: 0.0916  noobj_loss: 0.0535  bbox_loss: 0.0120  cls_loss: 0.0142  \n",
      "<<<iteration:[460/657] - total_loss: 0.2035  obj_loss: 0.0987  noobj_loss: 0.0555  bbox_loss: 0.0113  cls_loss: 0.0205  \n",
      "<<<iteration:[480/657] - total_loss: 0.1886  obj_loss: 0.1012  noobj_loss: 0.0576  bbox_loss: 0.0092  cls_loss: 0.0124  \n",
      "<<<iteration:[500/657] - total_loss: 0.2100  obj_loss: 0.1015  noobj_loss: 0.0566  bbox_loss: 0.0128  cls_loss: 0.0163  \n",
      "<<<iteration:[520/657] - total_loss: 0.1983  obj_loss: 0.1024  noobj_loss: 0.0548  bbox_loss: 0.0107  cls_loss: 0.0150  \n",
      "<<<iteration:[540/657] - total_loss: 0.1908  obj_loss: 0.0936  noobj_loss: 0.0514  bbox_loss: 0.0115  cls_loss: 0.0139  \n",
      "<<<iteration:[560/657] - total_loss: 0.2054  obj_loss: 0.0956  noobj_loss: 0.0578  bbox_loss: 0.0129  cls_loss: 0.0166  \n",
      "<<<iteration:[580/657] - total_loss: 0.2064  obj_loss: 0.0971  noobj_loss: 0.0533  bbox_loss: 0.0123  cls_loss: 0.0212  \n",
      "<<<iteration:[600/657] - total_loss: 0.1994  obj_loss: 0.0987  noobj_loss: 0.0617  bbox_loss: 0.0110  cls_loss: 0.0149  \n",
      "<<<iteration:[620/657] - total_loss: 0.1980  obj_loss: 0.1000  noobj_loss: 0.0581  bbox_loss: 0.0106  cls_loss: 0.0161  \n",
      "<<<iteration:[640/657] - total_loss: 0.2014  obj_loss: 0.0977  noobj_loss: 0.0634  bbox_loss: 0.0114  cls_loss: 0.0151  \n",
      "\n",
      "epoch:46/100 - Train Loss: 0.2050, Val Loss: 0.2493\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2173  obj_loss: 0.1099  noobj_loss: 0.0608  bbox_loss: 0.0119  cls_loss: 0.0172  \n",
      "<<<iteration:[40/657] - total_loss: 0.2052  obj_loss: 0.1007  noobj_loss: 0.0593  bbox_loss: 0.0112  cls_loss: 0.0187  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[60/657] - total_loss: 0.2141  obj_loss: 0.0866  noobj_loss: 0.0538  bbox_loss: 0.0162  cls_loss: 0.0197  \n",
      "<<<iteration:[80/657] - total_loss: 0.1951  obj_loss: 0.1012  noobj_loss: 0.0545  bbox_loss: 0.0102  cls_loss: 0.0155  \n",
      "<<<iteration:[100/657] - total_loss: 0.1902  obj_loss: 0.0928  noobj_loss: 0.0642  bbox_loss: 0.0107  cls_loss: 0.0119  \n",
      "<<<iteration:[120/657] - total_loss: 0.2045  obj_loss: 0.1116  noobj_loss: 0.0566  bbox_loss: 0.0100  cls_loss: 0.0146  \n",
      "<<<iteration:[140/657] - total_loss: 0.1982  obj_loss: 0.0928  noobj_loss: 0.0629  bbox_loss: 0.0112  cls_loss: 0.0181  \n",
      "<<<iteration:[160/657] - total_loss: 0.2153  obj_loss: 0.1027  noobj_loss: 0.0647  bbox_loss: 0.0121  cls_loss: 0.0196  \n",
      "<<<iteration:[180/657] - total_loss: 0.2038  obj_loss: 0.0999  noobj_loss: 0.0564  bbox_loss: 0.0114  cls_loss: 0.0189  \n",
      "<<<iteration:[200/657] - total_loss: 0.2009  obj_loss: 0.0994  noobj_loss: 0.0624  bbox_loss: 0.0106  cls_loss: 0.0174  \n",
      "<<<iteration:[220/657] - total_loss: 0.1929  obj_loss: 0.0948  noobj_loss: 0.0615  bbox_loss: 0.0105  cls_loss: 0.0149  \n",
      "<<<iteration:[240/657] - total_loss: 0.2100  obj_loss: 0.0991  noobj_loss: 0.0602  bbox_loss: 0.0130  cls_loss: 0.0158  \n",
      "<<<iteration:[260/657] - total_loss: 0.2003  obj_loss: 0.1052  noobj_loss: 0.0588  bbox_loss: 0.0104  cls_loss: 0.0139  \n",
      "<<<iteration:[280/657] - total_loss: 0.1947  obj_loss: 0.0915  noobj_loss: 0.0703  bbox_loss: 0.0105  cls_loss: 0.0155  \n",
      "<<<iteration:[300/657] - total_loss: 0.1939  obj_loss: 0.0839  noobj_loss: 0.0566  bbox_loss: 0.0127  cls_loss: 0.0180  \n",
      "<<<iteration:[320/657] - total_loss: 0.2122  obj_loss: 0.1050  noobj_loss: 0.0616  bbox_loss: 0.0116  cls_loss: 0.0183  \n",
      "<<<iteration:[340/657] - total_loss: 0.2000  obj_loss: 0.0982  noobj_loss: 0.0622  bbox_loss: 0.0112  cls_loss: 0.0144  \n",
      "<<<iteration:[360/657] - total_loss: 0.2069  obj_loss: 0.1096  noobj_loss: 0.0574  bbox_loss: 0.0104  cls_loss: 0.0165  \n",
      "<<<iteration:[380/657] - total_loss: 0.1899  obj_loss: 0.0893  noobj_loss: 0.0644  bbox_loss: 0.0106  cls_loss: 0.0153  \n",
      "<<<iteration:[400/657] - total_loss: 0.1869  obj_loss: 0.1018  noobj_loss: 0.0592  bbox_loss: 0.0082  cls_loss: 0.0144  \n",
      "<<<iteration:[420/657] - total_loss: 0.2041  obj_loss: 0.1025  noobj_loss: 0.0572  bbox_loss: 0.0117  cls_loss: 0.0145  \n",
      "<<<iteration:[440/657] - total_loss: 0.2109  obj_loss: 0.1008  noobj_loss: 0.0654  bbox_loss: 0.0127  cls_loss: 0.0142  \n",
      "<<<iteration:[460/657] - total_loss: 0.2117  obj_loss: 0.1146  noobj_loss: 0.0631  bbox_loss: 0.0103  cls_loss: 0.0141  \n",
      "<<<iteration:[480/657] - total_loss: 0.1751  obj_loss: 0.0860  noobj_loss: 0.0561  bbox_loss: 0.0095  cls_loss: 0.0135  \n",
      "<<<iteration:[500/657] - total_loss: 0.2018  obj_loss: 0.1037  noobj_loss: 0.0648  bbox_loss: 0.0101  cls_loss: 0.0154  \n",
      "<<<iteration:[520/657] - total_loss: 0.2064  obj_loss: 0.0988  noobj_loss: 0.0617  bbox_loss: 0.0125  cls_loss: 0.0143  \n",
      "<<<iteration:[540/657] - total_loss: 0.1904  obj_loss: 0.0987  noobj_loss: 0.0547  bbox_loss: 0.0095  cls_loss: 0.0168  \n",
      "<<<iteration:[560/657] - total_loss: 0.1888  obj_loss: 0.0925  noobj_loss: 0.0552  bbox_loss: 0.0111  cls_loss: 0.0133  \n",
      "<<<iteration:[580/657] - total_loss: 0.2033  obj_loss: 0.0931  noobj_loss: 0.0595  bbox_loss: 0.0131  cls_loss: 0.0150  \n",
      "<<<iteration:[600/657] - total_loss: 0.2041  obj_loss: 0.1076  noobj_loss: 0.0675  bbox_loss: 0.0098  cls_loss: 0.0139  \n",
      "<<<iteration:[620/657] - total_loss: 0.2281  obj_loss: 0.1135  noobj_loss: 0.0677  bbox_loss: 0.0124  cls_loss: 0.0186  \n",
      "<<<iteration:[640/657] - total_loss: 0.2304  obj_loss: 0.1091  noobj_loss: 0.0680  bbox_loss: 0.0140  cls_loss: 0.0174  \n",
      "\n",
      "epoch:47/100 - Train Loss: 0.2024, Val Loss: 0.2516\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2161  obj_loss: 0.1032  noobj_loss: 0.0605  bbox_loss: 0.0132  cls_loss: 0.0166  \n",
      "<<<iteration:[40/657] - total_loss: 0.1951  obj_loss: 0.0949  noobj_loss: 0.0627  bbox_loss: 0.0111  cls_loss: 0.0134  \n",
      "<<<iteration:[60/657] - total_loss: 0.2015  obj_loss: 0.1043  noobj_loss: 0.0570  bbox_loss: 0.0107  cls_loss: 0.0153  \n",
      "<<<iteration:[80/657] - total_loss: 0.2285  obj_loss: 0.1085  noobj_loss: 0.0623  bbox_loss: 0.0129  cls_loss: 0.0245  \n",
      "<<<iteration:[100/657] - total_loss: 0.2103  obj_loss: 0.1060  noobj_loss: 0.0561  bbox_loss: 0.0114  cls_loss: 0.0191  \n",
      "<<<iteration:[120/657] - total_loss: 0.1979  obj_loss: 0.0976  noobj_loss: 0.0665  bbox_loss: 0.0105  cls_loss: 0.0143  \n",
      "<<<iteration:[140/657] - total_loss: 0.2012  obj_loss: 0.1056  noobj_loss: 0.0588  bbox_loss: 0.0102  cls_loss: 0.0151  \n",
      "<<<iteration:[160/657] - total_loss: 0.2098  obj_loss: 0.1057  noobj_loss: 0.0588  bbox_loss: 0.0113  cls_loss: 0.0181  \n",
      "<<<iteration:[180/657] - total_loss: 0.2141  obj_loss: 0.1057  noobj_loss: 0.0653  bbox_loss: 0.0117  cls_loss: 0.0173  \n",
      "<<<iteration:[200/657] - total_loss: 0.1903  obj_loss: 0.0943  noobj_loss: 0.0576  bbox_loss: 0.0098  cls_loss: 0.0179  \n",
      "<<<iteration:[220/657] - total_loss: 0.1990  obj_loss: 0.0988  noobj_loss: 0.0604  bbox_loss: 0.0106  cls_loss: 0.0168  \n",
      "<<<iteration:[240/657] - total_loss: 0.2023  obj_loss: 0.0988  noobj_loss: 0.0655  bbox_loss: 0.0108  cls_loss: 0.0167  \n",
      "<<<iteration:[260/657] - total_loss: 0.1937  obj_loss: 0.0924  noobj_loss: 0.0624  bbox_loss: 0.0107  cls_loss: 0.0166  \n",
      "<<<iteration:[280/657] - total_loss: 0.2110  obj_loss: 0.0988  noobj_loss: 0.0630  bbox_loss: 0.0127  cls_loss: 0.0169  \n",
      "<<<iteration:[300/657] - total_loss: 0.2033  obj_loss: 0.1026  noobj_loss: 0.0572  bbox_loss: 0.0114  cls_loss: 0.0153  \n",
      "<<<iteration:[320/657] - total_loss: 0.2025  obj_loss: 0.1103  noobj_loss: 0.0621  bbox_loss: 0.0095  cls_loss: 0.0138  \n",
      "<<<iteration:[340/657] - total_loss: 0.2051  obj_loss: 0.1044  noobj_loss: 0.0615  bbox_loss: 0.0107  cls_loss: 0.0162  \n",
      "<<<iteration:[360/657] - total_loss: 0.1878  obj_loss: 0.0945  noobj_loss: 0.0548  bbox_loss: 0.0101  cls_loss: 0.0155  \n",
      "<<<iteration:[380/657] - total_loss: 0.2132  obj_loss: 0.1071  noobj_loss: 0.0674  bbox_loss: 0.0112  cls_loss: 0.0162  \n",
      "<<<iteration:[400/657] - total_loss: 0.1930  obj_loss: 0.1009  noobj_loss: 0.0638  bbox_loss: 0.0092  cls_loss: 0.0144  \n",
      "<<<iteration:[420/657] - total_loss: 0.2004  obj_loss: 0.1047  noobj_loss: 0.0583  bbox_loss: 0.0104  cls_loss: 0.0144  \n",
      "<<<iteration:[440/657] - total_loss: 0.1878  obj_loss: 0.0881  noobj_loss: 0.0597  bbox_loss: 0.0109  cls_loss: 0.0151  \n",
      "<<<iteration:[460/657] - total_loss: 0.2037  obj_loss: 0.1050  noobj_loss: 0.0611  bbox_loss: 0.0107  cls_loss: 0.0146  \n",
      "<<<iteration:[480/657] - total_loss: 0.2053  obj_loss: 0.0907  noobj_loss: 0.0724  bbox_loss: 0.0123  cls_loss: 0.0170  \n",
      "<<<iteration:[500/657] - total_loss: 0.1992  obj_loss: 0.0954  noobj_loss: 0.0613  bbox_loss: 0.0111  cls_loss: 0.0175  \n",
      "<<<iteration:[520/657] - total_loss: 0.2005  obj_loss: 0.1049  noobj_loss: 0.0584  bbox_loss: 0.0104  cls_loss: 0.0141  \n",
      "<<<iteration:[540/657] - total_loss: 0.2135  obj_loss: 0.1032  noobj_loss: 0.0611  bbox_loss: 0.0129  cls_loss: 0.0150  \n",
      "<<<iteration:[560/657] - total_loss: 0.2076  obj_loss: 0.1124  noobj_loss: 0.0530  bbox_loss: 0.0111  cls_loss: 0.0130  \n",
      "<<<iteration:[580/657] - total_loss: 0.1989  obj_loss: 0.0935  noobj_loss: 0.0656  bbox_loss: 0.0117  cls_loss: 0.0141  \n",
      "<<<iteration:[600/657] - total_loss: 0.2216  obj_loss: 0.1043  noobj_loss: 0.0576  bbox_loss: 0.0126  cls_loss: 0.0256  \n",
      "<<<iteration:[620/657] - total_loss: 0.1893  obj_loss: 0.0938  noobj_loss: 0.0619  bbox_loss: 0.0104  cls_loss: 0.0123  \n",
      "<<<iteration:[640/657] - total_loss: 0.2126  obj_loss: 0.1052  noobj_loss: 0.0593  bbox_loss: 0.0121  cls_loss: 0.0175  \n",
      "\n",
      "epoch:48/100 - Train Loss: 0.2035, Val Loss: 0.2490\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2252  obj_loss: 0.1020  noobj_loss: 0.0664  bbox_loss: 0.0142  cls_loss: 0.0189  \n",
      "<<<iteration:[40/657] - total_loss: 0.2508  obj_loss: 0.0911  noobj_loss: 0.0559  bbox_loss: 0.0222  cls_loss: 0.0210  \n",
      "<<<iteration:[60/657] - total_loss: 0.2642  obj_loss: 0.0976  noobj_loss: 0.0526  bbox_loss: 0.0232  cls_loss: 0.0245  \n",
      "<<<iteration:[80/657] - total_loss: 0.2317  obj_loss: 0.1017  noobj_loss: 0.0395  bbox_loss: 0.0181  cls_loss: 0.0199  \n",
      "<<<iteration:[100/657] - total_loss: 0.2330  obj_loss: 0.0996  noobj_loss: 0.0414  bbox_loss: 0.0178  cls_loss: 0.0239  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[120/657] - total_loss: 0.2159  obj_loss: 0.0972  noobj_loss: 0.0414  bbox_loss: 0.0155  cls_loss: 0.0204  \n",
      "<<<iteration:[140/657] - total_loss: 0.2608  obj_loss: 0.1002  noobj_loss: 0.0475  bbox_loss: 0.0220  cls_loss: 0.0267  \n",
      "<<<iteration:[160/657] - total_loss: 0.2263  obj_loss: 0.1047  noobj_loss: 0.0478  bbox_loss: 0.0155  cls_loss: 0.0202  \n",
      "<<<iteration:[180/657] - total_loss: 0.2094  obj_loss: 0.1043  noobj_loss: 0.0414  bbox_loss: 0.0137  cls_loss: 0.0161  \n",
      "<<<iteration:[200/657] - total_loss: 0.2386  obj_loss: 0.1103  noobj_loss: 0.0494  bbox_loss: 0.0166  cls_loss: 0.0207  \n",
      "<<<iteration:[220/657] - total_loss: 0.2379  obj_loss: 0.1151  noobj_loss: 0.0526  bbox_loss: 0.0155  cls_loss: 0.0189  \n",
      "<<<iteration:[240/657] - total_loss: 0.2144  obj_loss: 0.1022  noobj_loss: 0.0504  bbox_loss: 0.0139  cls_loss: 0.0174  \n",
      "<<<iteration:[260/657] - total_loss: 0.2003  obj_loss: 0.0952  noobj_loss: 0.0475  bbox_loss: 0.0130  cls_loss: 0.0162  \n",
      "<<<iteration:[280/657] - total_loss: 0.2271  obj_loss: 0.0920  noobj_loss: 0.0485  bbox_loss: 0.0180  cls_loss: 0.0206  \n",
      "<<<iteration:[300/657] - total_loss: 0.2307  obj_loss: 0.0889  noobj_loss: 0.0537  bbox_loss: 0.0178  cls_loss: 0.0259  \n",
      "<<<iteration:[320/657] - total_loss: 0.2063  obj_loss: 0.0857  noobj_loss: 0.0552  bbox_loss: 0.0148  cls_loss: 0.0189  \n",
      "<<<iteration:[340/657] - total_loss: 0.2041  obj_loss: 0.0942  noobj_loss: 0.0545  bbox_loss: 0.0125  cls_loss: 0.0202  \n",
      "<<<iteration:[360/657] - total_loss: 0.2226  obj_loss: 0.0893  noobj_loss: 0.0527  bbox_loss: 0.0170  cls_loss: 0.0219  \n",
      "<<<iteration:[380/657] - total_loss: 0.2014  obj_loss: 0.0977  noobj_loss: 0.0525  bbox_loss: 0.0120  cls_loss: 0.0174  \n",
      "<<<iteration:[400/657] - total_loss: 0.2152  obj_loss: 0.1018  noobj_loss: 0.0601  bbox_loss: 0.0136  cls_loss: 0.0151  \n",
      "<<<iteration:[420/657] - total_loss: 0.2023  obj_loss: 0.0854  noobj_loss: 0.0573  bbox_loss: 0.0132  cls_loss: 0.0223  \n",
      "<<<iteration:[440/657] - total_loss: 0.2017  obj_loss: 0.1020  noobj_loss: 0.0534  bbox_loss: 0.0116  cls_loss: 0.0149  \n",
      "<<<iteration:[460/657] - total_loss: 0.2099  obj_loss: 0.0987  noobj_loss: 0.0577  bbox_loss: 0.0132  cls_loss: 0.0165  \n",
      "<<<iteration:[480/657] - total_loss: 0.2149  obj_loss: 0.1035  noobj_loss: 0.0563  bbox_loss: 0.0136  cls_loss: 0.0151  \n",
      "<<<iteration:[500/657] - total_loss: 0.2323  obj_loss: 0.1025  noobj_loss: 0.0602  bbox_loss: 0.0160  cls_loss: 0.0199  \n",
      "<<<iteration:[520/657] - total_loss: 0.2173  obj_loss: 0.0925  noobj_loss: 0.0578  bbox_loss: 0.0152  cls_loss: 0.0201  \n",
      "<<<iteration:[540/657] - total_loss: 0.1944  obj_loss: 0.0863  noobj_loss: 0.0563  bbox_loss: 0.0129  cls_loss: 0.0156  \n",
      "<<<iteration:[560/657] - total_loss: 0.2057  obj_loss: 0.1021  noobj_loss: 0.0608  bbox_loss: 0.0112  cls_loss: 0.0170  \n",
      "<<<iteration:[580/657] - total_loss: 0.2303  obj_loss: 0.1135  noobj_loss: 0.0566  bbox_loss: 0.0138  cls_loss: 0.0193  \n",
      "<<<iteration:[600/657] - total_loss: 0.2153  obj_loss: 0.0972  noobj_loss: 0.0563  bbox_loss: 0.0142  cls_loss: 0.0191  \n",
      "<<<iteration:[620/657] - total_loss: 0.1982  obj_loss: 0.0951  noobj_loss: 0.0524  bbox_loss: 0.0116  cls_loss: 0.0192  \n",
      "<<<iteration:[640/657] - total_loss: 0.2079  obj_loss: 0.0894  noobj_loss: 0.0560  bbox_loss: 0.0140  cls_loss: 0.0204  \n",
      "\n",
      "epoch:49/100 - Train Loss: 0.2196, Val Loss: 0.2496\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2171  obj_loss: 0.0975  noobj_loss: 0.0603  bbox_loss: 0.0141  cls_loss: 0.0190  \n",
      "<<<iteration:[40/657] - total_loss: 0.2206  obj_loss: 0.1095  noobj_loss: 0.0575  bbox_loss: 0.0133  cls_loss: 0.0157  \n",
      "<<<iteration:[60/657] - total_loss: 0.2101  obj_loss: 0.1007  noobj_loss: 0.0637  bbox_loss: 0.0119  cls_loss: 0.0182  \n",
      "<<<iteration:[80/657] - total_loss: 0.2098  obj_loss: 0.0844  noobj_loss: 0.0524  bbox_loss: 0.0155  cls_loss: 0.0219  \n",
      "<<<iteration:[100/657] - total_loss: 0.2332  obj_loss: 0.1006  noobj_loss: 0.0609  bbox_loss: 0.0169  cls_loss: 0.0177  \n",
      "<<<iteration:[120/657] - total_loss: 0.2092  obj_loss: 0.0868  noobj_loss: 0.0579  bbox_loss: 0.0152  cls_loss: 0.0175  \n",
      "<<<iteration:[140/657] - total_loss: 0.2134  obj_loss: 0.1074  noobj_loss: 0.0563  bbox_loss: 0.0122  cls_loss: 0.0168  \n",
      "<<<iteration:[160/657] - total_loss: 0.2139  obj_loss: 0.1025  noobj_loss: 0.0618  bbox_loss: 0.0132  cls_loss: 0.0146  \n",
      "<<<iteration:[180/657] - total_loss: 0.2094  obj_loss: 0.0921  noobj_loss: 0.0672  bbox_loss: 0.0131  cls_loss: 0.0183  \n",
      "<<<iteration:[200/657] - total_loss: 0.2071  obj_loss: 0.0969  noobj_loss: 0.0619  bbox_loss: 0.0121  cls_loss: 0.0185  \n",
      "<<<iteration:[220/657] - total_loss: 0.2146  obj_loss: 0.1028  noobj_loss: 0.0622  bbox_loss: 0.0127  cls_loss: 0.0170  \n",
      "<<<iteration:[240/657] - total_loss: 0.2130  obj_loss: 0.0975  noobj_loss: 0.0585  bbox_loss: 0.0137  cls_loss: 0.0176  \n",
      "<<<iteration:[260/657] - total_loss: 0.2138  obj_loss: 0.0895  noobj_loss: 0.0535  bbox_loss: 0.0158  cls_loss: 0.0185  \n",
      "<<<iteration:[280/657] - total_loss: 0.2241  obj_loss: 0.0970  noobj_loss: 0.0570  bbox_loss: 0.0155  cls_loss: 0.0209  \n",
      "<<<iteration:[300/657] - total_loss: 0.2038  obj_loss: 0.0923  noobj_loss: 0.0623  bbox_loss: 0.0129  cls_loss: 0.0159  \n",
      "<<<iteration:[320/657] - total_loss: 0.2155  obj_loss: 0.0955  noobj_loss: 0.0575  bbox_loss: 0.0143  cls_loss: 0.0196  \n",
      "<<<iteration:[340/657] - total_loss: 0.2068  obj_loss: 0.1077  noobj_loss: 0.0522  bbox_loss: 0.0114  cls_loss: 0.0158  \n",
      "<<<iteration:[360/657] - total_loss: 0.2087  obj_loss: 0.1023  noobj_loss: 0.0615  bbox_loss: 0.0122  cls_loss: 0.0147  \n",
      "<<<iteration:[380/657] - total_loss: 0.2058  obj_loss: 0.0917  noobj_loss: 0.0534  bbox_loss: 0.0145  cls_loss: 0.0147  \n",
      "<<<iteration:[400/657] - total_loss: 0.2188  obj_loss: 0.0893  noobj_loss: 0.0647  bbox_loss: 0.0157  cls_loss: 0.0185  \n",
      "<<<iteration:[420/657] - total_loss: 0.1986  obj_loss: 0.0968  noobj_loss: 0.0602  bbox_loss: 0.0110  cls_loss: 0.0167  \n",
      "<<<iteration:[440/657] - total_loss: 0.1802  obj_loss: 0.0889  noobj_loss: 0.0528  bbox_loss: 0.0102  cls_loss: 0.0140  \n",
      "<<<iteration:[460/657] - total_loss: 0.1973  obj_loss: 0.0853  noobj_loss: 0.0583  bbox_loss: 0.0126  cls_loss: 0.0197  \n",
      "<<<iteration:[480/657] - total_loss: 0.1977  obj_loss: 0.0916  noobj_loss: 0.0562  bbox_loss: 0.0123  cls_loss: 0.0163  \n",
      "<<<iteration:[500/657] - total_loss: 0.1956  obj_loss: 0.0965  noobj_loss: 0.0542  bbox_loss: 0.0113  cls_loss: 0.0154  \n",
      "<<<iteration:[520/657] - total_loss: 0.1982  obj_loss: 0.0976  noobj_loss: 0.0603  bbox_loss: 0.0112  cls_loss: 0.0146  \n",
      "<<<iteration:[540/657] - total_loss: 0.2044  obj_loss: 0.0910  noobj_loss: 0.0497  bbox_loss: 0.0138  cls_loss: 0.0197  \n",
      "<<<iteration:[560/657] - total_loss: 0.2046  obj_loss: 0.1010  noobj_loss: 0.0507  bbox_loss: 0.0123  cls_loss: 0.0169  \n",
      "<<<iteration:[580/657] - total_loss: 0.2092  obj_loss: 0.1017  noobj_loss: 0.0632  bbox_loss: 0.0115  cls_loss: 0.0185  \n",
      "<<<iteration:[600/657] - total_loss: 0.2088  obj_loss: 0.0943  noobj_loss: 0.0629  bbox_loss: 0.0135  cls_loss: 0.0156  \n",
      "<<<iteration:[620/657] - total_loss: 0.2184  obj_loss: 0.1145  noobj_loss: 0.0548  bbox_loss: 0.0119  cls_loss: 0.0169  \n",
      "<<<iteration:[640/657] - total_loss: 0.1998  obj_loss: 0.1004  noobj_loss: 0.0568  bbox_loss: 0.0113  cls_loss: 0.0147  \n",
      "\n",
      "epoch:50/100 - Train Loss: 0.2079, Val Loss: 0.2474\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2019  obj_loss: 0.0962  noobj_loss: 0.0580  bbox_loss: 0.0119  cls_loss: 0.0174  \n",
      "<<<iteration:[40/657] - total_loss: 0.2245  obj_loss: 0.1012  noobj_loss: 0.0547  bbox_loss: 0.0157  cls_loss: 0.0175  \n",
      "<<<iteration:[60/657] - total_loss: 0.1885  obj_loss: 0.0894  noobj_loss: 0.0585  bbox_loss: 0.0113  cls_loss: 0.0134  \n",
      "<<<iteration:[80/657] - total_loss: 0.2161  obj_loss: 0.0967  noobj_loss: 0.0647  bbox_loss: 0.0136  cls_loss: 0.0188  \n",
      "<<<iteration:[100/657] - total_loss: 0.2000  obj_loss: 0.0954  noobj_loss: 0.0533  bbox_loss: 0.0121  cls_loss: 0.0176  \n",
      "<<<iteration:[120/657] - total_loss: 0.2057  obj_loss: 0.1014  noobj_loss: 0.0661  bbox_loss: 0.0114  cls_loss: 0.0143  \n",
      "<<<iteration:[140/657] - total_loss: 0.2088  obj_loss: 0.1127  noobj_loss: 0.0572  bbox_loss: 0.0105  cls_loss: 0.0150  \n",
      "<<<iteration:[160/657] - total_loss: 0.2003  obj_loss: 0.0940  noobj_loss: 0.0552  bbox_loss: 0.0124  cls_loss: 0.0167  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[180/657] - total_loss: 0.2112  obj_loss: 0.1082  noobj_loss: 0.0641  bbox_loss: 0.0114  cls_loss: 0.0138  \n",
      "<<<iteration:[200/657] - total_loss: 0.1976  obj_loss: 0.0955  noobj_loss: 0.0635  bbox_loss: 0.0108  cls_loss: 0.0162  \n",
      "<<<iteration:[220/657] - total_loss: 0.1969  obj_loss: 0.1009  noobj_loss: 0.0577  bbox_loss: 0.0105  cls_loss: 0.0147  \n",
      "<<<iteration:[240/657] - total_loss: 0.2055  obj_loss: 0.0927  noobj_loss: 0.0554  bbox_loss: 0.0132  cls_loss: 0.0190  \n",
      "<<<iteration:[260/657] - total_loss: 0.1965  obj_loss: 0.0985  noobj_loss: 0.0548  bbox_loss: 0.0108  cls_loss: 0.0168  \n",
      "<<<iteration:[280/657] - total_loss: 0.2087  obj_loss: 0.1080  noobj_loss: 0.0665  bbox_loss: 0.0109  cls_loss: 0.0132  \n",
      "<<<iteration:[300/657] - total_loss: 0.2102  obj_loss: 0.1029  noobj_loss: 0.0609  bbox_loss: 0.0120  cls_loss: 0.0170  \n",
      "<<<iteration:[320/657] - total_loss: 0.1999  obj_loss: 0.1048  noobj_loss: 0.0501  bbox_loss: 0.0110  cls_loss: 0.0149  \n",
      "<<<iteration:[340/657] - total_loss: 0.1977  obj_loss: 0.0968  noobj_loss: 0.0563  bbox_loss: 0.0114  cls_loss: 0.0158  \n",
      "<<<iteration:[360/657] - total_loss: 0.2071  obj_loss: 0.0950  noobj_loss: 0.0623  bbox_loss: 0.0130  cls_loss: 0.0160  \n",
      "<<<iteration:[380/657] - total_loss: 0.1946  obj_loss: 0.0940  noobj_loss: 0.0587  bbox_loss: 0.0112  cls_loss: 0.0154  \n",
      "<<<iteration:[400/657] - total_loss: 0.1836  obj_loss: 0.0868  noobj_loss: 0.0587  bbox_loss: 0.0106  cls_loss: 0.0146  \n",
      "<<<iteration:[420/657] - total_loss: 0.1866  obj_loss: 0.1009  noobj_loss: 0.0544  bbox_loss: 0.0090  cls_loss: 0.0135  \n",
      "<<<iteration:[440/657] - total_loss: 0.1944  obj_loss: 0.0953  noobj_loss: 0.0554  bbox_loss: 0.0111  cls_loss: 0.0159  \n",
      "<<<iteration:[460/657] - total_loss: 0.2080  obj_loss: 0.1019  noobj_loss: 0.0614  bbox_loss: 0.0119  cls_loss: 0.0158  \n",
      "<<<iteration:[480/657] - total_loss: 0.1846  obj_loss: 0.0853  noobj_loss: 0.0628  bbox_loss: 0.0107  cls_loss: 0.0145  \n",
      "<<<iteration:[500/657] - total_loss: 0.2078  obj_loss: 0.0916  noobj_loss: 0.0525  bbox_loss: 0.0138  cls_loss: 0.0207  \n",
      "<<<iteration:[520/657] - total_loss: 0.2030  obj_loss: 0.1009  noobj_loss: 0.0587  bbox_loss: 0.0113  cls_loss: 0.0165  \n",
      "<<<iteration:[540/657] - total_loss: 0.2028  obj_loss: 0.0990  noobj_loss: 0.0547  bbox_loss: 0.0113  cls_loss: 0.0200  \n",
      "<<<iteration:[560/657] - total_loss: 0.2085  obj_loss: 0.0953  noobj_loss: 0.0589  bbox_loss: 0.0133  cls_loss: 0.0170  \n",
      "<<<iteration:[580/657] - total_loss: 0.2031  obj_loss: 0.0998  noobj_loss: 0.0608  bbox_loss: 0.0110  cls_loss: 0.0178  \n",
      "<<<iteration:[600/657] - total_loss: 0.1943  obj_loss: 0.0895  noobj_loss: 0.0626  bbox_loss: 0.0113  cls_loss: 0.0170  \n",
      "<<<iteration:[620/657] - total_loss: 0.2065  obj_loss: 0.1029  noobj_loss: 0.0651  bbox_loss: 0.0112  cls_loss: 0.0151  \n",
      "<<<iteration:[640/657] - total_loss: 0.1891  obj_loss: 0.0890  noobj_loss: 0.0609  bbox_loss: 0.0106  cls_loss: 0.0165  \n",
      "\n",
      "epoch:51/100 - Train Loss: 0.2016, Val Loss: 0.2439\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2007  obj_loss: 0.0986  noobj_loss: 0.0618  bbox_loss: 0.0107  cls_loss: 0.0177  \n",
      "<<<iteration:[40/657] - total_loss: 0.2059  obj_loss: 0.1016  noobj_loss: 0.0592  bbox_loss: 0.0120  cls_loss: 0.0149  \n",
      "<<<iteration:[60/657] - total_loss: 0.2038  obj_loss: 0.0928  noobj_loss: 0.0560  bbox_loss: 0.0135  cls_loss: 0.0156  \n",
      "<<<iteration:[80/657] - total_loss: 0.1993  obj_loss: 0.0946  noobj_loss: 0.0648  bbox_loss: 0.0108  cls_loss: 0.0185  \n",
      "<<<iteration:[100/657] - total_loss: 0.1936  obj_loss: 0.0967  noobj_loss: 0.0680  bbox_loss: 0.0095  cls_loss: 0.0153  \n",
      "<<<iteration:[120/657] - total_loss: 0.2036  obj_loss: 0.0941  noobj_loss: 0.0632  bbox_loss: 0.0126  cls_loss: 0.0148  \n",
      "<<<iteration:[140/657] - total_loss: 0.2105  obj_loss: 0.1052  noobj_loss: 0.0577  bbox_loss: 0.0114  cls_loss: 0.0195  \n",
      "<<<iteration:[160/657] - total_loss: 0.2019  obj_loss: 0.0891  noobj_loss: 0.0640  bbox_loss: 0.0129  cls_loss: 0.0162  \n",
      "<<<iteration:[180/657] - total_loss: 0.2061  obj_loss: 0.1088  noobj_loss: 0.0612  bbox_loss: 0.0104  cls_loss: 0.0148  \n",
      "<<<iteration:[200/657] - total_loss: 0.1891  obj_loss: 0.0922  noobj_loss: 0.0560  bbox_loss: 0.0107  cls_loss: 0.0154  \n",
      "<<<iteration:[220/657] - total_loss: 0.1913  obj_loss: 0.0937  noobj_loss: 0.0578  bbox_loss: 0.0107  cls_loss: 0.0152  \n",
      "<<<iteration:[240/657] - total_loss: 0.1843  obj_loss: 0.0926  noobj_loss: 0.0522  bbox_loss: 0.0097  cls_loss: 0.0170  \n",
      "<<<iteration:[260/657] - total_loss: 0.2041  obj_loss: 0.1049  noobj_loss: 0.0551  bbox_loss: 0.0107  cls_loss: 0.0182  \n",
      "<<<iteration:[280/657] - total_loss: 0.1932  obj_loss: 0.0903  noobj_loss: 0.0601  bbox_loss: 0.0115  cls_loss: 0.0153  \n",
      "<<<iteration:[300/657] - total_loss: 0.1832  obj_loss: 0.0881  noobj_loss: 0.0544  bbox_loss: 0.0104  cls_loss: 0.0159  \n",
      "<<<iteration:[320/657] - total_loss: 0.1917  obj_loss: 0.0964  noobj_loss: 0.0638  bbox_loss: 0.0098  cls_loss: 0.0143  \n",
      "<<<iteration:[340/657] - total_loss: 0.2055  obj_loss: 0.0998  noobj_loss: 0.0635  bbox_loss: 0.0115  cls_loss: 0.0162  \n",
      "<<<iteration:[360/657] - total_loss: 0.2113  obj_loss: 0.1114  noobj_loss: 0.0611  bbox_loss: 0.0110  cls_loss: 0.0143  \n",
      "<<<iteration:[380/657] - total_loss: 0.2191  obj_loss: 0.1083  noobj_loss: 0.0630  bbox_loss: 0.0127  cls_loss: 0.0158  \n",
      "<<<iteration:[400/657] - total_loss: 0.1873  obj_loss: 0.0918  noobj_loss: 0.0503  bbox_loss: 0.0110  cls_loss: 0.0153  \n",
      "<<<iteration:[420/657] - total_loss: 0.1978  obj_loss: 0.0878  noobj_loss: 0.0632  bbox_loss: 0.0128  cls_loss: 0.0145  \n",
      "<<<iteration:[440/657] - total_loss: 0.1938  obj_loss: 0.0903  noobj_loss: 0.0657  bbox_loss: 0.0111  cls_loss: 0.0153  \n",
      "<<<iteration:[460/657] - total_loss: 0.2103  obj_loss: 0.1034  noobj_loss: 0.0552  bbox_loss: 0.0122  cls_loss: 0.0183  \n",
      "<<<iteration:[480/657] - total_loss: 0.1986  obj_loss: 0.1000  noobj_loss: 0.0640  bbox_loss: 0.0104  cls_loss: 0.0144  \n",
      "<<<iteration:[500/657] - total_loss: 0.1788  obj_loss: 0.0847  noobj_loss: 0.0595  bbox_loss: 0.0100  cls_loss: 0.0144  \n",
      "<<<iteration:[520/657] - total_loss: 0.2001  obj_loss: 0.1029  noobj_loss: 0.0537  bbox_loss: 0.0109  cls_loss: 0.0161  \n",
      "<<<iteration:[540/657] - total_loss: 0.2015  obj_loss: 0.1029  noobj_loss: 0.0622  bbox_loss: 0.0103  cls_loss: 0.0159  \n",
      "<<<iteration:[560/657] - total_loss: 0.1906  obj_loss: 0.0963  noobj_loss: 0.0479  bbox_loss: 0.0107  cls_loss: 0.0167  \n",
      "<<<iteration:[580/657] - total_loss: 0.1966  obj_loss: 0.1017  noobj_loss: 0.0596  bbox_loss: 0.0102  cls_loss: 0.0142  \n",
      "<<<iteration:[600/657] - total_loss: 0.1835  obj_loss: 0.0897  noobj_loss: 0.0550  bbox_loss: 0.0102  cls_loss: 0.0156  \n",
      "<<<iteration:[620/657] - total_loss: 0.2081  obj_loss: 0.1015  noobj_loss: 0.0560  bbox_loss: 0.0123  cls_loss: 0.0171  \n",
      "<<<iteration:[640/657] - total_loss: 0.1989  obj_loss: 0.1046  noobj_loss: 0.0672  bbox_loss: 0.0089  cls_loss: 0.0163  \n",
      "\n",
      "epoch:52/100 - Train Loss: 0.1981, Val Loss: 0.2448\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2042  obj_loss: 0.0970  noobj_loss: 0.0667  bbox_loss: 0.0116  cls_loss: 0.0159  \n",
      "<<<iteration:[40/657] - total_loss: 0.2305  obj_loss: 0.0918  noobj_loss: 0.0669  bbox_loss: 0.0181  cls_loss: 0.0148  \n",
      "<<<iteration:[60/657] - total_loss: 0.2021  obj_loss: 0.1057  noobj_loss: 0.0618  bbox_loss: 0.0100  cls_loss: 0.0154  \n",
      "<<<iteration:[80/657] - total_loss: 0.1898  obj_loss: 0.0932  noobj_loss: 0.0630  bbox_loss: 0.0101  cls_loss: 0.0145  \n",
      "<<<iteration:[100/657] - total_loss: 0.1962  obj_loss: 0.1004  noobj_loss: 0.0591  bbox_loss: 0.0104  cls_loss: 0.0142  \n",
      "<<<iteration:[120/657] - total_loss: 0.2000  obj_loss: 0.1041  noobj_loss: 0.0632  bbox_loss: 0.0095  cls_loss: 0.0167  \n",
      "<<<iteration:[140/657] - total_loss: 0.1829  obj_loss: 0.0920  noobj_loss: 0.0549  bbox_loss: 0.0099  cls_loss: 0.0141  \n",
      "<<<iteration:[160/657] - total_loss: 0.2073  obj_loss: 0.1076  noobj_loss: 0.0592  bbox_loss: 0.0115  cls_loss: 0.0127  \n",
      "<<<iteration:[180/657] - total_loss: 0.1871  obj_loss: 0.0873  noobj_loss: 0.0576  bbox_loss: 0.0111  cls_loss: 0.0157  \n",
      "<<<iteration:[200/657] - total_loss: 0.1960  obj_loss: 0.1025  noobj_loss: 0.0609  bbox_loss: 0.0098  cls_loss: 0.0141  \n",
      "<<<iteration:[220/657] - total_loss: 0.1879  obj_loss: 0.0945  noobj_loss: 0.0639  bbox_loss: 0.0098  cls_loss: 0.0122  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[240/657] - total_loss: 0.2057  obj_loss: 0.1011  noobj_loss: 0.0610  bbox_loss: 0.0116  cls_loss: 0.0163  \n",
      "<<<iteration:[260/657] - total_loss: 0.2131  obj_loss: 0.0994  noobj_loss: 0.0657  bbox_loss: 0.0124  cls_loss: 0.0187  \n",
      "<<<iteration:[280/657] - total_loss: 0.1916  obj_loss: 0.0987  noobj_loss: 0.0546  bbox_loss: 0.0107  cls_loss: 0.0123  \n",
      "<<<iteration:[300/657] - total_loss: 0.2096  obj_loss: 0.0966  noobj_loss: 0.0708  bbox_loss: 0.0122  cls_loss: 0.0167  \n",
      "<<<iteration:[320/657] - total_loss: 0.1871  obj_loss: 0.0936  noobj_loss: 0.0645  bbox_loss: 0.0089  cls_loss: 0.0166  \n",
      "<<<iteration:[340/657] - total_loss: 0.1966  obj_loss: 0.1046  noobj_loss: 0.0675  bbox_loss: 0.0088  cls_loss: 0.0143  \n",
      "<<<iteration:[360/657] - total_loss: 0.1987  obj_loss: 0.0973  noobj_loss: 0.0655  bbox_loss: 0.0105  cls_loss: 0.0162  \n",
      "<<<iteration:[380/657] - total_loss: 0.1885  obj_loss: 0.0897  noobj_loss: 0.0460  bbox_loss: 0.0126  cls_loss: 0.0128  \n",
      "<<<iteration:[400/657] - total_loss: 0.1998  obj_loss: 0.1021  noobj_loss: 0.0619  bbox_loss: 0.0103  cls_loss: 0.0153  \n",
      "<<<iteration:[420/657] - total_loss: 0.1957  obj_loss: 0.0986  noobj_loss: 0.0530  bbox_loss: 0.0113  cls_loss: 0.0141  \n",
      "<<<iteration:[440/657] - total_loss: 0.2051  obj_loss: 0.1032  noobj_loss: 0.0659  bbox_loss: 0.0112  cls_loss: 0.0131  \n",
      "<<<iteration:[460/657] - total_loss: 0.1877  obj_loss: 0.0978  noobj_loss: 0.0612  bbox_loss: 0.0091  cls_loss: 0.0140  \n",
      "<<<iteration:[480/657] - total_loss: 0.2009  obj_loss: 0.0913  noobj_loss: 0.0630  bbox_loss: 0.0121  cls_loss: 0.0175  \n",
      "<<<iteration:[500/657] - total_loss: 0.1955  obj_loss: 0.0932  noobj_loss: 0.0636  bbox_loss: 0.0109  cls_loss: 0.0162  \n",
      "<<<iteration:[520/657] - total_loss: 0.2066  obj_loss: 0.0940  noobj_loss: 0.0543  bbox_loss: 0.0141  cls_loss: 0.0148  \n",
      "<<<iteration:[540/657] - total_loss: 0.1776  obj_loss: 0.0821  noobj_loss: 0.0579  bbox_loss: 0.0104  cls_loss: 0.0144  \n",
      "<<<iteration:[560/657] - total_loss: 0.2144  obj_loss: 0.1086  noobj_loss: 0.0561  bbox_loss: 0.0120  cls_loss: 0.0175  \n",
      "<<<iteration:[580/657] - total_loss: 0.1955  obj_loss: 0.0857  noobj_loss: 0.0619  bbox_loss: 0.0125  cls_loss: 0.0164  \n",
      "<<<iteration:[600/657] - total_loss: 0.2036  obj_loss: 0.1044  noobj_loss: 0.0609  bbox_loss: 0.0104  cls_loss: 0.0166  \n",
      "<<<iteration:[620/657] - total_loss: 0.1975  obj_loss: 0.1003  noobj_loss: 0.0597  bbox_loss: 0.0109  cls_loss: 0.0128  \n",
      "<<<iteration:[640/657] - total_loss: 0.1883  obj_loss: 0.0925  noobj_loss: 0.0607  bbox_loss: 0.0101  cls_loss: 0.0149  \n",
      "\n",
      "epoch:53/100 - Train Loss: 0.1978, Val Loss: 0.2435\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2057  obj_loss: 0.0994  noobj_loss: 0.0599  bbox_loss: 0.0117  cls_loss: 0.0179  \n",
      "<<<iteration:[40/657] - total_loss: 0.2065  obj_loss: 0.1028  noobj_loss: 0.0589  bbox_loss: 0.0113  cls_loss: 0.0177  \n",
      "<<<iteration:[60/657] - total_loss: 0.1977  obj_loss: 0.0979  noobj_loss: 0.0557  bbox_loss: 0.0105  cls_loss: 0.0194  \n",
      "<<<iteration:[80/657] - total_loss: 0.1902  obj_loss: 0.0929  noobj_loss: 0.0575  bbox_loss: 0.0107  cls_loss: 0.0149  \n",
      "<<<iteration:[100/657] - total_loss: 0.1846  obj_loss: 0.0979  noobj_loss: 0.0527  bbox_loss: 0.0091  cls_loss: 0.0148  \n",
      "<<<iteration:[120/657] - total_loss: 0.1797  obj_loss: 0.0909  noobj_loss: 0.0560  bbox_loss: 0.0099  cls_loss: 0.0112  \n",
      "<<<iteration:[140/657] - total_loss: 0.1845  obj_loss: 0.0868  noobj_loss: 0.0613  bbox_loss: 0.0104  cls_loss: 0.0149  \n",
      "<<<iteration:[160/657] - total_loss: 0.1688  obj_loss: 0.0860  noobj_loss: 0.0508  bbox_loss: 0.0092  cls_loss: 0.0116  \n",
      "<<<iteration:[180/657] - total_loss: 0.1971  obj_loss: 0.1032  noobj_loss: 0.0602  bbox_loss: 0.0102  cls_loss: 0.0127  \n",
      "<<<iteration:[200/657] - total_loss: 0.1859  obj_loss: 0.0928  noobj_loss: 0.0569  bbox_loss: 0.0102  cls_loss: 0.0138  \n",
      "<<<iteration:[220/657] - total_loss: 0.1924  obj_loss: 0.0994  noobj_loss: 0.0535  bbox_loss: 0.0109  cls_loss: 0.0117  \n",
      "<<<iteration:[240/657] - total_loss: 0.1827  obj_loss: 0.0850  noobj_loss: 0.0617  bbox_loss: 0.0106  cls_loss: 0.0141  \n",
      "<<<iteration:[260/657] - total_loss: 0.1956  obj_loss: 0.0968  noobj_loss: 0.0581  bbox_loss: 0.0108  cls_loss: 0.0157  \n",
      "<<<iteration:[280/657] - total_loss: 0.2045  obj_loss: 0.1058  noobj_loss: 0.0685  bbox_loss: 0.0101  cls_loss: 0.0138  \n",
      "<<<iteration:[300/657] - total_loss: 0.2001  obj_loss: 0.1044  noobj_loss: 0.0548  bbox_loss: 0.0103  cls_loss: 0.0166  \n",
      "<<<iteration:[320/657] - total_loss: 0.2045  obj_loss: 0.1132  noobj_loss: 0.0586  bbox_loss: 0.0096  cls_loss: 0.0140  \n",
      "<<<iteration:[340/657] - total_loss: 0.1885  obj_loss: 0.0955  noobj_loss: 0.0618  bbox_loss: 0.0096  cls_loss: 0.0141  \n",
      "<<<iteration:[360/657] - total_loss: 0.1915  obj_loss: 0.0885  noobj_loss: 0.0658  bbox_loss: 0.0109  cls_loss: 0.0157  \n",
      "<<<iteration:[380/657] - total_loss: 0.1780  obj_loss: 0.0892  noobj_loss: 0.0605  bbox_loss: 0.0085  cls_loss: 0.0162  \n",
      "<<<iteration:[400/657] - total_loss: 0.1902  obj_loss: 0.0941  noobj_loss: 0.0698  bbox_loss: 0.0097  cls_loss: 0.0128  \n",
      "<<<iteration:[420/657] - total_loss: 0.1878  obj_loss: 0.0893  noobj_loss: 0.0638  bbox_loss: 0.0101  cls_loss: 0.0159  \n",
      "<<<iteration:[440/657] - total_loss: 0.1909  obj_loss: 0.0939  noobj_loss: 0.0675  bbox_loss: 0.0101  cls_loss: 0.0127  \n",
      "<<<iteration:[460/657] - total_loss: 0.1947  obj_loss: 0.1002  noobj_loss: 0.0604  bbox_loss: 0.0099  cls_loss: 0.0149  \n",
      "<<<iteration:[480/657] - total_loss: 0.1782  obj_loss: 0.0863  noobj_loss: 0.0588  bbox_loss: 0.0103  cls_loss: 0.0110  \n",
      "<<<iteration:[500/657] - total_loss: 0.1936  obj_loss: 0.0976  noobj_loss: 0.0616  bbox_loss: 0.0105  cls_loss: 0.0128  \n",
      "<<<iteration:[520/657] - total_loss: 0.1941  obj_loss: 0.0982  noobj_loss: 0.0654  bbox_loss: 0.0101  cls_loss: 0.0129  \n",
      "<<<iteration:[540/657] - total_loss: 0.2037  obj_loss: 0.0980  noobj_loss: 0.0554  bbox_loss: 0.0121  cls_loss: 0.0175  \n",
      "<<<iteration:[560/657] - total_loss: 0.1775  obj_loss: 0.0868  noobj_loss: 0.0494  bbox_loss: 0.0100  cls_loss: 0.0162  \n",
      "<<<iteration:[580/657] - total_loss: 0.1833  obj_loss: 0.0917  noobj_loss: 0.0606  bbox_loss: 0.0097  cls_loss: 0.0127  \n",
      "<<<iteration:[600/657] - total_loss: 0.1966  obj_loss: 0.0976  noobj_loss: 0.0592  bbox_loss: 0.0110  cls_loss: 0.0146  \n",
      "<<<iteration:[620/657] - total_loss: 0.1918  obj_loss: 0.0956  noobj_loss: 0.0561  bbox_loss: 0.0107  cls_loss: 0.0145  \n",
      "<<<iteration:[640/657] - total_loss: 0.1826  obj_loss: 0.0910  noobj_loss: 0.0586  bbox_loss: 0.0098  cls_loss: 0.0136  \n",
      "\n",
      "epoch:54/100 - Train Loss: 0.1904, Val Loss: 0.2385\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2116  obj_loss: 0.0929  noobj_loss: 0.0544  bbox_loss: 0.0145  cls_loss: 0.0188  \n",
      "<<<iteration:[40/657] - total_loss: 0.1788  obj_loss: 0.0923  noobj_loss: 0.0579  bbox_loss: 0.0089  cls_loss: 0.0132  \n",
      "<<<iteration:[60/657] - total_loss: 0.1847  obj_loss: 0.0881  noobj_loss: 0.0602  bbox_loss: 0.0101  cls_loss: 0.0158  \n",
      "<<<iteration:[80/657] - total_loss: 0.1811  obj_loss: 0.0936  noobj_loss: 0.0565  bbox_loss: 0.0096  cls_loss: 0.0109  \n",
      "<<<iteration:[100/657] - total_loss: 0.1995  obj_loss: 0.0976  noobj_loss: 0.0553  bbox_loss: 0.0119  cls_loss: 0.0149  \n",
      "<<<iteration:[120/657] - total_loss: 0.1950  obj_loss: 0.1055  noobj_loss: 0.0612  bbox_loss: 0.0090  cls_loss: 0.0139  \n",
      "<<<iteration:[140/657] - total_loss: 0.2026  obj_loss: 0.1012  noobj_loss: 0.0648  bbox_loss: 0.0106  cls_loss: 0.0161  \n",
      "<<<iteration:[160/657] - total_loss: 0.1879  obj_loss: 0.0983  noobj_loss: 0.0660  bbox_loss: 0.0086  cls_loss: 0.0139  \n",
      "<<<iteration:[180/657] - total_loss: 0.2111  obj_loss: 0.1040  noobj_loss: 0.0631  bbox_loss: 0.0122  cls_loss: 0.0146  \n",
      "<<<iteration:[200/657] - total_loss: 0.2040  obj_loss: 0.1100  noobj_loss: 0.0642  bbox_loss: 0.0097  cls_loss: 0.0136  \n",
      "<<<iteration:[220/657] - total_loss: 0.1823  obj_loss: 0.0861  noobj_loss: 0.0573  bbox_loss: 0.0109  cls_loss: 0.0130  \n",
      "<<<iteration:[240/657] - total_loss: 0.1914  obj_loss: 0.0885  noobj_loss: 0.0593  bbox_loss: 0.0108  cls_loss: 0.0192  \n",
      "<<<iteration:[260/657] - total_loss: 0.1922  obj_loss: 0.0924  noobj_loss: 0.0647  bbox_loss: 0.0106  cls_loss: 0.0145  \n",
      "<<<iteration:[280/657] - total_loss: 0.1920  obj_loss: 0.0938  noobj_loss: 0.0590  bbox_loss: 0.0104  cls_loss: 0.0166  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[300/657] - total_loss: 0.2083  obj_loss: 0.1017  noobj_loss: 0.0612  bbox_loss: 0.0119  cls_loss: 0.0166  \n",
      "<<<iteration:[320/657] - total_loss: 0.1827  obj_loss: 0.0917  noobj_loss: 0.0585  bbox_loss: 0.0098  cls_loss: 0.0129  \n",
      "<<<iteration:[340/657] - total_loss: 0.1908  obj_loss: 0.0866  noobj_loss: 0.0579  bbox_loss: 0.0119  cls_loss: 0.0157  \n",
      "<<<iteration:[360/657] - total_loss: 0.1864  obj_loss: 0.0936  noobj_loss: 0.0690  bbox_loss: 0.0088  cls_loss: 0.0145  \n",
      "<<<iteration:[380/657] - total_loss: 0.1959  obj_loss: 0.1008  noobj_loss: 0.0605  bbox_loss: 0.0100  cls_loss: 0.0152  \n",
      "<<<iteration:[400/657] - total_loss: 0.1922  obj_loss: 0.0981  noobj_loss: 0.0600  bbox_loss: 0.0100  cls_loss: 0.0140  \n",
      "<<<iteration:[420/657] - total_loss: 0.1943  obj_loss: 0.1052  noobj_loss: 0.0547  bbox_loss: 0.0094  cls_loss: 0.0148  \n",
      "<<<iteration:[440/657] - total_loss: 0.1875  obj_loss: 0.0909  noobj_loss: 0.0618  bbox_loss: 0.0099  cls_loss: 0.0160  \n",
      "<<<iteration:[460/657] - total_loss: 0.2081  obj_loss: 0.0945  noobj_loss: 0.0654  bbox_loss: 0.0129  cls_loss: 0.0164  \n",
      "<<<iteration:[480/657] - total_loss: 0.2011  obj_loss: 0.0963  noobj_loss: 0.0688  bbox_loss: 0.0112  cls_loss: 0.0145  \n",
      "<<<iteration:[500/657] - total_loss: 0.1930  obj_loss: 0.0956  noobj_loss: 0.0494  bbox_loss: 0.0113  cls_loss: 0.0160  \n",
      "<<<iteration:[520/657] - total_loss: 0.2002  obj_loss: 0.0948  noobj_loss: 0.0598  bbox_loss: 0.0127  cls_loss: 0.0119  \n",
      "<<<iteration:[540/657] - total_loss: 0.1772  obj_loss: 0.0839  noobj_loss: 0.0535  bbox_loss: 0.0104  cls_loss: 0.0143  \n",
      "<<<iteration:[560/657] - total_loss: 0.2074  obj_loss: 0.0921  noobj_loss: 0.0557  bbox_loss: 0.0139  cls_loss: 0.0177  \n",
      "<<<iteration:[580/657] - total_loss: 0.2158  obj_loss: 0.0994  noobj_loss: 0.0702  bbox_loss: 0.0124  cls_loss: 0.0191  \n",
      "<<<iteration:[600/657] - total_loss: 0.1917  obj_loss: 0.0947  noobj_loss: 0.0592  bbox_loss: 0.0108  cls_loss: 0.0136  \n",
      "<<<iteration:[620/657] - total_loss: 0.1901  obj_loss: 0.0883  noobj_loss: 0.0590  bbox_loss: 0.0115  cls_loss: 0.0145  \n",
      "<<<iteration:[640/657] - total_loss: 0.1834  obj_loss: 0.0889  noobj_loss: 0.0545  bbox_loss: 0.0104  cls_loss: 0.0153  \n",
      "\n",
      "epoch:55/100 - Train Loss: 0.1940, Val Loss: 0.2392\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1885  obj_loss: 0.0921  noobj_loss: 0.0621  bbox_loss: 0.0098  cls_loss: 0.0163  \n",
      "<<<iteration:[40/657] - total_loss: 0.2035  obj_loss: 0.0976  noobj_loss: 0.0616  bbox_loss: 0.0125  cls_loss: 0.0127  \n",
      "<<<iteration:[60/657] - total_loss: 0.1912  obj_loss: 0.0964  noobj_loss: 0.0625  bbox_loss: 0.0094  cls_loss: 0.0167  \n",
      "<<<iteration:[80/657] - total_loss: 0.1757  obj_loss: 0.0807  noobj_loss: 0.0641  bbox_loss: 0.0102  cls_loss: 0.0120  \n",
      "<<<iteration:[100/657] - total_loss: 0.1835  obj_loss: 0.0965  noobj_loss: 0.0555  bbox_loss: 0.0090  cls_loss: 0.0143  \n",
      "<<<iteration:[120/657] - total_loss: 0.2047  obj_loss: 0.0933  noobj_loss: 0.0716  bbox_loss: 0.0121  cls_loss: 0.0152  \n",
      "<<<iteration:[140/657] - total_loss: 0.1930  obj_loss: 0.1011  noobj_loss: 0.0614  bbox_loss: 0.0094  cls_loss: 0.0142  \n",
      "<<<iteration:[160/657] - total_loss: 0.1939  obj_loss: 0.1093  noobj_loss: 0.0581  bbox_loss: 0.0088  cls_loss: 0.0113  \n",
      "<<<iteration:[180/657] - total_loss: 0.1877  obj_loss: 0.0963  noobj_loss: 0.0590  bbox_loss: 0.0099  cls_loss: 0.0126  \n",
      "<<<iteration:[200/657] - total_loss: 0.1837  obj_loss: 0.0946  noobj_loss: 0.0555  bbox_loss: 0.0094  cls_loss: 0.0141  \n",
      "<<<iteration:[220/657] - total_loss: 0.1836  obj_loss: 0.0919  noobj_loss: 0.0598  bbox_loss: 0.0097  cls_loss: 0.0134  \n",
      "<<<iteration:[240/657] - total_loss: 0.2013  obj_loss: 0.1038  noobj_loss: 0.0600  bbox_loss: 0.0111  cls_loss: 0.0118  \n",
      "<<<iteration:[260/657] - total_loss: 0.2092  obj_loss: 0.1031  noobj_loss: 0.0566  bbox_loss: 0.0120  cls_loss: 0.0177  \n",
      "<<<iteration:[280/657] - total_loss: 0.2010  obj_loss: 0.1022  noobj_loss: 0.0652  bbox_loss: 0.0099  cls_loss: 0.0168  \n",
      "<<<iteration:[300/657] - total_loss: 0.2007  obj_loss: 0.0861  noobj_loss: 0.0595  bbox_loss: 0.0141  cls_loss: 0.0143  \n",
      "<<<iteration:[320/657] - total_loss: 0.1829  obj_loss: 0.0942  noobj_loss: 0.0585  bbox_loss: 0.0090  cls_loss: 0.0145  \n",
      "<<<iteration:[340/657] - total_loss: 0.1860  obj_loss: 0.0896  noobj_loss: 0.0594  bbox_loss: 0.0108  cls_loss: 0.0128  \n",
      "<<<iteration:[360/657] - total_loss: 0.1844  obj_loss: 0.0916  noobj_loss: 0.0592  bbox_loss: 0.0103  cls_loss: 0.0119  \n",
      "<<<iteration:[380/657] - total_loss: 0.1996  obj_loss: 0.1033  noobj_loss: 0.0640  bbox_loss: 0.0105  cls_loss: 0.0118  \n",
      "<<<iteration:[400/657] - total_loss: 0.1946  obj_loss: 0.0968  noobj_loss: 0.0611  bbox_loss: 0.0104  cls_loss: 0.0152  \n",
      "<<<iteration:[420/657] - total_loss: 0.1892  obj_loss: 0.0922  noobj_loss: 0.0549  bbox_loss: 0.0107  cls_loss: 0.0158  \n",
      "<<<iteration:[440/657] - total_loss: 0.1856  obj_loss: 0.0885  noobj_loss: 0.0535  bbox_loss: 0.0106  cls_loss: 0.0174  \n",
      "<<<iteration:[460/657] - total_loss: 0.1972  obj_loss: 0.0883  noobj_loss: 0.0557  bbox_loss: 0.0125  cls_loss: 0.0187  \n",
      "<<<iteration:[480/657] - total_loss: 0.1788  obj_loss: 0.0828  noobj_loss: 0.0688  bbox_loss: 0.0100  cls_loss: 0.0115  \n",
      "<<<iteration:[500/657] - total_loss: 0.1969  obj_loss: 0.0881  noobj_loss: 0.0601  bbox_loss: 0.0125  cls_loss: 0.0165  \n",
      "<<<iteration:[520/657] - total_loss: 0.1996  obj_loss: 0.0986  noobj_loss: 0.0595  bbox_loss: 0.0109  cls_loss: 0.0166  \n",
      "<<<iteration:[540/657] - total_loss: 0.1853  obj_loss: 0.0940  noobj_loss: 0.0546  bbox_loss: 0.0095  cls_loss: 0.0164  \n",
      "<<<iteration:[560/657] - total_loss: 0.1765  obj_loss: 0.0885  noobj_loss: 0.0577  bbox_loss: 0.0091  cls_loss: 0.0139  \n",
      "<<<iteration:[580/657] - total_loss: 0.1909  obj_loss: 0.1046  noobj_loss: 0.0578  bbox_loss: 0.0089  cls_loss: 0.0131  \n",
      "<<<iteration:[600/657] - total_loss: 0.1914  obj_loss: 0.0906  noobj_loss: 0.0675  bbox_loss: 0.0109  cls_loss: 0.0123  \n",
      "<<<iteration:[620/657] - total_loss: 0.1904  obj_loss: 0.0983  noobj_loss: 0.0618  bbox_loss: 0.0096  cls_loss: 0.0131  \n",
      "<<<iteration:[640/657] - total_loss: 0.1852  obj_loss: 0.0961  noobj_loss: 0.0574  bbox_loss: 0.0096  cls_loss: 0.0122  \n",
      "\n",
      "epoch:56/100 - Train Loss: 0.1909, Val Loss: 0.2366\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2046  obj_loss: 0.0975  noobj_loss: 0.0678  bbox_loss: 0.0114  cls_loss: 0.0163  \n",
      "<<<iteration:[40/657] - total_loss: 0.1868  obj_loss: 0.0874  noobj_loss: 0.0565  bbox_loss: 0.0112  cls_loss: 0.0152  \n",
      "<<<iteration:[60/657] - total_loss: 0.1834  obj_loss: 0.0944  noobj_loss: 0.0559  bbox_loss: 0.0095  cls_loss: 0.0137  \n",
      "<<<iteration:[80/657] - total_loss: 0.1868  obj_loss: 0.0976  noobj_loss: 0.0617  bbox_loss: 0.0094  cls_loss: 0.0114  \n",
      "<<<iteration:[100/657] - total_loss: 0.1864  obj_loss: 0.0970  noobj_loss: 0.0638  bbox_loss: 0.0090  cls_loss: 0.0125  \n",
      "<<<iteration:[120/657] - total_loss: 0.2114  obj_loss: 0.1016  noobj_loss: 0.0544  bbox_loss: 0.0135  cls_loss: 0.0150  \n",
      "<<<iteration:[140/657] - total_loss: 0.1816  obj_loss: 0.0903  noobj_loss: 0.0584  bbox_loss: 0.0098  cls_loss: 0.0131  \n",
      "<<<iteration:[160/657] - total_loss: 0.2001  obj_loss: 0.1021  noobj_loss: 0.0636  bbox_loss: 0.0109  cls_loss: 0.0115  \n",
      "<<<iteration:[180/657] - total_loss: 0.1735  obj_loss: 0.0887  noobj_loss: 0.0501  bbox_loss: 0.0092  cls_loss: 0.0135  \n",
      "<<<iteration:[200/657] - total_loss: 0.1938  obj_loss: 0.0924  noobj_loss: 0.0607  bbox_loss: 0.0108  cls_loss: 0.0172  \n",
      "<<<iteration:[220/657] - total_loss: 0.1952  obj_loss: 0.0955  noobj_loss: 0.0623  bbox_loss: 0.0108  cls_loss: 0.0145  \n",
      "<<<iteration:[240/657] - total_loss: 0.1809  obj_loss: 0.0904  noobj_loss: 0.0581  bbox_loss: 0.0098  cls_loss: 0.0125  \n",
      "<<<iteration:[260/657] - total_loss: 0.1773  obj_loss: 0.0851  noobj_loss: 0.0616  bbox_loss: 0.0093  cls_loss: 0.0146  \n",
      "<<<iteration:[280/657] - total_loss: 0.1961  obj_loss: 0.0971  noobj_loss: 0.0579  bbox_loss: 0.0112  cls_loss: 0.0139  \n",
      "<<<iteration:[300/657] - total_loss: 0.1823  obj_loss: 0.0890  noobj_loss: 0.0653  bbox_loss: 0.0097  cls_loss: 0.0122  \n",
      "<<<iteration:[320/657] - total_loss: 0.1848  obj_loss: 0.0934  noobj_loss: 0.0632  bbox_loss: 0.0093  cls_loss: 0.0133  \n",
      "<<<iteration:[340/657] - total_loss: 0.1995  obj_loss: 0.0944  noobj_loss: 0.0687  bbox_loss: 0.0111  cls_loss: 0.0151  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[360/657] - total_loss: 0.1919  obj_loss: 0.1016  noobj_loss: 0.0564  bbox_loss: 0.0096  cls_loss: 0.0139  \n",
      "<<<iteration:[380/657] - total_loss: 0.1736  obj_loss: 0.0858  noobj_loss: 0.0527  bbox_loss: 0.0097  cls_loss: 0.0128  \n",
      "<<<iteration:[400/657] - total_loss: 0.1870  obj_loss: 0.0971  noobj_loss: 0.0619  bbox_loss: 0.0086  cls_loss: 0.0162  \n",
      "<<<iteration:[420/657] - total_loss: 0.2016  obj_loss: 0.1027  noobj_loss: 0.0693  bbox_loss: 0.0100  cls_loss: 0.0145  \n",
      "<<<iteration:[440/657] - total_loss: 0.1719  obj_loss: 0.0866  noobj_loss: 0.0619  bbox_loss: 0.0087  cls_loss: 0.0106  \n",
      "<<<iteration:[460/657] - total_loss: 0.1897  obj_loss: 0.0913  noobj_loss: 0.0700  bbox_loss: 0.0102  cls_loss: 0.0124  \n",
      "<<<iteration:[480/657] - total_loss: 0.1997  obj_loss: 0.0951  noobj_loss: 0.0645  bbox_loss: 0.0115  cls_loss: 0.0148  \n",
      "<<<iteration:[500/657] - total_loss: 0.1920  obj_loss: 0.1010  noobj_loss: 0.0620  bbox_loss: 0.0095  cls_loss: 0.0124  \n",
      "<<<iteration:[520/657] - total_loss: 0.1919  obj_loss: 0.0981  noobj_loss: 0.0599  bbox_loss: 0.0100  cls_loss: 0.0139  \n",
      "<<<iteration:[540/657] - total_loss: 0.1663  obj_loss: 0.0834  noobj_loss: 0.0575  bbox_loss: 0.0088  cls_loss: 0.0101  \n",
      "<<<iteration:[560/657] - total_loss: 0.1804  obj_loss: 0.0910  noobj_loss: 0.0643  bbox_loss: 0.0092  cls_loss: 0.0110  \n",
      "<<<iteration:[580/657] - total_loss: 0.1807  obj_loss: 0.0948  noobj_loss: 0.0597  bbox_loss: 0.0088  cls_loss: 0.0119  \n",
      "<<<iteration:[600/657] - total_loss: 0.2029  obj_loss: 0.1009  noobj_loss: 0.0644  bbox_loss: 0.0113  cls_loss: 0.0136  \n",
      "<<<iteration:[620/657] - total_loss: 0.1848  obj_loss: 0.0931  noobj_loss: 0.0591  bbox_loss: 0.0096  cls_loss: 0.0142  \n",
      "<<<iteration:[640/657] - total_loss: 0.2009  obj_loss: 0.1030  noobj_loss: 0.0616  bbox_loss: 0.0105  cls_loss: 0.0147  \n",
      "\n",
      "epoch:57/100 - Train Loss: 0.1890, Val Loss: 0.2389\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1913  obj_loss: 0.0942  noobj_loss: 0.0661  bbox_loss: 0.0100  cls_loss: 0.0141  \n",
      "<<<iteration:[40/657] - total_loss: 0.1792  obj_loss: 0.0899  noobj_loss: 0.0578  bbox_loss: 0.0093  cls_loss: 0.0141  \n",
      "<<<iteration:[60/657] - total_loss: 0.1904  obj_loss: 0.1034  noobj_loss: 0.0630  bbox_loss: 0.0084  cls_loss: 0.0133  \n",
      "<<<iteration:[80/657] - total_loss: 0.1784  obj_loss: 0.0882  noobj_loss: 0.0582  bbox_loss: 0.0096  cls_loss: 0.0129  \n",
      "<<<iteration:[100/657] - total_loss: 0.1699  obj_loss: 0.0810  noobj_loss: 0.0609  bbox_loss: 0.0092  cls_loss: 0.0126  \n",
      "<<<iteration:[120/657] - total_loss: 0.1906  obj_loss: 0.1005  noobj_loss: 0.0652  bbox_loss: 0.0092  cls_loss: 0.0113  \n",
      "<<<iteration:[140/657] - total_loss: 0.1983  obj_loss: 0.0993  noobj_loss: 0.0581  bbox_loss: 0.0106  cls_loss: 0.0169  \n",
      "<<<iteration:[160/657] - total_loss: 0.2036  obj_loss: 0.0909  noobj_loss: 0.0583  bbox_loss: 0.0133  cls_loss: 0.0168  \n",
      "<<<iteration:[180/657] - total_loss: 0.1802  obj_loss: 0.0925  noobj_loss: 0.0587  bbox_loss: 0.0089  cls_loss: 0.0137  \n",
      "<<<iteration:[200/657] - total_loss: 0.1949  obj_loss: 0.1061  noobj_loss: 0.0573  bbox_loss: 0.0100  cls_loss: 0.0102  \n",
      "<<<iteration:[220/657] - total_loss: 0.1903  obj_loss: 0.0908  noobj_loss: 0.0627  bbox_loss: 0.0104  cls_loss: 0.0160  \n",
      "<<<iteration:[240/657] - total_loss: 0.1737  obj_loss: 0.0887  noobj_loss: 0.0562  bbox_loss: 0.0090  cls_loss: 0.0117  \n",
      "<<<iteration:[260/657] - total_loss: 0.1845  obj_loss: 0.0911  noobj_loss: 0.0622  bbox_loss: 0.0096  cls_loss: 0.0144  \n",
      "<<<iteration:[280/657] - total_loss: 0.1962  obj_loss: 0.1066  noobj_loss: 0.0572  bbox_loss: 0.0095  cls_loss: 0.0136  \n",
      "<<<iteration:[300/657] - total_loss: 0.1902  obj_loss: 0.0931  noobj_loss: 0.0648  bbox_loss: 0.0105  cls_loss: 0.0123  \n",
      "<<<iteration:[320/657] - total_loss: 0.1988  obj_loss: 0.0938  noobj_loss: 0.0693  bbox_loss: 0.0110  cls_loss: 0.0155  \n",
      "<<<iteration:[340/657] - total_loss: 0.2081  obj_loss: 0.1054  noobj_loss: 0.0597  bbox_loss: 0.0119  cls_loss: 0.0132  \n",
      "<<<iteration:[360/657] - total_loss: 0.2046  obj_loss: 0.1033  noobj_loss: 0.0535  bbox_loss: 0.0116  cls_loss: 0.0167  \n",
      "<<<iteration:[380/657] - total_loss: 0.1984  obj_loss: 0.1083  noobj_loss: 0.0599  bbox_loss: 0.0092  cls_loss: 0.0141  \n",
      "<<<iteration:[400/657] - total_loss: 0.1866  obj_loss: 0.0954  noobj_loss: 0.0610  bbox_loss: 0.0095  cls_loss: 0.0134  \n",
      "<<<iteration:[420/657] - total_loss: 0.1947  obj_loss: 0.0976  noobj_loss: 0.0658  bbox_loss: 0.0108  cls_loss: 0.0100  \n",
      "<<<iteration:[440/657] - total_loss: 0.1857  obj_loss: 0.0960  noobj_loss: 0.0547  bbox_loss: 0.0097  cls_loss: 0.0137  \n",
      "<<<iteration:[460/657] - total_loss: 0.1893  obj_loss: 0.0928  noobj_loss: 0.0675  bbox_loss: 0.0096  cls_loss: 0.0147  \n",
      "<<<iteration:[480/657] - total_loss: 0.1829  obj_loss: 0.0949  noobj_loss: 0.0604  bbox_loss: 0.0090  cls_loss: 0.0127  \n",
      "<<<iteration:[500/657] - total_loss: 0.1833  obj_loss: 0.0899  noobj_loss: 0.0553  bbox_loss: 0.0106  cls_loss: 0.0128  \n",
      "<<<iteration:[520/657] - total_loss: 0.1816  obj_loss: 0.0951  noobj_loss: 0.0603  bbox_loss: 0.0087  cls_loss: 0.0127  \n",
      "<<<iteration:[540/657] - total_loss: 0.1835  obj_loss: 0.0946  noobj_loss: 0.0620  bbox_loss: 0.0091  cls_loss: 0.0123  \n",
      "<<<iteration:[560/657] - total_loss: 0.1984  obj_loss: 0.1007  noobj_loss: 0.0586  bbox_loss: 0.0104  cls_loss: 0.0162  \n",
      "<<<iteration:[580/657] - total_loss: 0.1681  obj_loss: 0.0845  noobj_loss: 0.0572  bbox_loss: 0.0085  cls_loss: 0.0127  \n",
      "<<<iteration:[600/657] - total_loss: 0.1832  obj_loss: 0.0906  noobj_loss: 0.0556  bbox_loss: 0.0099  cls_loss: 0.0152  \n",
      "<<<iteration:[620/657] - total_loss: 0.1803  obj_loss: 0.0886  noobj_loss: 0.0612  bbox_loss: 0.0093  cls_loss: 0.0148  \n",
      "<<<iteration:[640/657] - total_loss: 0.1777  obj_loss: 0.0901  noobj_loss: 0.0614  bbox_loss: 0.0087  cls_loss: 0.0136  \n",
      "\n",
      "epoch:58/100 - Train Loss: 0.1878, Val Loss: 0.2363\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.2185  obj_loss: 0.1066  noobj_loss: 0.0630  bbox_loss: 0.0133  cls_loss: 0.0138  \n",
      "<<<iteration:[40/657] - total_loss: 0.1911  obj_loss: 0.1046  noobj_loss: 0.0607  bbox_loss: 0.0084  cls_loss: 0.0142  \n",
      "<<<iteration:[60/657] - total_loss: 0.1941  obj_loss: 0.0920  noobj_loss: 0.0575  bbox_loss: 0.0110  cls_loss: 0.0184  \n",
      "<<<iteration:[80/657] - total_loss: 0.1807  obj_loss: 0.0885  noobj_loss: 0.0496  bbox_loss: 0.0102  cls_loss: 0.0162  \n",
      "<<<iteration:[100/657] - total_loss: 0.2020  obj_loss: 0.1047  noobj_loss: 0.0597  bbox_loss: 0.0108  cls_loss: 0.0132  \n",
      "<<<iteration:[120/657] - total_loss: 0.1958  obj_loss: 0.0910  noobj_loss: 0.0647  bbox_loss: 0.0119  cls_loss: 0.0131  \n",
      "<<<iteration:[140/657] - total_loss: 0.1874  obj_loss: 0.0934  noobj_loss: 0.0555  bbox_loss: 0.0107  cls_loss: 0.0129  \n",
      "<<<iteration:[160/657] - total_loss: 0.1826  obj_loss: 0.0898  noobj_loss: 0.0612  bbox_loss: 0.0097  cls_loss: 0.0137  \n",
      "<<<iteration:[180/657] - total_loss: 0.1749  obj_loss: 0.0882  noobj_loss: 0.0575  bbox_loss: 0.0096  cls_loss: 0.0100  \n",
      "<<<iteration:[200/657] - total_loss: 0.1867  obj_loss: 0.0952  noobj_loss: 0.0533  bbox_loss: 0.0095  cls_loss: 0.0174  \n",
      "<<<iteration:[220/657] - total_loss: 0.1791  obj_loss: 0.0938  noobj_loss: 0.0602  bbox_loss: 0.0085  cls_loss: 0.0125  \n",
      "<<<iteration:[240/657] - total_loss: 0.1736  obj_loss: 0.0898  noobj_loss: 0.0566  bbox_loss: 0.0085  cls_loss: 0.0133  \n",
      "<<<iteration:[260/657] - total_loss: 0.1767  obj_loss: 0.0894  noobj_loss: 0.0541  bbox_loss: 0.0094  cls_loss: 0.0132  \n",
      "<<<iteration:[280/657] - total_loss: 0.1777  obj_loss: 0.0962  noobj_loss: 0.0615  bbox_loss: 0.0081  cls_loss: 0.0102  \n",
      "<<<iteration:[300/657] - total_loss: 0.1928  obj_loss: 0.0952  noobj_loss: 0.0546  bbox_loss: 0.0106  cls_loss: 0.0173  \n",
      "<<<iteration:[320/657] - total_loss: 0.1789  obj_loss: 0.0922  noobj_loss: 0.0550  bbox_loss: 0.0091  cls_loss: 0.0134  \n",
      "<<<iteration:[340/657] - total_loss: 0.1982  obj_loss: 0.1031  noobj_loss: 0.0651  bbox_loss: 0.0102  cls_loss: 0.0114  \n",
      "<<<iteration:[360/657] - total_loss: 0.1909  obj_loss: 0.0991  noobj_loss: 0.0649  bbox_loss: 0.0097  cls_loss: 0.0106  \n",
      "<<<iteration:[380/657] - total_loss: 0.1745  obj_loss: 0.0872  noobj_loss: 0.0473  bbox_loss: 0.0099  cls_loss: 0.0140  \n",
      "<<<iteration:[400/657] - total_loss: 0.1823  obj_loss: 0.0934  noobj_loss: 0.0624  bbox_loss: 0.0090  cls_loss: 0.0129  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[420/657] - total_loss: 0.1860  obj_loss: 0.0858  noobj_loss: 0.0685  bbox_loss: 0.0107  cls_loss: 0.0126  \n",
      "<<<iteration:[440/657] - total_loss: 0.1785  obj_loss: 0.0930  noobj_loss: 0.0548  bbox_loss: 0.0094  cls_loss: 0.0114  \n",
      "<<<iteration:[460/657] - total_loss: 0.1888  obj_loss: 0.0996  noobj_loss: 0.0550  bbox_loss: 0.0097  cls_loss: 0.0132  \n",
      "<<<iteration:[480/657] - total_loss: 0.1851  obj_loss: 0.0919  noobj_loss: 0.0623  bbox_loss: 0.0098  cls_loss: 0.0132  \n",
      "<<<iteration:[500/657] - total_loss: 0.1757  obj_loss: 0.0865  noobj_loss: 0.0616  bbox_loss: 0.0094  cls_loss: 0.0113  \n",
      "<<<iteration:[520/657] - total_loss: 0.1918  obj_loss: 0.0961  noobj_loss: 0.0648  bbox_loss: 0.0100  cls_loss: 0.0134  \n",
      "<<<iteration:[540/657] - total_loss: 0.2066  obj_loss: 0.0995  noobj_loss: 0.0631  bbox_loss: 0.0114  cls_loss: 0.0186  \n",
      "<<<iteration:[560/657] - total_loss: 0.1825  obj_loss: 0.0926  noobj_loss: 0.0641  bbox_loss: 0.0087  cls_loss: 0.0142  \n",
      "<<<iteration:[580/657] - total_loss: 0.1942  obj_loss: 0.0974  noobj_loss: 0.0621  bbox_loss: 0.0105  cls_loss: 0.0130  \n",
      "<<<iteration:[600/657] - total_loss: 0.1764  obj_loss: 0.0905  noobj_loss: 0.0563  bbox_loss: 0.0092  cls_loss: 0.0116  \n",
      "<<<iteration:[620/657] - total_loss: 0.1687  obj_loss: 0.0803  noobj_loss: 0.0600  bbox_loss: 0.0095  cls_loss: 0.0110  \n",
      "<<<iteration:[640/657] - total_loss: 0.2013  obj_loss: 0.1085  noobj_loss: 0.0636  bbox_loss: 0.0098  cls_loss: 0.0121  \n",
      "\n",
      "epoch:59/100 - Train Loss: 0.1867, Val Loss: 0.2342\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1844  obj_loss: 0.0915  noobj_loss: 0.0606  bbox_loss: 0.0095  cls_loss: 0.0152  \n",
      "<<<iteration:[40/657] - total_loss: 0.1759  obj_loss: 0.0942  noobj_loss: 0.0569  bbox_loss: 0.0084  cls_loss: 0.0114  \n",
      "<<<iteration:[60/657] - total_loss: 0.1901  obj_loss: 0.0956  noobj_loss: 0.0588  bbox_loss: 0.0102  cls_loss: 0.0138  \n",
      "<<<iteration:[80/657] - total_loss: 0.1866  obj_loss: 0.0919  noobj_loss: 0.0635  bbox_loss: 0.0104  cls_loss: 0.0110  \n",
      "<<<iteration:[100/657] - total_loss: 0.1739  obj_loss: 0.0820  noobj_loss: 0.0532  bbox_loss: 0.0104  cls_loss: 0.0131  \n",
      "<<<iteration:[120/657] - total_loss: 0.1868  obj_loss: 0.0945  noobj_loss: 0.0590  bbox_loss: 0.0098  cls_loss: 0.0136  \n",
      "<<<iteration:[140/657] - total_loss: 0.1906  obj_loss: 0.0933  noobj_loss: 0.0617  bbox_loss: 0.0109  cls_loss: 0.0121  \n",
      "<<<iteration:[160/657] - total_loss: 0.1923  obj_loss: 0.0971  noobj_loss: 0.0635  bbox_loss: 0.0100  cls_loss: 0.0135  \n",
      "<<<iteration:[180/657] - total_loss: 0.1759  obj_loss: 0.0896  noobj_loss: 0.0594  bbox_loss: 0.0091  cls_loss: 0.0110  \n",
      "<<<iteration:[200/657] - total_loss: 0.1855  obj_loss: 0.0929  noobj_loss: 0.0610  bbox_loss: 0.0094  cls_loss: 0.0152  \n",
      "<<<iteration:[220/657] - total_loss: 0.1841  obj_loss: 0.0921  noobj_loss: 0.0668  bbox_loss: 0.0097  cls_loss: 0.0100  \n",
      "<<<iteration:[240/657] - total_loss: 0.1751  obj_loss: 0.0901  noobj_loss: 0.0594  bbox_loss: 0.0085  cls_loss: 0.0129  \n",
      "<<<iteration:[260/657] - total_loss: 0.1785  obj_loss: 0.0830  noobj_loss: 0.0581  bbox_loss: 0.0108  cls_loss: 0.0126  \n",
      "<<<iteration:[280/657] - total_loss: 0.1991  obj_loss: 0.0965  noobj_loss: 0.0618  bbox_loss: 0.0117  cls_loss: 0.0133  \n",
      "<<<iteration:[300/657] - total_loss: 0.1909  obj_loss: 0.0943  noobj_loss: 0.0615  bbox_loss: 0.0105  cls_loss: 0.0133  \n",
      "<<<iteration:[320/657] - total_loss: 0.1803  obj_loss: 0.0938  noobj_loss: 0.0569  bbox_loss: 0.0093  cls_loss: 0.0117  \n",
      "<<<iteration:[340/657] - total_loss: 0.1931  obj_loss: 0.1011  noobj_loss: 0.0566  bbox_loss: 0.0105  cls_loss: 0.0111  \n",
      "<<<iteration:[360/657] - total_loss: 0.1716  obj_loss: 0.0830  noobj_loss: 0.0576  bbox_loss: 0.0094  cls_loss: 0.0127  \n",
      "<<<iteration:[380/657] - total_loss: 0.2012  obj_loss: 0.1076  noobj_loss: 0.0584  bbox_loss: 0.0097  cls_loss: 0.0157  \n",
      "<<<iteration:[400/657] - total_loss: 0.1829  obj_loss: 0.0932  noobj_loss: 0.0638  bbox_loss: 0.0088  cls_loss: 0.0138  \n",
      "<<<iteration:[420/657] - total_loss: 0.1992  obj_loss: 0.0992  noobj_loss: 0.0625  bbox_loss: 0.0104  cls_loss: 0.0167  \n",
      "<<<iteration:[440/657] - total_loss: 0.1730  obj_loss: 0.0885  noobj_loss: 0.0595  bbox_loss: 0.0085  cls_loss: 0.0122  \n",
      "<<<iteration:[460/657] - total_loss: 0.1996  obj_loss: 0.0967  noobj_loss: 0.0568  bbox_loss: 0.0118  cls_loss: 0.0155  \n",
      "<<<iteration:[480/657] - total_loss: 0.1834  obj_loss: 0.0915  noobj_loss: 0.0555  bbox_loss: 0.0097  cls_loss: 0.0157  \n",
      "<<<iteration:[500/657] - total_loss: 0.1777  obj_loss: 0.0907  noobj_loss: 0.0618  bbox_loss: 0.0089  cls_loss: 0.0117  \n",
      "<<<iteration:[520/657] - total_loss: 0.1829  obj_loss: 0.0905  noobj_loss: 0.0591  bbox_loss: 0.0100  cls_loss: 0.0128  \n",
      "<<<iteration:[540/657] - total_loss: 0.2009  obj_loss: 0.0947  noobj_loss: 0.0648  bbox_loss: 0.0123  cls_loss: 0.0122  \n",
      "<<<iteration:[560/657] - total_loss: 0.1832  obj_loss: 0.0946  noobj_loss: 0.0586  bbox_loss: 0.0095  cls_loss: 0.0119  \n",
      "<<<iteration:[580/657] - total_loss: 0.1811  obj_loss: 0.0995  noobj_loss: 0.0539  bbox_loss: 0.0088  cls_loss: 0.0109  \n",
      "<<<iteration:[600/657] - total_loss: 0.1714  obj_loss: 0.0862  noobj_loss: 0.0547  bbox_loss: 0.0091  cls_loss: 0.0124  \n",
      "<<<iteration:[620/657] - total_loss: 0.1930  obj_loss: 0.0921  noobj_loss: 0.0682  bbox_loss: 0.0101  cls_loss: 0.0162  \n",
      "<<<iteration:[640/657] - total_loss: 0.1765  obj_loss: 0.0917  noobj_loss: 0.0629  bbox_loss: 0.0087  cls_loss: 0.0101  \n",
      "\n",
      "epoch:60/100 - Train Loss: 0.1899, Val Loss: 0.3115\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1990  obj_loss: 0.0976  noobj_loss: 0.0537  bbox_loss: 0.0122  cls_loss: 0.0135  \n",
      "<<<iteration:[40/657] - total_loss: 0.1777  obj_loss: 0.0844  noobj_loss: 0.0510  bbox_loss: 0.0109  cls_loss: 0.0132  \n",
      "<<<iteration:[60/657] - total_loss: 0.1737  obj_loss: 0.0847  noobj_loss: 0.0475  bbox_loss: 0.0102  cls_loss: 0.0140  \n",
      "<<<iteration:[80/657] - total_loss: 0.1743  obj_loss: 0.0847  noobj_loss: 0.0587  bbox_loss: 0.0091  cls_loss: 0.0150  \n",
      "<<<iteration:[100/657] - total_loss: 0.1830  obj_loss: 0.0915  noobj_loss: 0.0609  bbox_loss: 0.0095  cls_loss: 0.0136  \n",
      "<<<iteration:[120/657] - total_loss: 0.1910  obj_loss: 0.0958  noobj_loss: 0.0653  bbox_loss: 0.0099  cls_loss: 0.0129  \n",
      "<<<iteration:[140/657] - total_loss: 0.1914  obj_loss: 0.1014  noobj_loss: 0.0594  bbox_loss: 0.0091  cls_loss: 0.0146  \n",
      "<<<iteration:[160/657] - total_loss: 0.1745  obj_loss: 0.0850  noobj_loss: 0.0583  bbox_loss: 0.0098  cls_loss: 0.0114  \n",
      "<<<iteration:[180/657] - total_loss: 0.1701  obj_loss: 0.0838  noobj_loss: 0.0604  bbox_loss: 0.0090  cls_loss: 0.0110  \n",
      "<<<iteration:[200/657] - total_loss: 0.1722  obj_loss: 0.0860  noobj_loss: 0.0582  bbox_loss: 0.0089  cls_loss: 0.0124  \n",
      "<<<iteration:[220/657] - total_loss: 0.1890  obj_loss: 0.0840  noobj_loss: 0.0609  bbox_loss: 0.0116  cls_loss: 0.0163  \n",
      "<<<iteration:[240/657] - total_loss: 0.1844  obj_loss: 0.0899  noobj_loss: 0.0676  bbox_loss: 0.0100  cls_loss: 0.0106  \n",
      "<<<iteration:[260/657] - total_loss: 0.1860  obj_loss: 0.0916  noobj_loss: 0.0612  bbox_loss: 0.0094  cls_loss: 0.0169  \n",
      "<<<iteration:[280/657] - total_loss: 0.1961  obj_loss: 0.0964  noobj_loss: 0.0603  bbox_loss: 0.0107  cls_loss: 0.0158  \n",
      "<<<iteration:[300/657] - total_loss: 0.1739  obj_loss: 0.0887  noobj_loss: 0.0555  bbox_loss: 0.0094  cls_loss: 0.0106  \n",
      "<<<iteration:[320/657] - total_loss: 0.1918  obj_loss: 0.0997  noobj_loss: 0.0619  bbox_loss: 0.0099  cls_loss: 0.0116  \n",
      "<<<iteration:[340/657] - total_loss: 0.1672  obj_loss: 0.0851  noobj_loss: 0.0614  bbox_loss: 0.0080  cls_loss: 0.0115  \n",
      "<<<iteration:[360/657] - total_loss: 0.1762  obj_loss: 0.0894  noobj_loss: 0.0635  bbox_loss: 0.0086  cls_loss: 0.0120  \n",
      "<<<iteration:[380/657] - total_loss: 0.1824  obj_loss: 0.0864  noobj_loss: 0.0573  bbox_loss: 0.0103  cls_loss: 0.0159  \n",
      "<<<iteration:[400/657] - total_loss: 0.1903  obj_loss: 0.1008  noobj_loss: 0.0657  bbox_loss: 0.0090  cls_loss: 0.0117  \n",
      "<<<iteration:[420/657] - total_loss: 0.1755  obj_loss: 0.0850  noobj_loss: 0.0643  bbox_loss: 0.0091  cls_loss: 0.0127  \n",
      "<<<iteration:[440/657] - total_loss: 0.1900  obj_loss: 0.0873  noobj_loss: 0.0650  bbox_loss: 0.0115  cls_loss: 0.0128  \n",
      "<<<iteration:[460/657] - total_loss: 0.1803  obj_loss: 0.0881  noobj_loss: 0.0616  bbox_loss: 0.0099  cls_loss: 0.0120  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[480/657] - total_loss: 0.1964  obj_loss: 0.1061  noobj_loss: 0.0678  bbox_loss: 0.0090  cls_loss: 0.0114  \n",
      "<<<iteration:[500/657] - total_loss: 0.1779  obj_loss: 0.0888  noobj_loss: 0.0605  bbox_loss: 0.0091  cls_loss: 0.0133  \n",
      "<<<iteration:[520/657] - total_loss: 0.1674  obj_loss: 0.0834  noobj_loss: 0.0609  bbox_loss: 0.0081  cls_loss: 0.0130  \n",
      "<<<iteration:[540/657] - total_loss: 0.1789  obj_loss: 0.0938  noobj_loss: 0.0627  bbox_loss: 0.0086  cls_loss: 0.0106  \n",
      "<<<iteration:[560/657] - total_loss: 0.1801  obj_loss: 0.0863  noobj_loss: 0.0620  bbox_loss: 0.0100  cls_loss: 0.0130  \n",
      "<<<iteration:[580/657] - total_loss: 0.1816  obj_loss: 0.0919  noobj_loss: 0.0542  bbox_loss: 0.0098  cls_loss: 0.0136  \n",
      "<<<iteration:[600/657] - total_loss: 0.1809  obj_loss: 0.0802  noobj_loss: 0.0628  bbox_loss: 0.0112  cls_loss: 0.0135  \n",
      "<<<iteration:[620/657] - total_loss: 0.1868  obj_loss: 0.0907  noobj_loss: 0.0598  bbox_loss: 0.0106  cls_loss: 0.0131  \n",
      "<<<iteration:[640/657] - total_loss: 0.1900  obj_loss: 0.0965  noobj_loss: 0.0584  bbox_loss: 0.0102  cls_loss: 0.0134  \n",
      "\n",
      "epoch:61/100 - Train Loss: 0.1825, Val Loss: 0.2365\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1952  obj_loss: 0.1006  noobj_loss: 0.0632  bbox_loss: 0.0102  cls_loss: 0.0122  \n",
      "<<<iteration:[40/657] - total_loss: 0.1846  obj_loss: 0.0917  noobj_loss: 0.0526  bbox_loss: 0.0105  cls_loss: 0.0140  \n",
      "<<<iteration:[60/657] - total_loss: 0.1690  obj_loss: 0.0861  noobj_loss: 0.0554  bbox_loss: 0.0085  cls_loss: 0.0125  \n",
      "<<<iteration:[80/657] - total_loss: 0.1863  obj_loss: 0.0976  noobj_loss: 0.0576  bbox_loss: 0.0093  cls_loss: 0.0131  \n",
      "<<<iteration:[100/657] - total_loss: 0.1715  obj_loss: 0.0881  noobj_loss: 0.0544  bbox_loss: 0.0088  cls_loss: 0.0120  \n",
      "<<<iteration:[120/657] - total_loss: 0.1800  obj_loss: 0.0922  noobj_loss: 0.0562  bbox_loss: 0.0093  cls_loss: 0.0132  \n",
      "<<<iteration:[140/657] - total_loss: 0.1877  obj_loss: 0.0943  noobj_loss: 0.0588  bbox_loss: 0.0093  cls_loss: 0.0177  \n",
      "<<<iteration:[160/657] - total_loss: 0.1867  obj_loss: 0.0830  noobj_loss: 0.0618  bbox_loss: 0.0119  cls_loss: 0.0132  \n",
      "<<<iteration:[180/657] - total_loss: 0.1830  obj_loss: 0.0903  noobj_loss: 0.0637  bbox_loss: 0.0095  cls_loss: 0.0133  \n",
      "<<<iteration:[200/657] - total_loss: 0.2096  obj_loss: 0.1044  noobj_loss: 0.0593  bbox_loss: 0.0114  cls_loss: 0.0185  \n",
      "<<<iteration:[220/657] - total_loss: 0.1824  obj_loss: 0.0901  noobj_loss: 0.0601  bbox_loss: 0.0099  cls_loss: 0.0129  \n",
      "<<<iteration:[240/657] - total_loss: 0.1669  obj_loss: 0.0801  noobj_loss: 0.0564  bbox_loss: 0.0092  cls_loss: 0.0128  \n",
      "<<<iteration:[260/657] - total_loss: 0.1854  obj_loss: 0.0969  noobj_loss: 0.0654  bbox_loss: 0.0087  cls_loss: 0.0125  \n",
      "<<<iteration:[280/657] - total_loss: 0.1747  obj_loss: 0.0913  noobj_loss: 0.0597  bbox_loss: 0.0083  cls_loss: 0.0121  \n",
      "<<<iteration:[300/657] - total_loss: 0.1823  obj_loss: 0.0916  noobj_loss: 0.0555  bbox_loss: 0.0097  cls_loss: 0.0145  \n",
      "<<<iteration:[320/657] - total_loss: 0.1779  obj_loss: 0.0904  noobj_loss: 0.0632  bbox_loss: 0.0088  cls_loss: 0.0121  \n",
      "<<<iteration:[340/657] - total_loss: 0.1731  obj_loss: 0.0789  noobj_loss: 0.0666  bbox_loss: 0.0095  cls_loss: 0.0137  \n",
      "<<<iteration:[360/657] - total_loss: 0.1660  obj_loss: 0.0839  noobj_loss: 0.0577  bbox_loss: 0.0087  cls_loss: 0.0096  \n",
      "<<<iteration:[380/657] - total_loss: 0.1724  obj_loss: 0.0898  noobj_loss: 0.0578  bbox_loss: 0.0082  cls_loss: 0.0130  \n",
      "<<<iteration:[400/657] - total_loss: 0.1704  obj_loss: 0.0912  noobj_loss: 0.0571  bbox_loss: 0.0082  cls_loss: 0.0098  \n",
      "<<<iteration:[420/657] - total_loss: 0.1738  obj_loss: 0.0853  noobj_loss: 0.0597  bbox_loss: 0.0091  cls_loss: 0.0130  \n",
      "<<<iteration:[440/657] - total_loss: 0.1872  obj_loss: 0.0925  noobj_loss: 0.0598  bbox_loss: 0.0106  cls_loss: 0.0119  \n",
      "<<<iteration:[460/657] - total_loss: 0.1770  obj_loss: 0.0924  noobj_loss: 0.0570  bbox_loss: 0.0087  cls_loss: 0.0124  \n",
      "<<<iteration:[480/657] - total_loss: 0.1824  obj_loss: 0.0906  noobj_loss: 0.0579  bbox_loss: 0.0096  cls_loss: 0.0149  \n",
      "<<<iteration:[500/657] - total_loss: 0.1813  obj_loss: 0.0961  noobj_loss: 0.0579  bbox_loss: 0.0089  cls_loss: 0.0116  \n",
      "<<<iteration:[520/657] - total_loss: 0.1885  obj_loss: 0.0904  noobj_loss: 0.0579  bbox_loss: 0.0107  cls_loss: 0.0154  \n",
      "<<<iteration:[540/657] - total_loss: 0.1823  obj_loss: 0.0892  noobj_loss: 0.0641  bbox_loss: 0.0100  cls_loss: 0.0109  \n",
      "<<<iteration:[560/657] - total_loss: 0.1908  obj_loss: 0.1032  noobj_loss: 0.0657  bbox_loss: 0.0083  cls_loss: 0.0131  \n",
      "<<<iteration:[580/657] - total_loss: 0.1712  obj_loss: 0.0893  noobj_loss: 0.0626  bbox_loss: 0.0080  cls_loss: 0.0108  \n",
      "<<<iteration:[600/657] - total_loss: 0.1773  obj_loss: 0.0924  noobj_loss: 0.0647  bbox_loss: 0.0083  cls_loss: 0.0109  \n",
      "<<<iteration:[620/657] - total_loss: 0.1830  obj_loss: 0.0910  noobj_loss: 0.0578  bbox_loss: 0.0101  cls_loss: 0.0127  \n",
      "<<<iteration:[640/657] - total_loss: 0.1812  obj_loss: 0.0973  noobj_loss: 0.0620  bbox_loss: 0.0083  cls_loss: 0.0115  \n",
      "\n",
      "epoch:62/100 - Train Loss: 0.1806, Val Loss: 0.2338\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1969  obj_loss: 0.1080  noobj_loss: 0.0664  bbox_loss: 0.0093  cls_loss: 0.0094  \n",
      "<<<iteration:[40/657] - total_loss: 0.1879  obj_loss: 0.0875  noobj_loss: 0.0592  bbox_loss: 0.0113  cls_loss: 0.0142  \n",
      "<<<iteration:[60/657] - total_loss: 0.1625  obj_loss: 0.0799  noobj_loss: 0.0645  bbox_loss: 0.0081  cls_loss: 0.0100  \n",
      "<<<iteration:[80/657] - total_loss: 0.1859  obj_loss: 0.0902  noobj_loss: 0.0561  bbox_loss: 0.0106  cls_loss: 0.0148  \n",
      "<<<iteration:[100/657] - total_loss: 0.1931  obj_loss: 0.0978  noobj_loss: 0.0578  bbox_loss: 0.0104  cls_loss: 0.0143  \n",
      "<<<iteration:[120/657] - total_loss: 0.1744  obj_loss: 0.0909  noobj_loss: 0.0587  bbox_loss: 0.0087  cls_loss: 0.0104  \n",
      "<<<iteration:[140/657] - total_loss: 0.1826  obj_loss: 0.0965  noobj_loss: 0.0547  bbox_loss: 0.0088  cls_loss: 0.0148  \n",
      "<<<iteration:[160/657] - total_loss: 0.1805  obj_loss: 0.0907  noobj_loss: 0.0585  bbox_loss: 0.0099  cls_loss: 0.0110  \n",
      "<<<iteration:[180/657] - total_loss: 0.1787  obj_loss: 0.0911  noobj_loss: 0.0562  bbox_loss: 0.0096  cls_loss: 0.0116  \n",
      "<<<iteration:[200/657] - total_loss: 0.1827  obj_loss: 0.0951  noobj_loss: 0.0587  bbox_loss: 0.0093  cls_loss: 0.0117  \n",
      "<<<iteration:[220/657] - total_loss: 0.1763  obj_loss: 0.0882  noobj_loss: 0.0660  bbox_loss: 0.0087  cls_loss: 0.0116  \n",
      "<<<iteration:[240/657] - total_loss: 0.1817  obj_loss: 0.0925  noobj_loss: 0.0606  bbox_loss: 0.0090  cls_loss: 0.0141  \n",
      "<<<iteration:[260/657] - total_loss: 0.1780  obj_loss: 0.0909  noobj_loss: 0.0545  bbox_loss: 0.0085  cls_loss: 0.0176  \n",
      "<<<iteration:[280/657] - total_loss: 0.1752  obj_loss: 0.0878  noobj_loss: 0.0589  bbox_loss: 0.0087  cls_loss: 0.0144  \n",
      "<<<iteration:[300/657] - total_loss: 0.1822  obj_loss: 0.0979  noobj_loss: 0.0569  bbox_loss: 0.0083  cls_loss: 0.0145  \n",
      "<<<iteration:[320/657] - total_loss: 0.1832  obj_loss: 0.0953  noobj_loss: 0.0544  bbox_loss: 0.0093  cls_loss: 0.0143  \n",
      "<<<iteration:[340/657] - total_loss: 0.1775  obj_loss: 0.0865  noobj_loss: 0.0607  bbox_loss: 0.0095  cls_loss: 0.0133  \n",
      "<<<iteration:[360/657] - total_loss: 0.1744  obj_loss: 0.0908  noobj_loss: 0.0635  bbox_loss: 0.0082  cls_loss: 0.0110  \n",
      "<<<iteration:[380/657] - total_loss: 0.1905  obj_loss: 0.0935  noobj_loss: 0.0641  bbox_loss: 0.0100  cls_loss: 0.0151  \n",
      "<<<iteration:[400/657] - total_loss: 0.1687  obj_loss: 0.0873  noobj_loss: 0.0586  bbox_loss: 0.0079  cls_loss: 0.0128  \n",
      "<<<iteration:[420/657] - total_loss: 0.1978  obj_loss: 0.1038  noobj_loss: 0.0660  bbox_loss: 0.0099  cls_loss: 0.0116  \n",
      "<<<iteration:[440/657] - total_loss: 0.1829  obj_loss: 0.0903  noobj_loss: 0.0644  bbox_loss: 0.0095  cls_loss: 0.0127  \n",
      "<<<iteration:[460/657] - total_loss: 0.1764  obj_loss: 0.0909  noobj_loss: 0.0613  bbox_loss: 0.0089  cls_loss: 0.0104  \n",
      "<<<iteration:[480/657] - total_loss: 0.1810  obj_loss: 0.0881  noobj_loss: 0.0559  bbox_loss: 0.0104  cls_loss: 0.0131  \n",
      "<<<iteration:[500/657] - total_loss: 0.1766  obj_loss: 0.0840  noobj_loss: 0.0540  bbox_loss: 0.0109  cls_loss: 0.0113  \n",
      "<<<iteration:[520/657] - total_loss: 0.1719  obj_loss: 0.0870  noobj_loss: 0.0573  bbox_loss: 0.0091  cls_loss: 0.0109  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[540/657] - total_loss: 0.1672  obj_loss: 0.0877  noobj_loss: 0.0524  bbox_loss: 0.0085  cls_loss: 0.0106  \n",
      "<<<iteration:[560/657] - total_loss: 0.1719  obj_loss: 0.0905  noobj_loss: 0.0530  bbox_loss: 0.0087  cls_loss: 0.0113  \n",
      "<<<iteration:[580/657] - total_loss: 0.1775  obj_loss: 0.0915  noobj_loss: 0.0592  bbox_loss: 0.0092  cls_loss: 0.0105  \n",
      "<<<iteration:[600/657] - total_loss: 0.1885  obj_loss: 0.0902  noobj_loss: 0.0598  bbox_loss: 0.0110  cls_loss: 0.0131  \n",
      "<<<iteration:[620/657] - total_loss: 0.1848  obj_loss: 0.0908  noobj_loss: 0.0577  bbox_loss: 0.0108  cls_loss: 0.0109  \n",
      "<<<iteration:[640/657] - total_loss: 0.1924  obj_loss: 0.1069  noobj_loss: 0.0594  bbox_loss: 0.0090  cls_loss: 0.0108  \n",
      "\n",
      "epoch:63/100 - Train Loss: 0.1818, Val Loss: 0.2339\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1823  obj_loss: 0.0945  noobj_loss: 0.0607  bbox_loss: 0.0090  cls_loss: 0.0123  \n",
      "<<<iteration:[40/657] - total_loss: 0.1722  obj_loss: 0.0780  noobj_loss: 0.0601  bbox_loss: 0.0104  cls_loss: 0.0120  \n",
      "<<<iteration:[60/657] - total_loss: 0.1780  obj_loss: 0.0867  noobj_loss: 0.0554  bbox_loss: 0.0101  cls_loss: 0.0130  \n",
      "<<<iteration:[80/657] - total_loss: 0.2012  obj_loss: 0.1038  noobj_loss: 0.0617  bbox_loss: 0.0105  cls_loss: 0.0142  \n",
      "<<<iteration:[100/657] - total_loss: 0.1826  obj_loss: 0.0995  noobj_loss: 0.0635  bbox_loss: 0.0082  cls_loss: 0.0103  \n",
      "<<<iteration:[120/657] - total_loss: 0.1834  obj_loss: 0.0942  noobj_loss: 0.0577  bbox_loss: 0.0095  cls_loss: 0.0128  \n",
      "<<<iteration:[140/657] - total_loss: 0.1785  obj_loss: 0.0924  noobj_loss: 0.0608  bbox_loss: 0.0089  cls_loss: 0.0112  \n",
      "<<<iteration:[160/657] - total_loss: 0.1875  obj_loss: 0.0988  noobj_loss: 0.0633  bbox_loss: 0.0091  cls_loss: 0.0117  \n",
      "<<<iteration:[180/657] - total_loss: 0.1841  obj_loss: 0.0946  noobj_loss: 0.0562  bbox_loss: 0.0097  cls_loss: 0.0128  \n",
      "<<<iteration:[200/657] - total_loss: 0.1753  obj_loss: 0.0968  noobj_loss: 0.0514  bbox_loss: 0.0081  cls_loss: 0.0121  \n",
      "<<<iteration:[220/657] - total_loss: 0.1765  obj_loss: 0.0918  noobj_loss: 0.0575  bbox_loss: 0.0085  cls_loss: 0.0136  \n",
      "<<<iteration:[240/657] - total_loss: 0.1842  obj_loss: 0.0931  noobj_loss: 0.0623  bbox_loss: 0.0099  cls_loss: 0.0103  \n",
      "<<<iteration:[260/657] - total_loss: 0.1785  obj_loss: 0.0954  noobj_loss: 0.0603  bbox_loss: 0.0085  cls_loss: 0.0102  \n",
      "<<<iteration:[280/657] - total_loss: 0.1885  obj_loss: 0.0947  noobj_loss: 0.0685  bbox_loss: 0.0093  cls_loss: 0.0129  \n",
      "<<<iteration:[300/657] - total_loss: 0.1810  obj_loss: 0.0959  noobj_loss: 0.0604  bbox_loss: 0.0086  cls_loss: 0.0119  \n",
      "<<<iteration:[320/657] - total_loss: 0.1745  obj_loss: 0.0880  noobj_loss: 0.0613  bbox_loss: 0.0088  cls_loss: 0.0121  \n",
      "<<<iteration:[340/657] - total_loss: 0.1867  obj_loss: 0.0914  noobj_loss: 0.0542  bbox_loss: 0.0112  cls_loss: 0.0121  \n",
      "<<<iteration:[360/657] - total_loss: 0.1755  obj_loss: 0.0931  noobj_loss: 0.0573  bbox_loss: 0.0082  cls_loss: 0.0128  \n",
      "<<<iteration:[380/657] - total_loss: 0.1959  obj_loss: 0.0949  noobj_loss: 0.0630  bbox_loss: 0.0115  cls_loss: 0.0119  \n",
      "<<<iteration:[400/657] - total_loss: 0.1574  obj_loss: 0.0770  noobj_loss: 0.0552  bbox_loss: 0.0083  cls_loss: 0.0112  \n",
      "<<<iteration:[420/657] - total_loss: 0.1812  obj_loss: 0.0878  noobj_loss: 0.0572  bbox_loss: 0.0105  cls_loss: 0.0123  \n",
      "<<<iteration:[440/657] - total_loss: 0.1903  obj_loss: 0.1076  noobj_loss: 0.0596  bbox_loss: 0.0083  cls_loss: 0.0112  \n",
      "<<<iteration:[460/657] - total_loss: 0.1754  obj_loss: 0.0886  noobj_loss: 0.0629  bbox_loss: 0.0090  cls_loss: 0.0103  \n",
      "<<<iteration:[480/657] - total_loss: 0.1816  obj_loss: 0.0945  noobj_loss: 0.0590  bbox_loss: 0.0088  cls_loss: 0.0136  \n",
      "<<<iteration:[500/657] - total_loss: 0.1788  obj_loss: 0.0964  noobj_loss: 0.0639  bbox_loss: 0.0075  cls_loss: 0.0129  \n",
      "<<<iteration:[520/657] - total_loss: 0.1780  obj_loss: 0.0908  noobj_loss: 0.0585  bbox_loss: 0.0093  cls_loss: 0.0116  \n",
      "<<<iteration:[540/657] - total_loss: 0.1783  obj_loss: 0.0882  noobj_loss: 0.0562  bbox_loss: 0.0100  cls_loss: 0.0118  \n",
      "<<<iteration:[560/657] - total_loss: 0.1756  obj_loss: 0.0911  noobj_loss: 0.0564  bbox_loss: 0.0090  cls_loss: 0.0111  \n",
      "<<<iteration:[580/657] - total_loss: 0.1714  obj_loss: 0.0909  noobj_loss: 0.0556  bbox_loss: 0.0081  cls_loss: 0.0121  \n",
      "<<<iteration:[600/657] - total_loss: 0.1712  obj_loss: 0.0881  noobj_loss: 0.0621  bbox_loss: 0.0082  cls_loss: 0.0112  \n",
      "<<<iteration:[620/657] - total_loss: 0.1635  obj_loss: 0.0774  noobj_loss: 0.0622  bbox_loss: 0.0086  cls_loss: 0.0120  \n",
      "<<<iteration:[640/657] - total_loss: 0.1855  obj_loss: 0.0891  noobj_loss: 0.0638  bbox_loss: 0.0101  cls_loss: 0.0140  \n",
      "\n",
      "epoch:64/100 - Train Loss: 0.1798, Val Loss: 0.2344\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1865  obj_loss: 0.0997  noobj_loss: 0.0637  bbox_loss: 0.0088  cls_loss: 0.0109  \n",
      "<<<iteration:[40/657] - total_loss: 0.1851  obj_loss: 0.0965  noobj_loss: 0.0641  bbox_loss: 0.0087  cls_loss: 0.0130  \n",
      "<<<iteration:[60/657] - total_loss: 0.1809  obj_loss: 0.0907  noobj_loss: 0.0642  bbox_loss: 0.0091  cls_loss: 0.0125  \n",
      "<<<iteration:[80/657] - total_loss: 0.1883  obj_loss: 0.1009  noobj_loss: 0.0620  bbox_loss: 0.0092  cls_loss: 0.0106  \n",
      "<<<iteration:[100/657] - total_loss: 0.1810  obj_loss: 0.0814  noobj_loss: 0.0519  bbox_loss: 0.0117  cls_loss: 0.0152  \n",
      "<<<iteration:[120/657] - total_loss: 0.1858  obj_loss: 0.0964  noobj_loss: 0.0633  bbox_loss: 0.0094  cls_loss: 0.0106  \n",
      "<<<iteration:[140/657] - total_loss: 0.1819  obj_loss: 0.0951  noobj_loss: 0.0603  bbox_loss: 0.0089  cls_loss: 0.0120  \n",
      "<<<iteration:[160/657] - total_loss: 0.1767  obj_loss: 0.0891  noobj_loss: 0.0557  bbox_loss: 0.0092  cls_loss: 0.0135  \n",
      "<<<iteration:[180/657] - total_loss: 0.1658  obj_loss: 0.0825  noobj_loss: 0.0574  bbox_loss: 0.0084  cls_loss: 0.0125  \n",
      "<<<iteration:[200/657] - total_loss: 0.1716  obj_loss: 0.0835  noobj_loss: 0.0576  bbox_loss: 0.0097  cls_loss: 0.0110  \n",
      "<<<iteration:[220/657] - total_loss: 0.1758  obj_loss: 0.0913  noobj_loss: 0.0627  bbox_loss: 0.0082  cls_loss: 0.0120  \n",
      "<<<iteration:[240/657] - total_loss: 0.1749  obj_loss: 0.0862  noobj_loss: 0.0602  bbox_loss: 0.0093  cls_loss: 0.0122  \n",
      "<<<iteration:[260/657] - total_loss: 0.1560  obj_loss: 0.0814  noobj_loss: 0.0544  bbox_loss: 0.0074  cls_loss: 0.0103  \n",
      "<<<iteration:[280/657] - total_loss: 0.1847  obj_loss: 0.0983  noobj_loss: 0.0569  bbox_loss: 0.0091  cls_loss: 0.0126  \n",
      "<<<iteration:[300/657] - total_loss: 0.1778  obj_loss: 0.0973  noobj_loss: 0.0583  bbox_loss: 0.0077  cls_loss: 0.0128  \n",
      "<<<iteration:[320/657] - total_loss: 0.1682  obj_loss: 0.0883  noobj_loss: 0.0561  bbox_loss: 0.0082  cls_loss: 0.0107  \n",
      "<<<iteration:[340/657] - total_loss: 0.1815  obj_loss: 0.0925  noobj_loss: 0.0566  bbox_loss: 0.0102  cls_loss: 0.0099  \n",
      "<<<iteration:[360/657] - total_loss: 0.1650  obj_loss: 0.0841  noobj_loss: 0.0552  bbox_loss: 0.0085  cls_loss: 0.0109  \n",
      "<<<iteration:[380/657] - total_loss: 0.1739  obj_loss: 0.0914  noobj_loss: 0.0615  bbox_loss: 0.0081  cls_loss: 0.0113  \n",
      "<<<iteration:[400/657] - total_loss: 0.1989  obj_loss: 0.0938  noobj_loss: 0.0622  bbox_loss: 0.0120  cls_loss: 0.0141  \n",
      "<<<iteration:[420/657] - total_loss: 0.1995  obj_loss: 0.0815  noobj_loss: 0.0575  bbox_loss: 0.0147  cls_loss: 0.0158  \n",
      "<<<iteration:[440/657] - total_loss: 0.1737  obj_loss: 0.0851  noobj_loss: 0.0586  bbox_loss: 0.0095  cls_loss: 0.0116  \n",
      "<<<iteration:[460/657] - total_loss: 0.1736  obj_loss: 0.0911  noobj_loss: 0.0641  bbox_loss: 0.0080  cls_loss: 0.0104  \n",
      "<<<iteration:[480/657] - total_loss: 0.1792  obj_loss: 0.0933  noobj_loss: 0.0591  bbox_loss: 0.0086  cls_loss: 0.0136  \n",
      "<<<iteration:[500/657] - total_loss: 0.1756  obj_loss: 0.0897  noobj_loss: 0.0537  bbox_loss: 0.0097  cls_loss: 0.0104  \n",
      "<<<iteration:[520/657] - total_loss: 0.1707  obj_loss: 0.0865  noobj_loss: 0.0590  bbox_loss: 0.0085  cls_loss: 0.0120  \n",
      "<<<iteration:[540/657] - total_loss: 0.1608  obj_loss: 0.0808  noobj_loss: 0.0589  bbox_loss: 0.0082  cls_loss: 0.0094  \n",
      "<<<iteration:[560/657] - total_loss: 0.1806  obj_loss: 0.0934  noobj_loss: 0.0627  bbox_loss: 0.0091  cls_loss: 0.0101  \n",
      "<<<iteration:[580/657] - total_loss: 0.1835  obj_loss: 0.0928  noobj_loss: 0.0607  bbox_loss: 0.0094  cls_loss: 0.0135  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[600/657] - total_loss: 0.1749  obj_loss: 0.0946  noobj_loss: 0.0572  bbox_loss: 0.0082  cls_loss: 0.0105  \n",
      "<<<iteration:[620/657] - total_loss: 0.1872  obj_loss: 0.0969  noobj_loss: 0.0642  bbox_loss: 0.0093  cls_loss: 0.0114  \n",
      "<<<iteration:[640/657] - total_loss: 0.1862  obj_loss: 0.1051  noobj_loss: 0.0639  bbox_loss: 0.0077  cls_loss: 0.0104  \n",
      "\n",
      "epoch:65/100 - Train Loss: 0.1780, Val Loss: 0.2319\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1844  obj_loss: 0.0961  noobj_loss: 0.0639  bbox_loss: 0.0088  cls_loss: 0.0124  \n",
      "<<<iteration:[40/657] - total_loss: 0.1745  obj_loss: 0.0923  noobj_loss: 0.0603  bbox_loss: 0.0081  cls_loss: 0.0116  \n",
      "<<<iteration:[60/657] - total_loss: 0.1877  obj_loss: 0.0908  noobj_loss: 0.0768  bbox_loss: 0.0093  cls_loss: 0.0121  \n",
      "<<<iteration:[80/657] - total_loss: 0.1977  obj_loss: 0.1020  noobj_loss: 0.0619  bbox_loss: 0.0103  cls_loss: 0.0130  \n",
      "<<<iteration:[100/657] - total_loss: 0.1786  obj_loss: 0.0935  noobj_loss: 0.0576  bbox_loss: 0.0086  cls_loss: 0.0135  \n",
      "<<<iteration:[120/657] - total_loss: 0.1830  obj_loss: 0.0959  noobj_loss: 0.0569  bbox_loss: 0.0090  cls_loss: 0.0137  \n",
      "<<<iteration:[140/657] - total_loss: 0.1978  obj_loss: 0.1018  noobj_loss: 0.0574  bbox_loss: 0.0110  cls_loss: 0.0122  \n",
      "<<<iteration:[160/657] - total_loss: 0.1635  obj_loss: 0.0825  noobj_loss: 0.0564  bbox_loss: 0.0085  cls_loss: 0.0104  \n",
      "<<<iteration:[180/657] - total_loss: 0.1560  obj_loss: 0.0801  noobj_loss: 0.0571  bbox_loss: 0.0076  cls_loss: 0.0093  \n",
      "<<<iteration:[200/657] - total_loss: 0.1668  obj_loss: 0.0799  noobj_loss: 0.0546  bbox_loss: 0.0099  cls_loss: 0.0099  \n",
      "<<<iteration:[220/657] - total_loss: 0.1689  obj_loss: 0.0849  noobj_loss: 0.0586  bbox_loss: 0.0081  cls_loss: 0.0144  \n",
      "<<<iteration:[240/657] - total_loss: 0.1917  obj_loss: 0.0958  noobj_loss: 0.0616  bbox_loss: 0.0101  cls_loss: 0.0144  \n",
      "<<<iteration:[260/657] - total_loss: 0.1784  obj_loss: 0.0943  noobj_loss: 0.0601  bbox_loss: 0.0087  cls_loss: 0.0106  \n",
      "<<<iteration:[280/657] - total_loss: 0.1797  obj_loss: 0.0871  noobj_loss: 0.0653  bbox_loss: 0.0091  cls_loss: 0.0142  \n",
      "<<<iteration:[300/657] - total_loss: 0.1745  obj_loss: 0.0812  noobj_loss: 0.0590  bbox_loss: 0.0100  cls_loss: 0.0138  \n",
      "<<<iteration:[320/657] - total_loss: 0.1673  obj_loss: 0.0764  noobj_loss: 0.0568  bbox_loss: 0.0100  cls_loss: 0.0126  \n",
      "<<<iteration:[340/657] - total_loss: 0.1912  obj_loss: 0.1005  noobj_loss: 0.0531  bbox_loss: 0.0106  cls_loss: 0.0114  \n",
      "<<<iteration:[360/657] - total_loss: 0.1762  obj_loss: 0.0892  noobj_loss: 0.0551  bbox_loss: 0.0096  cls_loss: 0.0116  \n",
      "<<<iteration:[380/657] - total_loss: 0.1683  obj_loss: 0.0862  noobj_loss: 0.0551  bbox_loss: 0.0090  cls_loss: 0.0096  \n",
      "<<<iteration:[400/657] - total_loss: 0.1824  obj_loss: 0.0961  noobj_loss: 0.0622  bbox_loss: 0.0089  cls_loss: 0.0104  \n",
      "<<<iteration:[420/657] - total_loss: 0.1847  obj_loss: 0.0907  noobj_loss: 0.0627  bbox_loss: 0.0102  cls_loss: 0.0116  \n",
      "<<<iteration:[440/657] - total_loss: 0.1705  obj_loss: 0.0925  noobj_loss: 0.0556  bbox_loss: 0.0078  cls_loss: 0.0112  \n",
      "<<<iteration:[460/657] - total_loss: 0.1826  obj_loss: 0.0919  noobj_loss: 0.0600  bbox_loss: 0.0097  cls_loss: 0.0120  \n",
      "<<<iteration:[480/657] - total_loss: 0.1798  obj_loss: 0.0939  noobj_loss: 0.0597  bbox_loss: 0.0093  cls_loss: 0.0095  \n",
      "<<<iteration:[500/657] - total_loss: 0.1796  obj_loss: 0.0924  noobj_loss: 0.0654  bbox_loss: 0.0079  cls_loss: 0.0150  \n",
      "<<<iteration:[520/657] - total_loss: 0.1820  obj_loss: 0.0928  noobj_loss: 0.0625  bbox_loss: 0.0093  cls_loss: 0.0116  \n",
      "<<<iteration:[540/657] - total_loss: 0.1773  obj_loss: 0.0942  noobj_loss: 0.0522  bbox_loss: 0.0092  cls_loss: 0.0112  \n",
      "<<<iteration:[560/657] - total_loss: 0.1814  obj_loss: 0.0987  noobj_loss: 0.0587  bbox_loss: 0.0084  cls_loss: 0.0114  \n",
      "<<<iteration:[580/657] - total_loss: 0.1885  obj_loss: 0.0864  noobj_loss: 0.0621  bbox_loss: 0.0110  cls_loss: 0.0159  \n",
      "<<<iteration:[600/657] - total_loss: 0.1869  obj_loss: 0.0939  noobj_loss: 0.0589  bbox_loss: 0.0106  cls_loss: 0.0105  \n",
      "<<<iteration:[620/657] - total_loss: 0.1702  obj_loss: 0.0908  noobj_loss: 0.0550  bbox_loss: 0.0082  cls_loss: 0.0107  \n",
      "<<<iteration:[640/657] - total_loss: 0.1689  obj_loss: 0.0891  noobj_loss: 0.0608  bbox_loss: 0.0080  cls_loss: 0.0093  \n",
      "\n",
      "epoch:66/100 - Train Loss: 0.1787, Val Loss: 0.2349\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1884  obj_loss: 0.0991  noobj_loss: 0.0645  bbox_loss: 0.0091  cls_loss: 0.0116  \n",
      "<<<iteration:[40/657] - total_loss: 0.1767  obj_loss: 0.0865  noobj_loss: 0.0544  bbox_loss: 0.0103  cls_loss: 0.0117  \n",
      "<<<iteration:[60/657] - total_loss: 0.1911  obj_loss: 0.1058  noobj_loss: 0.0651  bbox_loss: 0.0082  cls_loss: 0.0118  \n",
      "<<<iteration:[80/657] - total_loss: 0.1805  obj_loss: 0.0844  noobj_loss: 0.0663  bbox_loss: 0.0100  cls_loss: 0.0130  \n",
      "<<<iteration:[100/657] - total_loss: 0.1852  obj_loss: 0.0847  noobj_loss: 0.0545  bbox_loss: 0.0114  cls_loss: 0.0165  \n",
      "<<<iteration:[120/657] - total_loss: 0.1637  obj_loss: 0.0883  noobj_loss: 0.0540  bbox_loss: 0.0071  cls_loss: 0.0127  \n",
      "<<<iteration:[140/657] - total_loss: 0.1787  obj_loss: 0.0853  noobj_loss: 0.0622  bbox_loss: 0.0099  cls_loss: 0.0128  \n",
      "<<<iteration:[160/657] - total_loss: 0.1680  obj_loss: 0.0869  noobj_loss: 0.0561  bbox_loss: 0.0084  cls_loss: 0.0111  \n",
      "<<<iteration:[180/657] - total_loss: 0.1705  obj_loss: 0.0878  noobj_loss: 0.0605  bbox_loss: 0.0081  cls_loss: 0.0120  \n",
      "<<<iteration:[200/657] - total_loss: 0.1668  obj_loss: 0.0869  noobj_loss: 0.0638  bbox_loss: 0.0077  cls_loss: 0.0093  \n",
      "<<<iteration:[220/657] - total_loss: 0.1779  obj_loss: 0.0979  noobj_loss: 0.0587  bbox_loss: 0.0082  cls_loss: 0.0098  \n",
      "<<<iteration:[240/657] - total_loss: 0.1822  obj_loss: 0.0939  noobj_loss: 0.0595  bbox_loss: 0.0095  cls_loss: 0.0111  \n",
      "<<<iteration:[260/657] - total_loss: 0.1862  obj_loss: 0.1014  noobj_loss: 0.0597  bbox_loss: 0.0088  cls_loss: 0.0109  \n",
      "<<<iteration:[280/657] - total_loss: 0.1769  obj_loss: 0.0878  noobj_loss: 0.0638  bbox_loss: 0.0087  cls_loss: 0.0138  \n",
      "<<<iteration:[300/657] - total_loss: 0.1734  obj_loss: 0.0925  noobj_loss: 0.0550  bbox_loss: 0.0083  cls_loss: 0.0120  \n",
      "<<<iteration:[320/657] - total_loss: 0.1728  obj_loss: 0.0869  noobj_loss: 0.0584  bbox_loss: 0.0094  cls_loss: 0.0095  \n",
      "<<<iteration:[340/657] - total_loss: 0.1645  obj_loss: 0.0818  noobj_loss: 0.0546  bbox_loss: 0.0082  cls_loss: 0.0145  \n",
      "<<<iteration:[360/657] - total_loss: 0.1797  obj_loss: 0.0885  noobj_loss: 0.0682  bbox_loss: 0.0089  cls_loss: 0.0125  \n",
      "<<<iteration:[380/657] - total_loss: 0.1798  obj_loss: 0.0959  noobj_loss: 0.0615  bbox_loss: 0.0083  cls_loss: 0.0119  \n",
      "<<<iteration:[400/657] - total_loss: 0.1620  obj_loss: 0.0868  noobj_loss: 0.0558  bbox_loss: 0.0076  cls_loss: 0.0093  \n",
      "<<<iteration:[420/657] - total_loss: 0.1597  obj_loss: 0.0746  noobj_loss: 0.0568  bbox_loss: 0.0091  cls_loss: 0.0114  \n",
      "<<<iteration:[440/657] - total_loss: 0.1606  obj_loss: 0.0768  noobj_loss: 0.0592  bbox_loss: 0.0088  cls_loss: 0.0101  \n",
      "<<<iteration:[460/657] - total_loss: 0.1916  obj_loss: 0.0953  noobj_loss: 0.0615  bbox_loss: 0.0107  cls_loss: 0.0119  \n",
      "<<<iteration:[480/657] - total_loss: 0.1801  obj_loss: 0.0945  noobj_loss: 0.0641  bbox_loss: 0.0080  cls_loss: 0.0137  \n",
      "<<<iteration:[500/657] - total_loss: 0.1897  obj_loss: 0.0981  noobj_loss: 0.0651  bbox_loss: 0.0095  cls_loss: 0.0116  \n",
      "<<<iteration:[520/657] - total_loss: 0.1713  obj_loss: 0.0894  noobj_loss: 0.0555  bbox_loss: 0.0088  cls_loss: 0.0103  \n",
      "<<<iteration:[540/657] - total_loss: 0.1732  obj_loss: 0.0824  noobj_loss: 0.0613  bbox_loss: 0.0092  cls_loss: 0.0143  \n",
      "<<<iteration:[560/657] - total_loss: 0.1743  obj_loss: 0.0916  noobj_loss: 0.0575  bbox_loss: 0.0085  cls_loss: 0.0115  \n",
      "<<<iteration:[580/657] - total_loss: 0.1694  obj_loss: 0.0853  noobj_loss: 0.0557  bbox_loss: 0.0089  cls_loss: 0.0117  \n",
      "<<<iteration:[600/657] - total_loss: 0.1733  obj_loss: 0.0893  noobj_loss: 0.0534  bbox_loss: 0.0094  cls_loss: 0.0100  \n",
      "<<<iteration:[620/657] - total_loss: 0.1661  obj_loss: 0.0760  noobj_loss: 0.0602  bbox_loss: 0.0096  cls_loss: 0.0120  \n",
      "<<<iteration:[640/657] - total_loss: 0.1630  obj_loss: 0.0843  noobj_loss: 0.0519  bbox_loss: 0.0084  cls_loss: 0.0107  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:67/100 - Train Loss: 0.1745, Val Loss: 0.2307\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1832  obj_loss: 0.0960  noobj_loss: 0.0660  bbox_loss: 0.0085  cls_loss: 0.0117  \n",
      "<<<iteration:[40/657] - total_loss: 0.1831  obj_loss: 0.0945  noobj_loss: 0.0619  bbox_loss: 0.0084  cls_loss: 0.0155  \n",
      "<<<iteration:[60/657] - total_loss: 0.1725  obj_loss: 0.0911  noobj_loss: 0.0590  bbox_loss: 0.0081  cls_loss: 0.0113  \n",
      "<<<iteration:[80/657] - total_loss: 0.1780  obj_loss: 0.0922  noobj_loss: 0.0582  bbox_loss: 0.0092  cls_loss: 0.0108  \n",
      "<<<iteration:[100/657] - total_loss: 0.1605  obj_loss: 0.0789  noobj_loss: 0.0622  bbox_loss: 0.0082  cls_loss: 0.0095  \n",
      "<<<iteration:[120/657] - total_loss: 0.1692  obj_loss: 0.0882  noobj_loss: 0.0556  bbox_loss: 0.0085  cls_loss: 0.0106  \n",
      "<<<iteration:[140/657] - total_loss: 0.1731  obj_loss: 0.0890  noobj_loss: 0.0589  bbox_loss: 0.0085  cls_loss: 0.0120  \n",
      "<<<iteration:[160/657] - total_loss: 0.1830  obj_loss: 0.0996  noobj_loss: 0.0606  bbox_loss: 0.0083  cls_loss: 0.0114  \n",
      "<<<iteration:[180/657] - total_loss: 0.1840  obj_loss: 0.0962  noobj_loss: 0.0638  bbox_loss: 0.0089  cls_loss: 0.0112  \n",
      "<<<iteration:[200/657] - total_loss: 0.1564  obj_loss: 0.0745  noobj_loss: 0.0601  bbox_loss: 0.0080  cls_loss: 0.0118  \n",
      "<<<iteration:[220/657] - total_loss: 0.1829  obj_loss: 0.0974  noobj_loss: 0.0593  bbox_loss: 0.0085  cls_loss: 0.0132  \n",
      "<<<iteration:[240/657] - total_loss: 0.1647  obj_loss: 0.0847  noobj_loss: 0.0532  bbox_loss: 0.0086  cls_loss: 0.0105  \n",
      "<<<iteration:[260/657] - total_loss: 0.1781  obj_loss: 0.0891  noobj_loss: 0.0635  bbox_loss: 0.0091  cls_loss: 0.0116  \n",
      "<<<iteration:[280/657] - total_loss: 0.1625  obj_loss: 0.0853  noobj_loss: 0.0538  bbox_loss: 0.0080  cls_loss: 0.0104  \n",
      "<<<iteration:[300/657] - total_loss: 0.1664  obj_loss: 0.0867  noobj_loss: 0.0548  bbox_loss: 0.0081  cls_loss: 0.0118  \n",
      "<<<iteration:[320/657] - total_loss: 0.1700  obj_loss: 0.0802  noobj_loss: 0.0589  bbox_loss: 0.0092  cls_loss: 0.0145  \n",
      "<<<iteration:[340/657] - total_loss: 0.1698  obj_loss: 0.0898  noobj_loss: 0.0581  bbox_loss: 0.0081  cls_loss: 0.0107  \n",
      "<<<iteration:[360/657] - total_loss: 0.1774  obj_loss: 0.0856  noobj_loss: 0.0555  bbox_loss: 0.0108  cls_loss: 0.0101  \n",
      "<<<iteration:[380/657] - total_loss: 0.1844  obj_loss: 0.0909  noobj_loss: 0.0599  bbox_loss: 0.0104  cls_loss: 0.0114  \n",
      "<<<iteration:[400/657] - total_loss: 0.1956  obj_loss: 0.0932  noobj_loss: 0.0608  bbox_loss: 0.0121  cls_loss: 0.0114  \n",
      "<<<iteration:[420/657] - total_loss: 0.1788  obj_loss: 0.0985  noobj_loss: 0.0580  bbox_loss: 0.0080  cls_loss: 0.0113  \n",
      "<<<iteration:[440/657] - total_loss: 0.1822  obj_loss: 0.0976  noobj_loss: 0.0590  bbox_loss: 0.0084  cls_loss: 0.0132  \n",
      "<<<iteration:[460/657] - total_loss: 0.1663  obj_loss: 0.0845  noobj_loss: 0.0615  bbox_loss: 0.0080  cls_loss: 0.0110  \n",
      "<<<iteration:[480/657] - total_loss: 0.1777  obj_loss: 0.0930  noobj_loss: 0.0537  bbox_loss: 0.0095  cls_loss: 0.0105  \n",
      "<<<iteration:[500/657] - total_loss: 0.1719  obj_loss: 0.0922  noobj_loss: 0.0578  bbox_loss: 0.0080  cls_loss: 0.0108  \n",
      "<<<iteration:[520/657] - total_loss: 0.1736  obj_loss: 0.0890  noobj_loss: 0.0606  bbox_loss: 0.0090  cls_loss: 0.0094  \n",
      "<<<iteration:[540/657] - total_loss: 0.1723  obj_loss: 0.0851  noobj_loss: 0.0629  bbox_loss: 0.0089  cls_loss: 0.0115  \n",
      "<<<iteration:[560/657] - total_loss: 0.1689  obj_loss: 0.0807  noobj_loss: 0.0571  bbox_loss: 0.0096  cls_loss: 0.0117  \n",
      "<<<iteration:[580/657] - total_loss: 0.1786  obj_loss: 0.0867  noobj_loss: 0.0582  bbox_loss: 0.0098  cls_loss: 0.0141  \n",
      "<<<iteration:[600/657] - total_loss: 0.1799  obj_loss: 0.0932  noobj_loss: 0.0679  bbox_loss: 0.0086  cls_loss: 0.0097  \n",
      "<<<iteration:[620/657] - total_loss: 0.1721  obj_loss: 0.0860  noobj_loss: 0.0550  bbox_loss: 0.0095  cls_loss: 0.0110  \n",
      "<<<iteration:[640/657] - total_loss: 0.1645  obj_loss: 0.0804  noobj_loss: 0.0556  bbox_loss: 0.0092  cls_loss: 0.0104  \n",
      "\n",
      "epoch:68/100 - Train Loss: 0.1741, Val Loss: 0.2278\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1897  obj_loss: 0.0930  noobj_loss: 0.0623  bbox_loss: 0.0106  cls_loss: 0.0127  \n",
      "<<<iteration:[40/657] - total_loss: 0.1790  obj_loss: 0.0932  noobj_loss: 0.0603  bbox_loss: 0.0092  cls_loss: 0.0098  \n",
      "<<<iteration:[60/657] - total_loss: 0.1683  obj_loss: 0.0897  noobj_loss: 0.0548  bbox_loss: 0.0077  cls_loss: 0.0126  \n",
      "<<<iteration:[80/657] - total_loss: 0.1652  obj_loss: 0.0853  noobj_loss: 0.0585  bbox_loss: 0.0079  cls_loss: 0.0109  \n",
      "<<<iteration:[100/657] - total_loss: 0.1923  obj_loss: 0.1016  noobj_loss: 0.0575  bbox_loss: 0.0096  cls_loss: 0.0138  \n",
      "<<<iteration:[120/657] - total_loss: 0.1900  obj_loss: 0.0905  noobj_loss: 0.0620  bbox_loss: 0.0111  cls_loss: 0.0131  \n",
      "<<<iteration:[140/657] - total_loss: 0.1733  obj_loss: 0.0931  noobj_loss: 0.0595  bbox_loss: 0.0079  cls_loss: 0.0108  \n",
      "<<<iteration:[160/657] - total_loss: 0.1720  obj_loss: 0.0895  noobj_loss: 0.0557  bbox_loss: 0.0083  cls_loss: 0.0131  \n",
      "<<<iteration:[180/657] - total_loss: 0.1851  obj_loss: 0.1021  noobj_loss: 0.0640  bbox_loss: 0.0082  cls_loss: 0.0101  \n",
      "<<<iteration:[200/657] - total_loss: 0.1648  obj_loss: 0.0854  noobj_loss: 0.0525  bbox_loss: 0.0083  cls_loss: 0.0116  \n",
      "<<<iteration:[220/657] - total_loss: 0.1764  obj_loss: 0.0895  noobj_loss: 0.0572  bbox_loss: 0.0092  cls_loss: 0.0124  \n",
      "<<<iteration:[240/657] - total_loss: 0.1709  obj_loss: 0.0906  noobj_loss: 0.0597  bbox_loss: 0.0080  cls_loss: 0.0103  \n",
      "<<<iteration:[260/657] - total_loss: 0.1714  obj_loss: 0.0912  noobj_loss: 0.0536  bbox_loss: 0.0086  cls_loss: 0.0105  \n",
      "<<<iteration:[280/657] - total_loss: 0.1725  obj_loss: 0.0923  noobj_loss: 0.0577  bbox_loss: 0.0084  cls_loss: 0.0093  \n",
      "<<<iteration:[300/657] - total_loss: 0.1758  obj_loss: 0.0867  noobj_loss: 0.0654  bbox_loss: 0.0090  cls_loss: 0.0116  \n",
      "<<<iteration:[320/657] - total_loss: 0.1578  obj_loss: 0.0804  noobj_loss: 0.0616  bbox_loss: 0.0076  cls_loss: 0.0083  \n",
      "<<<iteration:[340/657] - total_loss: 0.1873  obj_loss: 0.0967  noobj_loss: 0.0623  bbox_loss: 0.0095  cls_loss: 0.0119  \n",
      "<<<iteration:[360/657] - total_loss: 0.1653  obj_loss: 0.0858  noobj_loss: 0.0623  bbox_loss: 0.0078  cls_loss: 0.0096  \n",
      "<<<iteration:[380/657] - total_loss: 0.1661  obj_loss: 0.0788  noobj_loss: 0.0569  bbox_loss: 0.0093  cls_loss: 0.0125  \n",
      "<<<iteration:[400/657] - total_loss: 0.1790  obj_loss: 0.0924  noobj_loss: 0.0674  bbox_loss: 0.0083  cls_loss: 0.0114  \n",
      "<<<iteration:[420/657] - total_loss: 0.1739  obj_loss: 0.0916  noobj_loss: 0.0590  bbox_loss: 0.0083  cls_loss: 0.0114  \n",
      "<<<iteration:[440/657] - total_loss: 0.1885  obj_loss: 0.0952  noobj_loss: 0.0656  bbox_loss: 0.0097  cls_loss: 0.0119  \n",
      "<<<iteration:[460/657] - total_loss: 0.1589  obj_loss: 0.0786  noobj_loss: 0.0561  bbox_loss: 0.0084  cls_loss: 0.0104  \n",
      "<<<iteration:[480/657] - total_loss: 0.1656  obj_loss: 0.0816  noobj_loss: 0.0602  bbox_loss: 0.0087  cls_loss: 0.0106  \n",
      "<<<iteration:[500/657] - total_loss: 0.1888  obj_loss: 0.0901  noobj_loss: 0.0656  bbox_loss: 0.0111  cls_loss: 0.0105  \n",
      "<<<iteration:[520/657] - total_loss: 0.1621  obj_loss: 0.0845  noobj_loss: 0.0485  bbox_loss: 0.0086  cls_loss: 0.0105  \n",
      "<<<iteration:[540/657] - total_loss: 0.1590  obj_loss: 0.0824  noobj_loss: 0.0560  bbox_loss: 0.0080  cls_loss: 0.0087  \n",
      "<<<iteration:[560/657] - total_loss: 0.1788  obj_loss: 0.0895  noobj_loss: 0.0641  bbox_loss: 0.0090  cls_loss: 0.0123  \n",
      "<<<iteration:[580/657] - total_loss: 0.1747  obj_loss: 0.0853  noobj_loss: 0.0640  bbox_loss: 0.0092  cls_loss: 0.0115  \n",
      "<<<iteration:[600/657] - total_loss: 0.1765  obj_loss: 0.0931  noobj_loss: 0.0615  bbox_loss: 0.0084  cls_loss: 0.0107  \n",
      "<<<iteration:[620/657] - total_loss: 0.1681  obj_loss: 0.0877  noobj_loss: 0.0598  bbox_loss: 0.0079  cls_loss: 0.0108  \n",
      "<<<iteration:[640/657] - total_loss: 0.1785  obj_loss: 0.0984  noobj_loss: 0.0649  bbox_loss: 0.0076  cls_loss: 0.0099  \n",
      "\n",
      "epoch:69/100 - Train Loss: 0.1738, Val Loss: 0.2273\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1653  obj_loss: 0.0861  noobj_loss: 0.0556  bbox_loss: 0.0085  cls_loss: 0.0088  \n",
      "<<<iteration:[40/657] - total_loss: 0.1659  obj_loss: 0.0849  noobj_loss: 0.0583  bbox_loss: 0.0080  cls_loss: 0.0118  \n",
      "<<<iteration:[60/657] - total_loss: 0.1784  obj_loss: 0.0966  noobj_loss: 0.0583  bbox_loss: 0.0083  cls_loss: 0.0112  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[80/657] - total_loss: 0.1786  obj_loss: 0.0984  noobj_loss: 0.0549  bbox_loss: 0.0084  cls_loss: 0.0106  \n",
      "<<<iteration:[100/657] - total_loss: 0.1689  obj_loss: 0.0859  noobj_loss: 0.0547  bbox_loss: 0.0090  cls_loss: 0.0105  \n",
      "<<<iteration:[120/657] - total_loss: 0.1851  obj_loss: 0.0956  noobj_loss: 0.0598  bbox_loss: 0.0095  cls_loss: 0.0120  \n",
      "<<<iteration:[140/657] - total_loss: 0.1614  obj_loss: 0.0814  noobj_loss: 0.0606  bbox_loss: 0.0082  cls_loss: 0.0089  \n",
      "<<<iteration:[160/657] - total_loss: 0.1702  obj_loss: 0.0923  noobj_loss: 0.0567  bbox_loss: 0.0076  cls_loss: 0.0114  \n",
      "<<<iteration:[180/657] - total_loss: 0.1637  obj_loss: 0.0816  noobj_loss: 0.0549  bbox_loss: 0.0091  cls_loss: 0.0092  \n",
      "<<<iteration:[200/657] - total_loss: 0.1640  obj_loss: 0.0859  noobj_loss: 0.0548  bbox_loss: 0.0084  cls_loss: 0.0088  \n",
      "<<<iteration:[220/657] - total_loss: 0.1712  obj_loss: 0.0953  noobj_loss: 0.0519  bbox_loss: 0.0079  cls_loss: 0.0102  \n",
      "<<<iteration:[240/657] - total_loss: 0.1853  obj_loss: 0.1012  noobj_loss: 0.0625  bbox_loss: 0.0086  cls_loss: 0.0101  \n",
      "<<<iteration:[260/657] - total_loss: 0.1798  obj_loss: 0.0989  noobj_loss: 0.0632  bbox_loss: 0.0075  cls_loss: 0.0118  \n",
      "<<<iteration:[280/657] - total_loss: 0.1902  obj_loss: 0.0967  noobj_loss: 0.0629  bbox_loss: 0.0101  cls_loss: 0.0116  \n",
      "<<<iteration:[300/657] - total_loss: 0.1689  obj_loss: 0.0817  noobj_loss: 0.0642  bbox_loss: 0.0087  cls_loss: 0.0118  \n",
      "<<<iteration:[320/657] - total_loss: 0.1732  obj_loss: 0.0909  noobj_loss: 0.0607  bbox_loss: 0.0083  cls_loss: 0.0104  \n",
      "<<<iteration:[340/657] - total_loss: 0.1822  obj_loss: 0.0994  noobj_loss: 0.0535  bbox_loss: 0.0088  cls_loss: 0.0121  \n",
      "<<<iteration:[360/657] - total_loss: 0.1827  obj_loss: 0.1003  noobj_loss: 0.0631  bbox_loss: 0.0085  cls_loss: 0.0081  \n",
      "<<<iteration:[380/657] - total_loss: 0.1657  obj_loss: 0.0793  noobj_loss: 0.0509  bbox_loss: 0.0102  cls_loss: 0.0099  \n",
      "<<<iteration:[400/657] - total_loss: 0.1736  obj_loss: 0.0919  noobj_loss: 0.0584  bbox_loss: 0.0084  cls_loss: 0.0107  \n",
      "<<<iteration:[420/657] - total_loss: 0.1706  obj_loss: 0.0872  noobj_loss: 0.0503  bbox_loss: 0.0089  cls_loss: 0.0136  \n",
      "<<<iteration:[440/657] - total_loss: 0.1772  obj_loss: 0.0918  noobj_loss: 0.0528  bbox_loss: 0.0093  cls_loss: 0.0127  \n",
      "<<<iteration:[460/657] - total_loss: 0.1708  obj_loss: 0.0834  noobj_loss: 0.0538  bbox_loss: 0.0094  cls_loss: 0.0134  \n",
      "<<<iteration:[480/657] - total_loss: 0.1683  obj_loss: 0.0845  noobj_loss: 0.0638  bbox_loss: 0.0081  cls_loss: 0.0117  \n",
      "<<<iteration:[500/657] - total_loss: 0.1597  obj_loss: 0.0794  noobj_loss: 0.0569  bbox_loss: 0.0085  cls_loss: 0.0095  \n",
      "<<<iteration:[520/657] - total_loss: 0.1793  obj_loss: 0.0963  noobj_loss: 0.0637  bbox_loss: 0.0085  cls_loss: 0.0087  \n",
      "<<<iteration:[540/657] - total_loss: 0.1628  obj_loss: 0.0817  noobj_loss: 0.0554  bbox_loss: 0.0084  cls_loss: 0.0115  \n",
      "<<<iteration:[560/657] - total_loss: 0.1629  obj_loss: 0.0852  noobj_loss: 0.0580  bbox_loss: 0.0076  cls_loss: 0.0108  \n",
      "<<<iteration:[580/657] - total_loss: 0.1551  obj_loss: 0.0781  noobj_loss: 0.0629  bbox_loss: 0.0072  cls_loss: 0.0096  \n",
      "<<<iteration:[600/657] - total_loss: 0.1786  obj_loss: 0.0952  noobj_loss: 0.0670  bbox_loss: 0.0078  cls_loss: 0.0109  \n",
      "<<<iteration:[620/657] - total_loss: 0.1689  obj_loss: 0.0865  noobj_loss: 0.0597  bbox_loss: 0.0082  cls_loss: 0.0115  \n",
      "<<<iteration:[640/657] - total_loss: 0.1529  obj_loss: 0.0740  noobj_loss: 0.0528  bbox_loss: 0.0085  cls_loss: 0.0101  \n",
      "\n",
      "epoch:70/100 - Train Loss: 0.1710, Val Loss: 0.2292\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1752  obj_loss: 0.0907  noobj_loss: 0.0536  bbox_loss: 0.0093  cls_loss: 0.0114  \n",
      "<<<iteration:[40/657] - total_loss: 0.1689  obj_loss: 0.0850  noobj_loss: 0.0584  bbox_loss: 0.0089  cls_loss: 0.0101  \n",
      "<<<iteration:[60/657] - total_loss: 0.1739  obj_loss: 0.0877  noobj_loss: 0.0628  bbox_loss: 0.0083  cls_loss: 0.0134  \n",
      "<<<iteration:[80/657] - total_loss: 0.1549  obj_loss: 0.0743  noobj_loss: 0.0568  bbox_loss: 0.0082  cls_loss: 0.0114  \n",
      "<<<iteration:[100/657] - total_loss: 0.1783  obj_loss: 0.0881  noobj_loss: 0.0560  bbox_loss: 0.0101  cls_loss: 0.0118  \n",
      "<<<iteration:[120/657] - total_loss: 0.1795  obj_loss: 0.0871  noobj_loss: 0.0582  bbox_loss: 0.0102  cls_loss: 0.0124  \n",
      "<<<iteration:[140/657] - total_loss: 0.1672  obj_loss: 0.0881  noobj_loss: 0.0577  bbox_loss: 0.0080  cls_loss: 0.0104  \n",
      "<<<iteration:[160/657] - total_loss: 0.1684  obj_loss: 0.0859  noobj_loss: 0.0567  bbox_loss: 0.0090  cls_loss: 0.0090  \n",
      "<<<iteration:[180/657] - total_loss: 0.1674  obj_loss: 0.0830  noobj_loss: 0.0538  bbox_loss: 0.0089  cls_loss: 0.0130  \n",
      "<<<iteration:[200/657] - total_loss: 0.1780  obj_loss: 0.0886  noobj_loss: 0.0741  bbox_loss: 0.0081  cls_loss: 0.0117  \n",
      "<<<iteration:[220/657] - total_loss: 0.1642  obj_loss: 0.0851  noobj_loss: 0.0573  bbox_loss: 0.0077  cls_loss: 0.0121  \n",
      "<<<iteration:[240/657] - total_loss: 0.1710  obj_loss: 0.0935  noobj_loss: 0.0545  bbox_loss: 0.0078  cls_loss: 0.0111  \n",
      "<<<iteration:[260/657] - total_loss: 0.1633  obj_loss: 0.0872  noobj_loss: 0.0502  bbox_loss: 0.0081  cls_loss: 0.0103  \n",
      "<<<iteration:[280/657] - total_loss: 0.1785  obj_loss: 0.0932  noobj_loss: 0.0623  bbox_loss: 0.0090  cls_loss: 0.0093  \n",
      "<<<iteration:[300/657] - total_loss: 0.1782  obj_loss: 0.0956  noobj_loss: 0.0538  bbox_loss: 0.0088  cls_loss: 0.0118  \n",
      "<<<iteration:[320/657] - total_loss: 0.1671  obj_loss: 0.0842  noobj_loss: 0.0531  bbox_loss: 0.0086  cls_loss: 0.0133  \n",
      "<<<iteration:[340/657] - total_loss: 0.1778  obj_loss: 0.0935  noobj_loss: 0.0637  bbox_loss: 0.0080  cls_loss: 0.0124  \n",
      "<<<iteration:[360/657] - total_loss: 0.1784  obj_loss: 0.0971  noobj_loss: 0.0668  bbox_loss: 0.0075  cls_loss: 0.0104  \n",
      "<<<iteration:[380/657] - total_loss: 0.1582  obj_loss: 0.0825  noobj_loss: 0.0582  bbox_loss: 0.0072  cls_loss: 0.0107  \n",
      "<<<iteration:[400/657] - total_loss: 0.1704  obj_loss: 0.0843  noobj_loss: 0.0583  bbox_loss: 0.0092  cls_loss: 0.0108  \n",
      "<<<iteration:[420/657] - total_loss: 0.1649  obj_loss: 0.0862  noobj_loss: 0.0612  bbox_loss: 0.0077  cls_loss: 0.0098  \n",
      "<<<iteration:[440/657] - total_loss: 0.1658  obj_loss: 0.0860  noobj_loss: 0.0556  bbox_loss: 0.0085  cls_loss: 0.0097  \n",
      "<<<iteration:[460/657] - total_loss: 0.1905  obj_loss: 0.0963  noobj_loss: 0.0650  bbox_loss: 0.0100  cls_loss: 0.0115  \n",
      "<<<iteration:[480/657] - total_loss: 0.1763  obj_loss: 0.0969  noobj_loss: 0.0590  bbox_loss: 0.0080  cls_loss: 0.0097  \n",
      "<<<iteration:[500/657] - total_loss: 0.1573  obj_loss: 0.0763  noobj_loss: 0.0584  bbox_loss: 0.0087  cls_loss: 0.0085  \n",
      "<<<iteration:[520/657] - total_loss: 0.1599  obj_loss: 0.0854  noobj_loss: 0.0558  bbox_loss: 0.0077  cls_loss: 0.0083  \n",
      "<<<iteration:[540/657] - total_loss: 0.1639  obj_loss: 0.0830  noobj_loss: 0.0542  bbox_loss: 0.0092  cls_loss: 0.0080  \n",
      "<<<iteration:[560/657] - total_loss: 0.1632  obj_loss: 0.0827  noobj_loss: 0.0622  bbox_loss: 0.0080  cls_loss: 0.0095  \n",
      "<<<iteration:[580/657] - total_loss: 0.1756  obj_loss: 0.1000  noobj_loss: 0.0625  bbox_loss: 0.0069  cls_loss: 0.0099  \n",
      "<<<iteration:[600/657] - total_loss: 0.1819  obj_loss: 0.0976  noobj_loss: 0.0625  bbox_loss: 0.0085  cls_loss: 0.0108  \n",
      "<<<iteration:[620/657] - total_loss: 0.1760  obj_loss: 0.0893  noobj_loss: 0.0618  bbox_loss: 0.0086  cls_loss: 0.0129  \n",
      "<<<iteration:[640/657] - total_loss: 0.1865  obj_loss: 0.0967  noobj_loss: 0.0628  bbox_loss: 0.0094  cls_loss: 0.0114  \n",
      "\n",
      "epoch:71/100 - Train Loss: 0.1708, Val Loss: 0.2276\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1839  obj_loss: 0.0987  noobj_loss: 0.0628  bbox_loss: 0.0085  cls_loss: 0.0112  \n",
      "<<<iteration:[40/657] - total_loss: 0.1622  obj_loss: 0.0903  noobj_loss: 0.0544  bbox_loss: 0.0072  cls_loss: 0.0086  \n",
      "<<<iteration:[60/657] - total_loss: 0.1746  obj_loss: 0.0882  noobj_loss: 0.0646  bbox_loss: 0.0087  cls_loss: 0.0105  \n",
      "<<<iteration:[80/657] - total_loss: 0.1686  obj_loss: 0.0881  noobj_loss: 0.0611  bbox_loss: 0.0083  cls_loss: 0.0084  \n",
      "<<<iteration:[100/657] - total_loss: 0.1725  obj_loss: 0.0896  noobj_loss: 0.0637  bbox_loss: 0.0079  cls_loss: 0.0118  \n",
      "<<<iteration:[120/657] - total_loss: 0.1708  obj_loss: 0.0912  noobj_loss: 0.0558  bbox_loss: 0.0083  cls_loss: 0.0101  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[140/657] - total_loss: 0.1644  obj_loss: 0.0834  noobj_loss: 0.0561  bbox_loss: 0.0084  cls_loss: 0.0109  \n",
      "<<<iteration:[160/657] - total_loss: 0.1674  obj_loss: 0.0844  noobj_loss: 0.0546  bbox_loss: 0.0088  cls_loss: 0.0116  \n",
      "<<<iteration:[180/657] - total_loss: 0.1676  obj_loss: 0.0915  noobj_loss: 0.0560  bbox_loss: 0.0074  cls_loss: 0.0111  \n",
      "<<<iteration:[200/657] - total_loss: 0.1717  obj_loss: 0.0847  noobj_loss: 0.0637  bbox_loss: 0.0092  cls_loss: 0.0090  \n",
      "<<<iteration:[220/657] - total_loss: 0.1739  obj_loss: 0.0900  noobj_loss: 0.0600  bbox_loss: 0.0086  cls_loss: 0.0108  \n",
      "<<<iteration:[240/657] - total_loss: 0.1804  obj_loss: 0.0982  noobj_loss: 0.0589  bbox_loss: 0.0083  cls_loss: 0.0112  \n",
      "<<<iteration:[260/657] - total_loss: 0.1596  obj_loss: 0.0829  noobj_loss: 0.0499  bbox_loss: 0.0084  cls_loss: 0.0097  \n",
      "<<<iteration:[280/657] - total_loss: 0.1793  obj_loss: 0.0929  noobj_loss: 0.0625  bbox_loss: 0.0087  cls_loss: 0.0115  \n",
      "<<<iteration:[300/657] - total_loss: 0.1607  obj_loss: 0.0798  noobj_loss: 0.0604  bbox_loss: 0.0082  cls_loss: 0.0096  \n",
      "<<<iteration:[320/657] - total_loss: 0.1674  obj_loss: 0.0857  noobj_loss: 0.0593  bbox_loss: 0.0083  cls_loss: 0.0105  \n",
      "<<<iteration:[340/657] - total_loss: 0.1827  obj_loss: 0.0921  noobj_loss: 0.0629  bbox_loss: 0.0090  cls_loss: 0.0139  \n",
      "<<<iteration:[360/657] - total_loss: 0.1737  obj_loss: 0.0886  noobj_loss: 0.0586  bbox_loss: 0.0093  cls_loss: 0.0091  \n",
      "<<<iteration:[380/657] - total_loss: 0.1824  obj_loss: 0.0905  noobj_loss: 0.0590  bbox_loss: 0.0094  cls_loss: 0.0156  \n",
      "<<<iteration:[400/657] - total_loss: 0.1686  obj_loss: 0.0874  noobj_loss: 0.0566  bbox_loss: 0.0084  cls_loss: 0.0109  \n",
      "<<<iteration:[420/657] - total_loss: 0.1827  obj_loss: 0.0971  noobj_loss: 0.0601  bbox_loss: 0.0087  cls_loss: 0.0122  \n",
      "<<<iteration:[440/657] - total_loss: 0.1693  obj_loss: 0.0874  noobj_loss: 0.0540  bbox_loss: 0.0088  cls_loss: 0.0110  \n",
      "<<<iteration:[460/657] - total_loss: 0.1721  obj_loss: 0.0881  noobj_loss: 0.0617  bbox_loss: 0.0087  cls_loss: 0.0094  \n",
      "<<<iteration:[480/657] - total_loss: 0.1532  obj_loss: 0.0779  noobj_loss: 0.0474  bbox_loss: 0.0081  cls_loss: 0.0111  \n",
      "<<<iteration:[500/657] - total_loss: 0.1877  obj_loss: 0.0967  noobj_loss: 0.0573  bbox_loss: 0.0100  cls_loss: 0.0121  \n",
      "<<<iteration:[520/657] - total_loss: 0.1732  obj_loss: 0.0866  noobj_loss: 0.0572  bbox_loss: 0.0095  cls_loss: 0.0105  \n",
      "<<<iteration:[540/657] - total_loss: 0.1645  obj_loss: 0.0932  noobj_loss: 0.0537  bbox_loss: 0.0072  cls_loss: 0.0086  \n",
      "<<<iteration:[560/657] - total_loss: 0.1776  obj_loss: 0.0961  noobj_loss: 0.0548  bbox_loss: 0.0087  cls_loss: 0.0107  \n",
      "<<<iteration:[580/657] - total_loss: 0.1743  obj_loss: 0.0892  noobj_loss: 0.0643  bbox_loss: 0.0085  cls_loss: 0.0105  \n",
      "<<<iteration:[600/657] - total_loss: 0.1665  obj_loss: 0.0806  noobj_loss: 0.0619  bbox_loss: 0.0090  cls_loss: 0.0097  \n",
      "<<<iteration:[620/657] - total_loss: 0.1779  obj_loss: 0.0981  noobj_loss: 0.0533  bbox_loss: 0.0084  cls_loss: 0.0112  \n",
      "<<<iteration:[640/657] - total_loss: 0.1710  obj_loss: 0.0860  noobj_loss: 0.0551  bbox_loss: 0.0091  cls_loss: 0.0121  \n",
      "\n",
      "epoch:72/100 - Train Loss: 0.1715, Val Loss: 0.2236\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1772  obj_loss: 0.0931  noobj_loss: 0.0573  bbox_loss: 0.0087  cls_loss: 0.0119  \n",
      "<<<iteration:[40/657] - total_loss: 0.1584  obj_loss: 0.0787  noobj_loss: 0.0599  bbox_loss: 0.0083  cls_loss: 0.0084  \n",
      "<<<iteration:[60/657] - total_loss: 0.1679  obj_loss: 0.0914  noobj_loss: 0.0561  bbox_loss: 0.0080  cls_loss: 0.0087  \n",
      "<<<iteration:[80/657] - total_loss: 0.1606  obj_loss: 0.0847  noobj_loss: 0.0532  bbox_loss: 0.0079  cls_loss: 0.0097  \n",
      "<<<iteration:[100/657] - total_loss: 0.1651  obj_loss: 0.0841  noobj_loss: 0.0597  bbox_loss: 0.0079  cls_loss: 0.0116  \n",
      "<<<iteration:[120/657] - total_loss: 0.1736  obj_loss: 0.0961  noobj_loss: 0.0566  bbox_loss: 0.0076  cls_loss: 0.0114  \n",
      "<<<iteration:[140/657] - total_loss: 0.1832  obj_loss: 0.0951  noobj_loss: 0.0567  bbox_loss: 0.0097  cls_loss: 0.0115  \n",
      "<<<iteration:[160/657] - total_loss: 0.1578  obj_loss: 0.0813  noobj_loss: 0.0567  bbox_loss: 0.0074  cls_loss: 0.0113  \n",
      "<<<iteration:[180/657] - total_loss: 0.1661  obj_loss: 0.0834  noobj_loss: 0.0535  bbox_loss: 0.0095  cls_loss: 0.0086  \n",
      "<<<iteration:[200/657] - total_loss: 0.1676  obj_loss: 0.0782  noobj_loss: 0.0537  bbox_loss: 0.0100  cls_loss: 0.0127  \n",
      "<<<iteration:[220/657] - total_loss: 0.1674  obj_loss: 0.0872  noobj_loss: 0.0470  bbox_loss: 0.0088  cls_loss: 0.0126  \n",
      "<<<iteration:[240/657] - total_loss: 0.1626  obj_loss: 0.0802  noobj_loss: 0.0610  bbox_loss: 0.0083  cls_loss: 0.0106  \n",
      "<<<iteration:[260/657] - total_loss: 0.1445  obj_loss: 0.0706  noobj_loss: 0.0590  bbox_loss: 0.0069  cls_loss: 0.0097  \n",
      "<<<iteration:[280/657] - total_loss: 0.1653  obj_loss: 0.0779  noobj_loss: 0.0639  bbox_loss: 0.0089  cls_loss: 0.0109  \n",
      "<<<iteration:[300/657] - total_loss: 0.1647  obj_loss: 0.0824  noobj_loss: 0.0537  bbox_loss: 0.0087  cls_loss: 0.0118  \n",
      "<<<iteration:[320/657] - total_loss: 0.1691  obj_loss: 0.0893  noobj_loss: 0.0539  bbox_loss: 0.0085  cls_loss: 0.0102  \n",
      "<<<iteration:[340/657] - total_loss: 0.1690  obj_loss: 0.0919  noobj_loss: 0.0632  bbox_loss: 0.0072  cls_loss: 0.0095  \n",
      "<<<iteration:[360/657] - total_loss: 0.1781  obj_loss: 0.0966  noobj_loss: 0.0626  bbox_loss: 0.0081  cls_loss: 0.0098  \n",
      "<<<iteration:[380/657] - total_loss: 0.1728  obj_loss: 0.0902  noobj_loss: 0.0553  bbox_loss: 0.0088  cls_loss: 0.0111  \n",
      "<<<iteration:[400/657] - total_loss: 0.1751  obj_loss: 0.0958  noobj_loss: 0.0560  bbox_loss: 0.0083  cls_loss: 0.0099  \n",
      "<<<iteration:[420/657] - total_loss: 0.1656  obj_loss: 0.0844  noobj_loss: 0.0638  bbox_loss: 0.0077  cls_loss: 0.0110  \n",
      "<<<iteration:[440/657] - total_loss: 0.1793  obj_loss: 0.0944  noobj_loss: 0.0649  bbox_loss: 0.0085  cls_loss: 0.0099  \n",
      "<<<iteration:[460/657] - total_loss: 0.1650  obj_loss: 0.0872  noobj_loss: 0.0595  bbox_loss: 0.0076  cls_loss: 0.0102  \n",
      "<<<iteration:[480/657] - total_loss: 0.1624  obj_loss: 0.0774  noobj_loss: 0.0600  bbox_loss: 0.0086  cls_loss: 0.0121  \n",
      "<<<iteration:[500/657] - total_loss: 0.1813  obj_loss: 0.0960  noobj_loss: 0.0571  bbox_loss: 0.0087  cls_loss: 0.0131  \n",
      "<<<iteration:[520/657] - total_loss: 0.1689  obj_loss: 0.0814  noobj_loss: 0.0546  bbox_loss: 0.0095  cls_loss: 0.0129  \n",
      "<<<iteration:[540/657] - total_loss: 0.1797  obj_loss: 0.0979  noobj_loss: 0.0613  bbox_loss: 0.0084  cls_loss: 0.0089  \n",
      "<<<iteration:[560/657] - total_loss: 0.1636  obj_loss: 0.0864  noobj_loss: 0.0527  bbox_loss: 0.0086  cls_loss: 0.0079  \n",
      "<<<iteration:[580/657] - total_loss: 0.1721  obj_loss: 0.0903  noobj_loss: 0.0601  bbox_loss: 0.0086  cls_loss: 0.0085  \n",
      "<<<iteration:[600/657] - total_loss: 0.1804  obj_loss: 0.0951  noobj_loss: 0.0561  bbox_loss: 0.0092  cls_loss: 0.0110  \n",
      "<<<iteration:[620/657] - total_loss: 0.1750  obj_loss: 0.0877  noobj_loss: 0.0591  bbox_loss: 0.0089  cls_loss: 0.0132  \n",
      "<<<iteration:[640/657] - total_loss: 0.1683  obj_loss: 0.0917  noobj_loss: 0.0533  bbox_loss: 0.0081  cls_loss: 0.0094  \n",
      "\n",
      "epoch:73/100 - Train Loss: 0.1692, Val Loss: 0.2250\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1821  obj_loss: 0.1011  noobj_loss: 0.0595  bbox_loss: 0.0077  cls_loss: 0.0125  \n",
      "<<<iteration:[40/657] - total_loss: 0.1485  obj_loss: 0.0727  noobj_loss: 0.0568  bbox_loss: 0.0077  cls_loss: 0.0089  \n",
      "<<<iteration:[60/657] - total_loss: 0.1691  obj_loss: 0.0891  noobj_loss: 0.0535  bbox_loss: 0.0085  cls_loss: 0.0107  \n",
      "<<<iteration:[80/657] - total_loss: 0.1663  obj_loss: 0.0883  noobj_loss: 0.0615  bbox_loss: 0.0075  cls_loss: 0.0097  \n",
      "<<<iteration:[100/657] - total_loss: 0.1729  obj_loss: 0.0948  noobj_loss: 0.0573  bbox_loss: 0.0077  cls_loss: 0.0107  \n",
      "<<<iteration:[120/657] - total_loss: 0.1808  obj_loss: 0.0994  noobj_loss: 0.0595  bbox_loss: 0.0081  cls_loss: 0.0111  \n",
      "<<<iteration:[140/657] - total_loss: 0.1577  obj_loss: 0.0760  noobj_loss: 0.0590  bbox_loss: 0.0084  cls_loss: 0.0102  \n",
      "<<<iteration:[160/657] - total_loss: 0.1544  obj_loss: 0.0807  noobj_loss: 0.0550  bbox_loss: 0.0072  cls_loss: 0.0103  \n",
      "<<<iteration:[180/657] - total_loss: 0.1681  obj_loss: 0.0861  noobj_loss: 0.0554  bbox_loss: 0.0086  cls_loss: 0.0115  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[200/657] - total_loss: 0.1618  obj_loss: 0.0860  noobj_loss: 0.0615  bbox_loss: 0.0073  cls_loss: 0.0087  \n",
      "<<<iteration:[220/657] - total_loss: 0.1674  obj_loss: 0.0824  noobj_loss: 0.0579  bbox_loss: 0.0089  cls_loss: 0.0115  \n",
      "<<<iteration:[240/657] - total_loss: 0.1745  obj_loss: 0.0837  noobj_loss: 0.0568  bbox_loss: 0.0105  cls_loss: 0.0098  \n",
      "<<<iteration:[260/657] - total_loss: 0.1647  obj_loss: 0.0819  noobj_loss: 0.0573  bbox_loss: 0.0088  cls_loss: 0.0099  \n",
      "<<<iteration:[280/657] - total_loss: 0.1811  obj_loss: 0.0933  noobj_loss: 0.0562  bbox_loss: 0.0093  cls_loss: 0.0132  \n",
      "<<<iteration:[300/657] - total_loss: 0.1805  obj_loss: 0.0869  noobj_loss: 0.0532  bbox_loss: 0.0109  cls_loss: 0.0123  \n",
      "<<<iteration:[320/657] - total_loss: 0.1610  obj_loss: 0.0863  noobj_loss: 0.0512  bbox_loss: 0.0080  cls_loss: 0.0093  \n",
      "<<<iteration:[340/657] - total_loss: 0.1711  obj_loss: 0.0855  noobj_loss: 0.0558  bbox_loss: 0.0097  cls_loss: 0.0092  \n",
      "<<<iteration:[360/657] - total_loss: 0.1600  obj_loss: 0.0788  noobj_loss: 0.0671  bbox_loss: 0.0078  cls_loss: 0.0085  \n",
      "<<<iteration:[380/657] - total_loss: 0.1607  obj_loss: 0.0819  noobj_loss: 0.0551  bbox_loss: 0.0082  cls_loss: 0.0101  \n",
      "<<<iteration:[400/657] - total_loss: 0.1610  obj_loss: 0.0839  noobj_loss: 0.0601  bbox_loss: 0.0075  cls_loss: 0.0094  \n",
      "<<<iteration:[420/657] - total_loss: 0.1641  obj_loss: 0.0781  noobj_loss: 0.0523  bbox_loss: 0.0100  cls_loss: 0.0098  \n",
      "<<<iteration:[440/657] - total_loss: 0.1712  obj_loss: 0.0893  noobj_loss: 0.0660  bbox_loss: 0.0078  cls_loss: 0.0098  \n",
      "<<<iteration:[460/657] - total_loss: 0.1849  obj_loss: 0.0971  noobj_loss: 0.0607  bbox_loss: 0.0092  cls_loss: 0.0116  \n",
      "<<<iteration:[480/657] - total_loss: 0.1916  obj_loss: 0.0912  noobj_loss: 0.0596  bbox_loss: 0.0109  cls_loss: 0.0162  \n",
      "<<<iteration:[500/657] - total_loss: 0.1628  obj_loss: 0.0843  noobj_loss: 0.0588  bbox_loss: 0.0080  cls_loss: 0.0092  \n",
      "<<<iteration:[520/657] - total_loss: 0.1729  obj_loss: 0.0945  noobj_loss: 0.0597  bbox_loss: 0.0079  cls_loss: 0.0089  \n",
      "<<<iteration:[540/657] - total_loss: 0.1787  obj_loss: 0.0945  noobj_loss: 0.0612  bbox_loss: 0.0085  cls_loss: 0.0111  \n",
      "<<<iteration:[560/657] - total_loss: 0.1684  obj_loss: 0.0910  noobj_loss: 0.0538  bbox_loss: 0.0078  cls_loss: 0.0117  \n",
      "<<<iteration:[580/657] - total_loss: 0.1706  obj_loss: 0.0931  noobj_loss: 0.0554  bbox_loss: 0.0076  cls_loss: 0.0119  \n",
      "<<<iteration:[600/657] - total_loss: 0.1685  obj_loss: 0.0840  noobj_loss: 0.0614  bbox_loss: 0.0087  cls_loss: 0.0102  \n",
      "<<<iteration:[620/657] - total_loss: 0.1688  obj_loss: 0.0906  noobj_loss: 0.0622  bbox_loss: 0.0076  cls_loss: 0.0093  \n",
      "<<<iteration:[640/657] - total_loss: 0.1705  obj_loss: 0.0969  noobj_loss: 0.0594  bbox_loss: 0.0072  cls_loss: 0.0079  \n",
      "\n",
      "epoch:74/100 - Train Loss: 0.1753, Val Loss: 0.3945\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1710  obj_loss: 0.0860  noobj_loss: 0.0566  bbox_loss: 0.0088  cls_loss: 0.0128  \n",
      "<<<iteration:[40/657] - total_loss: 0.1792  obj_loss: 0.0850  noobj_loss: 0.0555  bbox_loss: 0.0111  cls_loss: 0.0111  \n",
      "<<<iteration:[60/657] - total_loss: 0.1771  obj_loss: 0.0877  noobj_loss: 0.0633  bbox_loss: 0.0096  cls_loss: 0.0097  \n",
      "<<<iteration:[80/657] - total_loss: 0.1787  obj_loss: 0.0901  noobj_loss: 0.0563  bbox_loss: 0.0096  cls_loss: 0.0123  \n",
      "<<<iteration:[100/657] - total_loss: 0.1759  obj_loss: 0.0867  noobj_loss: 0.0612  bbox_loss: 0.0097  cls_loss: 0.0103  \n",
      "<<<iteration:[120/657] - total_loss: 0.1768  obj_loss: 0.0892  noobj_loss: 0.0583  bbox_loss: 0.0094  cls_loss: 0.0117  \n",
      "<<<iteration:[140/657] - total_loss: 0.1891  obj_loss: 0.0982  noobj_loss: 0.0563  bbox_loss: 0.0101  cls_loss: 0.0124  \n",
      "<<<iteration:[160/657] - total_loss: 0.1649  obj_loss: 0.0787  noobj_loss: 0.0552  bbox_loss: 0.0094  cls_loss: 0.0114  \n",
      "<<<iteration:[180/657] - total_loss: 0.1743  obj_loss: 0.0923  noobj_loss: 0.0650  bbox_loss: 0.0078  cls_loss: 0.0107  \n",
      "<<<iteration:[200/657] - total_loss: 0.1701  obj_loss: 0.0836  noobj_loss: 0.0627  bbox_loss: 0.0090  cls_loss: 0.0103  \n",
      "<<<iteration:[220/657] - total_loss: 0.1805  obj_loss: 0.0845  noobj_loss: 0.0669  bbox_loss: 0.0103  cls_loss: 0.0113  \n",
      "<<<iteration:[240/657] - total_loss: 0.1659  obj_loss: 0.0690  noobj_loss: 0.0490  bbox_loss: 0.0120  cls_loss: 0.0125  \n",
      "<<<iteration:[260/657] - total_loss: 0.1662  obj_loss: 0.0829  noobj_loss: 0.0656  bbox_loss: 0.0082  cls_loss: 0.0093  \n",
      "<<<iteration:[280/657] - total_loss: 0.1799  obj_loss: 0.0923  noobj_loss: 0.0599  bbox_loss: 0.0089  cls_loss: 0.0132  \n",
      "<<<iteration:[300/657] - total_loss: 0.1744  obj_loss: 0.0831  noobj_loss: 0.0636  bbox_loss: 0.0102  cls_loss: 0.0086  \n",
      "<<<iteration:[320/657] - total_loss: 0.1769  obj_loss: 0.0824  noobj_loss: 0.0665  bbox_loss: 0.0092  cls_loss: 0.0152  \n",
      "<<<iteration:[340/657] - total_loss: 0.1719  obj_loss: 0.0957  noobj_loss: 0.0535  bbox_loss: 0.0081  cls_loss: 0.0088  \n",
      "<<<iteration:[360/657] - total_loss: 0.1637  obj_loss: 0.0848  noobj_loss: 0.0581  bbox_loss: 0.0079  cls_loss: 0.0103  \n",
      "<<<iteration:[380/657] - total_loss: 0.1864  obj_loss: 0.0958  noobj_loss: 0.0653  bbox_loss: 0.0096  cls_loss: 0.0099  \n",
      "<<<iteration:[400/657] - total_loss: 0.1759  obj_loss: 0.0927  noobj_loss: 0.0630  bbox_loss: 0.0082  cls_loss: 0.0105  \n",
      "<<<iteration:[420/657] - total_loss: 0.1662  obj_loss: 0.0847  noobj_loss: 0.0604  bbox_loss: 0.0084  cls_loss: 0.0095  \n",
      "<<<iteration:[440/657] - total_loss: 0.1801  obj_loss: 0.0901  noobj_loss: 0.0619  bbox_loss: 0.0092  cls_loss: 0.0132  \n",
      "<<<iteration:[460/657] - total_loss: 0.1766  obj_loss: 0.0944  noobj_loss: 0.0555  bbox_loss: 0.0092  cls_loss: 0.0085  \n",
      "<<<iteration:[480/657] - total_loss: 0.1561  obj_loss: 0.0790  noobj_loss: 0.0517  bbox_loss: 0.0082  cls_loss: 0.0104  \n",
      "<<<iteration:[500/657] - total_loss: 0.1716  obj_loss: 0.0941  noobj_loss: 0.0614  bbox_loss: 0.0075  cls_loss: 0.0093  \n",
      "<<<iteration:[520/657] - total_loss: 0.1669  obj_loss: 0.0840  noobj_loss: 0.0587  bbox_loss: 0.0088  cls_loss: 0.0097  \n",
      "<<<iteration:[540/657] - total_loss: 0.1765  obj_loss: 0.0820  noobj_loss: 0.0621  bbox_loss: 0.0098  cls_loss: 0.0146  \n",
      "<<<iteration:[560/657] - total_loss: 0.1726  obj_loss: 0.0886  noobj_loss: 0.0618  bbox_loss: 0.0087  cls_loss: 0.0094  \n",
      "<<<iteration:[580/657] - total_loss: 0.1587  obj_loss: 0.0848  noobj_loss: 0.0502  bbox_loss: 0.0079  cls_loss: 0.0093  \n",
      "<<<iteration:[600/657] - total_loss: 0.1635  obj_loss: 0.0838  noobj_loss: 0.0570  bbox_loss: 0.0082  cls_loss: 0.0102  \n",
      "<<<iteration:[620/657] - total_loss: 0.1722  obj_loss: 0.0861  noobj_loss: 0.0561  bbox_loss: 0.0090  cls_loss: 0.0129  \n",
      "<<<iteration:[640/657] - total_loss: 0.1657  obj_loss: 0.0795  noobj_loss: 0.0550  bbox_loss: 0.0092  cls_loss: 0.0125  \n",
      "\n",
      "epoch:75/100 - Train Loss: 0.1723, Val Loss: 0.2247\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1687  obj_loss: 0.0877  noobj_loss: 0.0583  bbox_loss: 0.0083  cls_loss: 0.0105  \n",
      "<<<iteration:[40/657] - total_loss: 0.1707  obj_loss: 0.0854  noobj_loss: 0.0575  bbox_loss: 0.0092  cls_loss: 0.0107  \n",
      "<<<iteration:[60/657] - total_loss: 0.1633  obj_loss: 0.0857  noobj_loss: 0.0552  bbox_loss: 0.0079  cls_loss: 0.0106  \n",
      "<<<iteration:[80/657] - total_loss: 0.1851  obj_loss: 0.1020  noobj_loss: 0.0593  bbox_loss: 0.0084  cls_loss: 0.0117  \n",
      "<<<iteration:[100/657] - total_loss: 0.1683  obj_loss: 0.0893  noobj_loss: 0.0664  bbox_loss: 0.0073  cls_loss: 0.0091  \n",
      "<<<iteration:[120/657] - total_loss: 0.1670  obj_loss: 0.0894  noobj_loss: 0.0559  bbox_loss: 0.0079  cls_loss: 0.0104  \n",
      "<<<iteration:[140/657] - total_loss: 0.1510  obj_loss: 0.0792  noobj_loss: 0.0561  bbox_loss: 0.0072  cls_loss: 0.0078  \n",
      "<<<iteration:[160/657] - total_loss: 0.1744  obj_loss: 0.0831  noobj_loss: 0.0653  bbox_loss: 0.0096  cls_loss: 0.0109  \n",
      "<<<iteration:[180/657] - total_loss: 0.1698  obj_loss: 0.0839  noobj_loss: 0.0591  bbox_loss: 0.0090  cls_loss: 0.0114  \n",
      "<<<iteration:[200/657] - total_loss: 0.1619  obj_loss: 0.0853  noobj_loss: 0.0533  bbox_loss: 0.0080  cls_loss: 0.0101  \n",
      "<<<iteration:[220/657] - total_loss: 0.1768  obj_loss: 0.0873  noobj_loss: 0.0624  bbox_loss: 0.0095  cls_loss: 0.0109  \n",
      "<<<iteration:[240/657] - total_loss: 0.1647  obj_loss: 0.0876  noobj_loss: 0.0567  bbox_loss: 0.0077  cls_loss: 0.0100  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[260/657] - total_loss: 0.1725  obj_loss: 0.0981  noobj_loss: 0.0628  bbox_loss: 0.0066  cls_loss: 0.0099  \n",
      "<<<iteration:[280/657] - total_loss: 0.1722  obj_loss: 0.0836  noobj_loss: 0.0613  bbox_loss: 0.0091  cls_loss: 0.0127  \n",
      "<<<iteration:[300/657] - total_loss: 0.1616  obj_loss: 0.0799  noobj_loss: 0.0613  bbox_loss: 0.0083  cls_loss: 0.0096  \n",
      "<<<iteration:[320/657] - total_loss: 0.1702  obj_loss: 0.0847  noobj_loss: 0.0636  bbox_loss: 0.0086  cls_loss: 0.0108  \n",
      "<<<iteration:[340/657] - total_loss: 0.1643  obj_loss: 0.0849  noobj_loss: 0.0508  bbox_loss: 0.0088  cls_loss: 0.0098  \n",
      "<<<iteration:[360/657] - total_loss: 0.1663  obj_loss: 0.0886  noobj_loss: 0.0582  bbox_loss: 0.0080  cls_loss: 0.0085  \n",
      "<<<iteration:[380/657] - total_loss: 0.1674  obj_loss: 0.0850  noobj_loss: 0.0598  bbox_loss: 0.0085  cls_loss: 0.0098  \n",
      "<<<iteration:[400/657] - total_loss: 0.1732  obj_loss: 0.0906  noobj_loss: 0.0656  bbox_loss: 0.0081  cls_loss: 0.0091  \n",
      "<<<iteration:[420/657] - total_loss: 0.1650  obj_loss: 0.0845  noobj_loss: 0.0551  bbox_loss: 0.0087  cls_loss: 0.0093  \n",
      "<<<iteration:[440/657] - total_loss: 0.1712  obj_loss: 0.0951  noobj_loss: 0.0614  bbox_loss: 0.0072  cls_loss: 0.0095  \n",
      "<<<iteration:[460/657] - total_loss: 0.1620  obj_loss: 0.0832  noobj_loss: 0.0521  bbox_loss: 0.0086  cls_loss: 0.0094  \n",
      "<<<iteration:[480/657] - total_loss: 0.1668  obj_loss: 0.0892  noobj_loss: 0.0477  bbox_loss: 0.0090  cls_loss: 0.0090  \n",
      "<<<iteration:[500/657] - total_loss: 0.1593  obj_loss: 0.0794  noobj_loss: 0.0559  bbox_loss: 0.0084  cls_loss: 0.0101  \n",
      "<<<iteration:[520/657] - total_loss: 0.1649  obj_loss: 0.0809  noobj_loss: 0.0600  bbox_loss: 0.0089  cls_loss: 0.0097  \n",
      "<<<iteration:[540/657] - total_loss: 0.1760  obj_loss: 0.0813  noobj_loss: 0.0569  bbox_loss: 0.0110  cls_loss: 0.0110  \n",
      "<<<iteration:[560/657] - total_loss: 0.1654  obj_loss: 0.0781  noobj_loss: 0.0613  bbox_loss: 0.0092  cls_loss: 0.0103  \n",
      "<<<iteration:[580/657] - total_loss: 0.1723  obj_loss: 0.0877  noobj_loss: 0.0532  bbox_loss: 0.0090  cls_loss: 0.0130  \n",
      "<<<iteration:[600/657] - total_loss: 0.1630  obj_loss: 0.0856  noobj_loss: 0.0546  bbox_loss: 0.0080  cls_loss: 0.0101  \n",
      "<<<iteration:[620/657] - total_loss: 0.1699  obj_loss: 0.0833  noobj_loss: 0.0631  bbox_loss: 0.0086  cls_loss: 0.0119  \n",
      "<<<iteration:[640/657] - total_loss: 0.1772  obj_loss: 0.0910  noobj_loss: 0.0613  bbox_loss: 0.0089  cls_loss: 0.0108  \n",
      "\n",
      "epoch:76/100 - Train Loss: 0.1676, Val Loss: 0.2253\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1872  obj_loss: 0.0965  noobj_loss: 0.0654  bbox_loss: 0.0096  cls_loss: 0.0099  \n",
      "<<<iteration:[40/657] - total_loss: 0.1742  obj_loss: 0.0970  noobj_loss: 0.0575  bbox_loss: 0.0074  cls_loss: 0.0112  \n",
      "<<<iteration:[60/657] - total_loss: 0.1728  obj_loss: 0.0857  noobj_loss: 0.0540  bbox_loss: 0.0099  cls_loss: 0.0108  \n",
      "<<<iteration:[80/657] - total_loss: 0.1773  obj_loss: 0.0877  noobj_loss: 0.0551  bbox_loss: 0.0102  cls_loss: 0.0113  \n",
      "<<<iteration:[100/657] - total_loss: 0.1779  obj_loss: 0.0968  noobj_loss: 0.0555  bbox_loss: 0.0086  cls_loss: 0.0104  \n",
      "<<<iteration:[120/657] - total_loss: 0.1621  obj_loss: 0.0824  noobj_loss: 0.0543  bbox_loss: 0.0086  cls_loss: 0.0095  \n",
      "<<<iteration:[140/657] - total_loss: 0.1606  obj_loss: 0.0780  noobj_loss: 0.0618  bbox_loss: 0.0085  cls_loss: 0.0093  \n",
      "<<<iteration:[160/657] - total_loss: 0.1627  obj_loss: 0.0893  noobj_loss: 0.0573  bbox_loss: 0.0070  cls_loss: 0.0096  \n",
      "<<<iteration:[180/657] - total_loss: 0.1628  obj_loss: 0.0911  noobj_loss: 0.0553  bbox_loss: 0.0070  cls_loss: 0.0093  \n",
      "<<<iteration:[200/657] - total_loss: 0.1645  obj_loss: 0.0835  noobj_loss: 0.0517  bbox_loss: 0.0085  cls_loss: 0.0128  \n",
      "<<<iteration:[220/657] - total_loss: 0.1716  obj_loss: 0.0822  noobj_loss: 0.0584  bbox_loss: 0.0095  cls_loss: 0.0129  \n",
      "<<<iteration:[240/657] - total_loss: 0.1594  obj_loss: 0.0807  noobj_loss: 0.0528  bbox_loss: 0.0087  cls_loss: 0.0086  \n",
      "<<<iteration:[260/657] - total_loss: 0.1634  obj_loss: 0.0850  noobj_loss: 0.0568  bbox_loss: 0.0079  cls_loss: 0.0104  \n",
      "<<<iteration:[280/657] - total_loss: 0.1635  obj_loss: 0.0842  noobj_loss: 0.0575  bbox_loss: 0.0082  cls_loss: 0.0096  \n",
      "<<<iteration:[300/657] - total_loss: 0.1697  obj_loss: 0.0957  noobj_loss: 0.0570  bbox_loss: 0.0075  cls_loss: 0.0080  \n",
      "<<<iteration:[320/657] - total_loss: 0.1746  obj_loss: 0.0846  noobj_loss: 0.0571  bbox_loss: 0.0098  cls_loss: 0.0128  \n",
      "<<<iteration:[340/657] - total_loss: 0.1734  obj_loss: 0.0801  noobj_loss: 0.0529  bbox_loss: 0.0109  cls_loss: 0.0125  \n",
      "<<<iteration:[360/657] - total_loss: 0.1655  obj_loss: 0.0854  noobj_loss: 0.0566  bbox_loss: 0.0083  cls_loss: 0.0103  \n",
      "<<<iteration:[380/657] - total_loss: 0.1557  obj_loss: 0.0789  noobj_loss: 0.0522  bbox_loss: 0.0083  cls_loss: 0.0089  \n",
      "<<<iteration:[400/657] - total_loss: 0.1515  obj_loss: 0.0783  noobj_loss: 0.0545  bbox_loss: 0.0073  cls_loss: 0.0096  \n",
      "<<<iteration:[420/657] - total_loss: 0.1776  obj_loss: 0.0921  noobj_loss: 0.0627  bbox_loss: 0.0086  cls_loss: 0.0113  \n",
      "<<<iteration:[440/657] - total_loss: 0.1632  obj_loss: 0.0820  noobj_loss: 0.0577  bbox_loss: 0.0085  cls_loss: 0.0099  \n",
      "<<<iteration:[460/657] - total_loss: 0.1649  obj_loss: 0.0847  noobj_loss: 0.0538  bbox_loss: 0.0084  cls_loss: 0.0114  \n",
      "<<<iteration:[480/657] - total_loss: 0.1692  obj_loss: 0.0927  noobj_loss: 0.0646  bbox_loss: 0.0073  cls_loss: 0.0077  \n",
      "<<<iteration:[500/657] - total_loss: 0.1639  obj_loss: 0.0876  noobj_loss: 0.0583  bbox_loss: 0.0076  cls_loss: 0.0090  \n",
      "<<<iteration:[520/657] - total_loss: 0.1574  obj_loss: 0.0884  noobj_loss: 0.0578  bbox_loss: 0.0065  cls_loss: 0.0075  \n",
      "<<<iteration:[540/657] - total_loss: 0.1696  obj_loss: 0.0887  noobj_loss: 0.0533  bbox_loss: 0.0088  cls_loss: 0.0101  \n",
      "<<<iteration:[560/657] - total_loss: 0.1603  obj_loss: 0.0825  noobj_loss: 0.0502  bbox_loss: 0.0085  cls_loss: 0.0103  \n",
      "<<<iteration:[580/657] - total_loss: 0.1762  obj_loss: 0.0912  noobj_loss: 0.0560  bbox_loss: 0.0093  cls_loss: 0.0105  \n",
      "<<<iteration:[600/657] - total_loss: 0.1576  obj_loss: 0.0772  noobj_loss: 0.0561  bbox_loss: 0.0086  cls_loss: 0.0091  \n",
      "<<<iteration:[620/657] - total_loss: 0.1533  obj_loss: 0.0802  noobj_loss: 0.0529  bbox_loss: 0.0076  cls_loss: 0.0088  \n",
      "<<<iteration:[640/657] - total_loss: 0.1753  obj_loss: 0.0879  noobj_loss: 0.0619  bbox_loss: 0.0091  cls_loss: 0.0111  \n",
      "\n",
      "epoch:77/100 - Train Loss: 0.1669, Val Loss: 0.2228\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1694  obj_loss: 0.0799  noobj_loss: 0.0562  bbox_loss: 0.0103  cls_loss: 0.0100  \n",
      "<<<iteration:[40/657] - total_loss: 0.1887  obj_loss: 0.1020  noobj_loss: 0.0574  bbox_loss: 0.0096  cls_loss: 0.0101  \n",
      "<<<iteration:[60/657] - total_loss: 0.1604  obj_loss: 0.0869  noobj_loss: 0.0491  bbox_loss: 0.0078  cls_loss: 0.0097  \n",
      "<<<iteration:[80/657] - total_loss: 0.1690  obj_loss: 0.0885  noobj_loss: 0.0628  bbox_loss: 0.0079  cls_loss: 0.0098  \n",
      "<<<iteration:[100/657] - total_loss: 0.1755  obj_loss: 0.0937  noobj_loss: 0.0599  bbox_loss: 0.0078  cls_loss: 0.0130  \n",
      "<<<iteration:[120/657] - total_loss: 0.1586  obj_loss: 0.0761  noobj_loss: 0.0530  bbox_loss: 0.0088  cls_loss: 0.0120  \n",
      "<<<iteration:[140/657] - total_loss: 0.1565  obj_loss: 0.0832  noobj_loss: 0.0467  bbox_loss: 0.0079  cls_loss: 0.0102  \n",
      "<<<iteration:[160/657] - total_loss: 0.1812  obj_loss: 0.1018  noobj_loss: 0.0611  bbox_loss: 0.0078  cls_loss: 0.0098  \n",
      "<<<iteration:[180/657] - total_loss: 0.1643  obj_loss: 0.0875  noobj_loss: 0.0562  bbox_loss: 0.0077  cls_loss: 0.0104  \n",
      "<<<iteration:[200/657] - total_loss: 0.1690  obj_loss: 0.0815  noobj_loss: 0.0584  bbox_loss: 0.0093  cls_loss: 0.0117  \n",
      "<<<iteration:[220/657] - total_loss: 0.1562  obj_loss: 0.0778  noobj_loss: 0.0494  bbox_loss: 0.0087  cls_loss: 0.0103  \n",
      "<<<iteration:[240/657] - total_loss: 0.1657  obj_loss: 0.0922  noobj_loss: 0.0470  bbox_loss: 0.0080  cls_loss: 0.0102  \n",
      "<<<iteration:[260/657] - total_loss: 0.1693  obj_loss: 0.0927  noobj_loss: 0.0550  bbox_loss: 0.0080  cls_loss: 0.0089  \n",
      "<<<iteration:[280/657] - total_loss: 0.1547  obj_loss: 0.0808  noobj_loss: 0.0503  bbox_loss: 0.0078  cls_loss: 0.0097  \n",
      "<<<iteration:[300/657] - total_loss: 0.1588  obj_loss: 0.0843  noobj_loss: 0.0572  bbox_loss: 0.0074  cls_loss: 0.0090  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[320/657] - total_loss: 0.1760  obj_loss: 0.0950  noobj_loss: 0.0638  bbox_loss: 0.0079  cls_loss: 0.0099  \n",
      "<<<iteration:[340/657] - total_loss: 0.1595  obj_loss: 0.0809  noobj_loss: 0.0597  bbox_loss: 0.0080  cls_loss: 0.0087  \n",
      "<<<iteration:[360/657] - total_loss: 0.1531  obj_loss: 0.0777  noobj_loss: 0.0579  bbox_loss: 0.0072  cls_loss: 0.0106  \n",
      "<<<iteration:[380/657] - total_loss: 0.1661  obj_loss: 0.0871  noobj_loss: 0.0610  bbox_loss: 0.0078  cls_loss: 0.0093  \n",
      "<<<iteration:[400/657] - total_loss: 0.1685  obj_loss: 0.0856  noobj_loss: 0.0518  bbox_loss: 0.0092  cls_loss: 0.0109  \n",
      "<<<iteration:[420/657] - total_loss: 0.1627  obj_loss: 0.0853  noobj_loss: 0.0641  bbox_loss: 0.0073  cls_loss: 0.0088  \n",
      "<<<iteration:[440/657] - total_loss: 0.1673  obj_loss: 0.0862  noobj_loss: 0.0592  bbox_loss: 0.0084  cls_loss: 0.0094  \n",
      "<<<iteration:[460/657] - total_loss: 0.1579  obj_loss: 0.0864  noobj_loss: 0.0506  bbox_loss: 0.0075  cls_loss: 0.0090  \n",
      "<<<iteration:[480/657] - total_loss: 0.1641  obj_loss: 0.0927  noobj_loss: 0.0576  bbox_loss: 0.0068  cls_loss: 0.0086  \n",
      "<<<iteration:[500/657] - total_loss: 0.1842  obj_loss: 0.0965  noobj_loss: 0.0600  bbox_loss: 0.0090  cls_loss: 0.0127  \n",
      "<<<iteration:[520/657] - total_loss: 0.1706  obj_loss: 0.0912  noobj_loss: 0.0651  bbox_loss: 0.0077  cls_loss: 0.0085  \n",
      "<<<iteration:[540/657] - total_loss: 0.1624  obj_loss: 0.0866  noobj_loss: 0.0553  bbox_loss: 0.0077  cls_loss: 0.0096  \n",
      "<<<iteration:[560/657] - total_loss: 0.1580  obj_loss: 0.0806  noobj_loss: 0.0623  bbox_loss: 0.0074  cls_loss: 0.0092  \n",
      "<<<iteration:[580/657] - total_loss: 0.1585  obj_loss: 0.0760  noobj_loss: 0.0633  bbox_loss: 0.0084  cls_loss: 0.0087  \n",
      "<<<iteration:[600/657] - total_loss: 0.1609  obj_loss: 0.0818  noobj_loss: 0.0568  bbox_loss: 0.0082  cls_loss: 0.0096  \n",
      "<<<iteration:[620/657] - total_loss: 0.1711  obj_loss: 0.0902  noobj_loss: 0.0502  bbox_loss: 0.0088  cls_loss: 0.0117  \n",
      "<<<iteration:[640/657] - total_loss: 0.1550  obj_loss: 0.0817  noobj_loss: 0.0548  bbox_loss: 0.0073  cls_loss: 0.0093  \n",
      "\n",
      "epoch:78/100 - Train Loss: 0.1652, Val Loss: 0.2211\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1768  obj_loss: 0.0934  noobj_loss: 0.0658  bbox_loss: 0.0078  cls_loss: 0.0114  \n",
      "<<<iteration:[40/657] - total_loss: 0.1847  obj_loss: 0.0912  noobj_loss: 0.0539  bbox_loss: 0.0104  cls_loss: 0.0149  \n",
      "<<<iteration:[60/657] - total_loss: 0.1766  obj_loss: 0.0941  noobj_loss: 0.0561  bbox_loss: 0.0087  cls_loss: 0.0109  \n",
      "<<<iteration:[80/657] - total_loss: 0.1604  obj_loss: 0.0845  noobj_loss: 0.0548  bbox_loss: 0.0081  cls_loss: 0.0080  \n",
      "<<<iteration:[100/657] - total_loss: 0.1780  obj_loss: 0.0963  noobj_loss: 0.0562  bbox_loss: 0.0080  cls_loss: 0.0133  \n",
      "<<<iteration:[120/657] - total_loss: 0.1529  obj_loss: 0.0806  noobj_loss: 0.0568  bbox_loss: 0.0070  cls_loss: 0.0087  \n",
      "<<<iteration:[140/657] - total_loss: 0.1645  obj_loss: 0.0883  noobj_loss: 0.0627  bbox_loss: 0.0071  cls_loss: 0.0093  \n",
      "<<<iteration:[160/657] - total_loss: 0.1654  obj_loss: 0.0887  noobj_loss: 0.0575  bbox_loss: 0.0078  cls_loss: 0.0087  \n",
      "<<<iteration:[180/657] - total_loss: 0.1686  obj_loss: 0.0888  noobj_loss: 0.0547  bbox_loss: 0.0084  cls_loss: 0.0104  \n",
      "<<<iteration:[200/657] - total_loss: 0.1776  obj_loss: 0.0962  noobj_loss: 0.0517  bbox_loss: 0.0096  cls_loss: 0.0077  \n",
      "<<<iteration:[220/657] - total_loss: 0.1503  obj_loss: 0.0760  noobj_loss: 0.0619  bbox_loss: 0.0068  cls_loss: 0.0095  \n",
      "<<<iteration:[240/657] - total_loss: 0.1560  obj_loss: 0.0830  noobj_loss: 0.0542  bbox_loss: 0.0075  cls_loss: 0.0084  \n",
      "<<<iteration:[260/657] - total_loss: 0.1443  obj_loss: 0.0731  noobj_loss: 0.0516  bbox_loss: 0.0071  cls_loss: 0.0097  \n",
      "<<<iteration:[280/657] - total_loss: 0.1583  obj_loss: 0.0836  noobj_loss: 0.0566  bbox_loss: 0.0076  cls_loss: 0.0086  \n",
      "<<<iteration:[300/657] - total_loss: 0.1628  obj_loss: 0.0788  noobj_loss: 0.0563  bbox_loss: 0.0092  cls_loss: 0.0098  \n",
      "<<<iteration:[320/657] - total_loss: 0.1644  obj_loss: 0.0826  noobj_loss: 0.0596  bbox_loss: 0.0082  cls_loss: 0.0110  \n",
      "<<<iteration:[340/657] - total_loss: 0.1594  obj_loss: 0.0781  noobj_loss: 0.0530  bbox_loss: 0.0088  cls_loss: 0.0108  \n",
      "<<<iteration:[360/657] - total_loss: 0.1594  obj_loss: 0.0842  noobj_loss: 0.0515  bbox_loss: 0.0080  cls_loss: 0.0095  \n",
      "<<<iteration:[380/657] - total_loss: 0.1797  obj_loss: 0.0973  noobj_loss: 0.0685  bbox_loss: 0.0073  cls_loss: 0.0119  \n",
      "<<<iteration:[400/657] - total_loss: 0.1524  obj_loss: 0.0781  noobj_loss: 0.0568  bbox_loss: 0.0078  cls_loss: 0.0067  \n",
      "<<<iteration:[420/657] - total_loss: 0.1577  obj_loss: 0.0802  noobj_loss: 0.0609  bbox_loss: 0.0079  cls_loss: 0.0075  \n",
      "<<<iteration:[440/657] - total_loss: 0.1574  obj_loss: 0.0785  noobj_loss: 0.0537  bbox_loss: 0.0083  cls_loss: 0.0103  \n",
      "<<<iteration:[460/657] - total_loss: 0.1587  obj_loss: 0.0869  noobj_loss: 0.0485  bbox_loss: 0.0075  cls_loss: 0.0103  \n",
      "<<<iteration:[480/657] - total_loss: 0.1538  obj_loss: 0.0767  noobj_loss: 0.0563  bbox_loss: 0.0077  cls_loss: 0.0104  \n",
      "<<<iteration:[500/657] - total_loss: 0.1576  obj_loss: 0.0826  noobj_loss: 0.0530  bbox_loss: 0.0078  cls_loss: 0.0095  \n",
      "<<<iteration:[520/657] - total_loss: 0.1655  obj_loss: 0.0900  noobj_loss: 0.0612  bbox_loss: 0.0072  cls_loss: 0.0089  \n",
      "<<<iteration:[540/657] - total_loss: 0.1710  obj_loss: 0.0932  noobj_loss: 0.0595  bbox_loss: 0.0080  cls_loss: 0.0082  \n",
      "<<<iteration:[560/657] - total_loss: 0.1738  obj_loss: 0.0931  noobj_loss: 0.0577  bbox_loss: 0.0083  cls_loss: 0.0102  \n",
      "<<<iteration:[580/657] - total_loss: 0.1634  obj_loss: 0.0905  noobj_loss: 0.0599  bbox_loss: 0.0067  cls_loss: 0.0097  \n",
      "<<<iteration:[600/657] - total_loss: 0.1800  obj_loss: 0.0994  noobj_loss: 0.0586  bbox_loss: 0.0083  cls_loss: 0.0097  \n",
      "<<<iteration:[620/657] - total_loss: 0.1569  obj_loss: 0.0801  noobj_loss: 0.0557  bbox_loss: 0.0079  cls_loss: 0.0095  \n",
      "<<<iteration:[640/657] - total_loss: 0.1663  obj_loss: 0.0868  noobj_loss: 0.0584  bbox_loss: 0.0083  cls_loss: 0.0087  \n",
      "\n",
      "epoch:79/100 - Train Loss: 0.1637, Val Loss: 0.2219\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1644  obj_loss: 0.0891  noobj_loss: 0.0602  bbox_loss: 0.0072  cls_loss: 0.0094  \n",
      "<<<iteration:[40/657] - total_loss: 0.1549  obj_loss: 0.0784  noobj_loss: 0.0540  bbox_loss: 0.0080  cls_loss: 0.0097  \n",
      "<<<iteration:[60/657] - total_loss: 0.1575  obj_loss: 0.0857  noobj_loss: 0.0541  bbox_loss: 0.0072  cls_loss: 0.0089  \n",
      "<<<iteration:[80/657] - total_loss: 0.1818  obj_loss: 0.0948  noobj_loss: 0.0594  bbox_loss: 0.0091  cls_loss: 0.0118  \n",
      "<<<iteration:[100/657] - total_loss: 0.1562  obj_loss: 0.0838  noobj_loss: 0.0513  bbox_loss: 0.0080  cls_loss: 0.0069  \n",
      "<<<iteration:[120/657] - total_loss: 0.1798  obj_loss: 0.0949  noobj_loss: 0.0554  bbox_loss: 0.0087  cls_loss: 0.0138  \n",
      "<<<iteration:[140/657] - total_loss: 0.1452  obj_loss: 0.0752  noobj_loss: 0.0494  bbox_loss: 0.0076  cls_loss: 0.0072  \n",
      "<<<iteration:[160/657] - total_loss: 0.1678  obj_loss: 0.0887  noobj_loss: 0.0563  bbox_loss: 0.0084  cls_loss: 0.0091  \n",
      "<<<iteration:[180/657] - total_loss: 0.1678  obj_loss: 0.0913  noobj_loss: 0.0589  bbox_loss: 0.0078  cls_loss: 0.0082  \n",
      "<<<iteration:[200/657] - total_loss: 0.1529  obj_loss: 0.0801  noobj_loss: 0.0517  bbox_loss: 0.0074  cls_loss: 0.0099  \n",
      "<<<iteration:[220/657] - total_loss: 0.1517  obj_loss: 0.0792  noobj_loss: 0.0522  bbox_loss: 0.0076  cls_loss: 0.0082  \n",
      "<<<iteration:[240/657] - total_loss: 0.1723  obj_loss: 0.0867  noobj_loss: 0.0592  bbox_loss: 0.0094  cls_loss: 0.0090  \n",
      "<<<iteration:[260/657] - total_loss: 0.1631  obj_loss: 0.0882  noobj_loss: 0.0554  bbox_loss: 0.0078  cls_loss: 0.0082  \n",
      "<<<iteration:[280/657] - total_loss: 0.1636  obj_loss: 0.0821  noobj_loss: 0.0544  bbox_loss: 0.0094  cls_loss: 0.0072  \n",
      "<<<iteration:[300/657] - total_loss: 0.1495  obj_loss: 0.0762  noobj_loss: 0.0512  bbox_loss: 0.0079  cls_loss: 0.0084  \n",
      "<<<iteration:[320/657] - total_loss: 0.1640  obj_loss: 0.0853  noobj_loss: 0.0676  bbox_loss: 0.0068  cls_loss: 0.0109  \n",
      "<<<iteration:[340/657] - total_loss: 0.1596  obj_loss: 0.0874  noobj_loss: 0.0560  bbox_loss: 0.0071  cls_loss: 0.0086  \n",
      "<<<iteration:[360/657] - total_loss: 0.1582  obj_loss: 0.0815  noobj_loss: 0.0625  bbox_loss: 0.0073  cls_loss: 0.0090  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[380/657] - total_loss: 0.1546  obj_loss: 0.0831  noobj_loss: 0.0563  bbox_loss: 0.0067  cls_loss: 0.0100  \n",
      "<<<iteration:[400/657] - total_loss: 0.1716  obj_loss: 0.0863  noobj_loss: 0.0633  bbox_loss: 0.0086  cls_loss: 0.0108  \n",
      "<<<iteration:[420/657] - total_loss: 0.1647  obj_loss: 0.0834  noobj_loss: 0.0543  bbox_loss: 0.0088  cls_loss: 0.0102  \n",
      "<<<iteration:[440/657] - total_loss: 0.1708  obj_loss: 0.0939  noobj_loss: 0.0566  bbox_loss: 0.0077  cls_loss: 0.0104  \n",
      "<<<iteration:[460/657] - total_loss: 0.1569  obj_loss: 0.0855  noobj_loss: 0.0511  bbox_loss: 0.0075  cls_loss: 0.0082  \n",
      "<<<iteration:[480/657] - total_loss: 0.1520  obj_loss: 0.0799  noobj_loss: 0.0507  bbox_loss: 0.0076  cls_loss: 0.0088  \n",
      "<<<iteration:[500/657] - total_loss: 0.1637  obj_loss: 0.0906  noobj_loss: 0.0528  bbox_loss: 0.0073  cls_loss: 0.0102  \n",
      "<<<iteration:[520/657] - total_loss: 0.1566  obj_loss: 0.0772  noobj_loss: 0.0569  bbox_loss: 0.0084  cls_loss: 0.0088  \n",
      "<<<iteration:[540/657] - total_loss: 0.1612  obj_loss: 0.0850  noobj_loss: 0.0526  bbox_loss: 0.0084  cls_loss: 0.0081  \n",
      "<<<iteration:[560/657] - total_loss: 0.1548  obj_loss: 0.0822  noobj_loss: 0.0539  bbox_loss: 0.0072  cls_loss: 0.0096  \n",
      "<<<iteration:[580/657] - total_loss: 0.1646  obj_loss: 0.0787  noobj_loss: 0.0649  bbox_loss: 0.0086  cls_loss: 0.0104  \n",
      "<<<iteration:[600/657] - total_loss: 0.1682  obj_loss: 0.0835  noobj_loss: 0.0466  bbox_loss: 0.0102  cls_loss: 0.0103  \n",
      "<<<iteration:[620/657] - total_loss: 0.1686  obj_loss: 0.0866  noobj_loss: 0.0550  bbox_loss: 0.0086  cls_loss: 0.0115  \n",
      "<<<iteration:[640/657] - total_loss: 0.1690  obj_loss: 0.0860  noobj_loss: 0.0692  bbox_loss: 0.0075  cls_loss: 0.0108  \n",
      "\n",
      "epoch:80/100 - Train Loss: 0.1620, Val Loss: 0.2216\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1811  obj_loss: 0.1003  noobj_loss: 0.0693  bbox_loss: 0.0075  cls_loss: 0.0085  \n",
      "<<<iteration:[40/657] - total_loss: 0.1542  obj_loss: 0.0813  noobj_loss: 0.0598  bbox_loss: 0.0069  cls_loss: 0.0086  \n",
      "<<<iteration:[60/657] - total_loss: 0.1571  obj_loss: 0.0832  noobj_loss: 0.0555  bbox_loss: 0.0074  cls_loss: 0.0091  \n",
      "<<<iteration:[80/657] - total_loss: 0.1504  obj_loss: 0.0792  noobj_loss: 0.0558  bbox_loss: 0.0068  cls_loss: 0.0095  \n",
      "<<<iteration:[100/657] - total_loss: 0.1727  obj_loss: 0.0924  noobj_loss: 0.0630  bbox_loss: 0.0079  cls_loss: 0.0091  \n",
      "<<<iteration:[120/657] - total_loss: 0.1547  obj_loss: 0.0827  noobj_loss: 0.0519  bbox_loss: 0.0071  cls_loss: 0.0105  \n",
      "<<<iteration:[140/657] - total_loss: 0.1591  obj_loss: 0.0818  noobj_loss: 0.0586  bbox_loss: 0.0076  cls_loss: 0.0101  \n",
      "<<<iteration:[160/657] - total_loss: 0.1673  obj_loss: 0.0906  noobj_loss: 0.0577  bbox_loss: 0.0077  cls_loss: 0.0095  \n",
      "<<<iteration:[180/657] - total_loss: 0.1500  obj_loss: 0.0741  noobj_loss: 0.0572  bbox_loss: 0.0078  cls_loss: 0.0085  \n",
      "<<<iteration:[200/657] - total_loss: 0.1632  obj_loss: 0.0843  noobj_loss: 0.0575  bbox_loss: 0.0080  cls_loss: 0.0100  \n",
      "<<<iteration:[220/657] - total_loss: 0.1592  obj_loss: 0.0842  noobj_loss: 0.0527  bbox_loss: 0.0077  cls_loss: 0.0101  \n",
      "<<<iteration:[240/657] - total_loss: 0.1649  obj_loss: 0.0847  noobj_loss: 0.0603  bbox_loss: 0.0083  cls_loss: 0.0085  \n",
      "<<<iteration:[260/657] - total_loss: 0.1761  obj_loss: 0.0919  noobj_loss: 0.0647  bbox_loss: 0.0083  cls_loss: 0.0102  \n",
      "<<<iteration:[280/657] - total_loss: 0.1560  obj_loss: 0.0813  noobj_loss: 0.0587  bbox_loss: 0.0074  cls_loss: 0.0083  \n",
      "<<<iteration:[300/657] - total_loss: 0.1742  obj_loss: 0.0846  noobj_loss: 0.0585  bbox_loss: 0.0097  cls_loss: 0.0116  \n",
      "<<<iteration:[320/657] - total_loss: 0.1598  obj_loss: 0.0776  noobj_loss: 0.0551  bbox_loss: 0.0083  cls_loss: 0.0132  \n",
      "<<<iteration:[340/657] - total_loss: 0.1832  obj_loss: 0.1042  noobj_loss: 0.0629  bbox_loss: 0.0077  cls_loss: 0.0088  \n",
      "<<<iteration:[360/657] - total_loss: 0.1518  obj_loss: 0.0792  noobj_loss: 0.0552  bbox_loss: 0.0070  cls_loss: 0.0100  \n",
      "<<<iteration:[380/657] - total_loss: 0.1709  obj_loss: 0.0875  noobj_loss: 0.0609  bbox_loss: 0.0084  cls_loss: 0.0109  \n",
      "<<<iteration:[400/657] - total_loss: 0.1659  obj_loss: 0.0930  noobj_loss: 0.0556  bbox_loss: 0.0073  cls_loss: 0.0089  \n",
      "<<<iteration:[420/657] - total_loss: 0.1633  obj_loss: 0.0865  noobj_loss: 0.0565  bbox_loss: 0.0076  cls_loss: 0.0104  \n",
      "<<<iteration:[440/657] - total_loss: 0.1617  obj_loss: 0.0891  noobj_loss: 0.0498  bbox_loss: 0.0077  cls_loss: 0.0093  \n",
      "<<<iteration:[460/657] - total_loss: 0.1563  obj_loss: 0.0832  noobj_loss: 0.0472  bbox_loss: 0.0080  cls_loss: 0.0096  \n",
      "<<<iteration:[480/657] - total_loss: 0.1547  obj_loss: 0.0764  noobj_loss: 0.0446  bbox_loss: 0.0089  cls_loss: 0.0114  \n",
      "<<<iteration:[500/657] - total_loss: 0.1610  obj_loss: 0.0859  noobj_loss: 0.0588  bbox_loss: 0.0072  cls_loss: 0.0094  \n",
      "<<<iteration:[520/657] - total_loss: 0.1511  obj_loss: 0.0833  noobj_loss: 0.0534  bbox_loss: 0.0066  cls_loss: 0.0080  \n",
      "<<<iteration:[540/657] - total_loss: 0.1534  obj_loss: 0.0734  noobj_loss: 0.0600  bbox_loss: 0.0084  cls_loss: 0.0081  \n",
      "<<<iteration:[560/657] - total_loss: 0.1684  obj_loss: 0.0944  noobj_loss: 0.0532  bbox_loss: 0.0076  cls_loss: 0.0094  \n",
      "<<<iteration:[580/657] - total_loss: 0.1575  obj_loss: 0.0802  noobj_loss: 0.0544  bbox_loss: 0.0082  cls_loss: 0.0093  \n",
      "<<<iteration:[600/657] - total_loss: 0.1513  obj_loss: 0.0751  noobj_loss: 0.0505  bbox_loss: 0.0082  cls_loss: 0.0098  \n",
      "<<<iteration:[620/657] - total_loss: 0.1435  obj_loss: 0.0736  noobj_loss: 0.0530  bbox_loss: 0.0072  cls_loss: 0.0074  \n",
      "<<<iteration:[640/657] - total_loss: 0.1702  obj_loss: 0.0842  noobj_loss: 0.0571  bbox_loss: 0.0096  cls_loss: 0.0093  \n",
      "\n",
      "epoch:81/100 - Train Loss: 0.1614, Val Loss: 0.2217\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1756  obj_loss: 0.0917  noobj_loss: 0.0595  bbox_loss: 0.0086  cls_loss: 0.0114  \n",
      "<<<iteration:[40/657] - total_loss: 0.1518  obj_loss: 0.0754  noobj_loss: 0.0536  bbox_loss: 0.0080  cls_loss: 0.0098  \n",
      "<<<iteration:[60/657] - total_loss: 0.1586  obj_loss: 0.0857  noobj_loss: 0.0585  bbox_loss: 0.0070  cls_loss: 0.0085  \n",
      "<<<iteration:[80/657] - total_loss: 0.1630  obj_loss: 0.0909  noobj_loss: 0.0496  bbox_loss: 0.0076  cls_loss: 0.0091  \n",
      "<<<iteration:[100/657] - total_loss: 0.1560  obj_loss: 0.0745  noobj_loss: 0.0539  bbox_loss: 0.0089  cls_loss: 0.0102  \n",
      "<<<iteration:[120/657] - total_loss: 0.1566  obj_loss: 0.0881  noobj_loss: 0.0516  bbox_loss: 0.0071  cls_loss: 0.0075  \n",
      "<<<iteration:[140/657] - total_loss: 0.1570  obj_loss: 0.0809  noobj_loss: 0.0596  bbox_loss: 0.0074  cls_loss: 0.0093  \n",
      "<<<iteration:[160/657] - total_loss: 0.1634  obj_loss: 0.0886  noobj_loss: 0.0478  bbox_loss: 0.0077  cls_loss: 0.0124  \n",
      "<<<iteration:[180/657] - total_loss: 0.1599  obj_loss: 0.0818  noobj_loss: 0.0618  bbox_loss: 0.0078  cls_loss: 0.0082  \n",
      "<<<iteration:[200/657] - total_loss: 0.1665  obj_loss: 0.0913  noobj_loss: 0.0521  bbox_loss: 0.0078  cls_loss: 0.0102  \n",
      "<<<iteration:[220/657] - total_loss: 0.1540  obj_loss: 0.0788  noobj_loss: 0.0485  bbox_loss: 0.0086  cls_loss: 0.0079  \n",
      "<<<iteration:[240/657] - total_loss: 0.1665  obj_loss: 0.0973  noobj_loss: 0.0569  bbox_loss: 0.0067  cls_loss: 0.0073  \n",
      "<<<iteration:[260/657] - total_loss: 0.1653  obj_loss: 0.0859  noobj_loss: 0.0626  bbox_loss: 0.0077  cls_loss: 0.0097  \n",
      "<<<iteration:[280/657] - total_loss: 0.1492  obj_loss: 0.0772  noobj_loss: 0.0541  bbox_loss: 0.0071  cls_loss: 0.0093  \n",
      "<<<iteration:[300/657] - total_loss: 0.1766  obj_loss: 0.0935  noobj_loss: 0.0669  bbox_loss: 0.0079  cls_loss: 0.0104  \n",
      "<<<iteration:[320/657] - total_loss: 0.1575  obj_loss: 0.0788  noobj_loss: 0.0589  bbox_loss: 0.0078  cls_loss: 0.0100  \n",
      "<<<iteration:[340/657] - total_loss: 0.1572  obj_loss: 0.0852  noobj_loss: 0.0493  bbox_loss: 0.0075  cls_loss: 0.0100  \n",
      "<<<iteration:[360/657] - total_loss: 0.1561  obj_loss: 0.0798  noobj_loss: 0.0586  bbox_loss: 0.0075  cls_loss: 0.0098  \n",
      "<<<iteration:[380/657] - total_loss: 0.1507  obj_loss: 0.0811  noobj_loss: 0.0531  bbox_loss: 0.0069  cls_loss: 0.0086  \n",
      "<<<iteration:[400/657] - total_loss: 0.1577  obj_loss: 0.0842  noobj_loss: 0.0552  bbox_loss: 0.0073  cls_loss: 0.0094  \n",
      "<<<iteration:[420/657] - total_loss: 0.1777  obj_loss: 0.0901  noobj_loss: 0.0629  bbox_loss: 0.0089  cls_loss: 0.0115  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[440/657] - total_loss: 0.1612  obj_loss: 0.0905  noobj_loss: 0.0550  bbox_loss: 0.0070  cls_loss: 0.0083  \n",
      "<<<iteration:[460/657] - total_loss: 0.1561  obj_loss: 0.0839  noobj_loss: 0.0543  bbox_loss: 0.0074  cls_loss: 0.0079  \n",
      "<<<iteration:[480/657] - total_loss: 0.1583  obj_loss: 0.0833  noobj_loss: 0.0523  bbox_loss: 0.0081  cls_loss: 0.0083  \n",
      "<<<iteration:[500/657] - total_loss: 0.1737  obj_loss: 0.0886  noobj_loss: 0.0577  bbox_loss: 0.0091  cls_loss: 0.0107  \n",
      "<<<iteration:[520/657] - total_loss: 0.1586  obj_loss: 0.0781  noobj_loss: 0.0563  bbox_loss: 0.0083  cls_loss: 0.0108  \n",
      "<<<iteration:[540/657] - total_loss: 0.1671  obj_loss: 0.0836  noobj_loss: 0.0544  bbox_loss: 0.0090  cls_loss: 0.0113  \n",
      "<<<iteration:[560/657] - total_loss: 0.1667  obj_loss: 0.0825  noobj_loss: 0.0467  bbox_loss: 0.0102  cls_loss: 0.0097  \n",
      "<<<iteration:[580/657] - total_loss: 0.1428  obj_loss: 0.0693  noobj_loss: 0.0485  bbox_loss: 0.0082  cls_loss: 0.0082  \n",
      "<<<iteration:[600/657] - total_loss: 0.1660  obj_loss: 0.0881  noobj_loss: 0.0602  bbox_loss: 0.0077  cls_loss: 0.0091  \n",
      "<<<iteration:[620/657] - total_loss: 0.1523  obj_loss: 0.0765  noobj_loss: 0.0519  bbox_loss: 0.0080  cls_loss: 0.0100  \n",
      "<<<iteration:[640/657] - total_loss: 0.1631  obj_loss: 0.0819  noobj_loss: 0.0575  bbox_loss: 0.0086  cls_loss: 0.0094  \n",
      "\n",
      "epoch:82/100 - Train Loss: 0.1608, Val Loss: 0.2270\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1847  obj_loss: 0.0981  noobj_loss: 0.0610  bbox_loss: 0.0092  cls_loss: 0.0103  \n",
      "<<<iteration:[40/657] - total_loss: 0.1486  obj_loss: 0.0873  noobj_loss: 0.0441  bbox_loss: 0.0064  cls_loss: 0.0073  \n",
      "<<<iteration:[60/657] - total_loss: 0.1461  obj_loss: 0.0736  noobj_loss: 0.0569  bbox_loss: 0.0069  cls_loss: 0.0095  \n",
      "<<<iteration:[80/657] - total_loss: 0.1576  obj_loss: 0.0831  noobj_loss: 0.0554  bbox_loss: 0.0076  cls_loss: 0.0087  \n",
      "<<<iteration:[100/657] - total_loss: 0.1654  obj_loss: 0.0846  noobj_loss: 0.0530  bbox_loss: 0.0089  cls_loss: 0.0096  \n",
      "<<<iteration:[120/657] - total_loss: 0.1560  obj_loss: 0.0854  noobj_loss: 0.0539  bbox_loss: 0.0069  cls_loss: 0.0092  \n",
      "<<<iteration:[140/657] - total_loss: 0.1604  obj_loss: 0.0845  noobj_loss: 0.0582  bbox_loss: 0.0078  cls_loss: 0.0075  \n",
      "<<<iteration:[160/657] - total_loss: 0.1558  obj_loss: 0.0806  noobj_loss: 0.0566  bbox_loss: 0.0074  cls_loss: 0.0097  \n",
      "<<<iteration:[180/657] - total_loss: 0.1597  obj_loss: 0.0857  noobj_loss: 0.0569  bbox_loss: 0.0073  cls_loss: 0.0090  \n",
      "<<<iteration:[200/657] - total_loss: 0.1555  obj_loss: 0.0810  noobj_loss: 0.0546  bbox_loss: 0.0077  cls_loss: 0.0087  \n",
      "<<<iteration:[220/657] - total_loss: 0.1623  obj_loss: 0.0853  noobj_loss: 0.0650  bbox_loss: 0.0071  cls_loss: 0.0089  \n",
      "<<<iteration:[240/657] - total_loss: 0.1603  obj_loss: 0.0898  noobj_loss: 0.0574  bbox_loss: 0.0067  cls_loss: 0.0084  \n",
      "<<<iteration:[260/657] - total_loss: 0.1600  obj_loss: 0.0829  noobj_loss: 0.0526  bbox_loss: 0.0083  cls_loss: 0.0094  \n",
      "<<<iteration:[280/657] - total_loss: 0.1635  obj_loss: 0.0886  noobj_loss: 0.0543  bbox_loss: 0.0078  cls_loss: 0.0089  \n",
      "<<<iteration:[300/657] - total_loss: 0.1699  obj_loss: 0.0840  noobj_loss: 0.0579  bbox_loss: 0.0095  cls_loss: 0.0094  \n",
      "<<<iteration:[320/657] - total_loss: 0.1705  obj_loss: 0.0918  noobj_loss: 0.0538  bbox_loss: 0.0087  cls_loss: 0.0082  \n",
      "<<<iteration:[340/657] - total_loss: 0.1440  obj_loss: 0.0732  noobj_loss: 0.0556  bbox_loss: 0.0071  cls_loss: 0.0074  \n",
      "<<<iteration:[360/657] - total_loss: 0.1463  obj_loss: 0.0748  noobj_loss: 0.0499  bbox_loss: 0.0077  cls_loss: 0.0079  \n",
      "<<<iteration:[380/657] - total_loss: 0.1767  obj_loss: 0.0975  noobj_loss: 0.0574  bbox_loss: 0.0079  cls_loss: 0.0110  \n",
      "<<<iteration:[400/657] - total_loss: 0.1596  obj_loss: 0.0845  noobj_loss: 0.0562  bbox_loss: 0.0077  cls_loss: 0.0083  \n",
      "<<<iteration:[420/657] - total_loss: 0.1534  obj_loss: 0.0775  noobj_loss: 0.0585  bbox_loss: 0.0073  cls_loss: 0.0100  \n",
      "<<<iteration:[440/657] - total_loss: 0.1612  obj_loss: 0.0909  noobj_loss: 0.0505  bbox_loss: 0.0070  cls_loss: 0.0100  \n",
      "<<<iteration:[460/657] - total_loss: 0.1579  obj_loss: 0.0903  noobj_loss: 0.0538  bbox_loss: 0.0067  cls_loss: 0.0071  \n",
      "<<<iteration:[480/657] - total_loss: 0.1519  obj_loss: 0.0827  noobj_loss: 0.0553  bbox_loss: 0.0067  cls_loss: 0.0081  \n",
      "<<<iteration:[500/657] - total_loss: 0.1718  obj_loss: 0.0885  noobj_loss: 0.0566  bbox_loss: 0.0088  cls_loss: 0.0109  \n",
      "<<<iteration:[520/657] - total_loss: 0.1419  obj_loss: 0.0720  noobj_loss: 0.0475  bbox_loss: 0.0070  cls_loss: 0.0110  \n",
      "<<<iteration:[540/657] - total_loss: 0.1500  obj_loss: 0.0833  noobj_loss: 0.0520  bbox_loss: 0.0060  cls_loss: 0.0107  \n",
      "<<<iteration:[560/657] - total_loss: 0.1717  obj_loss: 0.0824  noobj_loss: 0.0591  bbox_loss: 0.0097  cls_loss: 0.0111  \n",
      "<<<iteration:[580/657] - total_loss: 0.1486  obj_loss: 0.0718  noobj_loss: 0.0489  bbox_loss: 0.0085  cls_loss: 0.0097  \n",
      "<<<iteration:[600/657] - total_loss: 0.1589  obj_loss: 0.0809  noobj_loss: 0.0548  bbox_loss: 0.0078  cls_loss: 0.0115  \n",
      "<<<iteration:[620/657] - total_loss: 0.1596  obj_loss: 0.0877  noobj_loss: 0.0578  bbox_loss: 0.0068  cls_loss: 0.0088  \n",
      "<<<iteration:[640/657] - total_loss: 0.1571  obj_loss: 0.0872  noobj_loss: 0.0517  bbox_loss: 0.0072  cls_loss: 0.0079  \n",
      "\n",
      "epoch:83/100 - Train Loss: 0.1600, Val Loss: 0.2190\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1748  obj_loss: 0.0886  noobj_loss: 0.0662  bbox_loss: 0.0085  cls_loss: 0.0108  \n",
      "<<<iteration:[40/657] - total_loss: 0.1607  obj_loss: 0.0871  noobj_loss: 0.0522  bbox_loss: 0.0080  cls_loss: 0.0077  \n",
      "<<<iteration:[60/657] - total_loss: 0.1581  obj_loss: 0.0779  noobj_loss: 0.0613  bbox_loss: 0.0081  cls_loss: 0.0092  \n",
      "<<<iteration:[80/657] - total_loss: 0.1727  obj_loss: 0.0944  noobj_loss: 0.0581  bbox_loss: 0.0081  cls_loss: 0.0088  \n",
      "<<<iteration:[100/657] - total_loss: 0.1606  obj_loss: 0.0890  noobj_loss: 0.0592  bbox_loss: 0.0066  cls_loss: 0.0088  \n",
      "<<<iteration:[120/657] - total_loss: 0.1689  obj_loss: 0.0895  noobj_loss: 0.0616  bbox_loss: 0.0078  cls_loss: 0.0096  \n",
      "<<<iteration:[140/657] - total_loss: 0.1644  obj_loss: 0.0901  noobj_loss: 0.0567  bbox_loss: 0.0071  cls_loss: 0.0102  \n",
      "<<<iteration:[160/657] - total_loss: 0.1470  obj_loss: 0.0752  noobj_loss: 0.0534  bbox_loss: 0.0073  cls_loss: 0.0085  \n",
      "<<<iteration:[180/657] - total_loss: 0.1472  obj_loss: 0.0759  noobj_loss: 0.0615  bbox_loss: 0.0068  cls_loss: 0.0066  \n",
      "<<<iteration:[200/657] - total_loss: 0.1439  obj_loss: 0.0798  noobj_loss: 0.0408  bbox_loss: 0.0070  cls_loss: 0.0087  \n",
      "<<<iteration:[220/657] - total_loss: 0.1501  obj_loss: 0.0785  noobj_loss: 0.0554  bbox_loss: 0.0071  cls_loss: 0.0083  \n",
      "<<<iteration:[240/657] - total_loss: 0.1723  obj_loss: 0.0890  noobj_loss: 0.0549  bbox_loss: 0.0093  cls_loss: 0.0092  \n",
      "<<<iteration:[260/657] - total_loss: 0.1488  obj_loss: 0.0786  noobj_loss: 0.0512  bbox_loss: 0.0072  cls_loss: 0.0084  \n",
      "<<<iteration:[280/657] - total_loss: 0.1530  obj_loss: 0.0811  noobj_loss: 0.0589  bbox_loss: 0.0070  cls_loss: 0.0076  \n",
      "<<<iteration:[300/657] - total_loss: 0.1599  obj_loss: 0.0838  noobj_loss: 0.0650  bbox_loss: 0.0070  cls_loss: 0.0086  \n",
      "<<<iteration:[320/657] - total_loss: 0.1645  obj_loss: 0.0863  noobj_loss: 0.0568  bbox_loss: 0.0075  cls_loss: 0.0122  \n",
      "<<<iteration:[340/657] - total_loss: 0.1588  obj_loss: 0.0824  noobj_loss: 0.0546  bbox_loss: 0.0077  cls_loss: 0.0104  \n",
      "<<<iteration:[360/657] - total_loss: 0.1572  obj_loss: 0.0816  noobj_loss: 0.0488  bbox_loss: 0.0081  cls_loss: 0.0108  \n",
      "<<<iteration:[380/657] - total_loss: 0.1478  obj_loss: 0.0754  noobj_loss: 0.0494  bbox_loss: 0.0079  cls_loss: 0.0082  \n",
      "<<<iteration:[400/657] - total_loss: 0.1667  obj_loss: 0.0847  noobj_loss: 0.0649  bbox_loss: 0.0077  cls_loss: 0.0108  \n",
      "<<<iteration:[420/657] - total_loss: 0.1485  obj_loss: 0.0752  noobj_loss: 0.0584  bbox_loss: 0.0070  cls_loss: 0.0093  \n",
      "<<<iteration:[440/657] - total_loss: 0.1658  obj_loss: 0.0912  noobj_loss: 0.0553  bbox_loss: 0.0076  cls_loss: 0.0090  \n",
      "<<<iteration:[460/657] - total_loss: 0.1593  obj_loss: 0.0862  noobj_loss: 0.0544  bbox_loss: 0.0071  cls_loss: 0.0103  \n",
      "<<<iteration:[480/657] - total_loss: 0.1573  obj_loss: 0.0856  noobj_loss: 0.0529  bbox_loss: 0.0072  cls_loss: 0.0092  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[500/657] - total_loss: 0.1757  obj_loss: 0.0949  noobj_loss: 0.0589  bbox_loss: 0.0079  cls_loss: 0.0116  \n",
      "<<<iteration:[520/657] - total_loss: 0.1580  obj_loss: 0.0859  noobj_loss: 0.0507  bbox_loss: 0.0075  cls_loss: 0.0091  \n",
      "<<<iteration:[540/657] - total_loss: 0.1732  obj_loss: 0.0949  noobj_loss: 0.0543  bbox_loss: 0.0084  cls_loss: 0.0092  \n",
      "<<<iteration:[560/657] - total_loss: 0.1631  obj_loss: 0.0771  noobj_loss: 0.0582  bbox_loss: 0.0094  cls_loss: 0.0099  \n",
      "<<<iteration:[580/657] - total_loss: 0.1601  obj_loss: 0.0871  noobj_loss: 0.0607  bbox_loss: 0.0068  cls_loss: 0.0088  \n",
      "<<<iteration:[600/657] - total_loss: 0.1621  obj_loss: 0.0849  noobj_loss: 0.0468  bbox_loss: 0.0088  cls_loss: 0.0099  \n",
      "<<<iteration:[620/657] - total_loss: 0.1580  obj_loss: 0.0859  noobj_loss: 0.0507  bbox_loss: 0.0073  cls_loss: 0.0103  \n",
      "<<<iteration:[640/657] - total_loss: 0.1410  obj_loss: 0.0639  noobj_loss: 0.0590  bbox_loss: 0.0077  cls_loss: 0.0092  \n",
      "\n",
      "epoch:84/100 - Train Loss: 0.1589, Val Loss: 0.2205\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1601  obj_loss: 0.0859  noobj_loss: 0.0570  bbox_loss: 0.0075  cls_loss: 0.0080  \n",
      "<<<iteration:[40/657] - total_loss: 0.1573  obj_loss: 0.0825  noobj_loss: 0.0478  bbox_loss: 0.0081  cls_loss: 0.0106  \n",
      "<<<iteration:[60/657] - total_loss: 0.1655  obj_loss: 0.0922  noobj_loss: 0.0581  bbox_loss: 0.0069  cls_loss: 0.0096  \n",
      "<<<iteration:[80/657] - total_loss: 0.1520  obj_loss: 0.0776  noobj_loss: 0.0526  bbox_loss: 0.0079  cls_loss: 0.0085  \n",
      "<<<iteration:[100/657] - total_loss: 0.1579  obj_loss: 0.0799  noobj_loss: 0.0636  bbox_loss: 0.0075  cls_loss: 0.0088  \n",
      "<<<iteration:[120/657] - total_loss: 0.1585  obj_loss: 0.0814  noobj_loss: 0.0618  bbox_loss: 0.0072  cls_loss: 0.0103  \n",
      "<<<iteration:[140/657] - total_loss: 0.1574  obj_loss: 0.0829  noobj_loss: 0.0574  bbox_loss: 0.0074  cls_loss: 0.0089  \n",
      "<<<iteration:[160/657] - total_loss: 0.1487  obj_loss: 0.0807  noobj_loss: 0.0582  bbox_loss: 0.0059  cls_loss: 0.0093  \n",
      "<<<iteration:[180/657] - total_loss: 0.1590  obj_loss: 0.0903  noobj_loss: 0.0588  bbox_loss: 0.0063  cls_loss: 0.0080  \n",
      "<<<iteration:[200/657] - total_loss: 0.1447  obj_loss: 0.0758  noobj_loss: 0.0501  bbox_loss: 0.0071  cls_loss: 0.0084  \n",
      "<<<iteration:[220/657] - total_loss: 0.1765  obj_loss: 0.0945  noobj_loss: 0.0622  bbox_loss: 0.0084  cls_loss: 0.0086  \n",
      "<<<iteration:[240/657] - total_loss: 0.1612  obj_loss: 0.0847  noobj_loss: 0.0572  bbox_loss: 0.0079  cls_loss: 0.0084  \n",
      "<<<iteration:[260/657] - total_loss: 0.1616  obj_loss: 0.0808  noobj_loss: 0.0539  bbox_loss: 0.0088  cls_loss: 0.0097  \n",
      "<<<iteration:[280/657] - total_loss: 0.1637  obj_loss: 0.0887  noobj_loss: 0.0559  bbox_loss: 0.0077  cls_loss: 0.0086  \n",
      "<<<iteration:[300/657] - total_loss: 0.1431  obj_loss: 0.0752  noobj_loss: 0.0536  bbox_loss: 0.0066  cls_loss: 0.0083  \n",
      "<<<iteration:[320/657] - total_loss: 0.1635  obj_loss: 0.0846  noobj_loss: 0.0607  bbox_loss: 0.0077  cls_loss: 0.0103  \n",
      "<<<iteration:[340/657] - total_loss: 0.1503  obj_loss: 0.0809  noobj_loss: 0.0525  bbox_loss: 0.0072  cls_loss: 0.0075  \n",
      "<<<iteration:[360/657] - total_loss: 0.1680  obj_loss: 0.0752  noobj_loss: 0.0593  bbox_loss: 0.0102  cls_loss: 0.0120  \n",
      "<<<iteration:[380/657] - total_loss: 0.1633  obj_loss: 0.0937  noobj_loss: 0.0501  bbox_loss: 0.0069  cls_loss: 0.0098  \n",
      "<<<iteration:[400/657] - total_loss: 0.1581  obj_loss: 0.0850  noobj_loss: 0.0533  bbox_loss: 0.0073  cls_loss: 0.0100  \n",
      "<<<iteration:[420/657] - total_loss: 0.1513  obj_loss: 0.0790  noobj_loss: 0.0513  bbox_loss: 0.0076  cls_loss: 0.0086  \n",
      "<<<iteration:[440/657] - total_loss: 0.1510  obj_loss: 0.0731  noobj_loss: 0.0598  bbox_loss: 0.0077  cls_loss: 0.0097  \n",
      "<<<iteration:[460/657] - total_loss: 0.1608  obj_loss: 0.0842  noobj_loss: 0.0563  bbox_loss: 0.0079  cls_loss: 0.0088  \n",
      "<<<iteration:[480/657] - total_loss: 0.1647  obj_loss: 0.0853  noobj_loss: 0.0539  bbox_loss: 0.0086  cls_loss: 0.0093  \n",
      "<<<iteration:[500/657] - total_loss: 0.1653  obj_loss: 0.0910  noobj_loss: 0.0515  bbox_loss: 0.0079  cls_loss: 0.0090  \n",
      "<<<iteration:[520/657] - total_loss: 0.1631  obj_loss: 0.0865  noobj_loss: 0.0543  bbox_loss: 0.0083  cls_loss: 0.0081  \n",
      "<<<iteration:[540/657] - total_loss: 0.1478  obj_loss: 0.0800  noobj_loss: 0.0485  bbox_loss: 0.0070  cls_loss: 0.0084  \n",
      "<<<iteration:[560/657] - total_loss: 0.1827  obj_loss: 0.0908  noobj_loss: 0.0543  bbox_loss: 0.0108  cls_loss: 0.0106  \n",
      "<<<iteration:[580/657] - total_loss: 0.1589  obj_loss: 0.0865  noobj_loss: 0.0541  bbox_loss: 0.0072  cls_loss: 0.0094  \n",
      "<<<iteration:[600/657] - total_loss: 0.1569  obj_loss: 0.0857  noobj_loss: 0.0568  bbox_loss: 0.0073  cls_loss: 0.0061  \n",
      "<<<iteration:[620/657] - total_loss: 0.1700  obj_loss: 0.0882  noobj_loss: 0.0607  bbox_loss: 0.0081  cls_loss: 0.0108  \n",
      "<<<iteration:[640/657] - total_loss: 0.1598  obj_loss: 0.0833  noobj_loss: 0.0623  bbox_loss: 0.0073  cls_loss: 0.0088  \n",
      "\n",
      "epoch:85/100 - Train Loss: 0.1594, Val Loss: 0.2202\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1750  obj_loss: 0.0976  noobj_loss: 0.0593  bbox_loss: 0.0077  cls_loss: 0.0092  \n",
      "<<<iteration:[40/657] - total_loss: 0.1494  obj_loss: 0.0788  noobj_loss: 0.0515  bbox_loss: 0.0074  cls_loss: 0.0080  \n",
      "<<<iteration:[60/657] - total_loss: 0.1572  obj_loss: 0.0835  noobj_loss: 0.0540  bbox_loss: 0.0071  cls_loss: 0.0109  \n",
      "<<<iteration:[80/657] - total_loss: 0.1408  obj_loss: 0.0717  noobj_loss: 0.0545  bbox_loss: 0.0071  cls_loss: 0.0064  \n",
      "<<<iteration:[100/657] - total_loss: 0.1597  obj_loss: 0.0912  noobj_loss: 0.0547  bbox_loss: 0.0068  cls_loss: 0.0074  \n",
      "<<<iteration:[120/657] - total_loss: 0.1514  obj_loss: 0.0790  noobj_loss: 0.0462  bbox_loss: 0.0080  cls_loss: 0.0092  \n",
      "<<<iteration:[140/657] - total_loss: 0.1687  obj_loss: 0.0913  noobj_loss: 0.0595  bbox_loss: 0.0075  cls_loss: 0.0100  \n",
      "<<<iteration:[160/657] - total_loss: 0.1532  obj_loss: 0.0784  noobj_loss: 0.0578  bbox_loss: 0.0075  cls_loss: 0.0083  \n",
      "<<<iteration:[180/657] - total_loss: 0.1679  obj_loss: 0.0942  noobj_loss: 0.0614  bbox_loss: 0.0064  cls_loss: 0.0110  \n",
      "<<<iteration:[200/657] - total_loss: 0.1580  obj_loss: 0.0816  noobj_loss: 0.0502  bbox_loss: 0.0081  cls_loss: 0.0106  \n",
      "<<<iteration:[220/657] - total_loss: 0.1574  obj_loss: 0.0805  noobj_loss: 0.0500  bbox_loss: 0.0082  cls_loss: 0.0107  \n",
      "<<<iteration:[240/657] - total_loss: 0.1521  obj_loss: 0.0796  noobj_loss: 0.0542  bbox_loss: 0.0073  cls_loss: 0.0089  \n",
      "<<<iteration:[260/657] - total_loss: 0.1600  obj_loss: 0.0855  noobj_loss: 0.0512  bbox_loss: 0.0080  cls_loss: 0.0090  \n",
      "<<<iteration:[280/657] - total_loss: 0.1746  obj_loss: 0.0970  noobj_loss: 0.0638  bbox_loss: 0.0073  cls_loss: 0.0092  \n",
      "<<<iteration:[300/657] - total_loss: 0.1654  obj_loss: 0.0849  noobj_loss: 0.0596  bbox_loss: 0.0084  cls_loss: 0.0089  \n",
      "<<<iteration:[320/657] - total_loss: 0.1417  obj_loss: 0.0757  noobj_loss: 0.0497  bbox_loss: 0.0067  cls_loss: 0.0074  \n",
      "<<<iteration:[340/657] - total_loss: 0.1468  obj_loss: 0.0768  noobj_loss: 0.0507  bbox_loss: 0.0072  cls_loss: 0.0084  \n",
      "<<<iteration:[360/657] - total_loss: 0.1523  obj_loss: 0.0760  noobj_loss: 0.0566  bbox_loss: 0.0080  cls_loss: 0.0080  \n",
      "<<<iteration:[380/657] - total_loss: 0.1524  obj_loss: 0.0799  noobj_loss: 0.0485  bbox_loss: 0.0077  cls_loss: 0.0099  \n",
      "<<<iteration:[400/657] - total_loss: 0.1606  obj_loss: 0.0847  noobj_loss: 0.0549  bbox_loss: 0.0078  cls_loss: 0.0096  \n",
      "<<<iteration:[420/657] - total_loss: 0.1567  obj_loss: 0.0822  noobj_loss: 0.0610  bbox_loss: 0.0072  cls_loss: 0.0081  \n",
      "<<<iteration:[440/657] - total_loss: 0.1673  obj_loss: 0.0920  noobj_loss: 0.0537  bbox_loss: 0.0078  cls_loss: 0.0097  \n",
      "<<<iteration:[460/657] - total_loss: 0.1457  obj_loss: 0.0697  noobj_loss: 0.0518  bbox_loss: 0.0081  cls_loss: 0.0096  \n",
      "<<<iteration:[480/657] - total_loss: 0.1431  obj_loss: 0.0771  noobj_loss: 0.0528  bbox_loss: 0.0065  cls_loss: 0.0071  \n",
      "<<<iteration:[500/657] - total_loss: 0.1730  obj_loss: 0.0955  noobj_loss: 0.0595  bbox_loss: 0.0074  cls_loss: 0.0106  \n",
      "<<<iteration:[520/657] - total_loss: 0.1583  obj_loss: 0.0796  noobj_loss: 0.0664  bbox_loss: 0.0072  cls_loss: 0.0093  \n",
      "<<<iteration:[540/657] - total_loss: 0.1617  obj_loss: 0.0903  noobj_loss: 0.0562  bbox_loss: 0.0069  cls_loss: 0.0089  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[560/657] - total_loss: 0.1522  obj_loss: 0.0806  noobj_loss: 0.0558  bbox_loss: 0.0070  cls_loss: 0.0085  \n",
      "<<<iteration:[580/657] - total_loss: 0.1532  obj_loss: 0.0775  noobj_loss: 0.0520  bbox_loss: 0.0080  cls_loss: 0.0098  \n",
      "<<<iteration:[600/657] - total_loss: 0.1516  obj_loss: 0.0749  noobj_loss: 0.0652  bbox_loss: 0.0069  cls_loss: 0.0095  \n",
      "<<<iteration:[620/657] - total_loss: 0.1585  obj_loss: 0.0901  noobj_loss: 0.0506  bbox_loss: 0.0070  cls_loss: 0.0079  \n",
      "<<<iteration:[640/657] - total_loss: 0.1477  obj_loss: 0.0696  noobj_loss: 0.0560  bbox_loss: 0.0080  cls_loss: 0.0100  \n",
      "\n",
      "epoch:86/100 - Train Loss: 0.1565, Val Loss: 0.2232\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1435  obj_loss: 0.0774  noobj_loss: 0.0512  bbox_loss: 0.0066  cls_loss: 0.0076  \n",
      "<<<iteration:[40/657] - total_loss: 0.1524  obj_loss: 0.0815  noobj_loss: 0.0548  bbox_loss: 0.0068  cls_loss: 0.0098  \n",
      "<<<iteration:[60/657] - total_loss: 0.1510  obj_loss: 0.0795  noobj_loss: 0.0535  bbox_loss: 0.0072  cls_loss: 0.0085  \n",
      "<<<iteration:[80/657] - total_loss: 0.1470  obj_loss: 0.0763  noobj_loss: 0.0497  bbox_loss: 0.0074  cls_loss: 0.0088  \n",
      "<<<iteration:[100/657] - total_loss: 0.1547  obj_loss: 0.0796  noobj_loss: 0.0533  bbox_loss: 0.0081  cls_loss: 0.0080  \n",
      "<<<iteration:[120/657] - total_loss: 0.1623  obj_loss: 0.0838  noobj_loss: 0.0524  bbox_loss: 0.0080  cls_loss: 0.0121  \n",
      "<<<iteration:[140/657] - total_loss: 0.1580  obj_loss: 0.0805  noobj_loss: 0.0509  bbox_loss: 0.0085  cls_loss: 0.0097  \n",
      "<<<iteration:[160/657] - total_loss: 0.1593  obj_loss: 0.0835  noobj_loss: 0.0605  bbox_loss: 0.0072  cls_loss: 0.0095  \n",
      "<<<iteration:[180/657] - total_loss: 0.1479  obj_loss: 0.0746  noobj_loss: 0.0531  bbox_loss: 0.0073  cls_loss: 0.0101  \n",
      "<<<iteration:[200/657] - total_loss: 0.1487  obj_loss: 0.0768  noobj_loss: 0.0552  bbox_loss: 0.0074  cls_loss: 0.0072  \n",
      "<<<iteration:[220/657] - total_loss: 0.1555  obj_loss: 0.0833  noobj_loss: 0.0538  bbox_loss: 0.0069  cls_loss: 0.0107  \n",
      "<<<iteration:[240/657] - total_loss: 0.1605  obj_loss: 0.0874  noobj_loss: 0.0519  bbox_loss: 0.0073  cls_loss: 0.0108  \n",
      "<<<iteration:[260/657] - total_loss: 0.1643  obj_loss: 0.0868  noobj_loss: 0.0573  bbox_loss: 0.0079  cls_loss: 0.0094  \n",
      "<<<iteration:[280/657] - total_loss: 0.1663  obj_loss: 0.0912  noobj_loss: 0.0580  bbox_loss: 0.0075  cls_loss: 0.0086  \n",
      "<<<iteration:[300/657] - total_loss: 0.1630  obj_loss: 0.0888  noobj_loss: 0.0551  bbox_loss: 0.0075  cls_loss: 0.0091  \n",
      "<<<iteration:[320/657] - total_loss: 0.1494  obj_loss: 0.0806  noobj_loss: 0.0513  bbox_loss: 0.0070  cls_loss: 0.0080  \n",
      "<<<iteration:[340/657] - total_loss: 0.1495  obj_loss: 0.0782  noobj_loss: 0.0552  bbox_loss: 0.0071  cls_loss: 0.0084  \n",
      "<<<iteration:[360/657] - total_loss: 0.1535  obj_loss: 0.0814  noobj_loss: 0.0545  bbox_loss: 0.0071  cls_loss: 0.0091  \n",
      "<<<iteration:[380/657] - total_loss: 0.1624  obj_loss: 0.0928  noobj_loss: 0.0522  bbox_loss: 0.0072  cls_loss: 0.0076  \n",
      "<<<iteration:[400/657] - total_loss: 0.1521  obj_loss: 0.0768  noobj_loss: 0.0551  bbox_loss: 0.0082  cls_loss: 0.0067  \n",
      "<<<iteration:[420/657] - total_loss: 0.1522  obj_loss: 0.0791  noobj_loss: 0.0482  bbox_loss: 0.0076  cls_loss: 0.0110  \n",
      "<<<iteration:[440/657] - total_loss: 0.1562  obj_loss: 0.0837  noobj_loss: 0.0527  bbox_loss: 0.0076  cls_loss: 0.0084  \n",
      "<<<iteration:[460/657] - total_loss: 0.1594  obj_loss: 0.0833  noobj_loss: 0.0541  bbox_loss: 0.0080  cls_loss: 0.0092  \n",
      "<<<iteration:[480/657] - total_loss: 0.1707  obj_loss: 0.0938  noobj_loss: 0.0563  bbox_loss: 0.0079  cls_loss: 0.0094  \n",
      "<<<iteration:[500/657] - total_loss: 0.1514  obj_loss: 0.0822  noobj_loss: 0.0491  bbox_loss: 0.0071  cls_loss: 0.0093  \n",
      "<<<iteration:[520/657] - total_loss: 0.1569  obj_loss: 0.0756  noobj_loss: 0.0559  bbox_loss: 0.0089  cls_loss: 0.0088  \n",
      "<<<iteration:[540/657] - total_loss: 0.1609  obj_loss: 0.0891  noobj_loss: 0.0546  bbox_loss: 0.0073  cls_loss: 0.0083  \n",
      "<<<iteration:[560/657] - total_loss: 0.1425  obj_loss: 0.0725  noobj_loss: 0.0508  bbox_loss: 0.0073  cls_loss: 0.0081  \n",
      "<<<iteration:[580/657] - total_loss: 0.1598  obj_loss: 0.0863  noobj_loss: 0.0577  bbox_loss: 0.0071  cls_loss: 0.0091  \n",
      "<<<iteration:[600/657] - total_loss: 0.1599  obj_loss: 0.0850  noobj_loss: 0.0562  bbox_loss: 0.0075  cls_loss: 0.0091  \n",
      "<<<iteration:[620/657] - total_loss: 0.1495  obj_loss: 0.0776  noobj_loss: 0.0534  bbox_loss: 0.0073  cls_loss: 0.0088  \n",
      "<<<iteration:[640/657] - total_loss: 0.1607  obj_loss: 0.0883  noobj_loss: 0.0526  bbox_loss: 0.0072  cls_loss: 0.0102  \n",
      "\n",
      "epoch:87/100 - Train Loss: 0.1555, Val Loss: 0.2202\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1677  obj_loss: 0.0870  noobj_loss: 0.0529  bbox_loss: 0.0086  cls_loss: 0.0114  \n",
      "<<<iteration:[40/657] - total_loss: 0.1709  obj_loss: 0.0944  noobj_loss: 0.0586  bbox_loss: 0.0077  cls_loss: 0.0087  \n",
      "<<<iteration:[60/657] - total_loss: 0.1376  obj_loss: 0.0731  noobj_loss: 0.0468  bbox_loss: 0.0067  cls_loss: 0.0077  \n",
      "<<<iteration:[80/657] - total_loss: 0.1545  obj_loss: 0.0806  noobj_loss: 0.0570  bbox_loss: 0.0072  cls_loss: 0.0092  \n",
      "<<<iteration:[100/657] - total_loss: 0.1481  obj_loss: 0.0775  noobj_loss: 0.0511  bbox_loss: 0.0075  cls_loss: 0.0077  \n",
      "<<<iteration:[120/657] - total_loss: 0.1666  obj_loss: 0.0949  noobj_loss: 0.0573  bbox_loss: 0.0069  cls_loss: 0.0087  \n",
      "<<<iteration:[140/657] - total_loss: 0.1608  obj_loss: 0.0852  noobj_loss: 0.0571  bbox_loss: 0.0077  cls_loss: 0.0086  \n",
      "<<<iteration:[160/657] - total_loss: 0.1612  obj_loss: 0.0841  noobj_loss: 0.0536  bbox_loss: 0.0084  cls_loss: 0.0085  \n",
      "<<<iteration:[180/657] - total_loss: 0.1636  obj_loss: 0.0904  noobj_loss: 0.0558  bbox_loss: 0.0076  cls_loss: 0.0071  \n",
      "<<<iteration:[200/657] - total_loss: 0.1652  obj_loss: 0.0923  noobj_loss: 0.0559  bbox_loss: 0.0071  cls_loss: 0.0094  \n",
      "<<<iteration:[220/657] - total_loss: 0.1565  obj_loss: 0.0809  noobj_loss: 0.0619  bbox_loss: 0.0072  cls_loss: 0.0087  \n",
      "<<<iteration:[240/657] - total_loss: 0.1463  obj_loss: 0.0790  noobj_loss: 0.0573  bbox_loss: 0.0063  cls_loss: 0.0072  \n",
      "<<<iteration:[260/657] - total_loss: 0.1435  obj_loss: 0.0694  noobj_loss: 0.0539  bbox_loss: 0.0077  cls_loss: 0.0085  \n",
      "<<<iteration:[280/657] - total_loss: 0.1398  obj_loss: 0.0738  noobj_loss: 0.0528  bbox_loss: 0.0065  cls_loss: 0.0071  \n",
      "<<<iteration:[300/657] - total_loss: 0.1655  obj_loss: 0.0939  noobj_loss: 0.0592  bbox_loss: 0.0068  cls_loss: 0.0078  \n",
      "<<<iteration:[320/657] - total_loss: 0.1515  obj_loss: 0.0853  noobj_loss: 0.0484  bbox_loss: 0.0069  cls_loss: 0.0076  \n",
      "<<<iteration:[340/657] - total_loss: 0.1599  obj_loss: 0.0890  noobj_loss: 0.0541  bbox_loss: 0.0071  cls_loss: 0.0085  \n",
      "<<<iteration:[360/657] - total_loss: 0.1497  obj_loss: 0.0753  noobj_loss: 0.0557  bbox_loss: 0.0078  cls_loss: 0.0074  \n",
      "<<<iteration:[380/657] - total_loss: 0.1314  obj_loss: 0.0668  noobj_loss: 0.0506  bbox_loss: 0.0064  cls_loss: 0.0072  \n",
      "<<<iteration:[400/657] - total_loss: 0.1505  obj_loss: 0.0846  noobj_loss: 0.0493  bbox_loss: 0.0065  cls_loss: 0.0087  \n",
      "<<<iteration:[420/657] - total_loss: 0.1561  obj_loss: 0.0870  noobj_loss: 0.0532  bbox_loss: 0.0070  cls_loss: 0.0075  \n",
      "<<<iteration:[440/657] - total_loss: 0.1591  obj_loss: 0.0800  noobj_loss: 0.0575  bbox_loss: 0.0080  cls_loss: 0.0103  \n",
      "<<<iteration:[460/657] - total_loss: 0.1478  obj_loss: 0.0796  noobj_loss: 0.0535  bbox_loss: 0.0065  cls_loss: 0.0087  \n",
      "<<<iteration:[480/657] - total_loss: 0.1545  obj_loss: 0.0817  noobj_loss: 0.0506  bbox_loss: 0.0077  cls_loss: 0.0089  \n",
      "<<<iteration:[500/657] - total_loss: 0.1500  obj_loss: 0.0788  noobj_loss: 0.0517  bbox_loss: 0.0067  cls_loss: 0.0116  \n",
      "<<<iteration:[520/657] - total_loss: 0.1427  obj_loss: 0.0731  noobj_loss: 0.0552  bbox_loss: 0.0066  cls_loss: 0.0092  \n",
      "<<<iteration:[540/657] - total_loss: 0.1689  obj_loss: 0.0902  noobj_loss: 0.0596  bbox_loss: 0.0079  cls_loss: 0.0092  \n",
      "<<<iteration:[560/657] - total_loss: 0.1565  obj_loss: 0.0835  noobj_loss: 0.0521  bbox_loss: 0.0074  cls_loss: 0.0102  \n",
      "<<<iteration:[580/657] - total_loss: 0.1556  obj_loss: 0.0891  noobj_loss: 0.0503  bbox_loss: 0.0066  cls_loss: 0.0084  \n",
      "<<<iteration:[600/657] - total_loss: 0.1661  obj_loss: 0.0886  noobj_loss: 0.0616  bbox_loss: 0.0077  cls_loss: 0.0084  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[620/657] - total_loss: 0.1510  obj_loss: 0.0801  noobj_loss: 0.0569  bbox_loss: 0.0067  cls_loss: 0.0088  \n",
      "<<<iteration:[640/657] - total_loss: 0.1485  obj_loss: 0.0726  noobj_loss: 0.0482  bbox_loss: 0.0087  cls_loss: 0.0085  \n",
      "\n",
      "epoch:88/100 - Train Loss: 0.1543, Val Loss: 0.2181\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1506  obj_loss: 0.0750  noobj_loss: 0.0600  bbox_loss: 0.0075  cls_loss: 0.0079  \n",
      "<<<iteration:[40/657] - total_loss: 0.1645  obj_loss: 0.0936  noobj_loss: 0.0463  bbox_loss: 0.0077  cls_loss: 0.0091  \n",
      "<<<iteration:[60/657] - total_loss: 0.1643  obj_loss: 0.0897  noobj_loss: 0.0575  bbox_loss: 0.0073  cls_loss: 0.0096  \n",
      "<<<iteration:[80/657] - total_loss: 0.1576  obj_loss: 0.0844  noobj_loss: 0.0547  bbox_loss: 0.0073  cls_loss: 0.0093  \n",
      "<<<iteration:[100/657] - total_loss: 0.1576  obj_loss: 0.0856  noobj_loss: 0.0526  bbox_loss: 0.0073  cls_loss: 0.0091  \n",
      "<<<iteration:[120/657] - total_loss: 0.1472  obj_loss: 0.0801  noobj_loss: 0.0516  bbox_loss: 0.0068  cls_loss: 0.0074  \n",
      "<<<iteration:[140/657] - total_loss: 0.1419  obj_loss: 0.0725  noobj_loss: 0.0564  bbox_loss: 0.0065  cls_loss: 0.0089  \n",
      "<<<iteration:[160/657] - total_loss: 0.1461  obj_loss: 0.0795  noobj_loss: 0.0557  bbox_loss: 0.0064  cls_loss: 0.0070  \n",
      "<<<iteration:[180/657] - total_loss: 0.1508  obj_loss: 0.0787  noobj_loss: 0.0504  bbox_loss: 0.0079  cls_loss: 0.0074  \n",
      "<<<iteration:[200/657] - total_loss: 0.1503  obj_loss: 0.0792  noobj_loss: 0.0529  bbox_loss: 0.0072  cls_loss: 0.0086  \n",
      "<<<iteration:[220/657] - total_loss: 0.1483  obj_loss: 0.0720  noobj_loss: 0.0576  bbox_loss: 0.0076  cls_loss: 0.0094  \n",
      "<<<iteration:[240/657] - total_loss: 0.1465  obj_loss: 0.0800  noobj_loss: 0.0442  bbox_loss: 0.0073  cls_loss: 0.0077  \n",
      "<<<iteration:[260/657] - total_loss: 0.1498  obj_loss: 0.0784  noobj_loss: 0.0477  bbox_loss: 0.0077  cls_loss: 0.0089  \n",
      "<<<iteration:[280/657] - total_loss: 0.1592  obj_loss: 0.0840  noobj_loss: 0.0578  bbox_loss: 0.0077  cls_loss: 0.0077  \n",
      "<<<iteration:[300/657] - total_loss: 0.1468  obj_loss: 0.0796  noobj_loss: 0.0524  bbox_loss: 0.0066  cls_loss: 0.0079  \n",
      "<<<iteration:[320/657] - total_loss: 0.1705  obj_loss: 0.0869  noobj_loss: 0.0537  bbox_loss: 0.0094  cls_loss: 0.0099  \n",
      "<<<iteration:[340/657] - total_loss: 0.1604  obj_loss: 0.0834  noobj_loss: 0.0498  bbox_loss: 0.0087  cls_loss: 0.0087  \n",
      "<<<iteration:[360/657] - total_loss: 0.1493  obj_loss: 0.0763  noobj_loss: 0.0518  bbox_loss: 0.0079  cls_loss: 0.0078  \n",
      "<<<iteration:[380/657] - total_loss: 0.1524  obj_loss: 0.0796  noobj_loss: 0.0478  bbox_loss: 0.0077  cls_loss: 0.0105  \n",
      "<<<iteration:[400/657] - total_loss: 0.1501  obj_loss: 0.0787  noobj_loss: 0.0574  bbox_loss: 0.0071  cls_loss: 0.0071  \n",
      "<<<iteration:[420/657] - total_loss: 0.1816  obj_loss: 0.1060  noobj_loss: 0.0545  bbox_loss: 0.0079  cls_loss: 0.0089  \n",
      "<<<iteration:[440/657] - total_loss: 0.1539  obj_loss: 0.0805  noobj_loss: 0.0518  bbox_loss: 0.0077  cls_loss: 0.0090  \n",
      "<<<iteration:[460/657] - total_loss: 0.1413  obj_loss: 0.0724  noobj_loss: 0.0529  bbox_loss: 0.0069  cls_loss: 0.0080  \n",
      "<<<iteration:[480/657] - total_loss: 0.1616  obj_loss: 0.0899  noobj_loss: 0.0606  bbox_loss: 0.0065  cls_loss: 0.0088  \n",
      "<<<iteration:[500/657] - total_loss: 0.1568  obj_loss: 0.0812  noobj_loss: 0.0560  bbox_loss: 0.0074  cls_loss: 0.0104  \n",
      "<<<iteration:[520/657] - total_loss: 0.1518  obj_loss: 0.0818  noobj_loss: 0.0552  bbox_loss: 0.0068  cls_loss: 0.0085  \n",
      "<<<iteration:[540/657] - total_loss: 0.1656  obj_loss: 0.0841  noobj_loss: 0.0577  bbox_loss: 0.0082  cls_loss: 0.0118  \n",
      "<<<iteration:[560/657] - total_loss: 0.1616  obj_loss: 0.0865  noobj_loss: 0.0517  bbox_loss: 0.0077  cls_loss: 0.0108  \n",
      "<<<iteration:[580/657] - total_loss: 0.1542  obj_loss: 0.0815  noobj_loss: 0.0529  bbox_loss: 0.0076  cls_loss: 0.0080  \n",
      "<<<iteration:[600/657] - total_loss: 0.1537  obj_loss: 0.0859  noobj_loss: 0.0543  bbox_loss: 0.0067  cls_loss: 0.0073  \n",
      "<<<iteration:[620/657] - total_loss: 0.1472  obj_loss: 0.0809  noobj_loss: 0.0507  bbox_loss: 0.0064  cls_loss: 0.0087  \n",
      "<<<iteration:[640/657] - total_loss: 0.1440  obj_loss: 0.0755  noobj_loss: 0.0535  bbox_loss: 0.0068  cls_loss: 0.0076  \n",
      "\n",
      "epoch:89/100 - Train Loss: 0.1540, Val Loss: 0.2241\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1761  obj_loss: 0.0931  noobj_loss: 0.0549  bbox_loss: 0.0092  cls_loss: 0.0093  \n",
      "<<<iteration:[40/657] - total_loss: 0.1416  obj_loss: 0.0761  noobj_loss: 0.0524  bbox_loss: 0.0066  cls_loss: 0.0063  \n",
      "<<<iteration:[60/657] - total_loss: 0.1687  obj_loss: 0.0898  noobj_loss: 0.0575  bbox_loss: 0.0082  cls_loss: 0.0090  \n",
      "<<<iteration:[80/657] - total_loss: 0.1392  obj_loss: 0.0729  noobj_loss: 0.0501  bbox_loss: 0.0068  cls_loss: 0.0074  \n",
      "<<<iteration:[100/657] - total_loss: 0.1619  obj_loss: 0.0876  noobj_loss: 0.0639  bbox_loss: 0.0068  cls_loss: 0.0082  \n",
      "<<<iteration:[120/657] - total_loss: 0.1610  obj_loss: 0.0847  noobj_loss: 0.0476  bbox_loss: 0.0084  cls_loss: 0.0105  \n",
      "<<<iteration:[140/657] - total_loss: 0.1531  obj_loss: 0.0818  noobj_loss: 0.0534  bbox_loss: 0.0070  cls_loss: 0.0095  \n",
      "<<<iteration:[160/657] - total_loss: 0.1516  obj_loss: 0.0802  noobj_loss: 0.0535  bbox_loss: 0.0072  cls_loss: 0.0088  \n",
      "<<<iteration:[180/657] - total_loss: 0.1662  obj_loss: 0.0873  noobj_loss: 0.0533  bbox_loss: 0.0085  cls_loss: 0.0096  \n",
      "<<<iteration:[200/657] - total_loss: 0.1584  obj_loss: 0.0838  noobj_loss: 0.0577  bbox_loss: 0.0073  cls_loss: 0.0094  \n",
      "<<<iteration:[220/657] - total_loss: 0.1378  obj_loss: 0.0674  noobj_loss: 0.0546  bbox_loss: 0.0067  cls_loss: 0.0098  \n",
      "<<<iteration:[240/657] - total_loss: 0.1400  obj_loss: 0.0696  noobj_loss: 0.0524  bbox_loss: 0.0068  cls_loss: 0.0102  \n",
      "<<<iteration:[260/657] - total_loss: 0.1517  obj_loss: 0.0803  noobj_loss: 0.0577  bbox_loss: 0.0070  cls_loss: 0.0076  \n",
      "<<<iteration:[280/657] - total_loss: 0.1586  obj_loss: 0.0901  noobj_loss: 0.0511  bbox_loss: 0.0066  cls_loss: 0.0100  \n",
      "<<<iteration:[300/657] - total_loss: 0.1542  obj_loss: 0.0837  noobj_loss: 0.0557  bbox_loss: 0.0070  cls_loss: 0.0076  \n",
      "<<<iteration:[320/657] - total_loss: 0.1640  obj_loss: 0.0942  noobj_loss: 0.0482  bbox_loss: 0.0075  cls_loss: 0.0080  \n",
      "<<<iteration:[340/657] - total_loss: 0.1440  obj_loss: 0.0762  noobj_loss: 0.0529  bbox_loss: 0.0067  cls_loss: 0.0077  \n",
      "<<<iteration:[360/657] - total_loss: 0.1650  obj_loss: 0.0923  noobj_loss: 0.0637  bbox_loss: 0.0064  cls_loss: 0.0089  \n",
      "<<<iteration:[380/657] - total_loss: 0.1455  obj_loss: 0.0763  noobj_loss: 0.0535  bbox_loss: 0.0070  cls_loss: 0.0076  \n",
      "<<<iteration:[400/657] - total_loss: 0.1650  obj_loss: 0.0941  noobj_loss: 0.0561  bbox_loss: 0.0070  cls_loss: 0.0077  \n",
      "<<<iteration:[420/657] - total_loss: 0.1592  obj_loss: 0.0823  noobj_loss: 0.0571  bbox_loss: 0.0079  cls_loss: 0.0086  \n",
      "<<<iteration:[440/657] - total_loss: 0.1550  obj_loss: 0.0831  noobj_loss: 0.0557  bbox_loss: 0.0072  cls_loss: 0.0080  \n",
      "<<<iteration:[460/657] - total_loss: 0.1431  obj_loss: 0.0738  noobj_loss: 0.0476  bbox_loss: 0.0071  cls_loss: 0.0101  \n",
      "<<<iteration:[480/657] - total_loss: 0.1316  obj_loss: 0.0658  noobj_loss: 0.0475  bbox_loss: 0.0068  cls_loss: 0.0081  \n",
      "<<<iteration:[500/657] - total_loss: 0.1453  obj_loss: 0.0719  noobj_loss: 0.0506  bbox_loss: 0.0079  cls_loss: 0.0084  \n",
      "<<<iteration:[520/657] - total_loss: 0.1517  obj_loss: 0.0836  noobj_loss: 0.0581  bbox_loss: 0.0064  cls_loss: 0.0073  \n",
      "<<<iteration:[540/657] - total_loss: 0.1496  obj_loss: 0.0788  noobj_loss: 0.0497  bbox_loss: 0.0077  cls_loss: 0.0076  \n",
      "<<<iteration:[560/657] - total_loss: 0.1541  obj_loss: 0.0795  noobj_loss: 0.0525  bbox_loss: 0.0078  cls_loss: 0.0092  \n",
      "<<<iteration:[580/657] - total_loss: 0.1471  obj_loss: 0.0818  noobj_loss: 0.0556  bbox_loss: 0.0060  cls_loss: 0.0072  \n",
      "<<<iteration:[600/657] - total_loss: 0.1417  obj_loss: 0.0804  noobj_loss: 0.0445  bbox_loss: 0.0062  cls_loss: 0.0081  \n",
      "<<<iteration:[620/657] - total_loss: 0.1575  obj_loss: 0.0820  noobj_loss: 0.0581  bbox_loss: 0.0078  cls_loss: 0.0075  \n",
      "<<<iteration:[640/657] - total_loss: 0.1715  obj_loss: 0.0938  noobj_loss: 0.0629  bbox_loss: 0.0073  cls_loss: 0.0097  \n",
      "\n",
      "epoch:90/100 - Train Loss: 0.1535, Val Loss: 0.2200\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1708  obj_loss: 0.0903  noobj_loss: 0.0645  bbox_loss: 0.0077  cls_loss: 0.0095  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[40/657] - total_loss: 0.1522  obj_loss: 0.0846  noobj_loss: 0.0498  bbox_loss: 0.0068  cls_loss: 0.0086  \n",
      "<<<iteration:[60/657] - total_loss: 0.1649  obj_loss: 0.0899  noobj_loss: 0.0626  bbox_loss: 0.0067  cls_loss: 0.0103  \n",
      "<<<iteration:[80/657] - total_loss: 0.1489  obj_loss: 0.0810  noobj_loss: 0.0577  bbox_loss: 0.0064  cls_loss: 0.0071  \n",
      "<<<iteration:[100/657] - total_loss: 0.1516  obj_loss: 0.0819  noobj_loss: 0.0573  bbox_loss: 0.0067  cls_loss: 0.0076  \n",
      "<<<iteration:[120/657] - total_loss: 0.1484  obj_loss: 0.0823  noobj_loss: 0.0510  bbox_loss: 0.0065  cls_loss: 0.0082  \n",
      "<<<iteration:[140/657] - total_loss: 0.1454  obj_loss: 0.0816  noobj_loss: 0.0445  bbox_loss: 0.0065  cls_loss: 0.0090  \n",
      "<<<iteration:[160/657] - total_loss: 0.1370  obj_loss: 0.0652  noobj_loss: 0.0463  bbox_loss: 0.0082  cls_loss: 0.0076  \n",
      "<<<iteration:[180/657] - total_loss: 0.1588  obj_loss: 0.0897  noobj_loss: 0.0487  bbox_loss: 0.0072  cls_loss: 0.0086  \n",
      "<<<iteration:[200/657] - total_loss: 0.1446  obj_loss: 0.0804  noobj_loss: 0.0455  bbox_loss: 0.0066  cls_loss: 0.0082  \n",
      "<<<iteration:[220/657] - total_loss: 0.1569  obj_loss: 0.0743  noobj_loss: 0.0540  bbox_loss: 0.0086  cls_loss: 0.0126  \n",
      "<<<iteration:[240/657] - total_loss: 0.1691  obj_loss: 0.0847  noobj_loss: 0.0556  bbox_loss: 0.0087  cls_loss: 0.0129  \n",
      "<<<iteration:[260/657] - total_loss: 0.1509  obj_loss: 0.0822  noobj_loss: 0.0507  bbox_loss: 0.0067  cls_loss: 0.0097  \n",
      "<<<iteration:[280/657] - total_loss: 0.1458  obj_loss: 0.0830  noobj_loss: 0.0499  bbox_loss: 0.0059  cls_loss: 0.0082  \n",
      "<<<iteration:[300/657] - total_loss: 0.1413  obj_loss: 0.0734  noobj_loss: 0.0534  bbox_loss: 0.0065  cls_loss: 0.0087  \n",
      "<<<iteration:[320/657] - total_loss: 0.1721  obj_loss: 0.0917  noobj_loss: 0.0571  bbox_loss: 0.0085  cls_loss: 0.0092  \n",
      "<<<iteration:[340/657] - total_loss: 0.1615  obj_loss: 0.0906  noobj_loss: 0.0556  bbox_loss: 0.0068  cls_loss: 0.0090  \n",
      "<<<iteration:[360/657] - total_loss: 0.1622  obj_loss: 0.0851  noobj_loss: 0.0558  bbox_loss: 0.0083  cls_loss: 0.0080  \n",
      "<<<iteration:[380/657] - total_loss: 0.1505  obj_loss: 0.0813  noobj_loss: 0.0489  bbox_loss: 0.0073  cls_loss: 0.0082  \n",
      "<<<iteration:[400/657] - total_loss: 0.1433  obj_loss: 0.0725  noobj_loss: 0.0492  bbox_loss: 0.0076  cls_loss: 0.0081  \n",
      "<<<iteration:[420/657] - total_loss: 0.1581  obj_loss: 0.0867  noobj_loss: 0.0555  bbox_loss: 0.0068  cls_loss: 0.0097  \n",
      "<<<iteration:[440/657] - total_loss: 0.1479  obj_loss: 0.0731  noobj_loss: 0.0588  bbox_loss: 0.0069  cls_loss: 0.0109  \n",
      "<<<iteration:[460/657] - total_loss: 0.1471  obj_loss: 0.0857  noobj_loss: 0.0509  bbox_loss: 0.0058  cls_loss: 0.0071  \n",
      "<<<iteration:[480/657] - total_loss: 0.1520  obj_loss: 0.0762  noobj_loss: 0.0524  bbox_loss: 0.0078  cls_loss: 0.0109  \n",
      "<<<iteration:[500/657] - total_loss: 0.1546  obj_loss: 0.0829  noobj_loss: 0.0589  bbox_loss: 0.0067  cls_loss: 0.0085  \n",
      "<<<iteration:[520/657] - total_loss: 0.1759  obj_loss: 0.0834  noobj_loss: 0.0545  bbox_loss: 0.0109  cls_loss: 0.0110  \n",
      "<<<iteration:[540/657] - total_loss: 0.1429  obj_loss: 0.0744  noobj_loss: 0.0532  bbox_loss: 0.0066  cls_loss: 0.0087  \n",
      "<<<iteration:[560/657] - total_loss: 0.1551  obj_loss: 0.0769  noobj_loss: 0.0556  bbox_loss: 0.0083  cls_loss: 0.0088  \n",
      "<<<iteration:[580/657] - total_loss: 0.1583  obj_loss: 0.0871  noobj_loss: 0.0563  bbox_loss: 0.0070  cls_loss: 0.0080  \n",
      "<<<iteration:[600/657] - total_loss: 0.1448  obj_loss: 0.0728  noobj_loss: 0.0522  bbox_loss: 0.0079  cls_loss: 0.0063  \n",
      "<<<iteration:[620/657] - total_loss: 0.1507  obj_loss: 0.0843  noobj_loss: 0.0513  bbox_loss: 0.0066  cls_loss: 0.0077  \n",
      "<<<iteration:[640/657] - total_loss: 0.1688  obj_loss: 0.0964  noobj_loss: 0.0582  bbox_loss: 0.0071  cls_loss: 0.0079  \n",
      "\n",
      "epoch:91/100 - Train Loss: 0.1540, Val Loss: 0.2180\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1563  obj_loss: 0.0835  noobj_loss: 0.0534  bbox_loss: 0.0074  cls_loss: 0.0090  \n",
      "<<<iteration:[40/657] - total_loss: 0.1646  obj_loss: 0.0802  noobj_loss: 0.0522  bbox_loss: 0.0094  cls_loss: 0.0112  \n",
      "<<<iteration:[60/657] - total_loss: 0.1498  obj_loss: 0.0813  noobj_loss: 0.0552  bbox_loss: 0.0066  cls_loss: 0.0081  \n",
      "<<<iteration:[80/657] - total_loss: 0.1599  obj_loss: 0.0862  noobj_loss: 0.0508  bbox_loss: 0.0077  cls_loss: 0.0098  \n",
      "<<<iteration:[100/657] - total_loss: 0.1550  obj_loss: 0.0882  noobj_loss: 0.0486  bbox_loss: 0.0068  cls_loss: 0.0087  \n",
      "<<<iteration:[120/657] - total_loss: 0.1657  obj_loss: 0.0862  noobj_loss: 0.0596  bbox_loss: 0.0083  cls_loss: 0.0082  \n",
      "<<<iteration:[140/657] - total_loss: 0.1561  obj_loss: 0.0764  noobj_loss: 0.0604  bbox_loss: 0.0084  cls_loss: 0.0074  \n",
      "<<<iteration:[160/657] - total_loss: 0.1647  obj_loss: 0.0920  noobj_loss: 0.0630  bbox_loss: 0.0067  cls_loss: 0.0076  \n",
      "<<<iteration:[180/657] - total_loss: 0.1442  obj_loss: 0.0795  noobj_loss: 0.0485  bbox_loss: 0.0063  cls_loss: 0.0090  \n",
      "<<<iteration:[200/657] - total_loss: 0.1438  obj_loss: 0.0749  noobj_loss: 0.0474  bbox_loss: 0.0072  cls_loss: 0.0091  \n",
      "<<<iteration:[220/657] - total_loss: 0.1572  obj_loss: 0.0873  noobj_loss: 0.0612  bbox_loss: 0.0064  cls_loss: 0.0073  \n",
      "<<<iteration:[240/657] - total_loss: 0.1613  obj_loss: 0.0896  noobj_loss: 0.0549  bbox_loss: 0.0074  cls_loss: 0.0075  \n",
      "<<<iteration:[260/657] - total_loss: 0.1522  obj_loss: 0.0826  noobj_loss: 0.0506  bbox_loss: 0.0071  cls_loss: 0.0089  \n",
      "<<<iteration:[280/657] - total_loss: 0.1413  obj_loss: 0.0754  noobj_loss: 0.0459  bbox_loss: 0.0071  cls_loss: 0.0075  \n",
      "<<<iteration:[300/657] - total_loss: 0.1666  obj_loss: 0.0962  noobj_loss: 0.0600  bbox_loss: 0.0066  cls_loss: 0.0074  \n",
      "<<<iteration:[320/657] - total_loss: 0.1535  obj_loss: 0.0805  noobj_loss: 0.0562  bbox_loss: 0.0074  cls_loss: 0.0078  \n",
      "<<<iteration:[340/657] - total_loss: 0.1422  obj_loss: 0.0738  noobj_loss: 0.0543  bbox_loss: 0.0064  cls_loss: 0.0094  \n",
      "<<<iteration:[360/657] - total_loss: 0.1546  obj_loss: 0.0816  noobj_loss: 0.0571  bbox_loss: 0.0065  cls_loss: 0.0118  \n",
      "<<<iteration:[380/657] - total_loss: 0.1655  obj_loss: 0.0865  noobj_loss: 0.0561  bbox_loss: 0.0084  cls_loss: 0.0092  \n",
      "<<<iteration:[400/657] - total_loss: 0.1443  obj_loss: 0.0778  noobj_loss: 0.0502  bbox_loss: 0.0065  cls_loss: 0.0087  \n",
      "<<<iteration:[420/657] - total_loss: 0.1383  obj_loss: 0.0723  noobj_loss: 0.0504  bbox_loss: 0.0068  cls_loss: 0.0068  \n",
      "<<<iteration:[440/657] - total_loss: 0.1426  obj_loss: 0.0731  noobj_loss: 0.0445  bbox_loss: 0.0075  cls_loss: 0.0096  \n",
      "<<<iteration:[460/657] - total_loss: 0.1454  obj_loss: 0.0728  noobj_loss: 0.0503  bbox_loss: 0.0078  cls_loss: 0.0086  \n",
      "<<<iteration:[480/657] - total_loss: 0.1530  obj_loss: 0.0799  noobj_loss: 0.0548  bbox_loss: 0.0072  cls_loss: 0.0097  \n",
      "<<<iteration:[500/657] - total_loss: 0.1622  obj_loss: 0.0804  noobj_loss: 0.0507  bbox_loss: 0.0093  cls_loss: 0.0102  \n",
      "<<<iteration:[520/657] - total_loss: 0.1465  obj_loss: 0.0772  noobj_loss: 0.0530  bbox_loss: 0.0066  cls_loss: 0.0097  \n",
      "<<<iteration:[540/657] - total_loss: 0.1589  obj_loss: 0.0887  noobj_loss: 0.0568  bbox_loss: 0.0069  cls_loss: 0.0072  \n",
      "<<<iteration:[560/657] - total_loss: 0.1580  obj_loss: 0.0820  noobj_loss: 0.0513  bbox_loss: 0.0081  cls_loss: 0.0096  \n",
      "<<<iteration:[580/657] - total_loss: 0.1479  obj_loss: 0.0742  noobj_loss: 0.0529  bbox_loss: 0.0077  cls_loss: 0.0089  \n",
      "<<<iteration:[600/657] - total_loss: 0.1535  obj_loss: 0.0841  noobj_loss: 0.0508  bbox_loss: 0.0069  cls_loss: 0.0098  \n",
      "<<<iteration:[620/657] - total_loss: 0.1638  obj_loss: 0.0903  noobj_loss: 0.0579  bbox_loss: 0.0074  cls_loss: 0.0074  \n",
      "<<<iteration:[640/657] - total_loss: 0.1444  obj_loss: 0.0756  noobj_loss: 0.0523  bbox_loss: 0.0070  cls_loss: 0.0077  \n",
      "\n",
      "epoch:92/100 - Train Loss: 0.1534, Val Loss: 0.2187\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1711  obj_loss: 0.0981  noobj_loss: 0.0594  bbox_loss: 0.0069  cls_loss: 0.0086  \n",
      "<<<iteration:[40/657] - total_loss: 0.1435  obj_loss: 0.0749  noobj_loss: 0.0519  bbox_loss: 0.0068  cls_loss: 0.0089  \n",
      "<<<iteration:[60/657] - total_loss: 0.1449  obj_loss: 0.0717  noobj_loss: 0.0506  bbox_loss: 0.0078  cls_loss: 0.0092  \n",
      "<<<iteration:[80/657] - total_loss: 0.1488  obj_loss: 0.0827  noobj_loss: 0.0487  bbox_loss: 0.0068  cls_loss: 0.0076  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[100/657] - total_loss: 0.1414  obj_loss: 0.0720  noobj_loss: 0.0545  bbox_loss: 0.0068  cls_loss: 0.0080  \n",
      "<<<iteration:[120/657] - total_loss: 0.1374  obj_loss: 0.0752  noobj_loss: 0.0469  bbox_loss: 0.0064  cls_loss: 0.0070  \n",
      "<<<iteration:[140/657] - total_loss: 0.1659  obj_loss: 0.0901  noobj_loss: 0.0586  bbox_loss: 0.0074  cls_loss: 0.0097  \n",
      "<<<iteration:[160/657] - total_loss: 0.1613  obj_loss: 0.0875  noobj_loss: 0.0585  bbox_loss: 0.0072  cls_loss: 0.0087  \n",
      "<<<iteration:[180/657] - total_loss: 0.1608  obj_loss: 0.0854  noobj_loss: 0.0578  bbox_loss: 0.0075  cls_loss: 0.0089  \n",
      "<<<iteration:[200/657] - total_loss: 0.1607  obj_loss: 0.0900  noobj_loss: 0.0465  bbox_loss: 0.0076  cls_loss: 0.0096  \n",
      "<<<iteration:[220/657] - total_loss: 0.1487  obj_loss: 0.0792  noobj_loss: 0.0554  bbox_loss: 0.0067  cls_loss: 0.0082  \n",
      "<<<iteration:[240/657] - total_loss: 0.1508  obj_loss: 0.0834  noobj_loss: 0.0544  bbox_loss: 0.0064  cls_loss: 0.0082  \n",
      "<<<iteration:[260/657] - total_loss: 0.1358  obj_loss: 0.0751  noobj_loss: 0.0467  bbox_loss: 0.0058  cls_loss: 0.0084  \n",
      "<<<iteration:[280/657] - total_loss: 0.1499  obj_loss: 0.0812  noobj_loss: 0.0515  bbox_loss: 0.0068  cls_loss: 0.0088  \n",
      "<<<iteration:[300/657] - total_loss: 0.1530  obj_loss: 0.0854  noobj_loss: 0.0482  bbox_loss: 0.0072  cls_loss: 0.0073  \n",
      "<<<iteration:[320/657] - total_loss: 0.1587  obj_loss: 0.0876  noobj_loss: 0.0561  bbox_loss: 0.0071  cls_loss: 0.0073  \n",
      "<<<iteration:[340/657] - total_loss: 0.1418  obj_loss: 0.0724  noobj_loss: 0.0537  bbox_loss: 0.0069  cls_loss: 0.0079  \n",
      "<<<iteration:[360/657] - total_loss: 0.1559  obj_loss: 0.0836  noobj_loss: 0.0612  bbox_loss: 0.0068  cls_loss: 0.0076  \n",
      "<<<iteration:[380/657] - total_loss: 0.1494  obj_loss: 0.0777  noobj_loss: 0.0568  bbox_loss: 0.0068  cls_loss: 0.0091  \n",
      "<<<iteration:[400/657] - total_loss: 0.1457  obj_loss: 0.0741  noobj_loss: 0.0528  bbox_loss: 0.0075  cls_loss: 0.0078  \n",
      "<<<iteration:[420/657] - total_loss: 0.1559  obj_loss: 0.0823  noobj_loss: 0.0582  bbox_loss: 0.0073  cls_loss: 0.0083  \n",
      "<<<iteration:[440/657] - total_loss: 0.1487  obj_loss: 0.0793  noobj_loss: 0.0539  bbox_loss: 0.0071  cls_loss: 0.0070  \n",
      "<<<iteration:[460/657] - total_loss: 0.1586  obj_loss: 0.0883  noobj_loss: 0.0496  bbox_loss: 0.0075  cls_loss: 0.0078  \n",
      "<<<iteration:[480/657] - total_loss: 0.1612  obj_loss: 0.0781  noobj_loss: 0.0570  bbox_loss: 0.0089  cls_loss: 0.0102  \n",
      "<<<iteration:[500/657] - total_loss: 0.1491  obj_loss: 0.0779  noobj_loss: 0.0517  bbox_loss: 0.0073  cls_loss: 0.0088  \n",
      "<<<iteration:[520/657] - total_loss: 0.1531  obj_loss: 0.0829  noobj_loss: 0.0510  bbox_loss: 0.0076  cls_loss: 0.0068  \n",
      "<<<iteration:[540/657] - total_loss: 0.1470  obj_loss: 0.0723  noobj_loss: 0.0567  bbox_loss: 0.0079  cls_loss: 0.0070  \n",
      "<<<iteration:[560/657] - total_loss: 0.1525  obj_loss: 0.0872  noobj_loss: 0.0517  bbox_loss: 0.0065  cls_loss: 0.0071  \n",
      "<<<iteration:[580/657] - total_loss: 0.1386  obj_loss: 0.0719  noobj_loss: 0.0522  bbox_loss: 0.0066  cls_loss: 0.0074  \n",
      "<<<iteration:[600/657] - total_loss: 0.1421  obj_loss: 0.0688  noobj_loss: 0.0553  bbox_loss: 0.0073  cls_loss: 0.0094  \n",
      "<<<iteration:[620/657] - total_loss: 0.1440  obj_loss: 0.0795  noobj_loss: 0.0533  bbox_loss: 0.0058  cls_loss: 0.0089  \n",
      "<<<iteration:[640/657] - total_loss: 0.1517  obj_loss: 0.0822  noobj_loss: 0.0514  bbox_loss: 0.0068  cls_loss: 0.0099  \n",
      "\n",
      "epoch:93/100 - Train Loss: 0.1505, Val Loss: 0.2159\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1588  obj_loss: 0.0882  noobj_loss: 0.0483  bbox_loss: 0.0074  cls_loss: 0.0093  \n",
      "<<<iteration:[40/657] - total_loss: 0.1409  obj_loss: 0.0697  noobj_loss: 0.0506  bbox_loss: 0.0076  cls_loss: 0.0080  \n",
      "<<<iteration:[60/657] - total_loss: 0.1410  obj_loss: 0.0754  noobj_loss: 0.0467  bbox_loss: 0.0067  cls_loss: 0.0088  \n",
      "<<<iteration:[80/657] - total_loss: 0.1522  obj_loss: 0.0835  noobj_loss: 0.0549  bbox_loss: 0.0064  cls_loss: 0.0093  \n",
      "<<<iteration:[100/657] - total_loss: 0.1800  obj_loss: 0.0938  noobj_loss: 0.0611  bbox_loss: 0.0081  cls_loss: 0.0150  \n",
      "<<<iteration:[120/657] - total_loss: 0.1494  obj_loss: 0.0740  noobj_loss: 0.0499  bbox_loss: 0.0084  cls_loss: 0.0085  \n",
      "<<<iteration:[140/657] - total_loss: 0.1404  obj_loss: 0.0711  noobj_loss: 0.0499  bbox_loss: 0.0070  cls_loss: 0.0094  \n",
      "<<<iteration:[160/657] - total_loss: 0.1452  obj_loss: 0.0777  noobj_loss: 0.0489  bbox_loss: 0.0070  cls_loss: 0.0081  \n",
      "<<<iteration:[180/657] - total_loss: 0.1587  obj_loss: 0.0872  noobj_loss: 0.0543  bbox_loss: 0.0071  cls_loss: 0.0088  \n",
      "<<<iteration:[200/657] - total_loss: 0.1601  obj_loss: 0.0876  noobj_loss: 0.0522  bbox_loss: 0.0077  cls_loss: 0.0077  \n",
      "<<<iteration:[220/657] - total_loss: 0.1534  obj_loss: 0.0709  noobj_loss: 0.0555  bbox_loss: 0.0094  cls_loss: 0.0080  \n",
      "<<<iteration:[240/657] - total_loss: 0.1438  obj_loss: 0.0740  noobj_loss: 0.0502  bbox_loss: 0.0072  cls_loss: 0.0086  \n",
      "<<<iteration:[260/657] - total_loss: 0.1482  obj_loss: 0.0824  noobj_loss: 0.0525  bbox_loss: 0.0062  cls_loss: 0.0087  \n",
      "<<<iteration:[280/657] - total_loss: 0.1470  obj_loss: 0.0770  noobj_loss: 0.0560  bbox_loss: 0.0069  cls_loss: 0.0075  \n",
      "<<<iteration:[300/657] - total_loss: 0.1565  obj_loss: 0.0850  noobj_loss: 0.0529  bbox_loss: 0.0069  cls_loss: 0.0107  \n",
      "<<<iteration:[320/657] - total_loss: 0.1365  obj_loss: 0.0699  noobj_loss: 0.0514  bbox_loss: 0.0067  cls_loss: 0.0076  \n",
      "<<<iteration:[340/657] - total_loss: 0.1516  obj_loss: 0.0861  noobj_loss: 0.0481  bbox_loss: 0.0068  cls_loss: 0.0076  \n",
      "<<<iteration:[360/657] - total_loss: 0.1481  obj_loss: 0.0783  noobj_loss: 0.0527  bbox_loss: 0.0070  cls_loss: 0.0087  \n",
      "<<<iteration:[380/657] - total_loss: 0.1461  obj_loss: 0.0767  noobj_loss: 0.0481  bbox_loss: 0.0073  cls_loss: 0.0089  \n",
      "<<<iteration:[400/657] - total_loss: 0.1383  obj_loss: 0.0735  noobj_loss: 0.0537  bbox_loss: 0.0061  cls_loss: 0.0077  \n",
      "<<<iteration:[420/657] - total_loss: 0.1602  obj_loss: 0.0881  noobj_loss: 0.0579  bbox_loss: 0.0071  cls_loss: 0.0078  \n",
      "<<<iteration:[440/657] - total_loss: 0.1478  obj_loss: 0.0802  noobj_loss: 0.0534  bbox_loss: 0.0068  cls_loss: 0.0070  \n",
      "<<<iteration:[460/657] - total_loss: 0.1366  obj_loss: 0.0666  noobj_loss: 0.0527  bbox_loss: 0.0073  cls_loss: 0.0072  \n",
      "<<<iteration:[480/657] - total_loss: 0.1520  obj_loss: 0.0854  noobj_loss: 0.0517  bbox_loss: 0.0066  cls_loss: 0.0079  \n",
      "<<<iteration:[500/657] - total_loss: 0.1423  obj_loss: 0.0742  noobj_loss: 0.0509  bbox_loss: 0.0069  cls_loss: 0.0083  \n",
      "<<<iteration:[520/657] - total_loss: 0.1430  obj_loss: 0.0745  noobj_loss: 0.0540  bbox_loss: 0.0065  cls_loss: 0.0089  \n",
      "<<<iteration:[540/657] - total_loss: 0.1544  obj_loss: 0.0831  noobj_loss: 0.0584  bbox_loss: 0.0068  cls_loss: 0.0080  \n",
      "<<<iteration:[560/657] - total_loss: 0.1479  obj_loss: 0.0819  noobj_loss: 0.0469  bbox_loss: 0.0067  cls_loss: 0.0092  \n",
      "<<<iteration:[580/657] - total_loss: 0.1592  obj_loss: 0.0881  noobj_loss: 0.0498  bbox_loss: 0.0073  cls_loss: 0.0096  \n",
      "<<<iteration:[600/657] - total_loss: 0.1570  obj_loss: 0.0893  noobj_loss: 0.0483  bbox_loss: 0.0071  cls_loss: 0.0079  \n",
      "<<<iteration:[620/657] - total_loss: 0.1473  obj_loss: 0.0812  noobj_loss: 0.0491  bbox_loss: 0.0068  cls_loss: 0.0078  \n",
      "<<<iteration:[640/657] - total_loss: 0.1527  obj_loss: 0.0804  noobj_loss: 0.0510  bbox_loss: 0.0077  cls_loss: 0.0085  \n",
      "\n",
      "epoch:94/100 - Train Loss: 0.1498, Val Loss: 0.2175\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1599  obj_loss: 0.0935  noobj_loss: 0.0524  bbox_loss: 0.0066  cls_loss: 0.0075  \n",
      "<<<iteration:[40/657] - total_loss: 0.1460  obj_loss: 0.0759  noobj_loss: 0.0505  bbox_loss: 0.0075  cls_loss: 0.0075  \n",
      "<<<iteration:[60/657] - total_loss: 0.1508  obj_loss: 0.0817  noobj_loss: 0.0550  bbox_loss: 0.0068  cls_loss: 0.0074  \n",
      "<<<iteration:[80/657] - total_loss: 0.1563  obj_loss: 0.0836  noobj_loss: 0.0559  bbox_loss: 0.0073  cls_loss: 0.0084  \n",
      "<<<iteration:[100/657] - total_loss: 0.1399  obj_loss: 0.0771  noobj_loss: 0.0469  bbox_loss: 0.0063  cls_loss: 0.0078  \n",
      "<<<iteration:[120/657] - total_loss: 0.1386  obj_loss: 0.0751  noobj_loss: 0.0502  bbox_loss: 0.0061  cls_loss: 0.0078  \n",
      "<<<iteration:[140/657] - total_loss: 0.1599  obj_loss: 0.0882  noobj_loss: 0.0512  bbox_loss: 0.0075  cls_loss: 0.0083  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[160/657] - total_loss: 0.1760  obj_loss: 0.1003  noobj_loss: 0.0498  bbox_loss: 0.0085  cls_loss: 0.0084  \n",
      "<<<iteration:[180/657] - total_loss: 0.1531  obj_loss: 0.0833  noobj_loss: 0.0478  bbox_loss: 0.0075  cls_loss: 0.0083  \n",
      "<<<iteration:[200/657] - total_loss: 0.1578  obj_loss: 0.0796  noobj_loss: 0.0549  bbox_loss: 0.0082  cls_loss: 0.0098  \n",
      "<<<iteration:[220/657] - total_loss: 0.1400  obj_loss: 0.0724  noobj_loss: 0.0501  bbox_loss: 0.0070  cls_loss: 0.0077  \n",
      "<<<iteration:[240/657] - total_loss: 0.1454  obj_loss: 0.0811  noobj_loss: 0.0545  bbox_loss: 0.0059  cls_loss: 0.0076  \n",
      "<<<iteration:[260/657] - total_loss: 0.1473  obj_loss: 0.0770  noobj_loss: 0.0597  bbox_loss: 0.0066  cls_loss: 0.0076  \n",
      "<<<iteration:[280/657] - total_loss: 0.1575  obj_loss: 0.0837  noobj_loss: 0.0532  bbox_loss: 0.0075  cls_loss: 0.0096  \n",
      "<<<iteration:[300/657] - total_loss: 0.1569  obj_loss: 0.0869  noobj_loss: 0.0531  bbox_loss: 0.0069  cls_loss: 0.0087  \n",
      "<<<iteration:[320/657] - total_loss: 0.1515  obj_loss: 0.0847  noobj_loss: 0.0523  bbox_loss: 0.0065  cls_loss: 0.0080  \n",
      "<<<iteration:[340/657] - total_loss: 0.1534  obj_loss: 0.0850  noobj_loss: 0.0512  bbox_loss: 0.0066  cls_loss: 0.0099  \n",
      "<<<iteration:[360/657] - total_loss: 0.1467  obj_loss: 0.0783  noobj_loss: 0.0569  bbox_loss: 0.0064  cls_loss: 0.0078  \n",
      "<<<iteration:[380/657] - total_loss: 0.1376  obj_loss: 0.0748  noobj_loss: 0.0493  bbox_loss: 0.0061  cls_loss: 0.0076  \n",
      "<<<iteration:[400/657] - total_loss: 0.1674  obj_loss: 0.0893  noobj_loss: 0.0600  bbox_loss: 0.0080  cls_loss: 0.0082  \n",
      "<<<iteration:[420/657] - total_loss: 0.1684  obj_loss: 0.0875  noobj_loss: 0.0500  bbox_loss: 0.0093  cls_loss: 0.0092  \n",
      "<<<iteration:[440/657] - total_loss: 0.1493  obj_loss: 0.0776  noobj_loss: 0.0508  bbox_loss: 0.0073  cls_loss: 0.0098  \n",
      "<<<iteration:[460/657] - total_loss: 0.1509  obj_loss: 0.0785  noobj_loss: 0.0563  bbox_loss: 0.0074  cls_loss: 0.0073  \n",
      "<<<iteration:[480/657] - total_loss: 0.1528  obj_loss: 0.0817  noobj_loss: 0.0496  bbox_loss: 0.0075  cls_loss: 0.0090  \n",
      "<<<iteration:[500/657] - total_loss: 0.1522  obj_loss: 0.0751  noobj_loss: 0.0593  bbox_loss: 0.0078  cls_loss: 0.0082  \n",
      "<<<iteration:[520/657] - total_loss: 0.1547  obj_loss: 0.0872  noobj_loss: 0.0505  bbox_loss: 0.0067  cls_loss: 0.0085  \n",
      "<<<iteration:[540/657] - total_loss: 0.1606  obj_loss: 0.0877  noobj_loss: 0.0564  bbox_loss: 0.0071  cls_loss: 0.0092  \n",
      "<<<iteration:[560/657] - total_loss: 0.1370  obj_loss: 0.0690  noobj_loss: 0.0503  bbox_loss: 0.0071  cls_loss: 0.0074  \n",
      "<<<iteration:[580/657] - total_loss: 0.1420  obj_loss: 0.0738  noobj_loss: 0.0520  bbox_loss: 0.0068  cls_loss: 0.0081  \n",
      "<<<iteration:[600/657] - total_loss: 0.1517  obj_loss: 0.0831  noobj_loss: 0.0491  bbox_loss: 0.0071  cls_loss: 0.0084  \n",
      "<<<iteration:[620/657] - total_loss: 0.1411  obj_loss: 0.0753  noobj_loss: 0.0543  bbox_loss: 0.0065  cls_loss: 0.0062  \n",
      "<<<iteration:[640/657] - total_loss: 0.1255  obj_loss: 0.0597  noobj_loss: 0.0482  bbox_loss: 0.0066  cls_loss: 0.0090  \n",
      "\n",
      "epoch:95/100 - Train Loss: 0.1512, Val Loss: 0.2244\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1592  obj_loss: 0.0852  noobj_loss: 0.0561  bbox_loss: 0.0078  cls_loss: 0.0070  \n",
      "<<<iteration:[40/657] - total_loss: 0.1509  obj_loss: 0.0781  noobj_loss: 0.0527  bbox_loss: 0.0076  cls_loss: 0.0083  \n",
      "<<<iteration:[60/657] - total_loss: 0.1436  obj_loss: 0.0779  noobj_loss: 0.0539  bbox_loss: 0.0061  cls_loss: 0.0081  \n",
      "<<<iteration:[80/657] - total_loss: 0.1552  obj_loss: 0.0791  noobj_loss: 0.0552  bbox_loss: 0.0081  cls_loss: 0.0081  \n",
      "<<<iteration:[100/657] - total_loss: 0.1479  obj_loss: 0.0797  noobj_loss: 0.0442  bbox_loss: 0.0075  cls_loss: 0.0084  \n",
      "<<<iteration:[120/657] - total_loss: 0.1449  obj_loss: 0.0786  noobj_loss: 0.0552  bbox_loss: 0.0065  cls_loss: 0.0062  \n",
      "<<<iteration:[140/657] - total_loss: 0.1417  obj_loss: 0.0734  noobj_loss: 0.0454  bbox_loss: 0.0073  cls_loss: 0.0089  \n",
      "<<<iteration:[160/657] - total_loss: 0.1506  obj_loss: 0.0805  noobj_loss: 0.0512  bbox_loss: 0.0071  cls_loss: 0.0088  \n",
      "<<<iteration:[180/657] - total_loss: 0.1651  obj_loss: 0.0931  noobj_loss: 0.0497  bbox_loss: 0.0079  cls_loss: 0.0076  \n",
      "<<<iteration:[200/657] - total_loss: 0.1459  obj_loss: 0.0811  noobj_loss: 0.0528  bbox_loss: 0.0062  cls_loss: 0.0076  \n",
      "<<<iteration:[220/657] - total_loss: 0.1386  obj_loss: 0.0711  noobj_loss: 0.0466  bbox_loss: 0.0071  cls_loss: 0.0088  \n",
      "<<<iteration:[240/657] - total_loss: 0.1500  obj_loss: 0.0836  noobj_loss: 0.0544  bbox_loss: 0.0065  cls_loss: 0.0068  \n",
      "<<<iteration:[260/657] - total_loss: 0.1476  obj_loss: 0.0797  noobj_loss: 0.0485  bbox_loss: 0.0073  cls_loss: 0.0073  \n",
      "<<<iteration:[280/657] - total_loss: 0.1445  obj_loss: 0.0771  noobj_loss: 0.0543  bbox_loss: 0.0065  cls_loss: 0.0080  \n",
      "<<<iteration:[300/657] - total_loss: 0.1463  obj_loss: 0.0818  noobj_loss: 0.0539  bbox_loss: 0.0060  cls_loss: 0.0074  \n",
      "<<<iteration:[320/657] - total_loss: 0.1469  obj_loss: 0.0763  noobj_loss: 0.0488  bbox_loss: 0.0073  cls_loss: 0.0095  \n",
      "<<<iteration:[340/657] - total_loss: 0.1501  obj_loss: 0.0850  noobj_loss: 0.0524  bbox_loss: 0.0060  cls_loss: 0.0088  \n",
      "<<<iteration:[360/657] - total_loss: 0.1304  obj_loss: 0.0708  noobj_loss: 0.0476  bbox_loss: 0.0057  cls_loss: 0.0072  \n",
      "<<<iteration:[380/657] - total_loss: 0.1387  obj_loss: 0.0730  noobj_loss: 0.0512  bbox_loss: 0.0066  cls_loss: 0.0070  \n",
      "<<<iteration:[400/657] - total_loss: 0.1462  obj_loss: 0.0791  noobj_loss: 0.0494  bbox_loss: 0.0071  cls_loss: 0.0068  \n",
      "<<<iteration:[420/657] - total_loss: 0.1587  obj_loss: 0.0880  noobj_loss: 0.0525  bbox_loss: 0.0071  cls_loss: 0.0091  \n",
      "<<<iteration:[440/657] - total_loss: 0.1485  obj_loss: 0.0819  noobj_loss: 0.0550  bbox_loss: 0.0062  cls_loss: 0.0079  \n",
      "<<<iteration:[460/657] - total_loss: 0.1518  obj_loss: 0.0802  noobj_loss: 0.0584  bbox_loss: 0.0069  cls_loss: 0.0081  \n",
      "<<<iteration:[480/657] - total_loss: 0.1327  obj_loss: 0.0667  noobj_loss: 0.0515  bbox_loss: 0.0067  cls_loss: 0.0069  \n",
      "<<<iteration:[500/657] - total_loss: 0.1529  obj_loss: 0.0880  noobj_loss: 0.0475  bbox_loss: 0.0068  cls_loss: 0.0070  \n",
      "<<<iteration:[520/657] - total_loss: 0.1477  obj_loss: 0.0755  noobj_loss: 0.0568  bbox_loss: 0.0073  cls_loss: 0.0070  \n",
      "<<<iteration:[540/657] - total_loss: 0.1518  obj_loss: 0.0794  noobj_loss: 0.0491  bbox_loss: 0.0078  cls_loss: 0.0090  \n",
      "<<<iteration:[560/657] - total_loss: 0.1438  obj_loss: 0.0754  noobj_loss: 0.0532  bbox_loss: 0.0069  cls_loss: 0.0073  \n",
      "<<<iteration:[580/657] - total_loss: 0.1587  obj_loss: 0.0857  noobj_loss: 0.0494  bbox_loss: 0.0077  cls_loss: 0.0099  \n",
      "<<<iteration:[600/657] - total_loss: 0.1658  obj_loss: 0.0908  noobj_loss: 0.0530  bbox_loss: 0.0084  cls_loss: 0.0064  \n",
      "<<<iteration:[620/657] - total_loss: 0.1514  obj_loss: 0.0786  noobj_loss: 0.0489  bbox_loss: 0.0079  cls_loss: 0.0090  \n",
      "<<<iteration:[640/657] - total_loss: 0.1380  obj_loss: 0.0775  noobj_loss: 0.0458  bbox_loss: 0.0064  cls_loss: 0.0058  \n",
      "\n",
      "epoch:96/100 - Train Loss: 0.1483, Val Loss: 0.2162\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1752  obj_loss: 0.0836  noobj_loss: 0.0572  bbox_loss: 0.0108  cls_loss: 0.0090  \n",
      "<<<iteration:[40/657] - total_loss: 0.1492  obj_loss: 0.0833  noobj_loss: 0.0534  bbox_loss: 0.0064  cls_loss: 0.0073  \n",
      "<<<iteration:[60/657] - total_loss: 0.1421  obj_loss: 0.0773  noobj_loss: 0.0524  bbox_loss: 0.0063  cls_loss: 0.0069  \n",
      "<<<iteration:[80/657] - total_loss: 0.1553  obj_loss: 0.0852  noobj_loss: 0.0536  bbox_loss: 0.0073  cls_loss: 0.0067  \n",
      "<<<iteration:[100/657] - total_loss: 0.1471  obj_loss: 0.0803  noobj_loss: 0.0553  bbox_loss: 0.0064  cls_loss: 0.0071  \n",
      "<<<iteration:[120/657] - total_loss: 0.1553  obj_loss: 0.0848  noobj_loss: 0.0492  bbox_loss: 0.0072  cls_loss: 0.0101  \n",
      "<<<iteration:[140/657] - total_loss: 0.1444  obj_loss: 0.0786  noobj_loss: 0.0500  bbox_loss: 0.0067  cls_loss: 0.0072  \n",
      "<<<iteration:[160/657] - total_loss: 0.1362  obj_loss: 0.0729  noobj_loss: 0.0490  bbox_loss: 0.0061  cls_loss: 0.0081  \n",
      "<<<iteration:[180/657] - total_loss: 0.1284  obj_loss: 0.0669  noobj_loss: 0.0443  bbox_loss: 0.0066  cls_loss: 0.0065  \n",
      "<<<iteration:[200/657] - total_loss: 0.1429  obj_loss: 0.0687  noobj_loss: 0.0504  bbox_loss: 0.0080  cls_loss: 0.0088  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[220/657] - total_loss: 0.1391  obj_loss: 0.0772  noobj_loss: 0.0472  bbox_loss: 0.0062  cls_loss: 0.0074  \n",
      "<<<iteration:[240/657] - total_loss: 0.1559  obj_loss: 0.0773  noobj_loss: 0.0546  bbox_loss: 0.0086  cls_loss: 0.0083  \n",
      "<<<iteration:[260/657] - total_loss: 0.1603  obj_loss: 0.0791  noobj_loss: 0.0589  bbox_loss: 0.0078  cls_loss: 0.0127  \n",
      "<<<iteration:[280/657] - total_loss: 0.1440  obj_loss: 0.0708  noobj_loss: 0.0574  bbox_loss: 0.0073  cls_loss: 0.0081  \n",
      "<<<iteration:[300/657] - total_loss: 0.1707  obj_loss: 0.0901  noobj_loss: 0.0566  bbox_loss: 0.0083  cls_loss: 0.0109  \n",
      "<<<iteration:[320/657] - total_loss: 0.1427  obj_loss: 0.0784  noobj_loss: 0.0491  bbox_loss: 0.0061  cls_loss: 0.0093  \n",
      "<<<iteration:[340/657] - total_loss: 0.1454  obj_loss: 0.0712  noobj_loss: 0.0533  bbox_loss: 0.0074  cls_loss: 0.0104  \n",
      "<<<iteration:[360/657] - total_loss: 0.1560  obj_loss: 0.0837  noobj_loss: 0.0558  bbox_loss: 0.0073  cls_loss: 0.0080  \n",
      "<<<iteration:[380/657] - total_loss: 0.1541  obj_loss: 0.0839  noobj_loss: 0.0545  bbox_loss: 0.0070  cls_loss: 0.0079  \n",
      "<<<iteration:[400/657] - total_loss: 0.1540  obj_loss: 0.0872  noobj_loss: 0.0527  bbox_loss: 0.0067  cls_loss: 0.0069  \n",
      "<<<iteration:[420/657] - total_loss: 0.1591  obj_loss: 0.0889  noobj_loss: 0.0495  bbox_loss: 0.0073  cls_loss: 0.0088  \n",
      "<<<iteration:[440/657] - total_loss: 0.1437  obj_loss: 0.0738  noobj_loss: 0.0527  bbox_loss: 0.0071  cls_loss: 0.0083  \n",
      "<<<iteration:[460/657] - total_loss: 0.1410  obj_loss: 0.0796  noobj_loss: 0.0453  bbox_loss: 0.0060  cls_loss: 0.0088  \n",
      "<<<iteration:[480/657] - total_loss: 0.1336  obj_loss: 0.0712  noobj_loss: 0.0447  bbox_loss: 0.0065  cls_loss: 0.0074  \n",
      "<<<iteration:[500/657] - total_loss: 0.1500  obj_loss: 0.0800  noobj_loss: 0.0503  bbox_loss: 0.0074  cls_loss: 0.0079  \n",
      "<<<iteration:[520/657] - total_loss: 0.1565  obj_loss: 0.0849  noobj_loss: 0.0480  bbox_loss: 0.0076  cls_loss: 0.0097  \n",
      "<<<iteration:[540/657] - total_loss: 0.1472  obj_loss: 0.0706  noobj_loss: 0.0600  bbox_loss: 0.0074  cls_loss: 0.0095  \n",
      "<<<iteration:[560/657] - total_loss: 0.1558  obj_loss: 0.0867  noobj_loss: 0.0574  bbox_loss: 0.0065  cls_loss: 0.0078  \n",
      "<<<iteration:[580/657] - total_loss: 0.1585  obj_loss: 0.0873  noobj_loss: 0.0534  bbox_loss: 0.0071  cls_loss: 0.0090  \n",
      "<<<iteration:[600/657] - total_loss: 0.1383  obj_loss: 0.0756  noobj_loss: 0.0492  bbox_loss: 0.0060  cls_loss: 0.0082  \n",
      "<<<iteration:[620/657] - total_loss: 0.1473  obj_loss: 0.0823  noobj_loss: 0.0487  bbox_loss: 0.0067  cls_loss: 0.0070  \n",
      "<<<iteration:[640/657] - total_loss: 0.1488  obj_loss: 0.0816  noobj_loss: 0.0560  bbox_loss: 0.0064  cls_loss: 0.0075  \n",
      "\n",
      "epoch:97/100 - Train Loss: 0.1494, Val Loss: 0.2195\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1548  obj_loss: 0.0858  noobj_loss: 0.0558  bbox_loss: 0.0068  cls_loss: 0.0072  \n",
      "<<<iteration:[40/657] - total_loss: 0.1415  obj_loss: 0.0757  noobj_loss: 0.0530  bbox_loss: 0.0065  cls_loss: 0.0070  \n",
      "<<<iteration:[60/657] - total_loss: 0.1436  obj_loss: 0.0785  noobj_loss: 0.0540  bbox_loss: 0.0063  cls_loss: 0.0067  \n",
      "<<<iteration:[80/657] - total_loss: 0.1494  obj_loss: 0.0748  noobj_loss: 0.0540  bbox_loss: 0.0078  cls_loss: 0.0085  \n",
      "<<<iteration:[100/657] - total_loss: 0.1337  obj_loss: 0.0733  noobj_loss: 0.0464  bbox_loss: 0.0061  cls_loss: 0.0064  \n",
      "<<<iteration:[120/657] - total_loss: 0.1414  obj_loss: 0.0737  noobj_loss: 0.0480  bbox_loss: 0.0071  cls_loss: 0.0079  \n",
      "<<<iteration:[140/657] - total_loss: 0.1388  obj_loss: 0.0730  noobj_loss: 0.0512  bbox_loss: 0.0067  cls_loss: 0.0068  \n",
      "<<<iteration:[160/657] - total_loss: 0.1594  obj_loss: 0.0880  noobj_loss: 0.0639  bbox_loss: 0.0064  cls_loss: 0.0075  \n",
      "<<<iteration:[180/657] - total_loss: 0.1403  obj_loss: 0.0795  noobj_loss: 0.0467  bbox_loss: 0.0060  cls_loss: 0.0073  \n",
      "<<<iteration:[200/657] - total_loss: 0.1524  obj_loss: 0.0819  noobj_loss: 0.0513  bbox_loss: 0.0072  cls_loss: 0.0089  \n",
      "<<<iteration:[220/657] - total_loss: 0.1503  obj_loss: 0.0791  noobj_loss: 0.0556  bbox_loss: 0.0069  cls_loss: 0.0090  \n",
      "<<<iteration:[240/657] - total_loss: 0.1594  obj_loss: 0.0851  noobj_loss: 0.0615  bbox_loss: 0.0073  cls_loss: 0.0070  \n",
      "<<<iteration:[260/657] - total_loss: 0.1440  obj_loss: 0.0799  noobj_loss: 0.0493  bbox_loss: 0.0065  cls_loss: 0.0072  \n",
      "<<<iteration:[280/657] - total_loss: 0.1575  obj_loss: 0.0860  noobj_loss: 0.0582  bbox_loss: 0.0069  cls_loss: 0.0077  \n",
      "<<<iteration:[300/657] - total_loss: 0.1284  obj_loss: 0.0701  noobj_loss: 0.0378  bbox_loss: 0.0063  cls_loss: 0.0079  \n",
      "<<<iteration:[320/657] - total_loss: 0.1513  obj_loss: 0.0782  noobj_loss: 0.0481  bbox_loss: 0.0080  cls_loss: 0.0089  \n",
      "<<<iteration:[340/657] - total_loss: 0.1456  obj_loss: 0.0835  noobj_loss: 0.0511  bbox_loss: 0.0061  cls_loss: 0.0061  \n",
      "<<<iteration:[360/657] - total_loss: 0.1464  obj_loss: 0.0728  noobj_loss: 0.0566  bbox_loss: 0.0077  cls_loss: 0.0067  \n",
      "<<<iteration:[380/657] - total_loss: 0.1640  obj_loss: 0.0892  noobj_loss: 0.0552  bbox_loss: 0.0074  cls_loss: 0.0101  \n",
      "<<<iteration:[400/657] - total_loss: 0.1394  obj_loss: 0.0762  noobj_loss: 0.0470  bbox_loss: 0.0065  cls_loss: 0.0070  \n",
      "<<<iteration:[420/657] - total_loss: 0.1433  obj_loss: 0.0753  noobj_loss: 0.0459  bbox_loss: 0.0076  cls_loss: 0.0069  \n",
      "<<<iteration:[440/657] - total_loss: 0.1406  obj_loss: 0.0733  noobj_loss: 0.0449  bbox_loss: 0.0074  cls_loss: 0.0080  \n",
      "<<<iteration:[460/657] - total_loss: 0.1477  obj_loss: 0.0815  noobj_loss: 0.0513  bbox_loss: 0.0067  cls_loss: 0.0073  \n",
      "<<<iteration:[480/657] - total_loss: 0.1381  obj_loss: 0.0723  noobj_loss: 0.0519  bbox_loss: 0.0064  cls_loss: 0.0079  \n",
      "<<<iteration:[500/657] - total_loss: 0.1558  obj_loss: 0.0837  noobj_loss: 0.0534  bbox_loss: 0.0074  cls_loss: 0.0084  \n",
      "<<<iteration:[520/657] - total_loss: 0.1618  obj_loss: 0.0925  noobj_loss: 0.0497  bbox_loss: 0.0073  cls_loss: 0.0081  \n",
      "<<<iteration:[540/657] - total_loss: 0.1523  obj_loss: 0.0785  noobj_loss: 0.0483  bbox_loss: 0.0080  cls_loss: 0.0097  \n",
      "<<<iteration:[560/657] - total_loss: 0.1535  obj_loss: 0.0780  noobj_loss: 0.0492  bbox_loss: 0.0083  cls_loss: 0.0094  \n",
      "<<<iteration:[580/657] - total_loss: 0.1549  obj_loss: 0.0859  noobj_loss: 0.0500  bbox_loss: 0.0071  cls_loss: 0.0083  \n",
      "<<<iteration:[600/657] - total_loss: 0.1445  obj_loss: 0.0767  noobj_loss: 0.0575  bbox_loss: 0.0064  cls_loss: 0.0069  \n",
      "<<<iteration:[620/657] - total_loss: 0.1453  obj_loss: 0.0804  noobj_loss: 0.0500  bbox_loss: 0.0062  cls_loss: 0.0087  \n",
      "<<<iteration:[640/657] - total_loss: 0.1590  obj_loss: 0.0774  noobj_loss: 0.0555  bbox_loss: 0.0090  cls_loss: 0.0087  \n",
      "\n",
      "epoch:98/100 - Train Loss: 0.1485, Val Loss: 0.2157\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1569  obj_loss: 0.0879  noobj_loss: 0.0515  bbox_loss: 0.0068  cls_loss: 0.0091  \n",
      "<<<iteration:[40/657] - total_loss: 0.1518  obj_loss: 0.0853  noobj_loss: 0.0556  bbox_loss: 0.0062  cls_loss: 0.0077  \n",
      "<<<iteration:[60/657] - total_loss: 0.1541  obj_loss: 0.0838  noobj_loss: 0.0550  bbox_loss: 0.0073  cls_loss: 0.0062  \n",
      "<<<iteration:[80/657] - total_loss: 0.1497  obj_loss: 0.0843  noobj_loss: 0.0512  bbox_loss: 0.0065  cls_loss: 0.0072  \n",
      "<<<iteration:[100/657] - total_loss: 0.1454  obj_loss: 0.0826  noobj_loss: 0.0509  bbox_loss: 0.0061  cls_loss: 0.0067  \n",
      "<<<iteration:[120/657] - total_loss: 0.1591  obj_loss: 0.0849  noobj_loss: 0.0538  bbox_loss: 0.0076  cls_loss: 0.0092  \n",
      "<<<iteration:[140/657] - total_loss: 0.1375  obj_loss: 0.0744  noobj_loss: 0.0463  bbox_loss: 0.0063  cls_loss: 0.0087  \n",
      "<<<iteration:[160/657] - total_loss: 0.1428  obj_loss: 0.0791  noobj_loss: 0.0477  bbox_loss: 0.0066  cls_loss: 0.0070  \n",
      "<<<iteration:[180/657] - total_loss: 0.1599  obj_loss: 0.0913  noobj_loss: 0.0558  bbox_loss: 0.0066  cls_loss: 0.0075  \n",
      "<<<iteration:[200/657] - total_loss: 0.1484  obj_loss: 0.0812  noobj_loss: 0.0473  bbox_loss: 0.0070  cls_loss: 0.0086  \n",
      "<<<iteration:[220/657] - total_loss: 0.1387  obj_loss: 0.0707  noobj_loss: 0.0490  bbox_loss: 0.0073  cls_loss: 0.0072  \n",
      "<<<iteration:[240/657] - total_loss: 0.1550  obj_loss: 0.0763  noobj_loss: 0.0540  bbox_loss: 0.0087  cls_loss: 0.0082  \n",
      "<<<iteration:[260/657] - total_loss: 0.1477  obj_loss: 0.0819  noobj_loss: 0.0513  bbox_loss: 0.0066  cls_loss: 0.0072  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[280/657] - total_loss: 0.1445  obj_loss: 0.0789  noobj_loss: 0.0563  bbox_loss: 0.0059  cls_loss: 0.0077  \n",
      "<<<iteration:[300/657] - total_loss: 0.1606  obj_loss: 0.0885  noobj_loss: 0.0532  bbox_loss: 0.0071  cls_loss: 0.0098  \n",
      "<<<iteration:[320/657] - total_loss: 0.1287  obj_loss: 0.0668  noobj_loss: 0.0483  bbox_loss: 0.0062  cls_loss: 0.0067  \n",
      "<<<iteration:[340/657] - total_loss: 0.1427  obj_loss: 0.0786  noobj_loss: 0.0505  bbox_loss: 0.0065  cls_loss: 0.0066  \n",
      "<<<iteration:[360/657] - total_loss: 0.1374  obj_loss: 0.0719  noobj_loss: 0.0457  bbox_loss: 0.0067  cls_loss: 0.0091  \n",
      "<<<iteration:[380/657] - total_loss: 0.1410  obj_loss: 0.0711  noobj_loss: 0.0536  bbox_loss: 0.0069  cls_loss: 0.0085  \n",
      "<<<iteration:[400/657] - total_loss: 0.1567  obj_loss: 0.0812  noobj_loss: 0.0499  bbox_loss: 0.0084  cls_loss: 0.0085  \n",
      "<<<iteration:[420/657] - total_loss: 0.1467  obj_loss: 0.0764  noobj_loss: 0.0546  bbox_loss: 0.0067  cls_loss: 0.0092  \n",
      "<<<iteration:[440/657] - total_loss: 0.1550  obj_loss: 0.0866  noobj_loss: 0.0541  bbox_loss: 0.0069  cls_loss: 0.0069  \n",
      "<<<iteration:[460/657] - total_loss: 0.1380  obj_loss: 0.0731  noobj_loss: 0.0499  bbox_loss: 0.0064  cls_loss: 0.0080  \n",
      "<<<iteration:[480/657] - total_loss: 0.1351  obj_loss: 0.0668  noobj_loss: 0.0483  bbox_loss: 0.0075  cls_loss: 0.0068  \n",
      "<<<iteration:[500/657] - total_loss: 0.1551  obj_loss: 0.0872  noobj_loss: 0.0519  bbox_loss: 0.0064  cls_loss: 0.0098  \n",
      "<<<iteration:[520/657] - total_loss: 0.1415  obj_loss: 0.0739  noobj_loss: 0.0532  bbox_loss: 0.0068  cls_loss: 0.0071  \n",
      "<<<iteration:[540/657] - total_loss: 0.1496  obj_loss: 0.0746  noobj_loss: 0.0462  bbox_loss: 0.0088  cls_loss: 0.0081  \n",
      "<<<iteration:[560/657] - total_loss: 0.1663  obj_loss: 0.0952  noobj_loss: 0.0587  bbox_loss: 0.0069  cls_loss: 0.0072  \n",
      "<<<iteration:[580/657] - total_loss: 0.1317  obj_loss: 0.0655  noobj_loss: 0.0433  bbox_loss: 0.0075  cls_loss: 0.0071  \n",
      "<<<iteration:[600/657] - total_loss: 0.1418  obj_loss: 0.0708  noobj_loss: 0.0478  bbox_loss: 0.0075  cls_loss: 0.0096  \n",
      "<<<iteration:[620/657] - total_loss: 0.1492  obj_loss: 0.0859  noobj_loss: 0.0496  bbox_loss: 0.0063  cls_loss: 0.0069  \n",
      "<<<iteration:[640/657] - total_loss: 0.1543  obj_loss: 0.0826  noobj_loss: 0.0553  bbox_loss: 0.0072  cls_loss: 0.0080  \n",
      "\n",
      "epoch:99/100 - Train Loss: 0.1469, Val Loss: 0.2157\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.1586  obj_loss: 0.0823  noobj_loss: 0.0537  bbox_loss: 0.0081  cls_loss: 0.0088  \n",
      "<<<iteration:[40/657] - total_loss: 0.1373  obj_loss: 0.0778  noobj_loss: 0.0433  bbox_loss: 0.0062  cls_loss: 0.0068  \n",
      "<<<iteration:[60/657] - total_loss: 0.1544  obj_loss: 0.0813  noobj_loss: 0.0641  bbox_loss: 0.0068  cls_loss: 0.0072  \n",
      "<<<iteration:[80/657] - total_loss: 0.1468  obj_loss: 0.0831  noobj_loss: 0.0507  bbox_loss: 0.0062  cls_loss: 0.0076  \n",
      "<<<iteration:[100/657] - total_loss: 0.1325  obj_loss: 0.0739  noobj_loss: 0.0444  bbox_loss: 0.0060  cls_loss: 0.0064  \n",
      "<<<iteration:[120/657] - total_loss: 0.1432  obj_loss: 0.0797  noobj_loss: 0.0494  bbox_loss: 0.0063  cls_loss: 0.0072  \n",
      "<<<iteration:[140/657] - total_loss: 0.1646  obj_loss: 0.0914  noobj_loss: 0.0632  bbox_loss: 0.0065  cls_loss: 0.0092  \n",
      "<<<iteration:[160/657] - total_loss: 0.1403  obj_loss: 0.0726  noobj_loss: 0.0547  bbox_loss: 0.0066  cls_loss: 0.0076  \n",
      "<<<iteration:[180/657] - total_loss: 0.1420  obj_loss: 0.0750  noobj_loss: 0.0520  bbox_loss: 0.0067  cls_loss: 0.0072  \n",
      "<<<iteration:[200/657] - total_loss: 0.1359  obj_loss: 0.0711  noobj_loss: 0.0458  bbox_loss: 0.0071  cls_loss: 0.0066  \n",
      "<<<iteration:[220/657] - total_loss: 0.1469  obj_loss: 0.0729  noobj_loss: 0.0510  bbox_loss: 0.0081  cls_loss: 0.0081  \n",
      "<<<iteration:[240/657] - total_loss: 0.1455  obj_loss: 0.0736  noobj_loss: 0.0536  bbox_loss: 0.0074  cls_loss: 0.0080  \n",
      "<<<iteration:[260/657] - total_loss: 0.1527  obj_loss: 0.0819  noobj_loss: 0.0502  bbox_loss: 0.0074  cls_loss: 0.0085  \n",
      "<<<iteration:[280/657] - total_loss: 0.1506  obj_loss: 0.0804  noobj_loss: 0.0559  bbox_loss: 0.0066  cls_loss: 0.0093  \n",
      "<<<iteration:[300/657] - total_loss: 0.1458  obj_loss: 0.0827  noobj_loss: 0.0473  bbox_loss: 0.0065  cls_loss: 0.0066  \n",
      "<<<iteration:[320/657] - total_loss: 0.1496  obj_loss: 0.0855  noobj_loss: 0.0509  bbox_loss: 0.0065  cls_loss: 0.0062  \n",
      "<<<iteration:[340/657] - total_loss: 0.1406  obj_loss: 0.0697  noobj_loss: 0.0453  bbox_loss: 0.0081  cls_loss: 0.0078  \n",
      "<<<iteration:[360/657] - total_loss: 0.1456  obj_loss: 0.0743  noobj_loss: 0.0509  bbox_loss: 0.0073  cls_loss: 0.0091  \n",
      "<<<iteration:[380/657] - total_loss: 0.1525  obj_loss: 0.0777  noobj_loss: 0.0551  bbox_loss: 0.0071  cls_loss: 0.0115  \n",
      "<<<iteration:[400/657] - total_loss: 0.1581  obj_loss: 0.0865  noobj_loss: 0.0493  bbox_loss: 0.0078  cls_loss: 0.0079  \n",
      "<<<iteration:[420/657] - total_loss: 0.1446  obj_loss: 0.0791  noobj_loss: 0.0515  bbox_loss: 0.0064  cls_loss: 0.0077  \n",
      "<<<iteration:[440/657] - total_loss: 0.1279  obj_loss: 0.0680  noobj_loss: 0.0477  bbox_loss: 0.0060  cls_loss: 0.0059  \n",
      "<<<iteration:[460/657] - total_loss: 0.1531  obj_loss: 0.0827  noobj_loss: 0.0537  bbox_loss: 0.0070  cls_loss: 0.0086  \n",
      "<<<iteration:[480/657] - total_loss: 0.1343  obj_loss: 0.0680  noobj_loss: 0.0461  bbox_loss: 0.0071  cls_loss: 0.0076  \n",
      "<<<iteration:[500/657] - total_loss: 0.1468  obj_loss: 0.0790  noobj_loss: 0.0455  bbox_loss: 0.0073  cls_loss: 0.0087  \n",
      "<<<iteration:[520/657] - total_loss: 0.1401  obj_loss: 0.0789  noobj_loss: 0.0487  bbox_loss: 0.0060  cls_loss: 0.0069  \n",
      "<<<iteration:[540/657] - total_loss: 0.1477  obj_loss: 0.0814  noobj_loss: 0.0496  bbox_loss: 0.0068  cls_loss: 0.0075  \n",
      "<<<iteration:[560/657] - total_loss: 0.1569  obj_loss: 0.0845  noobj_loss: 0.0535  bbox_loss: 0.0074  cls_loss: 0.0084  \n",
      "<<<iteration:[580/657] - total_loss: 0.1396  obj_loss: 0.0793  noobj_loss: 0.0442  bbox_loss: 0.0059  cls_loss: 0.0087  \n",
      "<<<iteration:[600/657] - total_loss: 0.1457  obj_loss: 0.0805  noobj_loss: 0.0469  bbox_loss: 0.0067  cls_loss: 0.0081  \n",
      "<<<iteration:[620/657] - total_loss: 0.1543  obj_loss: 0.0883  noobj_loss: 0.0529  bbox_loss: 0.0062  cls_loss: 0.0086  \n",
      "<<<iteration:[640/657] - total_loss: 0.1370  obj_loss: 0.0741  noobj_loss: 0.0472  bbox_loss: 0.0064  cls_loss: 0.0070  \n",
      "\n",
      "epoch:100/100 - Train Loss: 0.1459, Val Loss: 0.2144\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c02bd030b5a432492bfb7ae81ddf177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train bbox Loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train class Loss</td><td>█▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train obj Loss</td><td>▁▃▄▄▅▆▇▇█████▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▄</td></tr><tr><td>Val Loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val bbox Loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val class Loss</td><td>█▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val obj Loss</td><td>▁▂▃▄▄▅▇▇█▇█▇████▇▇▇▇▆▇▇▇▇▇▆▇▇▆▆▇▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>0.14586</td></tr><tr><td>Train bbox Loss</td><td>0.00683</td></tr><tr><td>Train class Loss</td><td>0.00788</td></tr><tr><td>Train obj Loss</td><td>0.07852</td></tr><tr><td>Val Loss</td><td>0.21441</td></tr><tr><td>Val bbox Loss</td><td>0.01237</td></tr><tr><td>Val class Loss</td><td>0.01651</td></tr><tr><td>Val obj Loss</td><td>0.10315</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-bird-3</strong> at: <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/q3h05unm' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/q3h05unm</a><br/> View job at <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDkxNzUzMA==/version_details/v1' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDkxNzUzMA==/version_details/v1</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231103_142641-q3h05unm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_epoch = 0\n",
    "best_score = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders, model, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "#     train_loss[\"obj_loss\"] += obj_loss\n",
    "#     train_loss[\"noobj_loss\"] += noobj_loss\n",
    "#     train_loss[\"bbox_loss\"] += bbox_loss\n",
    "#     train_loss[\"cls_loss\"] += cls_loss\n",
    "    wandb.log({\"Train Loss\": train_loss['total_loss'],\n",
    "               \"Train obj Loss\":train_loss[\"obj_loss\"],\n",
    "               \"Train bbox Loss\":train_loss[\"bbox_loss\"],\n",
    "               \"Train class Loss\":train_loss[\"cls_loss\"],\n",
    "               \"Val Loss\": val_loss['total_loss'],\n",
    "               \"Val obj Loss\":val_loss[\"obj_loss\"],\n",
    "               \"Val bbox Loss\":val_loss[\"bbox_loss\"],\n",
    "               \"Val class Loss\":val_loss[\"cls_loss\"],})\n",
    "    print(f\"\\nepoch:{epoch+1}/{num_epochs} - Train Loss: {train_loss['total_loss']:.4f}, Val Loss: {val_loss['total_loss']:.4f}\\n\")\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model(model.state_dict(), f'model_{epoch+1}.pth', save_dir=f\"./trained_model/{BACKBONE}_{PART}_LR{LR}_IP{PATCH_FACTOR}_pretrained\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637805f",
   "metadata": {},
   "source": [
    "## Test Dataset Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb53d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d867479",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0594e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, num_classes, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "#     model = YOLO_SWIN(num_classes=num_classes)\n",
    "    model=resnet50_cbam(pretrained=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af5b0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=448\n",
    "transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1e6b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path=\"./trained_model/YOLO_SWIN_T_body_LR0.0001_AUG30/model_90.pth\"\n",
    "ckpt_path=\"/workspace/Plastic_Bottle_defect_detection/trained_model/YOLO_RESNET_CBAM_neck_LR0.0001_IP50_Pretrained/model_100.pth\"\n",
    "model = load_model(ckpt_path, NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb2aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making augmented images-- augmented factor:0\n",
      "total length of augmented images: 0\n"
     ]
    }
   ],
   "source": [
    "NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "test_dataset=PET_dataset(\"neck\" ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='test', transformer=transformer, aug=None)\n",
    "test_dataloaders = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a2e8599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adf189c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_predict(image, model, conf_thres=0.2, iou_threshold=0.1):\n",
    "    predictions = model(image)\n",
    "    prediction = predictions.detach().cpu().squeeze(dim=0)\n",
    "    f_map=prediction\n",
    "\n",
    "#     print(prediction.shape)\n",
    "    \n",
    "    grid_size = prediction.shape[-1]\n",
    "    y_grid, x_grid = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size))\n",
    "    stride_size = IMAGE_SIZE/grid_size\n",
    "\n",
    "    conf = prediction[[0,5], ...].reshape(1, -1)\n",
    "    xc = (prediction[[1,6], ...] * IMAGE_SIZE + x_grid*stride_size).reshape(1,-1)\n",
    "    yc = (prediction[[2,7], ...] * IMAGE_SIZE + y_grid*stride_size).reshape(1,-1)\n",
    "    w = (prediction[[3,8], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    h = (prediction[[4,9], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    cls = torch.max(prediction[10:, ...].reshape(NUM_CLASSES, -1), dim=0).indices.tile(1,2)\n",
    "    \n",
    "    x_min = xc - w/2\n",
    "    y_min = yc - h/2\n",
    "    x_max = xc + w/2\n",
    "    y_max = yc + h/2\n",
    "\n",
    "    prediction_res = torch.cat([x_min, y_min, x_max, y_max, conf, cls], dim=0)\n",
    "    prediction_res = prediction_res.transpose(0,1)\n",
    "\n",
    "    # x_min과 y_min이 음수가 되지않고, x_max와 y_max가 이미지 크기를 넘지 않게 제한\n",
    "    prediction_res[:, 2].clip(min=0, max=image.shape[1]) \n",
    "    prediction_res[:, 3].clip(min=0, max=image.shape[0])\n",
    "        \n",
    "    pred_res = prediction_res[prediction_res[:, 4] > conf_thres]\n",
    "    nms_index = torchvision.ops.nms(boxes=pred_res[:, 0:4], scores=pred_res[:, 4], iou_threshold=iou_threshold)\n",
    "    pred_res_ = pred_res[nms_index].numpy()\n",
    "    \n",
    "    n_obj = pred_res_.shape[0]\n",
    "    bboxes = np.zeros(shape=(n_obj, 4), dtype=np.float32)\n",
    "    bboxes[:, 0:2] = (pred_res_[:, 0:2] + pred_res_[:, 2:4]) / 2\n",
    "    bboxes[:, 2:4] = pred_res_[:, 2:4] - pred_res_[:, 0:2]\n",
    "    scores = pred_res_[:, 4]\n",
    "    class_ids = pred_res_[:, 5]\n",
    "    \n",
    "    # 이미지 값이 들어가면 모델을 통해서, 후처리까지 포함된 yolo 포멧의 box좌표, 그 좌표에 대한 confidence score\n",
    "    # 그리고 class id를 반환\n",
    "    return bboxes, scores, class_ids,f_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4b23b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_images = []\n",
    "pred_labels =[]\n",
    "feature_maps=[]\n",
    "\n",
    "for index, batch in enumerate(test_dataloaders):\n",
    "    images = batch[0].to(device)\n",
    "    bboxes, scores, class_ids, fmap = model_predict(images, model, conf_thres=0.1, iou_threshold=0.1)\n",
    "    \n",
    "    if len(bboxes) > 0:\n",
    "        prediction_yolo = np.concatenate([bboxes, scores[:, np.newaxis], class_ids[:, np.newaxis]], axis=1)\n",
    "    else:\n",
    "        prediction_yolo = np.array([])\n",
    "    \n",
    "    # 텐서형의 이미지를 다시 unnormalize를 시키고, 다시 chw를 hwc로 바꾸고 넘파이로 바꾼다.\n",
    "    np_image = make_grid(images[0], normalize=True).cpu().permute(1,2,0).numpy()\n",
    "    pred_images.append(np_image)\n",
    "    pred_labels.append(prediction_yolo)\n",
    "    feature_maps.append(fmap)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c8d0fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cea3903a8044298dc8f087905e4a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=24), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(index=(0,len(pred_images)-1))\n",
    "def show_result(index=0):\n",
    "    print(pred_labels[index])\n",
    "    if len(pred_labels[index]) > 0:\n",
    "        result = visualize(pred_images[index], pred_labels[index][:, 0:4], pred_labels[index][:, 5])\n",
    "    else:\n",
    "        result = pred_images[index]\n",
    "        \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e21a11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e8fa0f0c7741b4a6d8a477ee7831d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=24), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#feature map에서 0,5번쨰에 해당하는 objectness 투사\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(index=(0,len(pred_images)-1))\n",
    "def show_result(index=0):\n",
    "    print(pred_labels[index])\n",
    "    if len(pred_labels[index]) > 0:\n",
    "        result = visualize(pred_images[index], pred_labels[index][:, 0:4], pred_labels[index][:, 5])\n",
    "    else:\n",
    "        result = pred_images[index]\n",
    "    \n",
    "    f_map=feature_maps[index]\n",
    "    zero_canvas=np.zeros((448,448))\n",
    "\n",
    "    cv_re1=cv2.resize(f_map[0,:,:].numpy(),(448,448))\n",
    "    cv_re2=cv2.resize(f_map[5,:,:].numpy(),(448,448))\n",
    "    zero_canvas=zero_canvas+cv_re1+cv_re2\n",
    "\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    rows = 1\n",
    "    cols = 2\n",
    "    ax1 = fig.add_subplot(rows, cols, 1)\n",
    "    ax1.imshow(result)\n",
    "    ax1.set_title('Detection')\n",
    "    ax1.axis(\"off\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(rows, cols, 2)\n",
    "    ax2.imshow(zero_canvas)\n",
    "    ax2.set_title('feature map-objectness')\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c749262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10c99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de590e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
