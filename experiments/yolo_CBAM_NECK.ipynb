{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8153b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2938f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6998f56",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a602257",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'Unformed': 0, 'Burr': 1}\n",
    "CLASS_ID_TO_NAME = {0: 'Unformed', 1: 'Burr'}\n",
    "BOX_COLOR = {'Unformed':(200, 0, 0), 'Burr':(0, 0, 200)}\n",
    "TEXT_COLOR = (255, 255, 255)\n",
    "\n",
    "def save_model(model_state, model_name, save_dir=\"./trained_model\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))\n",
    "\n",
    "\n",
    "def visualize_bbox(image, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    x_center, y_center, w, h = bbox\n",
    "    x_min = int(x_center - w/2)\n",
    "    y_min = int(y_center - h/2)\n",
    "    x_max = int(x_center + w/2)\n",
    "    y_max = int(y_center + h/2)\n",
    "    \n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color[class_name], thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(image, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color[class_name], -1)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "#         print('category_id: ',category_id)\n",
    "        class_name = CLASS_ID_TO_NAME[category_id.item()]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957496f",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "395a1431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PET_dataset():\n",
    "    def __init__(self,part,neck_dir,body_dir,phase, transformer=None, aug=None, aug_factor=0):\n",
    "        self.neck_dir=neck_dir\n",
    "        self.body_dir=body_dir\n",
    "        self.part=part\n",
    "        self.phase=phase\n",
    "        self.transformer=transformer\n",
    "        self.aug=aug\n",
    "        self.aug_factor=aug_factor\n",
    "        if(self.part==\"body\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.body_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.body_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        elif(self.part==\"neck\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.neck_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.neck_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        \n",
    "        self.auged_img_list, self.auged_label_list=self.make_aug_list(self.image_files, self.label_files)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if(self.aug==None):\n",
    "            filename, image = self.get_image(self.part, index)\n",
    "            bboxes, class_ids = self.get_label(self.part, index)\n",
    "\n",
    "            if(self.transformer):\n",
    "                transformed_data=self.transformer(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "                image = transformed_data['image']\n",
    "                bboxes = np.array(transformed_data['bboxes'])\n",
    "                class_ids = np.array(transformed_data['class_ids'])\n",
    "\n",
    "\n",
    "            target = {}\n",
    "    #         print(f'bboxes:{bboxes}\\nclass_ids:{class_ids}\\nlen_bboxes:{len(bboxes)}\\nlen_class_ids:{len(class_ids)}')\n",
    "    #         print(f'filename: {filename}')\n",
    "            target[\"boxes\"] = torch.Tensor(bboxes).float()\n",
    "            target[\"labels\"] = torch.Tensor(class_ids).long()\n",
    "\n",
    "            ###\n",
    "            bboxes=torch.Tensor(bboxes).float()\n",
    "            class_ids=torch.Tensor(class_ids).long()\n",
    "            target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "            ###\n",
    "        else:\n",
    "            image=self.auged_img_list[index][1]\n",
    "            target=self.auged_label_list[index]\n",
    "            filename=self.auged_img_list[index][0]\n",
    "        return image, target, filename\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        length=0\n",
    "        if(self.aug==None):\n",
    "            length=len(self.image_files)\n",
    "        else:\n",
    "            length=len(self.auged_img_list)\n",
    "        return length\n",
    "    \n",
    "    def make_aug_list(self,ori_image_list,ori_label_files):\n",
    "        aug_image_list=[]\n",
    "        aug_label_list=[]\n",
    "        \n",
    "        print(f\"start making augmented images-- augmented factor:{self.aug_factor}\")\n",
    "        for i in range(len(ori_image_list)):\n",
    "            filename, ori_image = self.get_image(self.part, i)\n",
    "            ori_bboxes, ori_class_ids = self.get_label(self.part, i)\n",
    "            for j in range(self.aug_factor):\n",
    "                auged_data=self.aug(image=ori_image, bboxes=ori_bboxes, class_ids=ori_class_ids)\n",
    "                image = auged_data['image']\n",
    "                bboxes = np.array(auged_data['bboxes'])\n",
    "                class_ids = np.array(auged_data['class_ids'])\n",
    "                \n",
    "                bboxes=torch.Tensor(bboxes).float()\n",
    "                class_ids=torch.Tensor(class_ids).long()\n",
    "                \n",
    "                aug_image_list.append((filename, image))\n",
    "                aug_label_list.append(np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1))\n",
    "        \n",
    "        print(f\"total length of augmented images: {len(aug_image_list)}\")\n",
    "        \n",
    "        return aug_image_list, aug_label_list\n",
    "        \n",
    "    \n",
    "    def get_image(self, part, index): # 이미지 불러오는 함수\n",
    "        filename = self.image_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body called!-> {self.part}\")\n",
    "            image_path = self.body_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck called!-> {self.part}\")\n",
    "            image_path = self.neck_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return filename, image\n",
    "    \n",
    "    def get_label(self, part, index): # label (box좌표, class_id) 불러오는 함수\n",
    "        label_filename=self.label_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body label called!-> {self.part}\")\n",
    "            label_path = self.body_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck label called!-> {self.part}\")\n",
    "            label_path = self.neck_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        with open(label_path, 'r') as file:\n",
    "            labels = file.readlines()\n",
    "        \n",
    "        class_ids=[]\n",
    "        bboxes=[]\n",
    "        for label in labels:\n",
    "            label=label.replace(\"\\n\", \"\")\n",
    "            obj=label.split(' ')[0]\n",
    "            coor=label.split(' ')[1:]\n",
    "            obj=int(obj)\n",
    "            coor=list(map(float, coor))\n",
    "            class_ids.append(obj)\n",
    "            bboxes.append(coor)\n",
    "            \n",
    "        return bboxes, class_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "214f686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 448\n",
    "\n",
    "transformer = A.Compose([ \n",
    "        # bounding box의 변환, augmentation에서 albumentations는 Detection 학습을 할 때 굉장히 유용하다. \n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        # albumentations 라이브러리에서는 Normalization을 먼저 진행해 주고 tensor화를 진행해 주어야한다.\n",
    "    ],\n",
    "    # box 위치에 대한 transformation도 함께 진행된다. \n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "augmentator=A.Compose([\n",
    "#     A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.7),\n",
    "#     A.Sharpen(p=0.7),\n",
    "    A.BBoxSafeRandomCrop(p=0.6),\n",
    "    A.VerticalFlip (p=0.5),\n",
    "    A.HueSaturationValue(p=0.5),\n",
    "    A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), target_list, filename_list\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d997e857",
   "metadata": {},
   "source": [
    "# NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "# BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "\n",
    "NECK_PATH = '/home/host_data/PET_data_IP_AUG/aug_patched_Neck/'\n",
    "BODY_PATH = '/home/host_data/PET_data_image_patching/Body'\n",
    "# trainset_yes_aug=PET_dataset(part='neck',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=augmentator, aug_factor=5)\n",
    "trainset_no_aug=PET_dataset(part='neck',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='valid', transformer=transformer, aug=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "745d4296",
   "metadata": {},
   "source": [
    "len(trainset_no_aug)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08066fa2",
   "metadata": {},
   "source": [
    "@interact(index=(0, len(trainset_no_aug)-1))\n",
    "\n",
    "def show_sample(index=0):\n",
    "    image, target, filename = trainset_no_aug[index]\n",
    "    image=image.permute(1,2,0).numpy()\n",
    "    img_H, img_W, _ = image.shape\n",
    "    print(filename)\n",
    "    print(image.shape)\n",
    "#     print(image)\n",
    "\n",
    "#     bboxes = target['boxes']\n",
    "#     class_ids = target[\"labels\"]\n",
    "    \n",
    "    ###\n",
    "    bboxes = target[:, 0:4]\n",
    "    class_ids = target[:, 4]\n",
    "    ###\n",
    "    bboxes[:, [0,2]] *= img_W\n",
    "    bboxes[:, [1,3]] *= img_H\n",
    "\n",
    "    canvas = visualize(image, bboxes, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show_sample()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39a26f95",
   "metadata": {},
   "source": [
    "@interact(index=(0, len(trainset_yes_aug)-1))\n",
    "\n",
    "def show_sample(index=0):\n",
    "    image, target, filename = trainset_yes_aug[index]\n",
    "    image=image.permute(1,2,0).numpy()\n",
    "    img_H, img_W, _ = image.shape\n",
    "    print(filename)\n",
    "    print(image.shape)\n",
    "#     print(image)\n",
    "\n",
    "#     bboxes = target['boxes']\n",
    "#     class_ids = target[\"labels\"]\n",
    "    ###\n",
    "    bboxes = target[:, 0:4]\n",
    "    class_ids = target[:, 4]\n",
    "    ###\n",
    "    bboxes[:, [0,2]] *= img_W\n",
    "    bboxes[:, [1,3]] *= img_H\n",
    "    print(bboxes)\n",
    "\n",
    "    canvas = visualize(image, bboxes, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eba35b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ba29e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "            elif pool_type=='lp':\n",
    "                lp_pool = F.lp_pool2d( x, 2, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( lp_pool )\n",
    "            elif pool_type=='lse':\n",
    "                # LSE pool only\n",
    "                lse_pool = logsumexp_2d(x)\n",
    "                channel_att_raw = self.mlp( lse_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "\n",
    "        scale = F.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return x * scale\n",
    "\n",
    "def logsumexp_2d(tensor):\n",
    "    tensor_flatten = tensor.view(tensor.size(0), tensor.size(1), -1)\n",
    "    s, _ = torch.max(tensor_flatten, dim=2, keepdim=True)\n",
    "    outputs = s + (tensor_flatten - s).exp().sum(dim=2, keepdim=True).log()\n",
    "    return outputs\n",
    "\n",
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n",
    "\n",
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        self.spatial = BasicConv(2, 1, kernel_size, stride=1, padding=(kernel_size-1) // 2, relu=False)\n",
    "    def forward(self, x):\n",
    "        x_compress = self.compress(x)\n",
    "        x_out = self.spatial(x_compress)\n",
    "        scale = F.sigmoid(x_out) # broadcasting\n",
    "        return x * scale\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.no_spatial=no_spatial\n",
    "        if not no_spatial:\n",
    "            self.SpatialGate = SpatialGate()\n",
    "    def forward(self, x):\n",
    "        x_out = self.ChannelGate(x)\n",
    "        if not self.no_spatial:\n",
    "            x_out = self.SpatialGate(x_out)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "258ceb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.nn import init\n",
    "# from .cbam import *\n",
    "# from .bam import *\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_cbam=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        if use_cbam:\n",
    "            self.cbam = CBAM( planes, 16 )\n",
    "        else:\n",
    "            self.cbam = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        if not self.cbam is None:\n",
    "            out = self.cbam(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_cbam=False):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        if use_cbam:\n",
    "            self.cbam = CBAM( planes * 4, 16 )\n",
    "        else:\n",
    "            self.cbam = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        if not self.cbam is None:\n",
    "            out = self.cbam(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers,  network_type, num_classes, att_type=None):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.network_type = network_type\n",
    "        # different model config between ImageNet and CIFAR \n",
    "        if network_type == \"ImageNet\":\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            self.avgpool = nn.AvgPool2d(7)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if att_type=='BAM':\n",
    "            self.bam1 = BAM(64*block.expansion)\n",
    "            self.bam2 = BAM(128*block.expansion)\n",
    "            self.bam3 = BAM(256*block.expansion)\n",
    "        else:\n",
    "            self.bam1, self.bam2, self.bam3 = None, None, None\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64,  layers[0], att_type=att_type)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, att_type=att_type)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, att_type=att_type)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, att_type=att_type)\n",
    "\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(512, 12, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        init.kaiming_normal(self.fc.weight)\n",
    "        for key in self.state_dict():\n",
    "            if key.split('.')[-1]==\"weight\":\n",
    "                if \"conv\" in key:\n",
    "                    init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n",
    "                if \"bn\" in key:\n",
    "                    if \"SpatialGate\" in key:\n",
    "                        self.state_dict()[key][...] = 0\n",
    "                    else:\n",
    "                        self.state_dict()[key][...] = 1\n",
    "            elif key.split(\".\")[-1]=='bias':\n",
    "                self.state_dict()[key][...] = 0\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, att_type=None):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, use_cbam=att_type=='CBAM'))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, use_cbam=att_type=='CBAM'))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        if self.network_type == \"ImageNet\":\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        if not self.bam1 is None:\n",
    "            x = self.bam1(x)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        if not self.bam2 is None:\n",
    "            x = self.bam2(x)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        if not self.bam3 is None:\n",
    "            x = self.bam3(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "#         if self.network_type == \"ImageNet\":\n",
    "#             x = self.avgpool(x)\n",
    "#         else:\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        x = self.final_conv(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def ResidualNet(network_type, depth, num_classes, att_type):\n",
    "\n",
    "    assert network_type in [\"ImageNet\", \"CIFAR10\", \"CIFAR100\"], \"network type should be ImageNet or CIFAR10 / CIFAR100\"\n",
    "    assert depth in [18, 34, 50, 101], 'network depth should be 18, 34, 50 or 101'\n",
    "\n",
    "    if depth == 18:\n",
    "        model = ResNet(BasicBlock, [2, 2, 2, 2], network_type, num_classes, att_type)\n",
    "\n",
    "    elif depth == 34:\n",
    "        model = ResNet(BasicBlock, [3, 4, 6, 3], network_type, num_classes, att_type)\n",
    "\n",
    "    elif depth == 50:\n",
    "        model = ResNet(Bottleneck, [3, 4, 6, 3], network_type, num_classes, att_type)\n",
    "\n",
    "    elif depth == 101:\n",
    "        model = ResNet(Bottleneck, [3, 4, 23, 3], network_type, num_classes, att_type)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4848e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.cbam.ChannelGate.mlp.1.weight', 'layer1.0.cbam.ChannelGate.mlp.1.bias', 'layer1.0.cbam.ChannelGate.mlp.3.weight', 'layer1.0.cbam.ChannelGate.mlp.3.bias', 'layer1.0.cbam.SpatialGate.spatial.conv.weight', 'layer1.0.cbam.SpatialGate.spatial.bn.weight', 'layer1.0.cbam.SpatialGate.spatial.bn.bias', 'layer1.0.cbam.SpatialGate.spatial.bn.running_mean', 'layer1.0.cbam.SpatialGate.spatial.bn.running_var', 'layer1.0.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.cbam.ChannelGate.mlp.1.weight', 'layer1.1.cbam.ChannelGate.mlp.1.bias', 'layer1.1.cbam.ChannelGate.mlp.3.weight', 'layer1.1.cbam.ChannelGate.mlp.3.bias', 'layer1.1.cbam.SpatialGate.spatial.conv.weight', 'layer1.1.cbam.SpatialGate.spatial.bn.weight', 'layer1.1.cbam.SpatialGate.spatial.bn.bias', 'layer1.1.cbam.SpatialGate.spatial.bn.running_mean', 'layer1.1.cbam.SpatialGate.spatial.bn.running_var', 'layer1.1.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.cbam.ChannelGate.mlp.1.weight', 'layer1.2.cbam.ChannelGate.mlp.1.bias', 'layer1.2.cbam.ChannelGate.mlp.3.weight', 'layer1.2.cbam.ChannelGate.mlp.3.bias', 'layer1.2.cbam.SpatialGate.spatial.conv.weight', 'layer1.2.cbam.SpatialGate.spatial.bn.weight', 'layer1.2.cbam.SpatialGate.spatial.bn.bias', 'layer1.2.cbam.SpatialGate.spatial.bn.running_mean', 'layer1.2.cbam.SpatialGate.spatial.bn.running_var', 'layer1.2.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.0.cbam.ChannelGate.mlp.1.weight', 'layer2.0.cbam.ChannelGate.mlp.1.bias', 'layer2.0.cbam.ChannelGate.mlp.3.weight', 'layer2.0.cbam.ChannelGate.mlp.3.bias', 'layer2.0.cbam.SpatialGate.spatial.conv.weight', 'layer2.0.cbam.SpatialGate.spatial.bn.weight', 'layer2.0.cbam.SpatialGate.spatial.bn.bias', 'layer2.0.cbam.SpatialGate.spatial.bn.running_mean', 'layer2.0.cbam.SpatialGate.spatial.bn.running_var', 'layer2.0.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.cbam.ChannelGate.mlp.1.weight', 'layer2.1.cbam.ChannelGate.mlp.1.bias', 'layer2.1.cbam.ChannelGate.mlp.3.weight', 'layer2.1.cbam.ChannelGate.mlp.3.bias', 'layer2.1.cbam.SpatialGate.spatial.conv.weight', 'layer2.1.cbam.SpatialGate.spatial.bn.weight', 'layer2.1.cbam.SpatialGate.spatial.bn.bias', 'layer2.1.cbam.SpatialGate.spatial.bn.running_mean', 'layer2.1.cbam.SpatialGate.spatial.bn.running_var', 'layer2.1.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.cbam.ChannelGate.mlp.1.weight', 'layer2.2.cbam.ChannelGate.mlp.1.bias', 'layer2.2.cbam.ChannelGate.mlp.3.weight', 'layer2.2.cbam.ChannelGate.mlp.3.bias', 'layer2.2.cbam.SpatialGate.spatial.conv.weight', 'layer2.2.cbam.SpatialGate.spatial.bn.weight', 'layer2.2.cbam.SpatialGate.spatial.bn.bias', 'layer2.2.cbam.SpatialGate.spatial.bn.running_mean', 'layer2.2.cbam.SpatialGate.spatial.bn.running_var', 'layer2.2.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.cbam.ChannelGate.mlp.1.weight', 'layer2.3.cbam.ChannelGate.mlp.1.bias', 'layer2.3.cbam.ChannelGate.mlp.3.weight', 'layer2.3.cbam.ChannelGate.mlp.3.bias', 'layer2.3.cbam.SpatialGate.spatial.conv.weight', 'layer2.3.cbam.SpatialGate.spatial.bn.weight', 'layer2.3.cbam.SpatialGate.spatial.bn.bias', 'layer2.3.cbam.SpatialGate.spatial.bn.running_mean', 'layer2.3.cbam.SpatialGate.spatial.bn.running_var', 'layer2.3.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.0.cbam.ChannelGate.mlp.1.weight', 'layer3.0.cbam.ChannelGate.mlp.1.bias', 'layer3.0.cbam.ChannelGate.mlp.3.weight', 'layer3.0.cbam.ChannelGate.mlp.3.bias', 'layer3.0.cbam.SpatialGate.spatial.conv.weight', 'layer3.0.cbam.SpatialGate.spatial.bn.weight', 'layer3.0.cbam.SpatialGate.spatial.bn.bias', 'layer3.0.cbam.SpatialGate.spatial.bn.running_mean', 'layer3.0.cbam.SpatialGate.spatial.bn.running_var', 'layer3.0.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.cbam.ChannelGate.mlp.1.weight', 'layer3.1.cbam.ChannelGate.mlp.1.bias', 'layer3.1.cbam.ChannelGate.mlp.3.weight', 'layer3.1.cbam.ChannelGate.mlp.3.bias', 'layer3.1.cbam.SpatialGate.spatial.conv.weight', 'layer3.1.cbam.SpatialGate.spatial.bn.weight', 'layer3.1.cbam.SpatialGate.spatial.bn.bias', 'layer3.1.cbam.SpatialGate.spatial.bn.running_mean', 'layer3.1.cbam.SpatialGate.spatial.bn.running_var', 'layer3.1.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.cbam.ChannelGate.mlp.1.weight', 'layer3.2.cbam.ChannelGate.mlp.1.bias', 'layer3.2.cbam.ChannelGate.mlp.3.weight', 'layer3.2.cbam.ChannelGate.mlp.3.bias', 'layer3.2.cbam.SpatialGate.spatial.conv.weight', 'layer3.2.cbam.SpatialGate.spatial.bn.weight', 'layer3.2.cbam.SpatialGate.spatial.bn.bias', 'layer3.2.cbam.SpatialGate.spatial.bn.running_mean', 'layer3.2.cbam.SpatialGate.spatial.bn.running_var', 'layer3.2.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.cbam.ChannelGate.mlp.1.weight', 'layer3.3.cbam.ChannelGate.mlp.1.bias', 'layer3.3.cbam.ChannelGate.mlp.3.weight', 'layer3.3.cbam.ChannelGate.mlp.3.bias', 'layer3.3.cbam.SpatialGate.spatial.conv.weight', 'layer3.3.cbam.SpatialGate.spatial.bn.weight', 'layer3.3.cbam.SpatialGate.spatial.bn.bias', 'layer3.3.cbam.SpatialGate.spatial.bn.running_mean', 'layer3.3.cbam.SpatialGate.spatial.bn.running_var', 'layer3.3.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.cbam.ChannelGate.mlp.1.weight', 'layer3.4.cbam.ChannelGate.mlp.1.bias', 'layer3.4.cbam.ChannelGate.mlp.3.weight', 'layer3.4.cbam.ChannelGate.mlp.3.bias', 'layer3.4.cbam.SpatialGate.spatial.conv.weight', 'layer3.4.cbam.SpatialGate.spatial.bn.weight', 'layer3.4.cbam.SpatialGate.spatial.bn.bias', 'layer3.4.cbam.SpatialGate.spatial.bn.running_mean', 'layer3.4.cbam.SpatialGate.spatial.bn.running_var', 'layer3.4.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.cbam.ChannelGate.mlp.1.weight', 'layer3.5.cbam.ChannelGate.mlp.1.bias', 'layer3.5.cbam.ChannelGate.mlp.3.weight', 'layer3.5.cbam.ChannelGate.mlp.3.bias', 'layer3.5.cbam.SpatialGate.spatial.conv.weight', 'layer3.5.cbam.SpatialGate.spatial.bn.weight', 'layer3.5.cbam.SpatialGate.spatial.bn.bias', 'layer3.5.cbam.SpatialGate.spatial.bn.running_mean', 'layer3.5.cbam.SpatialGate.spatial.bn.running_var', 'layer3.5.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.0.cbam.ChannelGate.mlp.1.weight', 'layer4.0.cbam.ChannelGate.mlp.1.bias', 'layer4.0.cbam.ChannelGate.mlp.3.weight', 'layer4.0.cbam.ChannelGate.mlp.3.bias', 'layer4.0.cbam.SpatialGate.spatial.conv.weight', 'layer4.0.cbam.SpatialGate.spatial.bn.weight', 'layer4.0.cbam.SpatialGate.spatial.bn.bias', 'layer4.0.cbam.SpatialGate.spatial.bn.running_mean', 'layer4.0.cbam.SpatialGate.spatial.bn.running_var', 'layer4.0.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.cbam.ChannelGate.mlp.1.weight', 'layer4.1.cbam.ChannelGate.mlp.1.bias', 'layer4.1.cbam.ChannelGate.mlp.3.weight', 'layer4.1.cbam.ChannelGate.mlp.3.bias', 'layer4.1.cbam.SpatialGate.spatial.conv.weight', 'layer4.1.cbam.SpatialGate.spatial.bn.weight', 'layer4.1.cbam.SpatialGate.spatial.bn.bias', 'layer4.1.cbam.SpatialGate.spatial.bn.running_mean', 'layer4.1.cbam.SpatialGate.spatial.bn.running_var', 'layer4.1.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.cbam.ChannelGate.mlp.1.weight', 'layer4.2.cbam.ChannelGate.mlp.1.bias', 'layer4.2.cbam.ChannelGate.mlp.3.weight', 'layer4.2.cbam.ChannelGate.mlp.3.bias', 'layer4.2.cbam.SpatialGate.spatial.conv.weight', 'layer4.2.cbam.SpatialGate.spatial.bn.weight', 'layer4.2.cbam.SpatialGate.spatial.bn.bias', 'layer4.2.cbam.SpatialGate.spatial.bn.running_mean', 'layer4.2.cbam.SpatialGate.spatial.bn.running_var', 'layer4.2.cbam.SpatialGate.spatial.bn.num_batches_tracked', 'fc.weight', 'fc.bias', 'final_conv.weight'])\n"
     ]
    }
   ],
   "source": [
    "ckpt_path=\"/workspace/Plastic_Bottle_defect_detection/trained_model/YOLO_RESNET_CBAM_neck_LR0.0001_IP50_nonPretrain/model_100.pth\"\n",
    "state_dict_my = torch.load(ckpt_path)\n",
    "print(state_dict_my.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1832a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['module.conv1.weight', 'module.bn1.weight', 'module.bn1.bias', 'module.bn1.running_mean', 'module.bn1.running_var', 'module.layer1.0.conv1.weight', 'module.layer1.0.bn1.weight', 'module.layer1.0.bn1.bias', 'module.layer1.0.bn1.running_mean', 'module.layer1.0.bn1.running_var', 'module.layer1.0.conv2.weight', 'module.layer1.0.bn2.weight', 'module.layer1.0.bn2.bias', 'module.layer1.0.bn2.running_mean', 'module.layer1.0.bn2.running_var', 'module.layer1.0.conv3.weight', 'module.layer1.0.bn3.weight', 'module.layer1.0.bn3.bias', 'module.layer1.0.bn3.running_mean', 'module.layer1.0.bn3.running_var', 'module.layer1.0.downsample.0.weight', 'module.layer1.0.downsample.1.weight', 'module.layer1.0.downsample.1.bias', 'module.layer1.0.downsample.1.running_mean', 'module.layer1.0.downsample.1.running_var', 'module.layer1.0.cbam.ChannelGate.mlp.1.weight', 'module.layer1.0.cbam.ChannelGate.mlp.1.bias', 'module.layer1.0.cbam.ChannelGate.mlp.3.weight', 'module.layer1.0.cbam.ChannelGate.mlp.3.bias', 'module.layer1.0.cbam.SpatialGate.spatial.conv.weight', 'module.layer1.0.cbam.SpatialGate.spatial.bn.weight', 'module.layer1.0.cbam.SpatialGate.spatial.bn.bias', 'module.layer1.0.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer1.0.cbam.SpatialGate.spatial.bn.running_var', 'module.layer1.1.conv1.weight', 'module.layer1.1.bn1.weight', 'module.layer1.1.bn1.bias', 'module.layer1.1.bn1.running_mean', 'module.layer1.1.bn1.running_var', 'module.layer1.1.conv2.weight', 'module.layer1.1.bn2.weight', 'module.layer1.1.bn2.bias', 'module.layer1.1.bn2.running_mean', 'module.layer1.1.bn2.running_var', 'module.layer1.1.conv3.weight', 'module.layer1.1.bn3.weight', 'module.layer1.1.bn3.bias', 'module.layer1.1.bn3.running_mean', 'module.layer1.1.bn3.running_var', 'module.layer1.1.cbam.ChannelGate.mlp.1.weight', 'module.layer1.1.cbam.ChannelGate.mlp.1.bias', 'module.layer1.1.cbam.ChannelGate.mlp.3.weight', 'module.layer1.1.cbam.ChannelGate.mlp.3.bias', 'module.layer1.1.cbam.SpatialGate.spatial.conv.weight', 'module.layer1.1.cbam.SpatialGate.spatial.bn.weight', 'module.layer1.1.cbam.SpatialGate.spatial.bn.bias', 'module.layer1.1.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer1.1.cbam.SpatialGate.spatial.bn.running_var', 'module.layer1.2.conv1.weight', 'module.layer1.2.bn1.weight', 'module.layer1.2.bn1.bias', 'module.layer1.2.bn1.running_mean', 'module.layer1.2.bn1.running_var', 'module.layer1.2.conv2.weight', 'module.layer1.2.bn2.weight', 'module.layer1.2.bn2.bias', 'module.layer1.2.bn2.running_mean', 'module.layer1.2.bn2.running_var', 'module.layer1.2.conv3.weight', 'module.layer1.2.bn3.weight', 'module.layer1.2.bn3.bias', 'module.layer1.2.bn3.running_mean', 'module.layer1.2.bn3.running_var', 'module.layer1.2.cbam.ChannelGate.mlp.1.weight', 'module.layer1.2.cbam.ChannelGate.mlp.1.bias', 'module.layer1.2.cbam.ChannelGate.mlp.3.weight', 'module.layer1.2.cbam.ChannelGate.mlp.3.bias', 'module.layer1.2.cbam.SpatialGate.spatial.conv.weight', 'module.layer1.2.cbam.SpatialGate.spatial.bn.weight', 'module.layer1.2.cbam.SpatialGate.spatial.bn.bias', 'module.layer1.2.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer1.2.cbam.SpatialGate.spatial.bn.running_var', 'module.layer2.0.conv1.weight', 'module.layer2.0.bn1.weight', 'module.layer2.0.bn1.bias', 'module.layer2.0.bn1.running_mean', 'module.layer2.0.bn1.running_var', 'module.layer2.0.conv2.weight', 'module.layer2.0.bn2.weight', 'module.layer2.0.bn2.bias', 'module.layer2.0.bn2.running_mean', 'module.layer2.0.bn2.running_var', 'module.layer2.0.conv3.weight', 'module.layer2.0.bn3.weight', 'module.layer2.0.bn3.bias', 'module.layer2.0.bn3.running_mean', 'module.layer2.0.bn3.running_var', 'module.layer2.0.downsample.0.weight', 'module.layer2.0.downsample.1.weight', 'module.layer2.0.downsample.1.bias', 'module.layer2.0.downsample.1.running_mean', 'module.layer2.0.downsample.1.running_var', 'module.layer2.0.cbam.ChannelGate.mlp.1.weight', 'module.layer2.0.cbam.ChannelGate.mlp.1.bias', 'module.layer2.0.cbam.ChannelGate.mlp.3.weight', 'module.layer2.0.cbam.ChannelGate.mlp.3.bias', 'module.layer2.0.cbam.SpatialGate.spatial.conv.weight', 'module.layer2.0.cbam.SpatialGate.spatial.bn.weight', 'module.layer2.0.cbam.SpatialGate.spatial.bn.bias', 'module.layer2.0.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer2.0.cbam.SpatialGate.spatial.bn.running_var', 'module.layer2.1.conv1.weight', 'module.layer2.1.bn1.weight', 'module.layer2.1.bn1.bias', 'module.layer2.1.bn1.running_mean', 'module.layer2.1.bn1.running_var', 'module.layer2.1.conv2.weight', 'module.layer2.1.bn2.weight', 'module.layer2.1.bn2.bias', 'module.layer2.1.bn2.running_mean', 'module.layer2.1.bn2.running_var', 'module.layer2.1.conv3.weight', 'module.layer2.1.bn3.weight', 'module.layer2.1.bn3.bias', 'module.layer2.1.bn3.running_mean', 'module.layer2.1.bn3.running_var', 'module.layer2.1.cbam.ChannelGate.mlp.1.weight', 'module.layer2.1.cbam.ChannelGate.mlp.1.bias', 'module.layer2.1.cbam.ChannelGate.mlp.3.weight', 'module.layer2.1.cbam.ChannelGate.mlp.3.bias', 'module.layer2.1.cbam.SpatialGate.spatial.conv.weight', 'module.layer2.1.cbam.SpatialGate.spatial.bn.weight', 'module.layer2.1.cbam.SpatialGate.spatial.bn.bias', 'module.layer2.1.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer2.1.cbam.SpatialGate.spatial.bn.running_var', 'module.layer2.2.conv1.weight', 'module.layer2.2.bn1.weight', 'module.layer2.2.bn1.bias', 'module.layer2.2.bn1.running_mean', 'module.layer2.2.bn1.running_var', 'module.layer2.2.conv2.weight', 'module.layer2.2.bn2.weight', 'module.layer2.2.bn2.bias', 'module.layer2.2.bn2.running_mean', 'module.layer2.2.bn2.running_var', 'module.layer2.2.conv3.weight', 'module.layer2.2.bn3.weight', 'module.layer2.2.bn3.bias', 'module.layer2.2.bn3.running_mean', 'module.layer2.2.bn3.running_var', 'module.layer2.2.cbam.ChannelGate.mlp.1.weight', 'module.layer2.2.cbam.ChannelGate.mlp.1.bias', 'module.layer2.2.cbam.ChannelGate.mlp.3.weight', 'module.layer2.2.cbam.ChannelGate.mlp.3.bias', 'module.layer2.2.cbam.SpatialGate.spatial.conv.weight', 'module.layer2.2.cbam.SpatialGate.spatial.bn.weight', 'module.layer2.2.cbam.SpatialGate.spatial.bn.bias', 'module.layer2.2.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer2.2.cbam.SpatialGate.spatial.bn.running_var', 'module.layer2.3.conv1.weight', 'module.layer2.3.bn1.weight', 'module.layer2.3.bn1.bias', 'module.layer2.3.bn1.running_mean', 'module.layer2.3.bn1.running_var', 'module.layer2.3.conv2.weight', 'module.layer2.3.bn2.weight', 'module.layer2.3.bn2.bias', 'module.layer2.3.bn2.running_mean', 'module.layer2.3.bn2.running_var', 'module.layer2.3.conv3.weight', 'module.layer2.3.bn3.weight', 'module.layer2.3.bn3.bias', 'module.layer2.3.bn3.running_mean', 'module.layer2.3.bn3.running_var', 'module.layer2.3.cbam.ChannelGate.mlp.1.weight', 'module.layer2.3.cbam.ChannelGate.mlp.1.bias', 'module.layer2.3.cbam.ChannelGate.mlp.3.weight', 'module.layer2.3.cbam.ChannelGate.mlp.3.bias', 'module.layer2.3.cbam.SpatialGate.spatial.conv.weight', 'module.layer2.3.cbam.SpatialGate.spatial.bn.weight', 'module.layer2.3.cbam.SpatialGate.spatial.bn.bias', 'module.layer2.3.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer2.3.cbam.SpatialGate.spatial.bn.running_var', 'module.layer3.0.conv1.weight', 'module.layer3.0.bn1.weight', 'module.layer3.0.bn1.bias', 'module.layer3.0.bn1.running_mean', 'module.layer3.0.bn1.running_var', 'module.layer3.0.conv2.weight', 'module.layer3.0.bn2.weight', 'module.layer3.0.bn2.bias', 'module.layer3.0.bn2.running_mean', 'module.layer3.0.bn2.running_var', 'module.layer3.0.conv3.weight', 'module.layer3.0.bn3.weight', 'module.layer3.0.bn3.bias', 'module.layer3.0.bn3.running_mean', 'module.layer3.0.bn3.running_var', 'module.layer3.0.downsample.0.weight', 'module.layer3.0.downsample.1.weight', 'module.layer3.0.downsample.1.bias', 'module.layer3.0.downsample.1.running_mean', 'module.layer3.0.downsample.1.running_var', 'module.layer3.0.cbam.ChannelGate.mlp.1.weight', 'module.layer3.0.cbam.ChannelGate.mlp.1.bias', 'module.layer3.0.cbam.ChannelGate.mlp.3.weight', 'module.layer3.0.cbam.ChannelGate.mlp.3.bias', 'module.layer3.0.cbam.SpatialGate.spatial.conv.weight', 'module.layer3.0.cbam.SpatialGate.spatial.bn.weight', 'module.layer3.0.cbam.SpatialGate.spatial.bn.bias', 'module.layer3.0.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer3.0.cbam.SpatialGate.spatial.bn.running_var', 'module.layer3.1.conv1.weight', 'module.layer3.1.bn1.weight', 'module.layer3.1.bn1.bias', 'module.layer3.1.bn1.running_mean', 'module.layer3.1.bn1.running_var', 'module.layer3.1.conv2.weight', 'module.layer3.1.bn2.weight', 'module.layer3.1.bn2.bias', 'module.layer3.1.bn2.running_mean', 'module.layer3.1.bn2.running_var', 'module.layer3.1.conv3.weight', 'module.layer3.1.bn3.weight', 'module.layer3.1.bn3.bias', 'module.layer3.1.bn3.running_mean', 'module.layer3.1.bn3.running_var', 'module.layer3.1.cbam.ChannelGate.mlp.1.weight', 'module.layer3.1.cbam.ChannelGate.mlp.1.bias', 'module.layer3.1.cbam.ChannelGate.mlp.3.weight', 'module.layer3.1.cbam.ChannelGate.mlp.3.bias', 'module.layer3.1.cbam.SpatialGate.spatial.conv.weight', 'module.layer3.1.cbam.SpatialGate.spatial.bn.weight', 'module.layer3.1.cbam.SpatialGate.spatial.bn.bias', 'module.layer3.1.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer3.1.cbam.SpatialGate.spatial.bn.running_var', 'module.layer3.2.conv1.weight', 'module.layer3.2.bn1.weight', 'module.layer3.2.bn1.bias', 'module.layer3.2.bn1.running_mean', 'module.layer3.2.bn1.running_var', 'module.layer3.2.conv2.weight', 'module.layer3.2.bn2.weight', 'module.layer3.2.bn2.bias', 'module.layer3.2.bn2.running_mean', 'module.layer3.2.bn2.running_var', 'module.layer3.2.conv3.weight', 'module.layer3.2.bn3.weight', 'module.layer3.2.bn3.bias', 'module.layer3.2.bn3.running_mean', 'module.layer3.2.bn3.running_var', 'module.layer3.2.cbam.ChannelGate.mlp.1.weight', 'module.layer3.2.cbam.ChannelGate.mlp.1.bias', 'module.layer3.2.cbam.ChannelGate.mlp.3.weight', 'module.layer3.2.cbam.ChannelGate.mlp.3.bias', 'module.layer3.2.cbam.SpatialGate.spatial.conv.weight', 'module.layer3.2.cbam.SpatialGate.spatial.bn.weight', 'module.layer3.2.cbam.SpatialGate.spatial.bn.bias', 'module.layer3.2.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer3.2.cbam.SpatialGate.spatial.bn.running_var', 'module.layer3.3.conv1.weight', 'module.layer3.3.bn1.weight', 'module.layer3.3.bn1.bias', 'module.layer3.3.bn1.running_mean', 'module.layer3.3.bn1.running_var', 'module.layer3.3.conv2.weight', 'module.layer3.3.bn2.weight', 'module.layer3.3.bn2.bias', 'module.layer3.3.bn2.running_mean', 'module.layer3.3.bn2.running_var', 'module.layer3.3.conv3.weight', 'module.layer3.3.bn3.weight', 'module.layer3.3.bn3.bias', 'module.layer3.3.bn3.running_mean', 'module.layer3.3.bn3.running_var', 'module.layer3.3.cbam.ChannelGate.mlp.1.weight', 'module.layer3.3.cbam.ChannelGate.mlp.1.bias', 'module.layer3.3.cbam.ChannelGate.mlp.3.weight', 'module.layer3.3.cbam.ChannelGate.mlp.3.bias', 'module.layer3.3.cbam.SpatialGate.spatial.conv.weight', 'module.layer3.3.cbam.SpatialGate.spatial.bn.weight', 'module.layer3.3.cbam.SpatialGate.spatial.bn.bias', 'module.layer3.3.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer3.3.cbam.SpatialGate.spatial.bn.running_var', 'module.layer3.4.conv1.weight', 'module.layer3.4.bn1.weight', 'module.layer3.4.bn1.bias', 'module.layer3.4.bn1.running_mean', 'module.layer3.4.bn1.running_var', 'module.layer3.4.conv2.weight', 'module.layer3.4.bn2.weight', 'module.layer3.4.bn2.bias', 'module.layer3.4.bn2.running_mean', 'module.layer3.4.bn2.running_var', 'module.layer3.4.conv3.weight', 'module.layer3.4.bn3.weight', 'module.layer3.4.bn3.bias', 'module.layer3.4.bn3.running_mean', 'module.layer3.4.bn3.running_var', 'module.layer3.4.cbam.ChannelGate.mlp.1.weight', 'module.layer3.4.cbam.ChannelGate.mlp.1.bias', 'module.layer3.4.cbam.ChannelGate.mlp.3.weight', 'module.layer3.4.cbam.ChannelGate.mlp.3.bias', 'module.layer3.4.cbam.SpatialGate.spatial.conv.weight', 'module.layer3.4.cbam.SpatialGate.spatial.bn.weight', 'module.layer3.4.cbam.SpatialGate.spatial.bn.bias', 'module.layer3.4.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer3.4.cbam.SpatialGate.spatial.bn.running_var', 'module.layer3.5.conv1.weight', 'module.layer3.5.bn1.weight', 'module.layer3.5.bn1.bias', 'module.layer3.5.bn1.running_mean', 'module.layer3.5.bn1.running_var', 'module.layer3.5.conv2.weight', 'module.layer3.5.bn2.weight', 'module.layer3.5.bn2.bias', 'module.layer3.5.bn2.running_mean', 'module.layer3.5.bn2.running_var', 'module.layer3.5.conv3.weight', 'module.layer3.5.bn3.weight', 'module.layer3.5.bn3.bias', 'module.layer3.5.bn3.running_mean', 'module.layer3.5.bn3.running_var', 'module.layer3.5.cbam.ChannelGate.mlp.1.weight', 'module.layer3.5.cbam.ChannelGate.mlp.1.bias', 'module.layer3.5.cbam.ChannelGate.mlp.3.weight', 'module.layer3.5.cbam.ChannelGate.mlp.3.bias', 'module.layer3.5.cbam.SpatialGate.spatial.conv.weight', 'module.layer3.5.cbam.SpatialGate.spatial.bn.weight', 'module.layer3.5.cbam.SpatialGate.spatial.bn.bias', 'module.layer3.5.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer3.5.cbam.SpatialGate.spatial.bn.running_var', 'module.layer4.0.conv1.weight', 'module.layer4.0.bn1.weight', 'module.layer4.0.bn1.bias', 'module.layer4.0.bn1.running_mean', 'module.layer4.0.bn1.running_var', 'module.layer4.0.conv2.weight', 'module.layer4.0.bn2.weight', 'module.layer4.0.bn2.bias', 'module.layer4.0.bn2.running_mean', 'module.layer4.0.bn2.running_var', 'module.layer4.0.conv3.weight', 'module.layer4.0.bn3.weight', 'module.layer4.0.bn3.bias', 'module.layer4.0.bn3.running_mean', 'module.layer4.0.bn3.running_var', 'module.layer4.0.downsample.0.weight', 'module.layer4.0.downsample.1.weight', 'module.layer4.0.downsample.1.bias', 'module.layer4.0.downsample.1.running_mean', 'module.layer4.0.downsample.1.running_var', 'module.layer4.0.cbam.ChannelGate.mlp.1.weight', 'module.layer4.0.cbam.ChannelGate.mlp.1.bias', 'module.layer4.0.cbam.ChannelGate.mlp.3.weight', 'module.layer4.0.cbam.ChannelGate.mlp.3.bias', 'module.layer4.0.cbam.SpatialGate.spatial.conv.weight', 'module.layer4.0.cbam.SpatialGate.spatial.bn.weight', 'module.layer4.0.cbam.SpatialGate.spatial.bn.bias', 'module.layer4.0.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer4.0.cbam.SpatialGate.spatial.bn.running_var', 'module.layer4.1.conv1.weight', 'module.layer4.1.bn1.weight', 'module.layer4.1.bn1.bias', 'module.layer4.1.bn1.running_mean', 'module.layer4.1.bn1.running_var', 'module.layer4.1.conv2.weight', 'module.layer4.1.bn2.weight', 'module.layer4.1.bn2.bias', 'module.layer4.1.bn2.running_mean', 'module.layer4.1.bn2.running_var', 'module.layer4.1.conv3.weight', 'module.layer4.1.bn3.weight', 'module.layer4.1.bn3.bias', 'module.layer4.1.bn3.running_mean', 'module.layer4.1.bn3.running_var', 'module.layer4.1.cbam.ChannelGate.mlp.1.weight', 'module.layer4.1.cbam.ChannelGate.mlp.1.bias', 'module.layer4.1.cbam.ChannelGate.mlp.3.weight', 'module.layer4.1.cbam.ChannelGate.mlp.3.bias', 'module.layer4.1.cbam.SpatialGate.spatial.conv.weight', 'module.layer4.1.cbam.SpatialGate.spatial.bn.weight', 'module.layer4.1.cbam.SpatialGate.spatial.bn.bias', 'module.layer4.1.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer4.1.cbam.SpatialGate.spatial.bn.running_var', 'module.layer4.2.conv1.weight', 'module.layer4.2.bn1.weight', 'module.layer4.2.bn1.bias', 'module.layer4.2.bn1.running_mean', 'module.layer4.2.bn1.running_var', 'module.layer4.2.conv2.weight', 'module.layer4.2.bn2.weight', 'module.layer4.2.bn2.bias', 'module.layer4.2.bn2.running_mean', 'module.layer4.2.bn2.running_var', 'module.layer4.2.conv3.weight', 'module.layer4.2.bn3.weight', 'module.layer4.2.bn3.bias', 'module.layer4.2.bn3.running_mean', 'module.layer4.2.bn3.running_var', 'module.layer4.2.cbam.ChannelGate.mlp.1.weight', 'module.layer4.2.cbam.ChannelGate.mlp.1.bias', 'module.layer4.2.cbam.ChannelGate.mlp.3.weight', 'module.layer4.2.cbam.ChannelGate.mlp.3.bias', 'module.layer4.2.cbam.SpatialGate.spatial.conv.weight', 'module.layer4.2.cbam.SpatialGate.spatial.bn.weight', 'module.layer4.2.cbam.SpatialGate.spatial.bn.bias', 'module.layer4.2.cbam.SpatialGate.spatial.bn.running_mean', 'module.layer4.2.cbam.SpatialGate.spatial.bn.running_var', 'module.fc.weight', 'module.fc.bias']\n"
     ]
    }
   ],
   "source": [
    "ckpt_path=\"/workspace/Plastic_Bottle_defect_detection/experiments/trained_model/RESNET50_CBAM_new_name_wrap.pth\"\n",
    "state_dict = torch.load(ckpt_path)\n",
    "print(list(state_dict[\"state_dict\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8abdefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.0.conv3.weight\n",
      "module.layer1.0.bn3.weight\n",
      "module.layer1.0.bn3.bias\n",
      "module.layer1.0.bn3.running_mean\n",
      "module.layer1.0.bn3.running_var\n",
      "module.layer1.0.downsample.0.weight\n",
      "module.layer1.0.downsample.1.weight\n",
      "module.layer1.0.downsample.1.bias\n",
      "module.layer1.0.downsample.1.running_mean\n",
      "module.layer1.0.downsample.1.running_var\n",
      "module.layer1.1.conv3.weight\n",
      "module.layer1.1.bn3.weight\n",
      "module.layer1.1.bn3.bias\n",
      "module.layer1.1.bn3.running_mean\n",
      "module.layer1.1.bn3.running_var\n",
      "module.layer1.2.conv3.weight\n",
      "module.layer1.2.bn3.weight\n",
      "module.layer1.2.bn3.bias\n",
      "module.layer1.2.bn3.running_mean\n",
      "module.layer1.2.bn3.running_var\n",
      "module.layer2.0.conv3.weight\n",
      "module.layer2.0.bn3.weight\n",
      "module.layer2.0.bn3.bias\n",
      "module.layer2.0.bn3.running_mean\n",
      "module.layer2.0.bn3.running_var\n",
      "module.layer2.1.conv3.weight\n",
      "module.layer2.1.bn3.weight\n",
      "module.layer2.1.bn3.bias\n",
      "module.layer2.1.bn3.running_mean\n",
      "module.layer2.1.bn3.running_var\n",
      "module.layer2.2.conv3.weight\n",
      "module.layer2.2.bn3.weight\n",
      "module.layer2.2.bn3.bias\n",
      "module.layer2.2.bn3.running_mean\n",
      "module.layer2.2.bn3.running_var\n",
      "module.layer2.3.conv3.weight\n",
      "module.layer2.3.bn3.weight\n",
      "module.layer2.3.bn3.bias\n",
      "module.layer2.3.bn3.running_mean\n",
      "module.layer2.3.bn3.running_var\n",
      "module.layer3.0.conv3.weight\n",
      "module.layer3.0.bn3.weight\n",
      "module.layer3.0.bn3.bias\n",
      "module.layer3.0.bn3.running_mean\n",
      "module.layer3.0.bn3.running_var\n",
      "module.layer3.1.conv3.weight\n",
      "module.layer3.1.bn3.weight\n",
      "module.layer3.1.bn3.bias\n",
      "module.layer3.1.bn3.running_mean\n",
      "module.layer3.1.bn3.running_var\n",
      "module.layer3.2.conv3.weight\n",
      "module.layer3.2.bn3.weight\n",
      "module.layer3.2.bn3.bias\n",
      "module.layer3.2.bn3.running_mean\n",
      "module.layer3.2.bn3.running_var\n",
      "module.layer3.3.conv3.weight\n",
      "module.layer3.3.bn3.weight\n",
      "module.layer3.3.bn3.bias\n",
      "module.layer3.3.bn3.running_mean\n",
      "module.layer3.3.bn3.running_var\n",
      "module.layer3.4.conv3.weight\n",
      "module.layer3.4.bn3.weight\n",
      "module.layer3.4.bn3.bias\n",
      "module.layer3.4.bn3.running_mean\n",
      "module.layer3.4.bn3.running_var\n",
      "module.layer3.5.conv3.weight\n",
      "module.layer3.5.bn3.weight\n",
      "module.layer3.5.bn3.bias\n",
      "module.layer3.5.bn3.running_mean\n",
      "module.layer3.5.bn3.running_var\n",
      "module.layer4.0.conv3.weight\n",
      "module.layer4.0.bn3.weight\n",
      "module.layer4.0.bn3.bias\n",
      "module.layer4.0.bn3.running_mean\n",
      "module.layer4.0.bn3.running_var\n",
      "module.layer4.1.conv3.weight\n",
      "module.layer4.1.bn3.weight\n",
      "module.layer4.1.bn3.bias\n",
      "module.layer4.1.bn3.running_mean\n",
      "module.layer4.1.bn3.running_var\n",
      "module.layer4.2.conv3.weight\n",
      "module.layer4.2.bn3.weight\n",
      "module.layer4.2.bn3.bias\n",
      "module.layer4.2.bn3.running_mean\n",
      "module.layer4.2.bn3.running_var\n"
     ]
    }
   ],
   "source": [
    "for layer in list(state_dict[\"state_dict\"].keys()):\n",
    "    name=layer.split('module.')[1]\n",
    "#     print(name)\n",
    "    if(name not in list(state_dict_my)):\n",
    "        print(layer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f80b426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3770/1777674582.py:131: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.fc.weight)\n",
      "/tmp/ipykernel_3770/1777674582.py:135: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.cbam.ChannelGate.mlp.1.weight\", \"layer1.0.cbam.ChannelGate.mlp.1.bias\", \"layer1.0.cbam.ChannelGate.mlp.3.weight\", \"layer1.0.cbam.ChannelGate.mlp.3.bias\", \"layer1.0.cbam.SpatialGate.spatial.conv.weight\", \"layer1.0.cbam.SpatialGate.spatial.bn.weight\", \"layer1.0.cbam.SpatialGate.spatial.bn.bias\", \"layer1.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer1.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.cbam.ChannelGate.mlp.1.weight\", \"layer1.1.cbam.ChannelGate.mlp.1.bias\", \"layer1.1.cbam.ChannelGate.mlp.3.weight\", \"layer1.1.cbam.ChannelGate.mlp.3.bias\", \"layer1.1.cbam.SpatialGate.spatial.conv.weight\", \"layer1.1.cbam.SpatialGate.spatial.bn.weight\", \"layer1.1.cbam.SpatialGate.spatial.bn.bias\", \"layer1.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer1.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.cbam.ChannelGate.mlp.1.weight\", \"layer1.2.cbam.ChannelGate.mlp.1.bias\", \"layer1.2.cbam.ChannelGate.mlp.3.weight\", \"layer1.2.cbam.ChannelGate.mlp.3.bias\", \"layer1.2.cbam.SpatialGate.spatial.conv.weight\", \"layer1.2.cbam.SpatialGate.spatial.bn.weight\", \"layer1.2.cbam.SpatialGate.spatial.bn.bias\", \"layer1.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer1.2.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.cbam.ChannelGate.mlp.1.weight\", \"layer2.0.cbam.ChannelGate.mlp.1.bias\", \"layer2.0.cbam.ChannelGate.mlp.3.weight\", \"layer2.0.cbam.ChannelGate.mlp.3.bias\", \"layer2.0.cbam.SpatialGate.spatial.conv.weight\", \"layer2.0.cbam.SpatialGate.spatial.bn.weight\", \"layer2.0.cbam.SpatialGate.spatial.bn.bias\", \"layer2.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.cbam.ChannelGate.mlp.1.weight\", \"layer2.1.cbam.ChannelGate.mlp.1.bias\", \"layer2.1.cbam.ChannelGate.mlp.3.weight\", \"layer2.1.cbam.ChannelGate.mlp.3.bias\", \"layer2.1.cbam.SpatialGate.spatial.conv.weight\", \"layer2.1.cbam.SpatialGate.spatial.bn.weight\", \"layer2.1.cbam.SpatialGate.spatial.bn.bias\", \"layer2.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.cbam.ChannelGate.mlp.1.weight\", \"layer2.2.cbam.ChannelGate.mlp.1.bias\", \"layer2.2.cbam.ChannelGate.mlp.3.weight\", \"layer2.2.cbam.ChannelGate.mlp.3.bias\", \"layer2.2.cbam.SpatialGate.spatial.conv.weight\", \"layer2.2.cbam.SpatialGate.spatial.bn.weight\", \"layer2.2.cbam.SpatialGate.spatial.bn.bias\", \"layer2.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.2.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.cbam.ChannelGate.mlp.1.weight\", \"layer2.3.cbam.ChannelGate.mlp.1.bias\", \"layer2.3.cbam.ChannelGate.mlp.3.weight\", \"layer2.3.cbam.ChannelGate.mlp.3.bias\", \"layer2.3.cbam.SpatialGate.spatial.conv.weight\", \"layer2.3.cbam.SpatialGate.spatial.bn.weight\", \"layer2.3.cbam.SpatialGate.spatial.bn.bias\", \"layer2.3.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.3.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.cbam.ChannelGate.mlp.1.weight\", \"layer3.0.cbam.ChannelGate.mlp.1.bias\", \"layer3.0.cbam.ChannelGate.mlp.3.weight\", \"layer3.0.cbam.ChannelGate.mlp.3.bias\", \"layer3.0.cbam.SpatialGate.spatial.conv.weight\", \"layer3.0.cbam.SpatialGate.spatial.bn.weight\", \"layer3.0.cbam.SpatialGate.spatial.bn.bias\", \"layer3.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.cbam.ChannelGate.mlp.1.weight\", \"layer3.1.cbam.ChannelGate.mlp.1.bias\", \"layer3.1.cbam.ChannelGate.mlp.3.weight\", \"layer3.1.cbam.ChannelGate.mlp.3.bias\", \"layer3.1.cbam.SpatialGate.spatial.conv.weight\", \"layer3.1.cbam.SpatialGate.spatial.bn.weight\", \"layer3.1.cbam.SpatialGate.spatial.bn.bias\", \"layer3.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.cbam.ChannelGate.mlp.1.weight\", \"layer3.2.cbam.ChannelGate.mlp.1.bias\", \"layer3.2.cbam.ChannelGate.mlp.3.weight\", \"layer3.2.cbam.ChannelGate.mlp.3.bias\", \"layer3.2.cbam.SpatialGate.spatial.conv.weight\", \"layer3.2.cbam.SpatialGate.spatial.bn.weight\", \"layer3.2.cbam.SpatialGate.spatial.bn.bias\", \"layer3.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.2.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.cbam.ChannelGate.mlp.1.weight\", \"layer3.3.cbam.ChannelGate.mlp.1.bias\", \"layer3.3.cbam.ChannelGate.mlp.3.weight\", \"layer3.3.cbam.ChannelGate.mlp.3.bias\", \"layer3.3.cbam.SpatialGate.spatial.conv.weight\", \"layer3.3.cbam.SpatialGate.spatial.bn.weight\", \"layer3.3.cbam.SpatialGate.spatial.bn.bias\", \"layer3.3.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.3.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.cbam.ChannelGate.mlp.1.weight\", \"layer3.4.cbam.ChannelGate.mlp.1.bias\", \"layer3.4.cbam.ChannelGate.mlp.3.weight\", \"layer3.4.cbam.ChannelGate.mlp.3.bias\", \"layer3.4.cbam.SpatialGate.spatial.conv.weight\", \"layer3.4.cbam.SpatialGate.spatial.bn.weight\", \"layer3.4.cbam.SpatialGate.spatial.bn.bias\", \"layer3.4.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.4.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.cbam.ChannelGate.mlp.1.weight\", \"layer3.5.cbam.ChannelGate.mlp.1.bias\", \"layer3.5.cbam.ChannelGate.mlp.3.weight\", \"layer3.5.cbam.ChannelGate.mlp.3.bias\", \"layer3.5.cbam.SpatialGate.spatial.conv.weight\", \"layer3.5.cbam.SpatialGate.spatial.bn.weight\", \"layer3.5.cbam.SpatialGate.spatial.bn.bias\", \"layer3.5.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.5.cbam.SpatialGate.spatial.bn.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.cbam.ChannelGate.mlp.1.weight\", \"layer4.0.cbam.ChannelGate.mlp.1.bias\", \"layer4.0.cbam.ChannelGate.mlp.3.weight\", \"layer4.0.cbam.ChannelGate.mlp.3.bias\", \"layer4.0.cbam.SpatialGate.spatial.conv.weight\", \"layer4.0.cbam.SpatialGate.spatial.bn.weight\", \"layer4.0.cbam.SpatialGate.spatial.bn.bias\", \"layer4.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer4.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.cbam.ChannelGate.mlp.1.weight\", \"layer4.1.cbam.ChannelGate.mlp.1.bias\", \"layer4.1.cbam.ChannelGate.mlp.3.weight\", \"layer4.1.cbam.ChannelGate.mlp.3.bias\", \"layer4.1.cbam.SpatialGate.spatial.conv.weight\", \"layer4.1.cbam.SpatialGate.spatial.bn.weight\", \"layer4.1.cbam.SpatialGate.spatial.bn.bias\", \"layer4.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer4.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.cbam.ChannelGate.mlp.1.weight\", \"layer4.2.cbam.ChannelGate.mlp.1.bias\", \"layer4.2.cbam.ChannelGate.mlp.3.weight\", \"layer4.2.cbam.ChannelGate.mlp.3.bias\", \"layer4.2.cbam.SpatialGate.spatial.conv.weight\", \"layer4.2.cbam.SpatialGate.spatial.bn.weight\", \"layer4.2.cbam.SpatialGate.spatial.bn.bias\", \"layer4.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer4.2.cbam.SpatialGate.spatial.bn.running_var\", \"fc.weight\", \"fc.bias\", \"final_conv.weight\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"best_prec1\", \"state_dict\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     10\u001b[0m ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/Plastic_Bottle_defect_detection/experiments/trained_model/RESNET50_CBAM_new_name_wrap.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(ckpt_path, num_classes, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     model = YOLO_SWIN(num_classes=num_classes)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     model = YOLO_SWIN(num_classes=num_classes)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     model \u001b[38;5;241m=\u001b[39m ResNet(BasicBlock, [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m3\u001b[39m], network_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageNet\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mNUM_CLASSES, att_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBAM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1913\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1914\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1915\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1919\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"bn1.running_mean\", \"bn1.running_var\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.bn1.bias\", \"layer1.0.bn1.running_mean\", \"layer1.0.bn1.running_var\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.bn2.bias\", \"layer1.0.bn2.running_mean\", \"layer1.0.bn2.running_var\", \"layer1.0.cbam.ChannelGate.mlp.1.weight\", \"layer1.0.cbam.ChannelGate.mlp.1.bias\", \"layer1.0.cbam.ChannelGate.mlp.3.weight\", \"layer1.0.cbam.ChannelGate.mlp.3.bias\", \"layer1.0.cbam.SpatialGate.spatial.conv.weight\", \"layer1.0.cbam.SpatialGate.spatial.bn.weight\", \"layer1.0.cbam.SpatialGate.spatial.bn.bias\", \"layer1.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer1.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.bn1.bias\", \"layer1.1.bn1.running_mean\", \"layer1.1.bn1.running_var\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.bn2.bias\", \"layer1.1.bn2.running_mean\", \"layer1.1.bn2.running_var\", \"layer1.1.cbam.ChannelGate.mlp.1.weight\", \"layer1.1.cbam.ChannelGate.mlp.1.bias\", \"layer1.1.cbam.ChannelGate.mlp.3.weight\", \"layer1.1.cbam.ChannelGate.mlp.3.bias\", \"layer1.1.cbam.SpatialGate.spatial.conv.weight\", \"layer1.1.cbam.SpatialGate.spatial.bn.weight\", \"layer1.1.cbam.SpatialGate.spatial.bn.bias\", \"layer1.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer1.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.cbam.ChannelGate.mlp.1.weight\", \"layer1.2.cbam.ChannelGate.mlp.1.bias\", \"layer1.2.cbam.ChannelGate.mlp.3.weight\", \"layer1.2.cbam.ChannelGate.mlp.3.bias\", \"layer1.2.cbam.SpatialGate.spatial.conv.weight\", \"layer1.2.cbam.SpatialGate.spatial.bn.weight\", \"layer1.2.cbam.SpatialGate.spatial.bn.bias\", \"layer1.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer1.2.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.bn1.bias\", \"layer2.0.bn1.running_mean\", \"layer2.0.bn1.running_var\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.bn2.bias\", \"layer2.0.bn2.running_mean\", \"layer2.0.bn2.running_var\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.0.downsample.1.bias\", \"layer2.0.downsample.1.running_mean\", \"layer2.0.downsample.1.running_var\", \"layer2.0.cbam.ChannelGate.mlp.1.weight\", \"layer2.0.cbam.ChannelGate.mlp.1.bias\", \"layer2.0.cbam.ChannelGate.mlp.3.weight\", \"layer2.0.cbam.ChannelGate.mlp.3.bias\", \"layer2.0.cbam.SpatialGate.spatial.conv.weight\", \"layer2.0.cbam.SpatialGate.spatial.bn.weight\", \"layer2.0.cbam.SpatialGate.spatial.bn.bias\", \"layer2.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.bn1.bias\", \"layer2.1.bn1.running_mean\", \"layer2.1.bn1.running_var\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.bn2.bias\", \"layer2.1.bn2.running_mean\", \"layer2.1.bn2.running_var\", \"layer2.1.cbam.ChannelGate.mlp.1.weight\", \"layer2.1.cbam.ChannelGate.mlp.1.bias\", \"layer2.1.cbam.ChannelGate.mlp.3.weight\", \"layer2.1.cbam.ChannelGate.mlp.3.bias\", \"layer2.1.cbam.SpatialGate.spatial.conv.weight\", \"layer2.1.cbam.SpatialGate.spatial.bn.weight\", \"layer2.1.cbam.SpatialGate.spatial.bn.bias\", \"layer2.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.cbam.ChannelGate.mlp.1.weight\", \"layer2.2.cbam.ChannelGate.mlp.1.bias\", \"layer2.2.cbam.ChannelGate.mlp.3.weight\", \"layer2.2.cbam.ChannelGate.mlp.3.bias\", \"layer2.2.cbam.SpatialGate.spatial.conv.weight\", \"layer2.2.cbam.SpatialGate.spatial.bn.weight\", \"layer2.2.cbam.SpatialGate.spatial.bn.bias\", \"layer2.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.2.cbam.SpatialGate.spatial.bn.running_var\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.bn1.bias\", \"layer2.3.bn1.running_mean\", \"layer2.3.bn1.running_var\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.bn2.bias\", \"layer2.3.bn2.running_mean\", \"layer2.3.bn2.running_var\", \"layer2.3.cbam.ChannelGate.mlp.1.weight\", \"layer2.3.cbam.ChannelGate.mlp.1.bias\", \"layer2.3.cbam.ChannelGate.mlp.3.weight\", \"layer2.3.cbam.ChannelGate.mlp.3.bias\", \"layer2.3.cbam.SpatialGate.spatial.conv.weight\", \"layer2.3.cbam.SpatialGate.spatial.bn.weight\", \"layer2.3.cbam.SpatialGate.spatial.bn.bias\", \"layer2.3.cbam.SpatialGate.spatial.bn.running_mean\", \"layer2.3.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.bn1.bias\", \"layer3.0.bn1.running_mean\", \"layer3.0.bn1.running_var\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.bn2.bias\", \"layer3.0.bn2.running_mean\", \"layer3.0.bn2.running_var\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.0.downsample.1.bias\", \"layer3.0.downsample.1.running_mean\", \"layer3.0.downsample.1.running_var\", \"layer3.0.cbam.ChannelGate.mlp.1.weight\", \"layer3.0.cbam.ChannelGate.mlp.1.bias\", \"layer3.0.cbam.ChannelGate.mlp.3.weight\", \"layer3.0.cbam.ChannelGate.mlp.3.bias\", \"layer3.0.cbam.SpatialGate.spatial.conv.weight\", \"layer3.0.cbam.SpatialGate.spatial.bn.weight\", \"layer3.0.cbam.SpatialGate.spatial.bn.bias\", \"layer3.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.bn1.bias\", \"layer3.1.bn1.running_mean\", \"layer3.1.bn1.running_var\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.bn2.bias\", \"layer3.1.bn2.running_mean\", \"layer3.1.bn2.running_var\", \"layer3.1.cbam.ChannelGate.mlp.1.weight\", \"layer3.1.cbam.ChannelGate.mlp.1.bias\", \"layer3.1.cbam.ChannelGate.mlp.3.weight\", \"layer3.1.cbam.ChannelGate.mlp.3.bias\", \"layer3.1.cbam.SpatialGate.spatial.conv.weight\", \"layer3.1.cbam.SpatialGate.spatial.bn.weight\", \"layer3.1.cbam.SpatialGate.spatial.bn.bias\", \"layer3.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.cbam.ChannelGate.mlp.1.weight\", \"layer3.2.cbam.ChannelGate.mlp.1.bias\", \"layer3.2.cbam.ChannelGate.mlp.3.weight\", \"layer3.2.cbam.ChannelGate.mlp.3.bias\", \"layer3.2.cbam.SpatialGate.spatial.conv.weight\", \"layer3.2.cbam.SpatialGate.spatial.bn.weight\", \"layer3.2.cbam.SpatialGate.spatial.bn.bias\", \"layer3.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.2.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.bn1.bias\", \"layer3.3.bn1.running_mean\", \"layer3.3.bn1.running_var\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.bn2.bias\", \"layer3.3.bn2.running_mean\", \"layer3.3.bn2.running_var\", \"layer3.3.cbam.ChannelGate.mlp.1.weight\", \"layer3.3.cbam.ChannelGate.mlp.1.bias\", \"layer3.3.cbam.ChannelGate.mlp.3.weight\", \"layer3.3.cbam.ChannelGate.mlp.3.bias\", \"layer3.3.cbam.SpatialGate.spatial.conv.weight\", \"layer3.3.cbam.SpatialGate.spatial.bn.weight\", \"layer3.3.cbam.SpatialGate.spatial.bn.bias\", \"layer3.3.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.3.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.bn1.bias\", \"layer3.4.bn1.running_mean\", \"layer3.4.bn1.running_var\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.bn2.bias\", \"layer3.4.bn2.running_mean\", \"layer3.4.bn2.running_var\", \"layer3.4.cbam.ChannelGate.mlp.1.weight\", \"layer3.4.cbam.ChannelGate.mlp.1.bias\", \"layer3.4.cbam.ChannelGate.mlp.3.weight\", \"layer3.4.cbam.ChannelGate.mlp.3.bias\", \"layer3.4.cbam.SpatialGate.spatial.conv.weight\", \"layer3.4.cbam.SpatialGate.spatial.bn.weight\", \"layer3.4.cbam.SpatialGate.spatial.bn.bias\", \"layer3.4.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.4.cbam.SpatialGate.spatial.bn.running_var\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.bn1.bias\", \"layer3.5.bn1.running_mean\", \"layer3.5.bn1.running_var\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.bn2.bias\", \"layer3.5.bn2.running_mean\", \"layer3.5.bn2.running_var\", \"layer3.5.cbam.ChannelGate.mlp.1.weight\", \"layer3.5.cbam.ChannelGate.mlp.1.bias\", \"layer3.5.cbam.ChannelGate.mlp.3.weight\", \"layer3.5.cbam.ChannelGate.mlp.3.bias\", \"layer3.5.cbam.SpatialGate.spatial.conv.weight\", \"layer3.5.cbam.SpatialGate.spatial.bn.weight\", \"layer3.5.cbam.SpatialGate.spatial.bn.bias\", \"layer3.5.cbam.SpatialGate.spatial.bn.running_mean\", \"layer3.5.cbam.SpatialGate.spatial.bn.running_var\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.0.cbam.ChannelGate.mlp.1.weight\", \"layer4.0.cbam.ChannelGate.mlp.1.bias\", \"layer4.0.cbam.ChannelGate.mlp.3.weight\", \"layer4.0.cbam.ChannelGate.mlp.3.bias\", \"layer4.0.cbam.SpatialGate.spatial.conv.weight\", \"layer4.0.cbam.SpatialGate.spatial.bn.weight\", \"layer4.0.cbam.SpatialGate.spatial.bn.bias\", \"layer4.0.cbam.SpatialGate.spatial.bn.running_mean\", \"layer4.0.cbam.SpatialGate.spatial.bn.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\", \"layer4.1.cbam.ChannelGate.mlp.1.weight\", \"layer4.1.cbam.ChannelGate.mlp.1.bias\", \"layer4.1.cbam.ChannelGate.mlp.3.weight\", \"layer4.1.cbam.ChannelGate.mlp.3.bias\", \"layer4.1.cbam.SpatialGate.spatial.conv.weight\", \"layer4.1.cbam.SpatialGate.spatial.bn.weight\", \"layer4.1.cbam.SpatialGate.spatial.bn.bias\", \"layer4.1.cbam.SpatialGate.spatial.bn.running_mean\", \"layer4.1.cbam.SpatialGate.spatial.bn.running_var\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.bn1.bias\", \"layer4.2.bn1.running_mean\", \"layer4.2.bn1.running_var\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.bn2.bias\", \"layer4.2.bn2.running_mean\", \"layer4.2.bn2.running_var\", \"layer4.2.cbam.ChannelGate.mlp.1.weight\", \"layer4.2.cbam.ChannelGate.mlp.1.bias\", \"layer4.2.cbam.ChannelGate.mlp.3.weight\", \"layer4.2.cbam.ChannelGate.mlp.3.bias\", \"layer4.2.cbam.SpatialGate.spatial.conv.weight\", \"layer4.2.cbam.SpatialGate.spatial.bn.weight\", \"layer4.2.cbam.SpatialGate.spatial.bn.bias\", \"layer4.2.cbam.SpatialGate.spatial.bn.running_mean\", \"layer4.2.cbam.SpatialGate.spatial.bn.running_var\", \"fc.weight\", \"fc.bias\", \"final_conv.weight\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"best_prec1\", \"state_dict\". "
     ]
    }
   ],
   "source": [
    "def load_model(ckpt_path, num_classes, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "#     model = YOLO_SWIN(num_classes=num_classes)\n",
    "#     model = YOLO_SWIN(num_classes=num_classes)\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], network_type=\"ImageNet\", num_classes=NUM_CLASSES, att_type=\"CBAM\")\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "ckpt_path=\"/workspace/Plastic_Bottle_defect_detection/experiments/trained_model/RESNET50_CBAM_new_name_wrap.pth\"\n",
    "model = load_model(ckpt_path, NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3dd80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3770/1777674582.py:131: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.fc.weight)\n",
      "/tmp/ipykernel_3770/1777674582.py:135: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=64, out_features=4, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=4, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=128, out_features=8, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=8, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=128, out_features=8, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=8, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=128, out_features=8, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=8, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=128, out_features=8, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=8, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=256, out_features=16, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=16, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=512, out_features=32, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=32, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=512, out_features=32, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=32, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (cbam): CBAM(\n",
       "        (ChannelGate): ChannelGate(\n",
       "          (mlp): Sequential(\n",
       "            (0): Flatten()\n",
       "            (1): Linear(in_features=512, out_features=32, bias=True)\n",
       "            (2): ReLU()\n",
       "            (3): Linear(in_features=32, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (SpatialGate): SpatialGate(\n",
       "          (compress): ChannelPool()\n",
       "          (spatial): BasicConv(\n",
       "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (final_conv): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "# model = YOLO_SWIN(num_classes=NUM_CLASSES)\n",
    "model = ResNet(BasicBlock, [3, 4, 6, 3], network_type=\"ImageNet\", num_classes=NUM_CLASSES, att_type=\"CBAM\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43ecd862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "          Flatten-10                   [-1, 64]               0\n",
      "           Linear-11                    [-1, 4]             260\n",
      "             ReLU-12                    [-1, 4]               0\n",
      "           Linear-13                   [-1, 64]             320\n",
      "          Flatten-14                   [-1, 64]               0\n",
      "           Linear-15                    [-1, 4]             260\n",
      "             ReLU-16                    [-1, 4]               0\n",
      "           Linear-17                   [-1, 64]             320\n",
      "      ChannelGate-18         [-1, 64, 112, 112]               0\n",
      "      ChannelPool-19          [-1, 2, 112, 112]               0\n",
      "           Conv2d-20          [-1, 1, 112, 112]              98\n",
      "      BatchNorm2d-21          [-1, 1, 112, 112]               2\n",
      "        BasicConv-22          [-1, 1, 112, 112]               0\n",
      "      SpatialGate-23         [-1, 64, 112, 112]               0\n",
      "             CBAM-24         [-1, 64, 112, 112]               0\n",
      "             ReLU-25         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-26         [-1, 64, 112, 112]               0\n",
      "           Conv2d-27         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-28         [-1, 64, 112, 112]             128\n",
      "             ReLU-29         [-1, 64, 112, 112]               0\n",
      "           Conv2d-30         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-31         [-1, 64, 112, 112]             128\n",
      "          Flatten-32                   [-1, 64]               0\n",
      "           Linear-33                    [-1, 4]             260\n",
      "             ReLU-34                    [-1, 4]               0\n",
      "           Linear-35                   [-1, 64]             320\n",
      "          Flatten-36                   [-1, 64]               0\n",
      "           Linear-37                    [-1, 4]             260\n",
      "             ReLU-38                    [-1, 4]               0\n",
      "           Linear-39                   [-1, 64]             320\n",
      "      ChannelGate-40         [-1, 64, 112, 112]               0\n",
      "      ChannelPool-41          [-1, 2, 112, 112]               0\n",
      "           Conv2d-42          [-1, 1, 112, 112]              98\n",
      "      BatchNorm2d-43          [-1, 1, 112, 112]               2\n",
      "        BasicConv-44          [-1, 1, 112, 112]               0\n",
      "      SpatialGate-45         [-1, 64, 112, 112]               0\n",
      "             CBAM-46         [-1, 64, 112, 112]               0\n",
      "             ReLU-47         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-48         [-1, 64, 112, 112]               0\n",
      "           Conv2d-49         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-50         [-1, 64, 112, 112]             128\n",
      "             ReLU-51         [-1, 64, 112, 112]               0\n",
      "           Conv2d-52         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-53         [-1, 64, 112, 112]             128\n",
      "          Flatten-54                   [-1, 64]               0\n",
      "           Linear-55                    [-1, 4]             260\n",
      "             ReLU-56                    [-1, 4]               0\n",
      "           Linear-57                   [-1, 64]             320\n",
      "          Flatten-58                   [-1, 64]               0\n",
      "           Linear-59                    [-1, 4]             260\n",
      "             ReLU-60                    [-1, 4]               0\n",
      "           Linear-61                   [-1, 64]             320\n",
      "      ChannelGate-62         [-1, 64, 112, 112]               0\n",
      "      ChannelPool-63          [-1, 2, 112, 112]               0\n",
      "           Conv2d-64          [-1, 1, 112, 112]              98\n",
      "      BatchNorm2d-65          [-1, 1, 112, 112]               2\n",
      "        BasicConv-66          [-1, 1, 112, 112]               0\n",
      "      SpatialGate-67         [-1, 64, 112, 112]               0\n",
      "             CBAM-68         [-1, 64, 112, 112]               0\n",
      "             ReLU-69         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-70         [-1, 64, 112, 112]               0\n",
      "           Conv2d-71          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-72          [-1, 128, 56, 56]             256\n",
      "             ReLU-73          [-1, 128, 56, 56]               0\n",
      "           Conv2d-74          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-75          [-1, 128, 56, 56]             256\n",
      "           Conv2d-76          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-77          [-1, 128, 56, 56]             256\n",
      "          Flatten-78                  [-1, 128]               0\n",
      "           Linear-79                    [-1, 8]           1,032\n",
      "             ReLU-80                    [-1, 8]               0\n",
      "           Linear-81                  [-1, 128]           1,152\n",
      "          Flatten-82                  [-1, 128]               0\n",
      "           Linear-83                    [-1, 8]           1,032\n",
      "             ReLU-84                    [-1, 8]               0\n",
      "           Linear-85                  [-1, 128]           1,152\n",
      "      ChannelGate-86          [-1, 128, 56, 56]               0\n",
      "      ChannelPool-87            [-1, 2, 56, 56]               0\n",
      "           Conv2d-88            [-1, 1, 56, 56]              98\n",
      "      BatchNorm2d-89            [-1, 1, 56, 56]               2\n",
      "        BasicConv-90            [-1, 1, 56, 56]               0\n",
      "      SpatialGate-91          [-1, 128, 56, 56]               0\n",
      "             CBAM-92          [-1, 128, 56, 56]               0\n",
      "             ReLU-93          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-94          [-1, 128, 56, 56]               0\n",
      "           Conv2d-95          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-96          [-1, 128, 56, 56]             256\n",
      "             ReLU-97          [-1, 128, 56, 56]               0\n",
      "           Conv2d-98          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-99          [-1, 128, 56, 56]             256\n",
      "         Flatten-100                  [-1, 128]               0\n",
      "          Linear-101                    [-1, 8]           1,032\n",
      "            ReLU-102                    [-1, 8]               0\n",
      "          Linear-103                  [-1, 128]           1,152\n",
      "         Flatten-104                  [-1, 128]               0\n",
      "          Linear-105                    [-1, 8]           1,032\n",
      "            ReLU-106                    [-1, 8]               0\n",
      "          Linear-107                  [-1, 128]           1,152\n",
      "     ChannelGate-108          [-1, 128, 56, 56]               0\n",
      "     ChannelPool-109            [-1, 2, 56, 56]               0\n",
      "          Conv2d-110            [-1, 1, 56, 56]              98\n",
      "     BatchNorm2d-111            [-1, 1, 56, 56]               2\n",
      "       BasicConv-112            [-1, 1, 56, 56]               0\n",
      "     SpatialGate-113          [-1, 128, 56, 56]               0\n",
      "            CBAM-114          [-1, 128, 56, 56]               0\n",
      "            ReLU-115          [-1, 128, 56, 56]               0\n",
      "      BasicBlock-116          [-1, 128, 56, 56]               0\n",
      "          Conv2d-117          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-118          [-1, 128, 56, 56]             256\n",
      "            ReLU-119          [-1, 128, 56, 56]               0\n",
      "          Conv2d-120          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-121          [-1, 128, 56, 56]             256\n",
      "         Flatten-122                  [-1, 128]               0\n",
      "          Linear-123                    [-1, 8]           1,032\n",
      "            ReLU-124                    [-1, 8]               0\n",
      "          Linear-125                  [-1, 128]           1,152\n",
      "         Flatten-126                  [-1, 128]               0\n",
      "          Linear-127                    [-1, 8]           1,032\n",
      "            ReLU-128                    [-1, 8]               0\n",
      "          Linear-129                  [-1, 128]           1,152\n",
      "     ChannelGate-130          [-1, 128, 56, 56]               0\n",
      "     ChannelPool-131            [-1, 2, 56, 56]               0\n",
      "          Conv2d-132            [-1, 1, 56, 56]              98\n",
      "     BatchNorm2d-133            [-1, 1, 56, 56]               2\n",
      "       BasicConv-134            [-1, 1, 56, 56]               0\n",
      "     SpatialGate-135          [-1, 128, 56, 56]               0\n",
      "            CBAM-136          [-1, 128, 56, 56]               0\n",
      "            ReLU-137          [-1, 128, 56, 56]               0\n",
      "      BasicBlock-138          [-1, 128, 56, 56]               0\n",
      "          Conv2d-139          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-140          [-1, 128, 56, 56]             256\n",
      "            ReLU-141          [-1, 128, 56, 56]               0\n",
      "          Conv2d-142          [-1, 128, 56, 56]         147,456\n",
      "     BatchNorm2d-143          [-1, 128, 56, 56]             256\n",
      "         Flatten-144                  [-1, 128]               0\n",
      "          Linear-145                    [-1, 8]           1,032\n",
      "            ReLU-146                    [-1, 8]               0\n",
      "          Linear-147                  [-1, 128]           1,152\n",
      "         Flatten-148                  [-1, 128]               0\n",
      "          Linear-149                    [-1, 8]           1,032\n",
      "            ReLU-150                    [-1, 8]               0\n",
      "          Linear-151                  [-1, 128]           1,152\n",
      "     ChannelGate-152          [-1, 128, 56, 56]               0\n",
      "     ChannelPool-153            [-1, 2, 56, 56]               0\n",
      "          Conv2d-154            [-1, 1, 56, 56]              98\n",
      "     BatchNorm2d-155            [-1, 1, 56, 56]               2\n",
      "       BasicConv-156            [-1, 1, 56, 56]               0\n",
      "     SpatialGate-157          [-1, 128, 56, 56]               0\n",
      "            CBAM-158          [-1, 128, 56, 56]               0\n",
      "            ReLU-159          [-1, 128, 56, 56]               0\n",
      "      BasicBlock-160          [-1, 128, 56, 56]               0\n",
      "          Conv2d-161          [-1, 256, 28, 28]         294,912\n",
      "     BatchNorm2d-162          [-1, 256, 28, 28]             512\n",
      "            ReLU-163          [-1, 256, 28, 28]               0\n",
      "          Conv2d-164          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 28, 28]             512\n",
      "          Conv2d-166          [-1, 256, 28, 28]          32,768\n",
      "     BatchNorm2d-167          [-1, 256, 28, 28]             512\n",
      "         Flatten-168                  [-1, 256]               0\n",
      "          Linear-169                   [-1, 16]           4,112\n",
      "            ReLU-170                   [-1, 16]               0\n",
      "          Linear-171                  [-1, 256]           4,352\n",
      "         Flatten-172                  [-1, 256]               0\n",
      "          Linear-173                   [-1, 16]           4,112\n",
      "            ReLU-174                   [-1, 16]               0\n",
      "          Linear-175                  [-1, 256]           4,352\n",
      "     ChannelGate-176          [-1, 256, 28, 28]               0\n",
      "     ChannelPool-177            [-1, 2, 28, 28]               0\n",
      "          Conv2d-178            [-1, 1, 28, 28]              98\n",
      "     BatchNorm2d-179            [-1, 1, 28, 28]               2\n",
      "       BasicConv-180            [-1, 1, 28, 28]               0\n",
      "     SpatialGate-181          [-1, 256, 28, 28]               0\n",
      "            CBAM-182          [-1, 256, 28, 28]               0\n",
      "            ReLU-183          [-1, 256, 28, 28]               0\n",
      "      BasicBlock-184          [-1, 256, 28, 28]               0\n",
      "          Conv2d-185          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-186          [-1, 256, 28, 28]             512\n",
      "            ReLU-187          [-1, 256, 28, 28]               0\n",
      "          Conv2d-188          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-189          [-1, 256, 28, 28]             512\n",
      "         Flatten-190                  [-1, 256]               0\n",
      "          Linear-191                   [-1, 16]           4,112\n",
      "            ReLU-192                   [-1, 16]               0\n",
      "          Linear-193                  [-1, 256]           4,352\n",
      "         Flatten-194                  [-1, 256]               0\n",
      "          Linear-195                   [-1, 16]           4,112\n",
      "            ReLU-196                   [-1, 16]               0\n",
      "          Linear-197                  [-1, 256]           4,352\n",
      "     ChannelGate-198          [-1, 256, 28, 28]               0\n",
      "     ChannelPool-199            [-1, 2, 28, 28]               0\n",
      "          Conv2d-200            [-1, 1, 28, 28]              98\n",
      "     BatchNorm2d-201            [-1, 1, 28, 28]               2\n",
      "       BasicConv-202            [-1, 1, 28, 28]               0\n",
      "     SpatialGate-203          [-1, 256, 28, 28]               0\n",
      "            CBAM-204          [-1, 256, 28, 28]               0\n",
      "            ReLU-205          [-1, 256, 28, 28]               0\n",
      "      BasicBlock-206          [-1, 256, 28, 28]               0\n",
      "          Conv2d-207          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-208          [-1, 256, 28, 28]             512\n",
      "            ReLU-209          [-1, 256, 28, 28]               0\n",
      "          Conv2d-210          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-211          [-1, 256, 28, 28]             512\n",
      "         Flatten-212                  [-1, 256]               0\n",
      "          Linear-213                   [-1, 16]           4,112\n",
      "            ReLU-214                   [-1, 16]               0\n",
      "          Linear-215                  [-1, 256]           4,352\n",
      "         Flatten-216                  [-1, 256]               0\n",
      "          Linear-217                   [-1, 16]           4,112\n",
      "            ReLU-218                   [-1, 16]               0\n",
      "          Linear-219                  [-1, 256]           4,352\n",
      "     ChannelGate-220          [-1, 256, 28, 28]               0\n",
      "     ChannelPool-221            [-1, 2, 28, 28]               0\n",
      "          Conv2d-222            [-1, 1, 28, 28]              98\n",
      "     BatchNorm2d-223            [-1, 1, 28, 28]               2\n",
      "       BasicConv-224            [-1, 1, 28, 28]               0\n",
      "     SpatialGate-225          [-1, 256, 28, 28]               0\n",
      "            CBAM-226          [-1, 256, 28, 28]               0\n",
      "            ReLU-227          [-1, 256, 28, 28]               0\n",
      "      BasicBlock-228          [-1, 256, 28, 28]               0\n",
      "          Conv2d-229          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-230          [-1, 256, 28, 28]             512\n",
      "            ReLU-231          [-1, 256, 28, 28]               0\n",
      "          Conv2d-232          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-233          [-1, 256, 28, 28]             512\n",
      "         Flatten-234                  [-1, 256]               0\n",
      "          Linear-235                   [-1, 16]           4,112\n",
      "            ReLU-236                   [-1, 16]               0\n",
      "          Linear-237                  [-1, 256]           4,352\n",
      "         Flatten-238                  [-1, 256]               0\n",
      "          Linear-239                   [-1, 16]           4,112\n",
      "            ReLU-240                   [-1, 16]               0\n",
      "          Linear-241                  [-1, 256]           4,352\n",
      "     ChannelGate-242          [-1, 256, 28, 28]               0\n",
      "     ChannelPool-243            [-1, 2, 28, 28]               0\n",
      "          Conv2d-244            [-1, 1, 28, 28]              98\n",
      "     BatchNorm2d-245            [-1, 1, 28, 28]               2\n",
      "       BasicConv-246            [-1, 1, 28, 28]               0\n",
      "     SpatialGate-247          [-1, 256, 28, 28]               0\n",
      "            CBAM-248          [-1, 256, 28, 28]               0\n",
      "            ReLU-249          [-1, 256, 28, 28]               0\n",
      "      BasicBlock-250          [-1, 256, 28, 28]               0\n",
      "          Conv2d-251          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-252          [-1, 256, 28, 28]             512\n",
      "            ReLU-253          [-1, 256, 28, 28]               0\n",
      "          Conv2d-254          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 28, 28]             512\n",
      "         Flatten-256                  [-1, 256]               0\n",
      "          Linear-257                   [-1, 16]           4,112\n",
      "            ReLU-258                   [-1, 16]               0\n",
      "          Linear-259                  [-1, 256]           4,352\n",
      "         Flatten-260                  [-1, 256]               0\n",
      "          Linear-261                   [-1, 16]           4,112\n",
      "            ReLU-262                   [-1, 16]               0\n",
      "          Linear-263                  [-1, 256]           4,352\n",
      "     ChannelGate-264          [-1, 256, 28, 28]               0\n",
      "     ChannelPool-265            [-1, 2, 28, 28]               0\n",
      "          Conv2d-266            [-1, 1, 28, 28]              98\n",
      "     BatchNorm2d-267            [-1, 1, 28, 28]               2\n",
      "       BasicConv-268            [-1, 1, 28, 28]               0\n",
      "     SpatialGate-269          [-1, 256, 28, 28]               0\n",
      "            CBAM-270          [-1, 256, 28, 28]               0\n",
      "            ReLU-271          [-1, 256, 28, 28]               0\n",
      "      BasicBlock-272          [-1, 256, 28, 28]               0\n",
      "          Conv2d-273          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-274          [-1, 256, 28, 28]             512\n",
      "            ReLU-275          [-1, 256, 28, 28]               0\n",
      "          Conv2d-276          [-1, 256, 28, 28]         589,824\n",
      "     BatchNorm2d-277          [-1, 256, 28, 28]             512\n",
      "         Flatten-278                  [-1, 256]               0\n",
      "          Linear-279                   [-1, 16]           4,112\n",
      "            ReLU-280                   [-1, 16]               0\n",
      "          Linear-281                  [-1, 256]           4,352\n",
      "         Flatten-282                  [-1, 256]               0\n",
      "          Linear-283                   [-1, 16]           4,112\n",
      "            ReLU-284                   [-1, 16]               0\n",
      "          Linear-285                  [-1, 256]           4,352\n",
      "     ChannelGate-286          [-1, 256, 28, 28]               0\n",
      "     ChannelPool-287            [-1, 2, 28, 28]               0\n",
      "          Conv2d-288            [-1, 1, 28, 28]              98\n",
      "     BatchNorm2d-289            [-1, 1, 28, 28]               2\n",
      "       BasicConv-290            [-1, 1, 28, 28]               0\n",
      "     SpatialGate-291          [-1, 256, 28, 28]               0\n",
      "            CBAM-292          [-1, 256, 28, 28]               0\n",
      "            ReLU-293          [-1, 256, 28, 28]               0\n",
      "      BasicBlock-294          [-1, 256, 28, 28]               0\n",
      "          Conv2d-295          [-1, 512, 14, 14]       1,179,648\n",
      "     BatchNorm2d-296          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-297          [-1, 512, 14, 14]               0\n",
      "          Conv2d-298          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-299          [-1, 512, 14, 14]           1,024\n",
      "          Conv2d-300          [-1, 512, 14, 14]         131,072\n",
      "     BatchNorm2d-301          [-1, 512, 14, 14]           1,024\n",
      "         Flatten-302                  [-1, 512]               0\n",
      "          Linear-303                   [-1, 32]          16,416\n",
      "            ReLU-304                   [-1, 32]               0\n",
      "          Linear-305                  [-1, 512]          16,896\n",
      "         Flatten-306                  [-1, 512]               0\n",
      "          Linear-307                   [-1, 32]          16,416\n",
      "            ReLU-308                   [-1, 32]               0\n",
      "          Linear-309                  [-1, 512]          16,896\n",
      "     ChannelGate-310          [-1, 512, 14, 14]               0\n",
      "     ChannelPool-311            [-1, 2, 14, 14]               0\n",
      "          Conv2d-312            [-1, 1, 14, 14]              98\n",
      "     BatchNorm2d-313            [-1, 1, 14, 14]               2\n",
      "       BasicConv-314            [-1, 1, 14, 14]               0\n",
      "     SpatialGate-315          [-1, 512, 14, 14]               0\n",
      "            CBAM-316          [-1, 512, 14, 14]               0\n",
      "            ReLU-317          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-318          [-1, 512, 14, 14]               0\n",
      "          Conv2d-319          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-320          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-321          [-1, 512, 14, 14]               0\n",
      "          Conv2d-322          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-323          [-1, 512, 14, 14]           1,024\n",
      "         Flatten-324                  [-1, 512]               0\n",
      "          Linear-325                   [-1, 32]          16,416\n",
      "            ReLU-326                   [-1, 32]               0\n",
      "          Linear-327                  [-1, 512]          16,896\n",
      "         Flatten-328                  [-1, 512]               0\n",
      "          Linear-329                   [-1, 32]          16,416\n",
      "            ReLU-330                   [-1, 32]               0\n",
      "          Linear-331                  [-1, 512]          16,896\n",
      "     ChannelGate-332          [-1, 512, 14, 14]               0\n",
      "     ChannelPool-333            [-1, 2, 14, 14]               0\n",
      "          Conv2d-334            [-1, 1, 14, 14]              98\n",
      "     BatchNorm2d-335            [-1, 1, 14, 14]               2\n",
      "       BasicConv-336            [-1, 1, 14, 14]               0\n",
      "     SpatialGate-337          [-1, 512, 14, 14]               0\n",
      "            CBAM-338          [-1, 512, 14, 14]               0\n",
      "            ReLU-339          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-340          [-1, 512, 14, 14]               0\n",
      "          Conv2d-341          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-342          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-343          [-1, 512, 14, 14]               0\n",
      "          Conv2d-344          [-1, 512, 14, 14]       2,359,296\n",
      "     BatchNorm2d-345          [-1, 512, 14, 14]           1,024\n",
      "         Flatten-346                  [-1, 512]               0\n",
      "          Linear-347                   [-1, 32]          16,416\n",
      "            ReLU-348                   [-1, 32]               0\n",
      "          Linear-349                  [-1, 512]          16,896\n",
      "         Flatten-350                  [-1, 512]               0\n",
      "          Linear-351                   [-1, 32]          16,416\n",
      "            ReLU-352                   [-1, 32]               0\n",
      "          Linear-353                  [-1, 512]          16,896\n",
      "     ChannelGate-354          [-1, 512, 14, 14]               0\n",
      "     ChannelPool-355            [-1, 2, 14, 14]               0\n",
      "          Conv2d-356            [-1, 1, 14, 14]              98\n",
      "     BatchNorm2d-357            [-1, 1, 14, 14]               2\n",
      "       BasicConv-358            [-1, 1, 14, 14]               0\n",
      "     SpatialGate-359          [-1, 512, 14, 14]               0\n",
      "            CBAM-360          [-1, 512, 14, 14]               0\n",
      "            ReLU-361          [-1, 512, 14, 14]               0\n",
      "      BasicBlock-362          [-1, 512, 14, 14]               0\n",
      "          Conv2d-363             [-1, 12, 7, 7]           6,144\n",
      "================================================================\n",
      "Total params: 21,614,808\n",
      "Trainable params: 21,614,808\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 513.68\n",
      "Params size (MB): 82.45\n",
      "Estimated Total Size (MB): 598.43\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cf0424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 448, 448).to(device)\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df4b3a",
   "metadata": {},
   "source": [
    "## Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_LOSS():\n",
    "    def __init__(self, num_classes, device, lambda_coord=5., lambda_noobj=0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.grid_size = 7\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def __call__(self, predictions, targets):\n",
    "        self.batch_size, _, _, _ = predictions.shape\n",
    "        groundtruths = self.build_batch_target_grid(targets)\n",
    "        groundtruths = groundtruths.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iou1 = self.get_IoU(predictions[:, 1:5, ...], groundtruths[:, 1:5, ...])\n",
    "            iou2 = self.get_IoU(predictions[:, 6:10, ...], groundtruths[:, 1:5, ...])\n",
    "\n",
    "        ious = torch.stack([iou1, iou2], dim=1)\n",
    "        max_iou, best_box = ious.max(dim=1, keepdim=True)\n",
    "        max_iou = torch.cat([max_iou, max_iou], dim=1)\n",
    "        best_box = torch.cat([best_box.eq(0), best_box.eq(1)], dim=1)\n",
    "\n",
    "        predictions_ = predictions[:, :5*2, ...].reshape(self.batch_size, 2, 5, self.grid_size, self.grid_size)\n",
    "        obj_pred = predictions_[:, :, 0, ...]\n",
    "        xy_pred = predictions_[:, :, 1:3, ...]\n",
    "        wh_pred = predictions_[:, :, 3:5, ...]\n",
    "        cls_pred = predictions[:, 5*2:, ...]\n",
    "\n",
    "        groundtruths_ = groundtruths[:, :5, ...].reshape(self.batch_size, 1, 5, self.grid_size, self.grid_size)\n",
    "        obj_target = groundtruths_[:, :, 0, ...]\n",
    "        xy_target = groundtruths_[:, :, 1:3, ...]\n",
    "        wh_target= groundtruths_[:, :, 3:5, ...]\n",
    "        cls_target = groundtruths[:, 5:, ...]\n",
    "        \n",
    "        positive = obj_target * best_box\n",
    "\n",
    "        obj_loss = self.mse_loss(positive * obj_pred, positive * ious)\n",
    "        noobj_loss = self.mse_loss((1 - positive) * obj_pred, ious*0)\n",
    "        xy_loss = self.mse_loss(positive.unsqueeze(dim=2) * xy_pred, positive.unsqueeze(dim=2) * xy_target)\n",
    "        wh_loss = self.mse_loss(positive.unsqueeze(dim=2) * (wh_pred.sign() * (wh_pred.abs() + 1e-8).sqrt()),\n",
    "                           positive.unsqueeze(dim=2) * (wh_target + 1e-8).sqrt())\n",
    "        cls_loss = self.mse_loss(obj_target * cls_pred, cls_target)\n",
    "        \n",
    "        obj_loss /= self.batch_size\n",
    "        noobj_loss /= self.batch_size\n",
    "        bbox_loss = (xy_loss+wh_loss) / self.batch_size\n",
    "        cls_loss /= self.batch_size\n",
    "        \n",
    "        total_loss = obj_loss + self.lambda_noobj*noobj_loss + self.lambda_coord*bbox_loss + cls_loss\n",
    "        return total_loss, (obj_loss.item(), noobj_loss.item(), bbox_loss.item(), cls_loss.item())\n",
    "    \n",
    "    def build_target_grid(self, target):\n",
    "        target_grid = torch.zeros((1+4+self.num_classes, self.grid_size, self.grid_size), device=self.device)\n",
    "\n",
    "        for gt in target:\n",
    "            xc, yc, w, h, cls_id = gt\n",
    "            xn = (xc % (1/self.grid_size))\n",
    "            yn = (yc % (1/self.grid_size))\n",
    "            cls_id = int(cls_id)\n",
    "\n",
    "            i_grid = int(xc * self.grid_size)\n",
    "            j_grid = int(yc * self.grid_size)\n",
    "            target_grid[0, j_grid, i_grid] = 1\n",
    "            target_grid[1:5, j_grid, i_grid] = torch.Tensor([xn,yn,w,h])\n",
    "#             print(5+cls_id, j_grid, i_grid)\n",
    "            target_grid[5+cls_id, j_grid, i_grid] = 1\n",
    "\n",
    "        return target_grid\n",
    "    \n",
    "    def build_batch_target_grid(self, targets):\n",
    "        target_grid_batch = torch.stack([self.build_target_grid(target) for target in targets], dim=0)\n",
    "        return target_grid_batch\n",
    "    \n",
    "    def get_IoU(self, cbox1, cbox2):\n",
    "        box1 = self.xywh_to_xyxy(cbox1)\n",
    "        box2 = self.xywh_to_xyxy(cbox2)\n",
    "\n",
    "        x1 = torch.max(box1[:, 0, ...], box2[:, 0, ...])\n",
    "        y1 = torch.max(box1[:, 1, ...], box2[:, 1, ...])\n",
    "        x2 = torch.min(box1[:, 2, ...], box2[:, 2, ...])\n",
    "        y2 = torch.min(box1[:, 3, ...], box2[:, 3, ...])\n",
    "\n",
    "        intersection = (x2-x1).clamp(min=0) * (y2-y1).clamp(min=0)\n",
    "        union = abs(cbox1[:, 2, ...]*cbox1[:, 3, ...]) + \\\n",
    "                abs(cbox2[:, 2, ...]*cbox2[:, 3, ...]) - intersection\n",
    "\n",
    "        intersection[intersection.gt(0)] = intersection[intersection.gt(0)] / union[intersection.gt(0)]\n",
    "        return intersection\n",
    "    \n",
    "    def generate_xy_normed_grid(self):\n",
    "        y_offset, x_offset = torch.meshgrid(torch.arange(self.grid_size), torch.arange(self.grid_size))\n",
    "        xy_grid = torch.stack([x_offset, y_offset], dim=0)\n",
    "        xy_normed_grid = xy_grid / self.grid_size\n",
    "        return xy_normed_grid.to(self.device)\n",
    "\n",
    "    def xywh_to_xyxy(self, bboxes):\n",
    "        xy_normed_grid = self.generate_xy_normed_grid()\n",
    "        xcyc = bboxes[:,0:2,...] + xy_normed_grid.tile(self.batch_size, 1,1,1)\n",
    "        wh = bboxes[:,2:4,...]\n",
    "        x1y1 = xcyc - (wh/2)\n",
    "        x2y2 = xcyc + (wh/2)\n",
    "        return torch.cat([x1y1, x2y2], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbfb76",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "    \n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        running_loss = defaultdict(float)\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0].to(device)\n",
    "            targets = batch[1]\n",
    "            filenames = batch[2]\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"): # phase가 train 일때만 gradient 추적기능을 킨다.\n",
    "                predictions = model(images) #prediction shape=> B,12,7,7\n",
    "#             print(f\"predictions:{predictions}, \\ntargets: {targets}\\n\")\n",
    "            loss, (obj_loss, noobj_loss, bbox_loss, cls_loss) = criterion(predictions, targets)\n",
    "#             print(f\"loss:{loss}, obj_loss:{obj_loss}, noobj_loss:{noobj_loss}\\nbbox_loss:{bbox_loss}, cls_loss:{cls_loss}\\n--------------\\n\")\n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 현재 epoch단계에서 loss가 얼마인지 running loss 가출력\n",
    "                running_loss[\"total_loss\"] += loss.item()\n",
    "                running_loss[\"obj_loss\"] += obj_loss\n",
    "                running_loss[\"noobj_loss\"] += noobj_loss\n",
    "                running_loss[\"bbox_loss\"] += bbox_loss\n",
    "                running_loss[\"cls_loss\"] += cls_loss\n",
    "                \n",
    "                train_loss[\"total_loss\"] += loss.item()\n",
    "                train_loss[\"obj_loss\"] += obj_loss\n",
    "                train_loss[\"noobj_loss\"] += noobj_loss\n",
    "                train_loss[\"bbox_loss\"] += bbox_loss\n",
    "                train_loss[\"cls_loss\"] += cls_loss\n",
    "                \n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                    text = f\"<<<iteration:[{index}/{len(dataloaders[phase])}] - \"\n",
    "                    for k, v in running_loss.items():\n",
    "                        text += f\"{k}: {v/VERBOSE_FREQ:.4f}  \"\n",
    "                        running_loss[k] = 0.\n",
    "                    print(text)\n",
    "            else:\n",
    "                val_loss[\"total_loss\"] += loss.item()\n",
    "                val_loss[\"obj_loss\"] += obj_loss\n",
    "                val_loss[\"noobj_loss\"] += noobj_loss\n",
    "                val_loss[\"bbox_loss\"] += bbox_loss\n",
    "                val_loss[\"cls_loss\"] += cls_loss\n",
    "\n",
    "    for k in train_loss.keys():\n",
    "        train_loss[k] /= len(dataloaders[\"train\"])\n",
    "        val_loss[k] /= len(dataloaders[\"val\"])\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97049805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(part, NECK_PATH, BODY_PATH, batch_size=2, aug_factor=0):\n",
    "    IMAGE_SIZE = 448\n",
    "    transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    "    )\n",
    "    augmentator=A.Compose([\n",
    "    #     A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "    #     A.Sharpen(p=0.7),\n",
    "        A.BBoxSafeRandomCrop(p=0.6),\n",
    "        A.VerticalFlip (p=0.6),\n",
    "        A.HueSaturationValue(p=0.6),\n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    "    )\n",
    "    \n",
    "    dataloaders = {}\n",
    "#     train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "#     train_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=augmentator, aug_factor=aug_factor)\n",
    "    train_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer, aug=None)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#     val_dataset = Detection_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
    "    val_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='valid', transformer=transformer, aug=None)\n",
    "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    print(f\"trainset:{len(train_dataset)} validset:{len(val_dataset)}\")\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c260f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making augmented images-- augmented factor:0\n",
      "total length of augmented images: 0\n",
      "start making augmented images-- augmented factor:0\n",
      "total length of augmented images: 0\n",
      "trainset:10500 validset:1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3770/1777674582.py:131: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.fc.weight)\n",
      "/tmp/ipykernel_3770/1777674582.py:135: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n"
     ]
    }
   ],
   "source": [
    "# data_dir = \"/content/drive/MyDrive/fastCamMedicalProj/DATASET/DATASET/Detection/\"\n",
    "# NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "# BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "NECK_PATH = '/home/host_data/PET_data_image_patching/patched_Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data_image_patching/Body'\n",
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 16\n",
    "VERBOSE_FREQ = 20\n",
    "LR=0.0001\n",
    "AUG_FACTOR=0\n",
    "PATCH_FACTOR=50\n",
    "BACKBONE=\"YOLO_RESNET_CBAM\"\n",
    "PART=\"neck\"\n",
    "num_epochs = 100\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "\n",
    "dataloaders = build_dataloader(part=PART,NECK_PATH=NECK_PATH,BODY_PATH=BODY_PATH,batch_size=BATCH_SIZE, aug_factor=AUG_FACTOR)\n",
    "model = ResNet(BasicBlock, [3, 4, 6, 3], network_type=\"ImageNet\", num_classes=NUM_CLASSES, att_type=\"CBAM\")\n",
    "model = model.to(device)\n",
    "criterion = YOLO_LOSS(num_classes=NUM_CLASSES, device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "817b20fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgomduribo\u001b[0m (\u001b[33murp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/Plastic_Bottle_defect_detection/experiments/wandb/run-20231029_164736-a7s8x4ne</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/a7s8x4ne' target=\"_blank\">giddy-leaf-2</a></strong> to <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/a7s8x4ne' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/a7s8x4ne</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/a7s8x4ne?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f328c9bafa0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"yolo_cbam_neck_IMAGE_PATCH\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"architecture\": BACKBONE,\n",
    "    \"dataset\": PART,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"patch factor\":PATCH_FACTOR,\n",
    "    \"aug factor\":AUG_FACTOR,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b358951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3423.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[20/657] - total_loss: 7403.3331  obj_loss: 406.4925  noobj_loss: 12789.6373  bbox_loss: 116.8576  cls_loss: 17.7339  \n",
      "<<<iteration:[40/657] - total_loss: 69.6353  obj_loss: 1.2321  noobj_loss: 60.8392  bbox_loss: 6.4654  cls_loss: 5.6564  \n",
      "<<<iteration:[60/657] - total_loss: 38.2591  obj_loss: 0.5847  noobj_loss: 36.3274  bbox_loss: 3.2739  cls_loss: 3.1411  \n",
      "<<<iteration:[80/657] - total_loss: 30.2336  obj_loss: 0.5018  noobj_loss: 26.2221  bbox_loss: 2.8558  cls_loss: 2.3420  \n",
      "<<<iteration:[100/657] - total_loss: 24.2520  obj_loss: 0.3655  noobj_loss: 20.9246  bbox_loss: 2.2766  cls_loss: 2.0411  \n",
      "<<<iteration:[120/657] - total_loss: 18.9928  obj_loss: 0.3464  noobj_loss: 16.5720  bbox_loss: 1.7445  cls_loss: 1.6381  \n",
      "<<<iteration:[140/657] - total_loss: 16.1883  obj_loss: 0.2349  noobj_loss: 13.9078  bbox_loss: 1.5359  cls_loss: 1.3201  \n",
      "<<<iteration:[160/657] - total_loss: 15.8058  obj_loss: 0.1900  noobj_loss: 12.5080  bbox_loss: 1.5885  cls_loss: 1.4192  \n",
      "<<<iteration:[180/657] - total_loss: 13.5933  obj_loss: 0.1965  noobj_loss: 11.5308  bbox_loss: 1.2863  cls_loss: 1.2000  \n",
      "<<<iteration:[200/657] - total_loss: 12.4561  obj_loss: 0.1542  noobj_loss: 10.2439  bbox_loss: 1.2224  cls_loss: 1.0681  \n",
      "<<<iteration:[220/657] - total_loss: 12.7958  obj_loss: 0.1334  noobj_loss: 9.5216  bbox_loss: 1.3472  cls_loss: 1.1655  \n",
      "<<<iteration:[240/657] - total_loss: 11.5930  obj_loss: 0.1405  noobj_loss: 8.5848  bbox_loss: 1.2110  cls_loss: 1.1050  \n",
      "<<<iteration:[260/657] - total_loss: 10.0579  obj_loss: 0.1211  noobj_loss: 7.9526  bbox_loss: 1.0189  cls_loss: 0.8659  \n",
      "<<<iteration:[280/657] - total_loss: 9.7731  obj_loss: 0.1211  noobj_loss: 7.5441  bbox_loss: 1.0068  cls_loss: 0.8461  \n",
      "<<<iteration:[300/657] - total_loss: 9.4877  obj_loss: 0.1032  noobj_loss: 6.9685  bbox_loss: 1.0153  cls_loss: 0.8238  \n",
      "<<<iteration:[320/657] - total_loss: 8.2029  obj_loss: 0.0968  noobj_loss: 6.7602  bbox_loss: 0.7800  cls_loss: 0.8262  \n",
      "<<<iteration:[340/657] - total_loss: 7.2688  obj_loss: 0.0914  noobj_loss: 6.1723  bbox_loss: 0.6798  cls_loss: 0.6923  \n",
      "<<<iteration:[360/657] - total_loss: 7.0135  obj_loss: 0.0817  noobj_loss: 6.0413  bbox_loss: 0.6638  cls_loss: 0.5919  \n",
      "<<<iteration:[380/657] - total_loss: 7.7742  obj_loss: 0.0918  noobj_loss: 5.9365  bbox_loss: 0.8212  cls_loss: 0.6084  \n",
      "<<<iteration:[400/657] - total_loss: 7.1878  obj_loss: 0.0850  noobj_loss: 5.5767  bbox_loss: 0.7506  cls_loss: 0.5613  \n",
      "<<<iteration:[420/657] - total_loss: 6.9750  obj_loss: 0.0798  noobj_loss: 5.2915  bbox_loss: 0.7234  cls_loss: 0.6321  \n",
      "<<<iteration:[440/657] - total_loss: 7.1969  obj_loss: 0.0811  noobj_loss: 5.1258  bbox_loss: 0.7675  cls_loss: 0.7155  \n",
      "<<<iteration:[460/657] - total_loss: 7.5695  obj_loss: 0.0847  noobj_loss: 5.2501  bbox_loss: 0.8388  cls_loss: 0.6657  \n",
      "<<<iteration:[480/657] - total_loss: 5.9850  obj_loss: 0.0827  noobj_loss: 4.7408  bbox_loss: 0.5946  cls_loss: 0.5586  \n",
      "<<<iteration:[500/657] - total_loss: 5.8133  obj_loss: 0.0716  noobj_loss: 4.7691  bbox_loss: 0.5564  cls_loss: 0.5753  \n",
      "<<<iteration:[520/657] - total_loss: 5.7738  obj_loss: 0.0758  noobj_loss: 4.4974  bbox_loss: 0.5979  cls_loss: 0.4599  \n",
      "<<<iteration:[540/657] - total_loss: 5.5118  obj_loss: 0.0752  noobj_loss: 4.1896  bbox_loss: 0.5665  cls_loss: 0.5092  \n",
      "<<<iteration:[560/657] - total_loss: 5.3426  obj_loss: 0.0664  noobj_loss: 4.2771  bbox_loss: 0.5275  cls_loss: 0.5000  \n",
      "<<<iteration:[580/657] - total_loss: 6.5936  obj_loss: 0.0751  noobj_loss: 4.5638  bbox_loss: 0.7381  cls_loss: 0.5459  \n",
      "<<<iteration:[600/657] - total_loss: 5.6671  obj_loss: 0.0685  noobj_loss: 4.1646  bbox_loss: 0.5983  cls_loss: 0.5247  \n",
      "<<<iteration:[620/657] - total_loss: 5.4754  obj_loss: 0.0725  noobj_loss: 3.8060  bbox_loss: 0.5974  cls_loss: 0.5127  \n",
      "<<<iteration:[640/657] - total_loss: 4.3909  obj_loss: 0.0603  noobj_loss: 3.7304  bbox_loss: 0.4122  cls_loss: 0.4046  \n",
      "\n",
      "epoch:1/100 - Train Loss: 237.7525, Val Loss: 4.7667\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 4.1803  obj_loss: 0.0600  noobj_loss: 3.6019  bbox_loss: 0.3750  cls_loss: 0.4445  \n",
      "<<<iteration:[40/657] - total_loss: 4.3443  obj_loss: 0.0607  noobj_loss: 3.5963  bbox_loss: 0.4122  cls_loss: 0.4247  \n",
      "<<<iteration:[60/657] - total_loss: 4.3646  obj_loss: 0.0728  noobj_loss: 3.4089  bbox_loss: 0.4330  cls_loss: 0.4224  \n",
      "<<<iteration:[80/657] - total_loss: 4.0566  obj_loss: 0.0542  noobj_loss: 3.4274  bbox_loss: 0.3683  cls_loss: 0.4469  \n",
      "<<<iteration:[100/657] - total_loss: 4.1103  obj_loss: 0.0526  noobj_loss: 3.1854  bbox_loss: 0.4061  cls_loss: 0.4344  \n",
      "<<<iteration:[120/657] - total_loss: 4.4875  obj_loss: 0.0619  noobj_loss: 3.3728  bbox_loss: 0.4475  cls_loss: 0.5018  \n",
      "<<<iteration:[140/657] - total_loss: 3.9035  obj_loss: 0.0547  noobj_loss: 3.1889  bbox_loss: 0.3780  cls_loss: 0.3642  \n",
      "<<<iteration:[160/657] - total_loss: 3.6124  obj_loss: 0.0507  noobj_loss: 3.0917  bbox_loss: 0.3284  cls_loss: 0.3739  \n",
      "<<<iteration:[180/657] - total_loss: 3.9327  obj_loss: 0.0553  noobj_loss: 3.0335  bbox_loss: 0.3851  cls_loss: 0.4350  \n",
      "<<<iteration:[200/657] - total_loss: 3.7647  obj_loss: 0.0547  noobj_loss: 2.9873  bbox_loss: 0.3709  cls_loss: 0.3618  \n",
      "<<<iteration:[220/657] - total_loss: 3.5730  obj_loss: 0.0512  noobj_loss: 2.8561  bbox_loss: 0.3393  cls_loss: 0.3971  \n",
      "<<<iteration:[240/657] - total_loss: 3.7138  obj_loss: 0.0560  noobj_loss: 3.0160  bbox_loss: 0.3541  cls_loss: 0.3795  \n",
      "<<<iteration:[260/657] - total_loss: 3.3280  obj_loss: 0.0576  noobj_loss: 2.8680  bbox_loss: 0.2995  cls_loss: 0.3390  \n",
      "<<<iteration:[280/657] - total_loss: 3.4528  obj_loss: 0.0474  noobj_loss: 2.7185  bbox_loss: 0.3361  cls_loss: 0.3656  \n",
      "<<<iteration:[300/657] - total_loss: 3.4696  obj_loss: 0.0492  noobj_loss: 2.6322  bbox_loss: 0.3514  cls_loss: 0.3470  \n",
      "<<<iteration:[320/657] - total_loss: 3.4080  obj_loss: 0.0499  noobj_loss: 2.7571  bbox_loss: 0.3197  cls_loss: 0.3808  \n",
      "<<<iteration:[340/657] - total_loss: 3.4023  obj_loss: 0.0426  noobj_loss: 2.5561  bbox_loss: 0.3527  cls_loss: 0.3183  \n",
      "<<<iteration:[360/657] - total_loss: 3.3247  obj_loss: 0.0574  noobj_loss: 2.6208  bbox_loss: 0.3218  cls_loss: 0.3478  \n",
      "<<<iteration:[380/657] - total_loss: 3.2108  obj_loss: 0.0487  noobj_loss: 2.5840  bbox_loss: 0.3032  cls_loss: 0.3539  \n",
      "<<<iteration:[400/657] - total_loss: 3.2357  obj_loss: 0.0523  noobj_loss: 2.4911  bbox_loss: 0.3213  cls_loss: 0.3312  \n",
      "<<<iteration:[420/657] - total_loss: 3.2316  obj_loss: 0.0528  noobj_loss: 2.5852  bbox_loss: 0.3156  cls_loss: 0.3084  \n",
      "<<<iteration:[440/657] - total_loss: 3.2161  obj_loss: 0.0425  noobj_loss: 2.4574  bbox_loss: 0.3258  cls_loss: 0.3160  \n",
      "<<<iteration:[460/657] - total_loss: 3.2831  obj_loss: 0.0485  noobj_loss: 2.5723  bbox_loss: 0.3276  cls_loss: 0.3105  \n",
      "<<<iteration:[480/657] - total_loss: 2.9700  obj_loss: 0.0448  noobj_loss: 2.3260  bbox_loss: 0.2866  cls_loss: 0.3291  \n",
      "<<<iteration:[500/657] - total_loss: 2.9097  obj_loss: 0.0475  noobj_loss: 2.3043  bbox_loss: 0.2786  cls_loss: 0.3172  \n",
      "<<<iteration:[520/657] - total_loss: 3.0617  obj_loss: 0.0491  noobj_loss: 2.3712  bbox_loss: 0.2926  cls_loss: 0.3640  \n",
      "<<<iteration:[540/657] - total_loss: 3.0628  obj_loss: 0.0568  noobj_loss: 2.2315  bbox_loss: 0.3068  cls_loss: 0.3561  \n",
      "<<<iteration:[560/657] - total_loss: 2.8537  obj_loss: 0.0507  noobj_loss: 2.2489  bbox_loss: 0.2696  cls_loss: 0.3307  \n",
      "<<<iteration:[580/657] - total_loss: 2.7049  obj_loss: 0.0510  noobj_loss: 2.1679  bbox_loss: 0.2551  cls_loss: 0.2947  \n",
      "<<<iteration:[600/657] - total_loss: 2.7803  obj_loss: 0.0478  noobj_loss: 2.0443  bbox_loss: 0.2733  cls_loss: 0.3439  \n",
      "<<<iteration:[620/657] - total_loss: 2.7081  obj_loss: 0.0530  noobj_loss: 2.0655  bbox_loss: 0.2568  cls_loss: 0.3383  \n",
      "<<<iteration:[640/657] - total_loss: 2.8432  obj_loss: 0.0519  noobj_loss: 2.0616  bbox_loss: 0.2915  cls_loss: 0.3031  \n",
      "\n",
      "epoch:2/100 - Train Loss: 3.4270, Val Loss: 3.1108\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 2.7540  obj_loss: 0.0663  noobj_loss: 2.2172  bbox_loss: 0.2534  cls_loss: 0.3123  \n",
      "<<<iteration:[40/657] - total_loss: 2.8948  obj_loss: 0.0443  noobj_loss: 2.0034  bbox_loss: 0.3078  cls_loss: 0.3098  \n",
      "<<<iteration:[60/657] - total_loss: 2.6308  obj_loss: 0.0455  noobj_loss: 2.0231  bbox_loss: 0.2551  cls_loss: 0.2983  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[80/657] - total_loss: 2.5772  obj_loss: 0.0466  noobj_loss: 1.9511  bbox_loss: 0.2480  cls_loss: 0.3150  \n",
      "<<<iteration:[100/657] - total_loss: 2.4204  obj_loss: 0.0513  noobj_loss: 1.9643  bbox_loss: 0.2186  cls_loss: 0.2939  \n",
      "<<<iteration:[120/657] - total_loss: 2.5796  obj_loss: 0.0463  noobj_loss: 2.0003  bbox_loss: 0.2491  cls_loss: 0.2874  \n",
      "<<<iteration:[140/657] - total_loss: 2.6342  obj_loss: 0.0444  noobj_loss: 1.9614  bbox_loss: 0.2626  cls_loss: 0.2960  \n",
      "<<<iteration:[160/657] - total_loss: 2.4682  obj_loss: 0.0438  noobj_loss: 1.9363  bbox_loss: 0.2287  cls_loss: 0.3127  \n",
      "<<<iteration:[180/657] - total_loss: 2.3412  obj_loss: 0.0476  noobj_loss: 1.8461  bbox_loss: 0.2218  cls_loss: 0.2614  \n",
      "<<<iteration:[200/657] - total_loss: 2.3915  obj_loss: 0.0561  noobj_loss: 1.8890  bbox_loss: 0.2225  cls_loss: 0.2784  \n",
      "<<<iteration:[220/657] - total_loss: 2.4210  obj_loss: 0.0441  noobj_loss: 1.9142  bbox_loss: 0.2250  cls_loss: 0.2947  \n",
      "<<<iteration:[240/657] - total_loss: 2.4340  obj_loss: 0.0417  noobj_loss: 1.6662  bbox_loss: 0.2497  cls_loss: 0.3108  \n",
      "<<<iteration:[260/657] - total_loss: 2.2393  obj_loss: 0.0486  noobj_loss: 1.6830  bbox_loss: 0.2121  cls_loss: 0.2888  \n",
      "<<<iteration:[280/657] - total_loss: 3.0399  obj_loss: 0.0386  noobj_loss: 1.8480  bbox_loss: 0.3554  cls_loss: 0.3004  \n",
      "<<<iteration:[300/657] - total_loss: 2.1009  obj_loss: 0.0412  noobj_loss: 1.6858  bbox_loss: 0.1870  cls_loss: 0.2815  \n",
      "<<<iteration:[320/657] - total_loss: 2.3889  obj_loss: 0.0449  noobj_loss: 1.7828  bbox_loss: 0.2317  cls_loss: 0.2939  \n",
      "<<<iteration:[340/657] - total_loss: 2.3919  obj_loss: 0.0416  noobj_loss: 1.6765  bbox_loss: 0.2451  cls_loss: 0.2866  \n",
      "<<<iteration:[360/657] - total_loss: 2.1941  obj_loss: 0.0417  noobj_loss: 1.7510  bbox_loss: 0.1985  cls_loss: 0.2844  \n",
      "<<<iteration:[380/657] - total_loss: 2.4916  obj_loss: 0.0502  noobj_loss: 1.6813  bbox_loss: 0.2612  cls_loss: 0.2945  \n",
      "<<<iteration:[400/657] - total_loss: 2.1867  obj_loss: 0.0470  noobj_loss: 1.7362  bbox_loss: 0.2011  cls_loss: 0.2663  \n",
      "<<<iteration:[420/657] - total_loss: 2.2306  obj_loss: 0.0427  noobj_loss: 1.6402  bbox_loss: 0.2172  cls_loss: 0.2819  \n",
      "<<<iteration:[440/657] - total_loss: 2.1693  obj_loss: 0.0425  noobj_loss: 1.7586  bbox_loss: 0.2011  cls_loss: 0.2421  \n",
      "<<<iteration:[460/657] - total_loss: 2.4914  obj_loss: 0.0473  noobj_loss: 1.6788  bbox_loss: 0.2644  cls_loss: 0.2829  \n",
      "<<<iteration:[480/657] - total_loss: 2.1599  obj_loss: 0.0496  noobj_loss: 1.5036  bbox_loss: 0.2181  cls_loss: 0.2680  \n",
      "<<<iteration:[500/657] - total_loss: 2.0887  obj_loss: 0.0424  noobj_loss: 1.5051  bbox_loss: 0.2051  cls_loss: 0.2684  \n",
      "<<<iteration:[520/657] - total_loss: 2.0196  obj_loss: 0.0441  noobj_loss: 1.6128  bbox_loss: 0.1887  cls_loss: 0.2256  \n",
      "<<<iteration:[540/657] - total_loss: 2.1508  obj_loss: 0.0540  noobj_loss: 1.5538  bbox_loss: 0.2130  cls_loss: 0.2548  \n",
      "<<<iteration:[560/657] - total_loss: 2.0151  obj_loss: 0.0467  noobj_loss: 1.4769  bbox_loss: 0.1998  cls_loss: 0.2309  \n",
      "<<<iteration:[580/657] - total_loss: 2.0509  obj_loss: 0.0476  noobj_loss: 1.5322  bbox_loss: 0.1969  cls_loss: 0.2526  \n",
      "<<<iteration:[600/657] - total_loss: 1.9970  obj_loss: 0.0494  noobj_loss: 1.5106  bbox_loss: 0.1872  cls_loss: 0.2565  \n",
      "<<<iteration:[620/657] - total_loss: 2.0697  obj_loss: 0.0380  noobj_loss: 1.5210  bbox_loss: 0.2093  cls_loss: 0.2247  \n",
      "<<<iteration:[640/657] - total_loss: 1.9457  obj_loss: 0.0455  noobj_loss: 1.4449  bbox_loss: 0.1871  cls_loss: 0.2424  \n",
      "\n",
      "epoch:3/100 - Train Loss: 2.3329, Val Loss: 2.1589\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 2.1840  obj_loss: 0.0512  noobj_loss: 1.6107  bbox_loss: 0.2094  cls_loss: 0.2804  \n",
      "<<<iteration:[40/657] - total_loss: 1.9763  obj_loss: 0.0433  noobj_loss: 1.4586  bbox_loss: 0.1894  cls_loss: 0.2565  \n",
      "<<<iteration:[60/657] - total_loss: 1.8933  obj_loss: 0.0514  noobj_loss: 1.4473  bbox_loss: 0.1732  cls_loss: 0.2525  \n",
      "<<<iteration:[80/657] - total_loss: 1.9547  obj_loss: 0.0477  noobj_loss: 1.3976  bbox_loss: 0.1916  cls_loss: 0.2504  \n",
      "<<<iteration:[100/657] - total_loss: 1.8249  obj_loss: 0.0492  noobj_loss: 1.4074  bbox_loss: 0.1713  cls_loss: 0.2153  \n",
      "<<<iteration:[120/657] - total_loss: 1.9309  obj_loss: 0.0503  noobj_loss: 1.2779  bbox_loss: 0.1938  cls_loss: 0.2728  \n",
      "<<<iteration:[140/657] - total_loss: 23.4065  obj_loss: 0.0311  noobj_loss: 1.7753  bbox_loss: 4.4192  cls_loss: 0.3918  \n",
      "<<<iteration:[160/657] - total_loss: 8.2365  obj_loss: 0.0238  noobj_loss: 1.4471  bbox_loss: 1.4470  cls_loss: 0.2541  \n",
      "<<<iteration:[180/657] - total_loss: 4.4705  obj_loss: 0.0289  noobj_loss: 1.3591  bbox_loss: 0.7044  cls_loss: 0.2398  \n",
      "<<<iteration:[200/657] - total_loss: 3.1455  obj_loss: 0.0408  noobj_loss: 1.3807  bbox_loss: 0.4290  cls_loss: 0.2692  \n",
      "<<<iteration:[220/657] - total_loss: 2.6999  obj_loss: 0.0346  noobj_loss: 1.3540  bbox_loss: 0.3460  cls_loss: 0.2586  \n",
      "<<<iteration:[240/657] - total_loss: 2.3949  obj_loss: 0.0372  noobj_loss: 1.3910  bbox_loss: 0.2784  cls_loss: 0.2704  \n",
      "<<<iteration:[260/657] - total_loss: 2.2101  obj_loss: 0.0395  noobj_loss: 1.2763  bbox_loss: 0.2652  cls_loss: 0.2062  \n",
      "<<<iteration:[280/657] - total_loss: 2.2902  obj_loss: 0.0443  noobj_loss: 1.3006  bbox_loss: 0.2733  cls_loss: 0.2293  \n",
      "<<<iteration:[300/657] - total_loss: 2.2040  obj_loss: 0.0443  noobj_loss: 1.3472  bbox_loss: 0.2483  cls_loss: 0.2445  \n",
      "<<<iteration:[320/657] - total_loss: 2.0292  obj_loss: 0.0419  noobj_loss: 1.2610  bbox_loss: 0.2212  cls_loss: 0.2508  \n",
      "<<<iteration:[340/657] - total_loss: 2.2156  obj_loss: 0.0453  noobj_loss: 1.3471  bbox_loss: 0.2417  cls_loss: 0.2883  \n",
      "<<<iteration:[360/657] - total_loss: 1.8857  obj_loss: 0.0439  noobj_loss: 1.2259  bbox_loss: 0.1995  cls_loss: 0.2313  \n",
      "<<<iteration:[380/657] - total_loss: 1.9482  obj_loss: 0.0441  noobj_loss: 1.2673  bbox_loss: 0.2063  cls_loss: 0.2388  \n",
      "<<<iteration:[400/657] - total_loss: 1.8322  obj_loss: 0.0491  noobj_loss: 1.3644  bbox_loss: 0.1757  cls_loss: 0.2225  \n",
      "<<<iteration:[420/657] - total_loss: 1.8088  obj_loss: 0.0412  noobj_loss: 1.2562  bbox_loss: 0.1796  cls_loss: 0.2414  \n",
      "<<<iteration:[440/657] - total_loss: 1.8983  obj_loss: 0.0438  noobj_loss: 1.2019  bbox_loss: 0.2029  cls_loss: 0.2391  \n",
      "<<<iteration:[460/657] - total_loss: 1.7859  obj_loss: 0.0434  noobj_loss: 1.1851  bbox_loss: 0.1805  cls_loss: 0.2472  \n",
      "<<<iteration:[480/657] - total_loss: 1.6607  obj_loss: 0.0473  noobj_loss: 1.2003  bbox_loss: 0.1564  cls_loss: 0.2312  \n",
      "<<<iteration:[500/657] - total_loss: 2.5393  obj_loss: 0.0412  noobj_loss: 1.2389  bbox_loss: 0.3323  cls_loss: 0.2171  \n",
      "<<<iteration:[520/657] - total_loss: 2.0492  obj_loss: 0.0458  noobj_loss: 1.2849  bbox_loss: 0.2225  cls_loss: 0.2484  \n",
      "<<<iteration:[540/657] - total_loss: 1.6240  obj_loss: 0.0512  noobj_loss: 1.1765  bbox_loss: 0.1541  cls_loss: 0.2142  \n",
      "<<<iteration:[560/657] - total_loss: 1.7963  obj_loss: 0.0448  noobj_loss: 1.2941  bbox_loss: 0.1805  cls_loss: 0.2022  \n",
      "<<<iteration:[580/657] - total_loss: 1.7607  obj_loss: 0.0455  noobj_loss: 1.1858  bbox_loss: 0.1777  cls_loss: 0.2339  \n",
      "<<<iteration:[600/657] - total_loss: 1.6821  obj_loss: 0.0449  noobj_loss: 1.1366  bbox_loss: 0.1713  cls_loss: 0.2124  \n",
      "<<<iteration:[620/657] - total_loss: 1.6486  obj_loss: 0.0407  noobj_loss: 1.2308  bbox_loss: 0.1583  cls_loss: 0.2009  \n",
      "<<<iteration:[640/657] - total_loss: 2.2196  obj_loss: 0.0440  noobj_loss: 1.1447  bbox_loss: 0.2709  cls_loss: 0.2489  \n",
      "\n",
      "epoch:4/100 - Train Loss: 2.9529, Val Loss: 2.2476\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.6749  obj_loss: 0.0524  noobj_loss: 1.1231  bbox_loss: 0.1660  cls_loss: 0.2308  \n",
      "<<<iteration:[40/657] - total_loss: 1.6346  obj_loss: 0.0444  noobj_loss: 1.1873  bbox_loss: 0.1556  cls_loss: 0.2188  \n",
      "<<<iteration:[60/657] - total_loss: 1.7605  obj_loss: 0.0396  noobj_loss: 1.1455  bbox_loss: 0.1815  cls_loss: 0.2408  \n",
      "<<<iteration:[80/657] - total_loss: 1.4465  obj_loss: 0.0454  noobj_loss: 1.1130  bbox_loss: 0.1331  cls_loss: 0.1792  \n",
      "<<<iteration:[100/657] - total_loss: 1.6300  obj_loss: 0.0462  noobj_loss: 1.1604  bbox_loss: 0.1534  cls_loss: 0.2364  \n",
      "<<<iteration:[120/657] - total_loss: 1.5761  obj_loss: 0.0503  noobj_loss: 1.1147  bbox_loss: 0.1473  cls_loss: 0.2321  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[140/657] - total_loss: 1.5015  obj_loss: 0.0531  noobj_loss: 1.0478  bbox_loss: 0.1437  cls_loss: 0.2062  \n",
      "<<<iteration:[160/657] - total_loss: 1.6813  obj_loss: 0.0442  noobj_loss: 1.1575  bbox_loss: 0.1641  cls_loss: 0.2380  \n",
      "<<<iteration:[180/657] - total_loss: 1.5797  obj_loss: 0.0596  noobj_loss: 1.1354  bbox_loss: 0.1477  cls_loss: 0.2139  \n",
      "<<<iteration:[200/657] - total_loss: 1.5209  obj_loss: 0.0503  noobj_loss: 1.0568  bbox_loss: 0.1439  cls_loss: 0.2224  \n",
      "<<<iteration:[220/657] - total_loss: 1.6278  obj_loss: 0.0460  noobj_loss: 1.1452  bbox_loss: 0.1600  cls_loss: 0.2090  \n",
      "<<<iteration:[240/657] - total_loss: 1.4528  obj_loss: 0.0534  noobj_loss: 1.0610  bbox_loss: 0.1334  cls_loss: 0.2021  \n",
      "<<<iteration:[260/657] - total_loss: 1.4116  obj_loss: 0.0446  noobj_loss: 1.0193  bbox_loss: 0.1287  cls_loss: 0.2138  \n",
      "<<<iteration:[280/657] - total_loss: 1.6034  obj_loss: 0.0408  noobj_loss: 1.0250  bbox_loss: 0.1688  cls_loss: 0.2064  \n",
      "<<<iteration:[300/657] - total_loss: 1.5136  obj_loss: 0.0482  noobj_loss: 1.0490  bbox_loss: 0.1424  cls_loss: 0.2287  \n",
      "<<<iteration:[320/657] - total_loss: 1.6062  obj_loss: 0.0526  noobj_loss: 1.1858  bbox_loss: 0.1521  cls_loss: 0.2002  \n",
      "<<<iteration:[340/657] - total_loss: 1.4622  obj_loss: 0.0471  noobj_loss: 1.0283  bbox_loss: 0.1406  cls_loss: 0.1977  \n",
      "<<<iteration:[360/657] - total_loss: 1.6023  obj_loss: 0.0474  noobj_loss: 1.1896  bbox_loss: 0.1535  cls_loss: 0.1926  \n",
      "<<<iteration:[380/657] - total_loss: 1.5722  obj_loss: 0.0390  noobj_loss: 1.0480  bbox_loss: 0.1569  cls_loss: 0.2245  \n",
      "<<<iteration:[400/657] - total_loss: 1.4809  obj_loss: 0.0435  noobj_loss: 0.9910  bbox_loss: 0.1472  cls_loss: 0.2062  \n",
      "<<<iteration:[420/657] - total_loss: 1.4014  obj_loss: 0.0527  noobj_loss: 1.0381  bbox_loss: 0.1244  cls_loss: 0.2074  \n",
      "<<<iteration:[440/657] - total_loss: 1.3938  obj_loss: 0.0501  noobj_loss: 0.9942  bbox_loss: 0.1223  cls_loss: 0.2353  \n",
      "<<<iteration:[460/657] - total_loss: 1.5685  obj_loss: 0.0441  noobj_loss: 1.0170  bbox_loss: 0.1548  cls_loss: 0.2418  \n",
      "<<<iteration:[480/657] - total_loss: 1.3421  obj_loss: 0.0490  noobj_loss: 1.0149  bbox_loss: 0.1152  cls_loss: 0.2099  \n",
      "<<<iteration:[500/657] - total_loss: 1.4394  obj_loss: 0.0562  noobj_loss: 1.0322  bbox_loss: 0.1262  cls_loss: 0.2363  \n",
      "<<<iteration:[520/657] - total_loss: 1.4416  obj_loss: 0.0456  noobj_loss: 1.0456  bbox_loss: 0.1320  cls_loss: 0.2131  \n",
      "<<<iteration:[540/657] - total_loss: 1.4606  obj_loss: 0.0444  noobj_loss: 1.0104  bbox_loss: 0.1422  cls_loss: 0.1998  \n",
      "<<<iteration:[560/657] - total_loss: 1.3736  obj_loss: 0.0542  noobj_loss: 0.9400  bbox_loss: 0.1252  cls_loss: 0.2232  \n",
      "<<<iteration:[580/657] - total_loss: 1.3669  obj_loss: 0.0501  noobj_loss: 0.9184  bbox_loss: 0.1290  cls_loss: 0.2124  \n",
      "<<<iteration:[600/657] - total_loss: 1.4825  obj_loss: 0.0454  noobj_loss: 1.0082  bbox_loss: 0.1471  cls_loss: 0.1974  \n",
      "<<<iteration:[620/657] - total_loss: 2.5357  obj_loss: 0.0380  noobj_loss: 0.9675  bbox_loss: 0.3629  cls_loss: 0.1992  \n",
      "<<<iteration:[640/657] - total_loss: 2.2651  obj_loss: 0.0335  noobj_loss: 0.9746  bbox_loss: 0.3064  cls_loss: 0.2123  \n",
      "\n",
      "epoch:5/100 - Train Loss: 1.5728, Val Loss: 1.7516\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.4386  obj_loss: 0.0488  noobj_loss: 0.9370  bbox_loss: 0.1419  cls_loss: 0.2116  \n",
      "<<<iteration:[40/657] - total_loss: 1.4055  obj_loss: 0.0433  noobj_loss: 0.9757  bbox_loss: 0.1359  cls_loss: 0.1949  \n",
      "<<<iteration:[60/657] - total_loss: 1.4102  obj_loss: 0.0470  noobj_loss: 0.9302  bbox_loss: 0.1407  cls_loss: 0.1945  \n",
      "<<<iteration:[80/657] - total_loss: 1.3569  obj_loss: 0.0500  noobj_loss: 0.9185  bbox_loss: 0.1314  cls_loss: 0.1908  \n",
      "<<<iteration:[100/657] - total_loss: 1.2774  obj_loss: 0.0455  noobj_loss: 0.8812  bbox_loss: 0.1232  cls_loss: 0.1753  \n",
      "<<<iteration:[120/657] - total_loss: 1.2087  obj_loss: 0.0522  noobj_loss: 0.8748  bbox_loss: 0.1057  cls_loss: 0.1904  \n",
      "<<<iteration:[140/657] - total_loss: 1.2626  obj_loss: 0.0510  noobj_loss: 0.8881  bbox_loss: 0.1173  cls_loss: 0.1812  \n",
      "<<<iteration:[160/657] - total_loss: 1.3026  obj_loss: 0.0518  noobj_loss: 0.9099  bbox_loss: 0.1217  cls_loss: 0.1874  \n",
      "<<<iteration:[180/657] - total_loss: 1.3151  obj_loss: 0.0529  noobj_loss: 0.8837  bbox_loss: 0.1266  cls_loss: 0.1874  \n",
      "<<<iteration:[200/657] - total_loss: 1.3708  obj_loss: 0.0483  noobj_loss: 0.8831  bbox_loss: 0.1298  cls_loss: 0.2318  \n",
      "<<<iteration:[220/657] - total_loss: 1.4232  obj_loss: 0.0453  noobj_loss: 0.8995  bbox_loss: 0.1434  cls_loss: 0.2112  \n",
      "<<<iteration:[240/657] - total_loss: 1.3412  obj_loss: 0.0527  noobj_loss: 0.8757  bbox_loss: 0.1303  cls_loss: 0.1993  \n",
      "<<<iteration:[260/657] - total_loss: 1.3544  obj_loss: 0.0448  noobj_loss: 0.8827  bbox_loss: 0.1317  cls_loss: 0.2100  \n",
      "<<<iteration:[280/657] - total_loss: 1.2862  obj_loss: 0.0486  noobj_loss: 0.9093  bbox_loss: 0.1175  cls_loss: 0.1957  \n",
      "<<<iteration:[300/657] - total_loss: 1.3212  obj_loss: 0.0598  noobj_loss: 0.9052  bbox_loss: 0.1178  cls_loss: 0.2199  \n",
      "<<<iteration:[320/657] - total_loss: 1.3553  obj_loss: 0.0591  noobj_loss: 0.8922  bbox_loss: 0.1263  cls_loss: 0.2184  \n",
      "<<<iteration:[340/657] - total_loss: 1.2997  obj_loss: 0.0441  noobj_loss: 0.8604  bbox_loss: 0.1232  cls_loss: 0.2095  \n",
      "<<<iteration:[360/657] - total_loss: 1.2412  obj_loss: 0.0555  noobj_loss: 0.8697  bbox_loss: 0.1115  cls_loss: 0.1931  \n",
      "<<<iteration:[380/657] - total_loss: 1.2883  obj_loss: 0.0460  noobj_loss: 0.9416  bbox_loss: 0.1160  cls_loss: 0.1914  \n",
      "<<<iteration:[400/657] - total_loss: 1.3203  obj_loss: 0.0530  noobj_loss: 0.9228  bbox_loss: 0.1201  cls_loss: 0.2052  \n",
      "<<<iteration:[420/657] - total_loss: 1.2340  obj_loss: 0.0512  noobj_loss: 0.8603  bbox_loss: 0.1077  cls_loss: 0.2142  \n",
      "<<<iteration:[440/657] - total_loss: 1.3621  obj_loss: 0.0447  noobj_loss: 0.8977  bbox_loss: 0.1326  cls_loss: 0.2054  \n",
      "<<<iteration:[460/657] - total_loss: 1.1843  obj_loss: 0.0513  noobj_loss: 0.7905  bbox_loss: 0.1101  cls_loss: 0.1873  \n",
      "<<<iteration:[480/657] - total_loss: 1.2020  obj_loss: 0.0556  noobj_loss: 0.8751  bbox_loss: 0.1070  cls_loss: 0.1740  \n",
      "<<<iteration:[500/657] - total_loss: 1.2017  obj_loss: 0.0510  noobj_loss: 0.8321  bbox_loss: 0.1069  cls_loss: 0.2002  \n",
      "<<<iteration:[520/657] - total_loss: 1.3049  obj_loss: 0.0402  noobj_loss: 0.8501  bbox_loss: 0.1321  cls_loss: 0.1794  \n",
      "<<<iteration:[540/657] - total_loss: 1.2815  obj_loss: 0.0479  noobj_loss: 0.8994  bbox_loss: 0.1137  cls_loss: 0.2154  \n",
      "<<<iteration:[560/657] - total_loss: 1.2288  obj_loss: 0.0545  noobj_loss: 0.8013  bbox_loss: 0.1155  cls_loss: 0.1960  \n",
      "<<<iteration:[580/657] - total_loss: 1.1803  obj_loss: 0.0520  noobj_loss: 0.8550  bbox_loss: 0.0989  cls_loss: 0.2063  \n",
      "<<<iteration:[600/657] - total_loss: 1.2685  obj_loss: 0.0518  noobj_loss: 0.8443  bbox_loss: 0.1203  cls_loss: 0.1931  \n",
      "<<<iteration:[620/657] - total_loss: 1.2103  obj_loss: 0.0463  noobj_loss: 0.8699  bbox_loss: 0.1097  cls_loss: 0.1804  \n",
      "<<<iteration:[640/657] - total_loss: 1.1987  obj_loss: 0.0561  noobj_loss: 0.8380  bbox_loss: 0.1032  cls_loss: 0.2076  \n",
      "\n",
      "epoch:6/100 - Train Loss: 1.2901, Val Loss: 1.1526\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.2329  obj_loss: 0.0583  noobj_loss: 0.8564  bbox_loss: 0.1112  cls_loss: 0.1905  \n",
      "<<<iteration:[40/657] - total_loss: 1.2569  obj_loss: 0.0519  noobj_loss: 0.7851  bbox_loss: 0.1275  cls_loss: 0.1750  \n",
      "<<<iteration:[60/657] - total_loss: 1.2007  obj_loss: 0.0482  noobj_loss: 0.8201  bbox_loss: 0.1125  cls_loss: 0.1801  \n",
      "<<<iteration:[80/657] - total_loss: 1.2094  obj_loss: 0.0508  noobj_loss: 0.8189  bbox_loss: 0.1103  cls_loss: 0.1976  \n",
      "<<<iteration:[100/657] - total_loss: 1.2167  obj_loss: 0.0538  noobj_loss: 0.8278  bbox_loss: 0.1098  cls_loss: 0.2000  \n",
      "<<<iteration:[120/657] - total_loss: 1.1498  obj_loss: 0.0460  noobj_loss: 0.7812  bbox_loss: 0.1080  cls_loss: 0.1734  \n",
      "<<<iteration:[140/657] - total_loss: 1.1861  obj_loss: 0.0563  noobj_loss: 0.7702  bbox_loss: 0.1077  cls_loss: 0.2064  \n",
      "<<<iteration:[160/657] - total_loss: 1.1805  obj_loss: 0.0509  noobj_loss: 0.7607  bbox_loss: 0.1111  cls_loss: 0.1940  \n",
      "<<<iteration:[180/657] - total_loss: 1.2496  obj_loss: 0.0469  noobj_loss: 0.8194  bbox_loss: 0.1208  cls_loss: 0.1888  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[200/657] - total_loss: 1.2063  obj_loss: 0.0529  noobj_loss: 0.7885  bbox_loss: 0.1035  cls_loss: 0.2418  \n",
      "<<<iteration:[220/657] - total_loss: 1.2103  obj_loss: 0.0459  noobj_loss: 0.8160  bbox_loss: 0.1165  cls_loss: 0.1738  \n",
      "<<<iteration:[240/657] - total_loss: 1.6244  obj_loss: 0.0452  noobj_loss: 0.7923  bbox_loss: 0.1990  cls_loss: 0.1878  \n",
      "<<<iteration:[260/657] - total_loss: 1.1682  obj_loss: 0.0498  noobj_loss: 0.7447  bbox_loss: 0.1138  cls_loss: 0.1769  \n",
      "<<<iteration:[280/657] - total_loss: 1.1578  obj_loss: 0.0476  noobj_loss: 0.8211  bbox_loss: 0.1043  cls_loss: 0.1783  \n",
      "<<<iteration:[300/657] - total_loss: 1.1547  obj_loss: 0.0501  noobj_loss: 0.7910  bbox_loss: 0.1057  cls_loss: 0.1805  \n",
      "<<<iteration:[320/657] - total_loss: 1.2357  obj_loss: 0.0560  noobj_loss: 0.8210  bbox_loss: 0.1141  cls_loss: 0.1987  \n",
      "<<<iteration:[340/657] - total_loss: 1.1343  obj_loss: 0.0578  noobj_loss: 0.7124  bbox_loss: 0.1052  cls_loss: 0.1941  \n",
      "<<<iteration:[360/657] - total_loss: 1.0665  obj_loss: 0.0520  noobj_loss: 0.7617  bbox_loss: 0.0921  cls_loss: 0.1732  \n",
      "<<<iteration:[380/657] - total_loss: 1.2853  obj_loss: 0.0509  noobj_loss: 0.7988  bbox_loss: 0.1327  cls_loss: 0.1717  \n",
      "<<<iteration:[400/657] - total_loss: 1.1028  obj_loss: 0.0588  noobj_loss: 0.7633  bbox_loss: 0.0961  cls_loss: 0.1819  \n",
      "<<<iteration:[420/657] - total_loss: 1.1500  obj_loss: 0.0462  noobj_loss: 0.7797  bbox_loss: 0.1063  cls_loss: 0.1822  \n",
      "<<<iteration:[440/657] - total_loss: 1.1097  obj_loss: 0.0496  noobj_loss: 0.7151  bbox_loss: 0.1023  cls_loss: 0.1910  \n",
      "<<<iteration:[460/657] - total_loss: 1.1141  obj_loss: 0.0565  noobj_loss: 0.7216  bbox_loss: 0.1065  cls_loss: 0.1645  \n",
      "<<<iteration:[480/657] - total_loss: 1.1564  obj_loss: 0.0523  noobj_loss: 0.7901  bbox_loss: 0.1081  cls_loss: 0.1688  \n",
      "<<<iteration:[500/657] - total_loss: 1.3375  obj_loss: 0.0581  noobj_loss: 0.7887  bbox_loss: 0.1391  cls_loss: 0.1894  \n",
      "<<<iteration:[520/657] - total_loss: 1.3928  obj_loss: 0.0409  noobj_loss: 0.7658  bbox_loss: 0.1560  cls_loss: 0.1889  \n",
      "<<<iteration:[540/657] - total_loss: 1.2370  obj_loss: 0.0551  noobj_loss: 0.7266  bbox_loss: 0.1262  cls_loss: 0.1874  \n",
      "<<<iteration:[560/657] - total_loss: 3.0436  obj_loss: 0.0294  noobj_loss: 0.7187  bbox_loss: 0.4929  cls_loss: 0.1903  \n",
      "<<<iteration:[580/657] - total_loss: 1.3807  obj_loss: 0.0445  noobj_loss: 0.7412  bbox_loss: 0.1593  cls_loss: 0.1692  \n",
      "<<<iteration:[600/657] - total_loss: 1.2191  obj_loss: 0.0509  noobj_loss: 0.7510  bbox_loss: 0.1199  cls_loss: 0.1932  \n",
      "<<<iteration:[620/657] - total_loss: 1.1375  obj_loss: 0.0490  noobj_loss: 0.7102  bbox_loss: 0.1085  cls_loss: 0.1912  \n",
      "<<<iteration:[640/657] - total_loss: 1.1302  obj_loss: 0.0430  noobj_loss: 0.7304  bbox_loss: 0.1092  cls_loss: 0.1759  \n",
      "\n",
      "epoch:7/100 - Train Loss: 1.2643, Val Loss: 1.3950\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.2132  obj_loss: 0.0562  noobj_loss: 0.7718  bbox_loss: 0.1156  cls_loss: 0.1928  \n",
      "<<<iteration:[40/657] - total_loss: 1.0847  obj_loss: 0.0524  noobj_loss: 0.7030  bbox_loss: 0.1036  cls_loss: 0.1628  \n",
      "<<<iteration:[60/657] - total_loss: 1.1586  obj_loss: 0.0497  noobj_loss: 0.7496  bbox_loss: 0.1089  cls_loss: 0.1896  \n",
      "<<<iteration:[80/657] - total_loss: 1.0727  obj_loss: 0.0519  noobj_loss: 0.7101  bbox_loss: 0.0975  cls_loss: 0.1784  \n",
      "<<<iteration:[100/657] - total_loss: 1.0966  obj_loss: 0.0501  noobj_loss: 0.6951  bbox_loss: 0.1023  cls_loss: 0.1877  \n",
      "<<<iteration:[120/657] - total_loss: 1.1252  obj_loss: 0.0520  noobj_loss: 0.7546  bbox_loss: 0.1054  cls_loss: 0.1687  \n",
      "<<<iteration:[140/657] - total_loss: 1.0578  obj_loss: 0.0563  noobj_loss: 0.6901  bbox_loss: 0.0951  cls_loss: 0.1807  \n",
      "<<<iteration:[160/657] - total_loss: 1.0545  obj_loss: 0.0535  noobj_loss: 0.6727  bbox_loss: 0.0948  cls_loss: 0.1904  \n",
      "<<<iteration:[180/657] - total_loss: 1.1952  obj_loss: 0.0494  noobj_loss: 0.6998  bbox_loss: 0.1235  cls_loss: 0.1784  \n",
      "<<<iteration:[200/657] - total_loss: 1.0361  obj_loss: 0.0507  noobj_loss: 0.7182  bbox_loss: 0.0915  cls_loss: 0.1686  \n",
      "<<<iteration:[220/657] - total_loss: 1.0511  obj_loss: 0.0560  noobj_loss: 0.6890  bbox_loss: 0.0984  cls_loss: 0.1585  \n",
      "<<<iteration:[240/657] - total_loss: 1.6531  obj_loss: 0.0414  noobj_loss: 0.7007  bbox_loss: 0.2133  cls_loss: 0.1947  \n",
      "<<<iteration:[260/657] - total_loss: 1.1123  obj_loss: 0.0538  noobj_loss: 0.6438  bbox_loss: 0.1105  cls_loss: 0.1841  \n",
      "<<<iteration:[280/657] - total_loss: 1.0902  obj_loss: 0.0462  noobj_loss: 0.7039  bbox_loss: 0.1041  cls_loss: 0.1714  \n",
      "<<<iteration:[300/657] - total_loss: 0.9784  obj_loss: 0.0569  noobj_loss: 0.6380  bbox_loss: 0.0843  cls_loss: 0.1812  \n",
      "<<<iteration:[320/657] - total_loss: 1.0289  obj_loss: 0.0552  noobj_loss: 0.6525  bbox_loss: 0.0966  cls_loss: 0.1645  \n",
      "<<<iteration:[340/657] - total_loss: 1.5955  obj_loss: 0.0423  noobj_loss: 0.7056  bbox_loss: 0.2038  cls_loss: 0.1814  \n",
      "<<<iteration:[360/657] - total_loss: 1.1786  obj_loss: 0.0541  noobj_loss: 0.6677  bbox_loss: 0.1216  cls_loss: 0.1826  \n",
      "<<<iteration:[380/657] - total_loss: 0.9989  obj_loss: 0.0578  noobj_loss: 0.6682  bbox_loss: 0.0867  cls_loss: 0.1732  \n",
      "<<<iteration:[400/657] - total_loss: 0.9925  obj_loss: 0.0506  noobj_loss: 0.6718  bbox_loss: 0.0883  cls_loss: 0.1645  \n",
      "<<<iteration:[420/657] - total_loss: 1.0094  obj_loss: 0.0536  noobj_loss: 0.6758  bbox_loss: 0.0871  cls_loss: 0.1825  \n",
      "<<<iteration:[440/657] - total_loss: 0.9767  obj_loss: 0.0538  noobj_loss: 0.6537  bbox_loss: 0.0850  cls_loss: 0.1710  \n",
      "<<<iteration:[460/657] - total_loss: 1.1754  obj_loss: 0.0525  noobj_loss: 0.6497  bbox_loss: 0.1233  cls_loss: 0.1818  \n",
      "<<<iteration:[480/657] - total_loss: 1.0039  obj_loss: 0.0571  noobj_loss: 0.6441  bbox_loss: 0.0898  cls_loss: 0.1759  \n",
      "<<<iteration:[500/657] - total_loss: 0.9861  obj_loss: 0.0548  noobj_loss: 0.6471  bbox_loss: 0.0876  cls_loss: 0.1698  \n",
      "<<<iteration:[520/657] - total_loss: 0.9855  obj_loss: 0.0538  noobj_loss: 0.6530  bbox_loss: 0.0847  cls_loss: 0.1818  \n",
      "<<<iteration:[540/657] - total_loss: 1.0513  obj_loss: 0.0511  noobj_loss: 0.6663  bbox_loss: 0.0970  cls_loss: 0.1821  \n",
      "<<<iteration:[560/657] - total_loss: 0.9765  obj_loss: 0.0635  noobj_loss: 0.6500  bbox_loss: 0.0827  cls_loss: 0.1746  \n",
      "<<<iteration:[580/657] - total_loss: 0.9615  obj_loss: 0.0588  noobj_loss: 0.6510  bbox_loss: 0.0828  cls_loss: 0.1634  \n",
      "<<<iteration:[600/657] - total_loss: 0.9988  obj_loss: 0.0619  noobj_loss: 0.6686  bbox_loss: 0.0837  cls_loss: 0.1841  \n",
      "<<<iteration:[620/657] - total_loss: 1.0209  obj_loss: 0.0553  noobj_loss: 0.6209  bbox_loss: 0.0966  cls_loss: 0.1719  \n",
      "<<<iteration:[640/657] - total_loss: 0.9690  obj_loss: 0.0614  noobj_loss: 0.6731  bbox_loss: 0.0797  cls_loss: 0.1726  \n",
      "\n",
      "epoch:8/100 - Train Loss: 1.0878, Val Loss: 1.0539\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.0353  obj_loss: 0.0585  noobj_loss: 0.6875  bbox_loss: 0.0904  cls_loss: 0.1812  \n",
      "<<<iteration:[40/657] - total_loss: 0.9869  obj_loss: 0.0621  noobj_loss: 0.6564  bbox_loss: 0.0865  cls_loss: 0.1640  \n",
      "<<<iteration:[60/657] - total_loss: 1.0111  obj_loss: 0.0546  noobj_loss: 0.6970  bbox_loss: 0.0903  cls_loss: 0.1563  \n",
      "<<<iteration:[80/657] - total_loss: 1.0577  obj_loss: 0.0522  noobj_loss: 0.6864  bbox_loss: 0.0961  cls_loss: 0.1818  \n",
      "<<<iteration:[100/657] - total_loss: 0.9734  obj_loss: 0.0535  noobj_loss: 0.6053  bbox_loss: 0.0860  cls_loss: 0.1873  \n",
      "<<<iteration:[120/657] - total_loss: 0.9180  obj_loss: 0.0636  noobj_loss: 0.6156  bbox_loss: 0.0748  cls_loss: 0.1726  \n",
      "<<<iteration:[140/657] - total_loss: 1.1753  obj_loss: 0.0560  noobj_loss: 0.6503  bbox_loss: 0.1214  cls_loss: 0.1873  \n",
      "<<<iteration:[160/657] - total_loss: 1.0953  obj_loss: 0.0486  noobj_loss: 0.6017  bbox_loss: 0.1160  cls_loss: 0.1661  \n",
      "<<<iteration:[180/657] - total_loss: 0.9163  obj_loss: 0.0502  noobj_loss: 0.6004  bbox_loss: 0.0824  cls_loss: 0.1538  \n",
      "<<<iteration:[200/657] - total_loss: 0.9870  obj_loss: 0.0569  noobj_loss: 0.6606  bbox_loss: 0.0853  cls_loss: 0.1734  \n",
      "<<<iteration:[220/657] - total_loss: 1.0119  obj_loss: 0.0564  noobj_loss: 0.6386  bbox_loss: 0.0926  cls_loss: 0.1734  \n",
      "<<<iteration:[240/657] - total_loss: 0.9224  obj_loss: 0.0653  noobj_loss: 0.6115  bbox_loss: 0.0753  cls_loss: 0.1748  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[260/657] - total_loss: 0.9582  obj_loss: 0.0567  noobj_loss: 0.6212  bbox_loss: 0.0857  cls_loss: 0.1624  \n",
      "<<<iteration:[280/657] - total_loss: 0.9985  obj_loss: 0.0559  noobj_loss: 0.6137  bbox_loss: 0.0998  cls_loss: 0.1365  \n",
      "<<<iteration:[300/657] - total_loss: 0.9225  obj_loss: 0.0576  noobj_loss: 0.5808  bbox_loss: 0.0820  cls_loss: 0.1648  \n",
      "<<<iteration:[320/657] - total_loss: 0.9972  obj_loss: 0.0474  noobj_loss: 0.6147  bbox_loss: 0.0914  cls_loss: 0.1854  \n",
      "<<<iteration:[340/657] - total_loss: 0.9476  obj_loss: 0.0560  noobj_loss: 0.6102  bbox_loss: 0.0807  cls_loss: 0.1830  \n",
      "<<<iteration:[360/657] - total_loss: 0.8603  obj_loss: 0.0584  noobj_loss: 0.5996  bbox_loss: 0.0691  cls_loss: 0.1565  \n",
      "<<<iteration:[380/657] - total_loss: 0.9170  obj_loss: 0.0682  noobj_loss: 0.6140  bbox_loss: 0.0738  cls_loss: 0.1727  \n",
      "<<<iteration:[400/657] - total_loss: 1.0022  obj_loss: 0.0576  noobj_loss: 0.6446  bbox_loss: 0.0901  cls_loss: 0.1720  \n",
      "<<<iteration:[420/657] - total_loss: 0.9413  obj_loss: 0.0589  noobj_loss: 0.5905  bbox_loss: 0.0825  cls_loss: 0.1745  \n",
      "<<<iteration:[440/657] - total_loss: 1.1209  obj_loss: 0.0533  noobj_loss: 0.6152  bbox_loss: 0.1177  cls_loss: 0.1713  \n",
      "<<<iteration:[460/657] - total_loss: 0.9698  obj_loss: 0.0540  noobj_loss: 0.5960  bbox_loss: 0.0899  cls_loss: 0.1681  \n",
      "<<<iteration:[480/657] - total_loss: 0.9912  obj_loss: 0.0514  noobj_loss: 0.6401  bbox_loss: 0.0888  cls_loss: 0.1759  \n",
      "<<<iteration:[500/657] - total_loss: 1.1873  obj_loss: 0.0588  noobj_loss: 0.5619  bbox_loss: 0.1368  cls_loss: 0.1636  \n",
      "<<<iteration:[520/657] - total_loss: 0.8870  obj_loss: 0.0556  noobj_loss: 0.5478  bbox_loss: 0.0788  cls_loss: 0.1634  \n",
      "<<<iteration:[540/657] - total_loss: 0.9126  obj_loss: 0.0515  noobj_loss: 0.6073  bbox_loss: 0.0865  cls_loss: 0.1251  \n",
      "<<<iteration:[560/657] - total_loss: 0.9562  obj_loss: 0.0611  noobj_loss: 0.5684  bbox_loss: 0.0821  cls_loss: 0.2005  \n",
      "<<<iteration:[580/657] - total_loss: 0.8619  obj_loss: 0.0526  noobj_loss: 0.5945  bbox_loss: 0.0737  cls_loss: 0.1437  \n",
      "<<<iteration:[600/657] - total_loss: 1.0155  obj_loss: 0.0559  noobj_loss: 0.5664  bbox_loss: 0.0982  cls_loss: 0.1853  \n",
      "<<<iteration:[620/657] - total_loss: 0.9850  obj_loss: 0.0597  noobj_loss: 0.6266  bbox_loss: 0.0907  cls_loss: 0.1586  \n",
      "<<<iteration:[640/657] - total_loss: 0.8742  obj_loss: 0.0539  noobj_loss: 0.5937  bbox_loss: 0.0717  cls_loss: 0.1653  \n",
      "\n",
      "epoch:9/100 - Train Loss: 0.9787, Val Loss: 0.9428\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.9400  obj_loss: 0.0577  noobj_loss: 0.6185  bbox_loss: 0.0824  cls_loss: 0.1613  \n",
      "<<<iteration:[40/657] - total_loss: 1.0066  obj_loss: 0.0512  noobj_loss: 0.5757  bbox_loss: 0.0979  cls_loss: 0.1779  \n",
      "<<<iteration:[60/657] - total_loss: 0.9102  obj_loss: 0.0567  noobj_loss: 0.5804  bbox_loss: 0.0811  cls_loss: 0.1576  \n",
      "<<<iteration:[80/657] - total_loss: 0.9681  obj_loss: 0.0595  noobj_loss: 0.5575  bbox_loss: 0.0941  cls_loss: 0.1592  \n",
      "<<<iteration:[100/657] - total_loss: 0.8584  obj_loss: 0.0622  noobj_loss: 0.5662  bbox_loss: 0.0701  cls_loss: 0.1624  \n",
      "<<<iteration:[120/657] - total_loss: 0.8531  obj_loss: 0.0587  noobj_loss: 0.5679  bbox_loss: 0.0709  cls_loss: 0.1558  \n",
      "<<<iteration:[140/657] - total_loss: 0.9632  obj_loss: 0.0509  noobj_loss: 0.5999  bbox_loss: 0.0902  cls_loss: 0.1615  \n",
      "<<<iteration:[160/657] - total_loss: 0.9146  obj_loss: 0.0544  noobj_loss: 0.5729  bbox_loss: 0.0814  cls_loss: 0.1668  \n",
      "<<<iteration:[180/657] - total_loss: 1.0271  obj_loss: 0.0562  noobj_loss: 0.6278  bbox_loss: 0.0970  cls_loss: 0.1722  \n",
      "<<<iteration:[200/657] - total_loss: 0.9572  obj_loss: 0.0580  noobj_loss: 0.5690  bbox_loss: 0.0894  cls_loss: 0.1675  \n",
      "<<<iteration:[220/657] - total_loss: 0.9375  obj_loss: 0.0544  noobj_loss: 0.5848  bbox_loss: 0.0877  cls_loss: 0.1525  \n",
      "<<<iteration:[240/657] - total_loss: 0.9214  obj_loss: 0.0522  noobj_loss: 0.5564  bbox_loss: 0.0829  cls_loss: 0.1762  \n",
      "<<<iteration:[260/657] - total_loss: 0.8997  obj_loss: 0.0619  noobj_loss: 0.5640  bbox_loss: 0.0781  cls_loss: 0.1655  \n",
      "<<<iteration:[280/657] - total_loss: 0.9406  obj_loss: 0.0580  noobj_loss: 0.5902  bbox_loss: 0.0851  cls_loss: 0.1619  \n",
      "<<<iteration:[300/657] - total_loss: 0.8562  obj_loss: 0.0500  noobj_loss: 0.5214  bbox_loss: 0.0738  cls_loss: 0.1764  \n",
      "<<<iteration:[320/657] - total_loss: 0.7853  obj_loss: 0.0559  noobj_loss: 0.5334  bbox_loss: 0.0615  cls_loss: 0.1551  \n",
      "<<<iteration:[340/657] - total_loss: 0.8780  obj_loss: 0.0623  noobj_loss: 0.5860  bbox_loss: 0.0763  cls_loss: 0.1412  \n",
      "<<<iteration:[360/657] - total_loss: 1.2804  obj_loss: 0.0573  noobj_loss: 0.5575  bbox_loss: 0.1537  cls_loss: 0.1757  \n",
      "<<<iteration:[380/657] - total_loss: 0.9458  obj_loss: 0.0480  noobj_loss: 0.5267  bbox_loss: 0.0936  cls_loss: 0.1666  \n",
      "<<<iteration:[400/657] - total_loss: 0.8635  obj_loss: 0.0608  noobj_loss: 0.5339  bbox_loss: 0.0736  cls_loss: 0.1680  \n",
      "<<<iteration:[420/657] - total_loss: 1.0238  obj_loss: 0.0550  noobj_loss: 0.5808  bbox_loss: 0.1058  cls_loss: 0.1493  \n",
      "<<<iteration:[440/657] - total_loss: 0.8980  obj_loss: 0.0634  noobj_loss: 0.5394  bbox_loss: 0.0833  cls_loss: 0.1482  \n",
      "<<<iteration:[460/657] - total_loss: 0.8758  obj_loss: 0.0551  noobj_loss: 0.5455  bbox_loss: 0.0747  cls_loss: 0.1745  \n",
      "<<<iteration:[480/657] - total_loss: 0.9134  obj_loss: 0.0599  noobj_loss: 0.5530  bbox_loss: 0.0834  cls_loss: 0.1601  \n",
      "<<<iteration:[500/657] - total_loss: 0.9063  obj_loss: 0.0608  noobj_loss: 0.5380  bbox_loss: 0.0836  cls_loss: 0.1583  \n",
      "<<<iteration:[520/657] - total_loss: 0.8866  obj_loss: 0.0617  noobj_loss: 0.5614  bbox_loss: 0.0801  cls_loss: 0.1435  \n",
      "<<<iteration:[540/657] - total_loss: 0.8643  obj_loss: 0.0604  noobj_loss: 0.5420  bbox_loss: 0.0804  cls_loss: 0.1309  \n",
      "<<<iteration:[560/657] - total_loss: 0.8445  obj_loss: 0.0623  noobj_loss: 0.5398  bbox_loss: 0.0736  cls_loss: 0.1442  \n",
      "<<<iteration:[580/657] - total_loss: 0.8272  obj_loss: 0.0663  noobj_loss: 0.5006  bbox_loss: 0.0721  cls_loss: 0.1503  \n",
      "<<<iteration:[600/657] - total_loss: 13.8133  obj_loss: 0.0546  noobj_loss: 6.3370  bbox_loss: 1.8931  cls_loss: 1.1247  \n",
      "<<<iteration:[620/657] - total_loss: 15.0579  obj_loss: 0.0261  noobj_loss: 2.2096  bbox_loss: 2.4598  cls_loss: 1.6281  \n",
      "<<<iteration:[640/657] - total_loss: 11.8824  obj_loss: 0.0224  noobj_loss: 1.5315  bbox_loss: 1.9954  cls_loss: 1.1171  \n",
      "\n",
      "epoch:10/100 - Train Loss: 2.2900, Val Loss: 8.2222\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 8.3328  obj_loss: 0.0336  noobj_loss: 1.1851  bbox_loss: 1.4078  cls_loss: 0.6676  \n",
      "<<<iteration:[40/657] - total_loss: 6.1477  obj_loss: 0.0358  noobj_loss: 1.1007  bbox_loss: 1.0169  cls_loss: 0.4770  \n",
      "<<<iteration:[60/657] - total_loss: 4.4124  obj_loss: 0.0386  noobj_loss: 1.0661  bbox_loss: 0.7057  cls_loss: 0.3122  \n",
      "<<<iteration:[80/657] - total_loss: 6.9963  obj_loss: 0.0365  noobj_loss: 0.9667  bbox_loss: 1.1811  cls_loss: 0.5710  \n",
      "<<<iteration:[100/657] - total_loss: 4.7961  obj_loss: 0.0412  noobj_loss: 0.9942  bbox_loss: 0.7842  cls_loss: 0.3366  \n",
      "<<<iteration:[120/657] - total_loss: 6.3376  obj_loss: 0.0428  noobj_loss: 0.9583  bbox_loss: 1.0797  cls_loss: 0.4170  \n",
      "<<<iteration:[140/657] - total_loss: 4.9071  obj_loss: 0.0462  noobj_loss: 0.9040  bbox_loss: 0.8217  cls_loss: 0.3003  \n",
      "<<<iteration:[160/657] - total_loss: 4.5045  obj_loss: 0.0391  noobj_loss: 0.8933  bbox_loss: 0.7503  cls_loss: 0.2675  \n",
      "<<<iteration:[180/657] - total_loss: 3.6698  obj_loss: 0.0467  noobj_loss: 0.8301  bbox_loss: 0.5864  cls_loss: 0.2761  \n",
      "<<<iteration:[200/657] - total_loss: 5.5930  obj_loss: 0.0441  noobj_loss: 0.8336  bbox_loss: 0.9659  cls_loss: 0.3029  \n",
      "<<<iteration:[220/657] - total_loss: 5.2784  obj_loss: 0.0433  noobj_loss: 0.8145  bbox_loss: 0.9151  cls_loss: 0.2523  \n",
      "<<<iteration:[240/657] - total_loss: 4.7572  obj_loss: 0.0474  noobj_loss: 0.8257  bbox_loss: 0.8115  cls_loss: 0.2395  \n",
      "<<<iteration:[260/657] - total_loss: 2.3442  obj_loss: 0.0474  noobj_loss: 0.8228  bbox_loss: 0.3393  cls_loss: 0.1890  \n",
      "<<<iteration:[280/657] - total_loss: 5.1813  obj_loss: 0.0472  noobj_loss: 0.8064  bbox_loss: 0.9028  cls_loss: 0.2171  \n",
      "<<<iteration:[300/657] - total_loss: 2.9304  obj_loss: 0.0479  noobj_loss: 0.7372  bbox_loss: 0.4593  cls_loss: 0.2171  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[320/657] - total_loss: 4.3482  obj_loss: 0.0479  noobj_loss: 0.8173  bbox_loss: 0.7425  cls_loss: 0.1792  \n",
      "<<<iteration:[340/657] - total_loss: 2.9188  obj_loss: 0.0523  noobj_loss: 0.7377  bbox_loss: 0.4595  cls_loss: 0.2004  \n",
      "<<<iteration:[360/657] - total_loss: 3.8083  obj_loss: 0.0542  noobj_loss: 0.7396  bbox_loss: 0.6355  cls_loss: 0.2070  \n",
      "<<<iteration:[380/657] - total_loss: 3.6352  obj_loss: 0.0452  noobj_loss: 0.7277  bbox_loss: 0.6068  cls_loss: 0.1920  \n",
      "<<<iteration:[400/657] - total_loss: 5.1156  obj_loss: 0.0483  noobj_loss: 0.7389  bbox_loss: 0.9025  cls_loss: 0.1854  \n",
      "<<<iteration:[420/657] - total_loss: 5.7986  obj_loss: 0.0522  noobj_loss: 0.7156  bbox_loss: 1.0348  cls_loss: 0.2148  \n",
      "<<<iteration:[440/657] - total_loss: 4.2478  obj_loss: 0.0497  noobj_loss: 0.7435  bbox_loss: 0.7296  cls_loss: 0.1783  \n",
      "<<<iteration:[460/657] - total_loss: 4.3194  obj_loss: 0.0521  noobj_loss: 0.7280  bbox_loss: 0.7449  cls_loss: 0.1789  \n",
      "<<<iteration:[480/657] - total_loss: 3.9144  obj_loss: 0.0523  noobj_loss: 0.7008  bbox_loss: 0.6680  cls_loss: 0.1718  \n",
      "<<<iteration:[500/657] - total_loss: 3.0206  obj_loss: 0.0470  noobj_loss: 0.7030  bbox_loss: 0.4838  cls_loss: 0.2029  \n",
      "<<<iteration:[520/657] - total_loss: 2.4829  obj_loss: 0.0574  noobj_loss: 0.6889  bbox_loss: 0.3786  cls_loss: 0.1880  \n",
      "<<<iteration:[540/657] - total_loss: 2.8961  obj_loss: 0.0535  noobj_loss: 0.6736  bbox_loss: 0.4661  cls_loss: 0.1755  \n",
      "<<<iteration:[560/657] - total_loss: 3.6667  obj_loss: 0.0629  noobj_loss: 0.6992  bbox_loss: 0.6148  cls_loss: 0.1801  \n",
      "<<<iteration:[580/657] - total_loss: 4.5412  obj_loss: 0.0549  noobj_loss: 0.6812  bbox_loss: 0.7848  cls_loss: 0.2215  \n",
      "<<<iteration:[600/657] - total_loss: 2.1129  obj_loss: 0.0510  noobj_loss: 0.6419  bbox_loss: 0.3183  cls_loss: 0.1494  \n",
      "<<<iteration:[620/657] - total_loss: 3.2407  obj_loss: 0.0451  noobj_loss: 0.7244  bbox_loss: 0.5332  cls_loss: 0.1675  \n",
      "<<<iteration:[640/657] - total_loss: 4.3539  obj_loss: 0.0478  noobj_loss: 0.7276  bbox_loss: 0.7518  cls_loss: 0.1834  \n",
      "\n",
      "epoch:11/100 - Train Loss: 4.3554, Val Loss: 2.7499\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 4.7347  obj_loss: 0.0528  noobj_loss: 0.6916  bbox_loss: 0.8323  cls_loss: 0.1746  \n",
      "<<<iteration:[40/657] - total_loss: 4.7749  obj_loss: 0.0639  noobj_loss: 0.6313  bbox_loss: 0.8442  cls_loss: 0.1744  \n",
      "<<<iteration:[60/657] - total_loss: 4.8300  obj_loss: 0.0498  noobj_loss: 0.6910  bbox_loss: 0.8518  cls_loss: 0.1758  \n",
      "<<<iteration:[80/657] - total_loss: 3.8122  obj_loss: 0.0577  noobj_loss: 0.6391  bbox_loss: 0.6558  cls_loss: 0.1558  \n",
      "<<<iteration:[100/657] - total_loss: 4.1601  obj_loss: 0.0618  noobj_loss: 0.6719  bbox_loss: 0.7184  cls_loss: 0.1703  \n",
      "<<<iteration:[120/657] - total_loss: 3.4398  obj_loss: 0.0460  noobj_loss: 0.6428  bbox_loss: 0.5766  cls_loss: 0.1896  \n",
      "<<<iteration:[140/657] - total_loss: 4.6708  obj_loss: 0.0442  noobj_loss: 0.6607  bbox_loss: 0.8255  cls_loss: 0.1689  \n",
      "<<<iteration:[160/657] - total_loss: 2.9879  obj_loss: 0.0583  noobj_loss: 0.6136  bbox_loss: 0.4900  cls_loss: 0.1730  \n",
      "<<<iteration:[180/657] - total_loss: 2.4887  obj_loss: 0.0637  noobj_loss: 0.6403  bbox_loss: 0.3878  cls_loss: 0.1660  \n",
      "<<<iteration:[200/657] - total_loss: 1.3396  obj_loss: 0.0607  noobj_loss: 0.6066  bbox_loss: 0.1599  cls_loss: 0.1761  \n",
      "<<<iteration:[220/657] - total_loss: 2.7832  obj_loss: 0.0571  noobj_loss: 0.6422  bbox_loss: 0.4461  cls_loss: 0.1747  \n",
      "<<<iteration:[240/657] - total_loss: 2.2471  obj_loss: 0.0537  noobj_loss: 0.5980  bbox_loss: 0.3420  cls_loss: 0.1843  \n",
      "<<<iteration:[260/657] - total_loss: 3.7875  obj_loss: 0.0622  noobj_loss: 0.6228  bbox_loss: 0.6454  cls_loss: 0.1870  \n",
      "<<<iteration:[280/657] - total_loss: 2.4930  obj_loss: 0.0501  noobj_loss: 0.6017  bbox_loss: 0.3944  cls_loss: 0.1701  \n",
      "<<<iteration:[300/657] - total_loss: 2.1276  obj_loss: 0.0529  noobj_loss: 0.6195  bbox_loss: 0.3191  cls_loss: 0.1697  \n",
      "<<<iteration:[320/657] - total_loss: 3.2775  obj_loss: 0.0599  noobj_loss: 0.6410  bbox_loss: 0.5476  cls_loss: 0.1589  \n",
      "<<<iteration:[340/657] - total_loss: 1.3949  obj_loss: 0.0541  noobj_loss: 0.6127  bbox_loss: 0.1757  cls_loss: 0.1561  \n",
      "<<<iteration:[360/657] - total_loss: 1.6146  obj_loss: 0.0571  noobj_loss: 0.5651  bbox_loss: 0.2200  cls_loss: 0.1747  \n",
      "<<<iteration:[380/657] - total_loss: 3.6604  obj_loss: 0.0648  noobj_loss: 0.5790  bbox_loss: 0.6226  cls_loss: 0.1933  \n",
      "<<<iteration:[400/657] - total_loss: 2.7289  obj_loss: 0.0600  noobj_loss: 0.5605  bbox_loss: 0.4449  cls_loss: 0.1640  \n",
      "<<<iteration:[420/657] - total_loss: 1.8718  obj_loss: 0.0611  noobj_loss: 0.5957  bbox_loss: 0.2694  cls_loss: 0.1658  \n",
      "<<<iteration:[440/657] - total_loss: 4.0410  obj_loss: 0.0544  noobj_loss: 0.6665  bbox_loss: 0.7011  cls_loss: 0.1479  \n",
      "<<<iteration:[460/657] - total_loss: 2.4164  obj_loss: 0.0602  noobj_loss: 0.5941  bbox_loss: 0.3812  cls_loss: 0.1533  \n",
      "<<<iteration:[480/657] - total_loss: 3.4467  obj_loss: 0.0567  noobj_loss: 0.5286  bbox_loss: 0.5949  cls_loss: 0.1513  \n",
      "<<<iteration:[500/657] - total_loss: 3.3187  obj_loss: 0.0622  noobj_loss: 0.5832  bbox_loss: 0.5622  cls_loss: 0.1540  \n",
      "<<<iteration:[520/657] - total_loss: 1.4759  obj_loss: 0.0577  noobj_loss: 0.5704  bbox_loss: 0.1908  cls_loss: 0.1789  \n",
      "<<<iteration:[540/657] - total_loss: 3.0502  obj_loss: 0.0570  noobj_loss: 0.6007  bbox_loss: 0.5041  cls_loss: 0.1725  \n",
      "<<<iteration:[560/657] - total_loss: 2.0748  obj_loss: 0.0637  noobj_loss: 0.5468  bbox_loss: 0.3141  cls_loss: 0.1672  \n",
      "<<<iteration:[580/657] - total_loss: 2.2925  obj_loss: 0.0553  noobj_loss: 0.5516  bbox_loss: 0.3582  cls_loss: 0.1704  \n",
      "<<<iteration:[600/657] - total_loss: 2.2502  obj_loss: 0.0539  noobj_loss: 0.5861  bbox_loss: 0.3481  cls_loss: 0.1626  \n",
      "<<<iteration:[620/657] - total_loss: 2.6691  obj_loss: 0.0611  noobj_loss: 0.5514  bbox_loss: 0.4327  cls_loss: 0.1689  \n",
      "<<<iteration:[640/657] - total_loss: 2.8721  obj_loss: 0.0523  noobj_loss: 0.5810  bbox_loss: 0.4701  cls_loss: 0.1786  \n",
      "\n",
      "epoch:12/100 - Train Loss: 2.9530, Val Loss: 2.1188\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 3.1876  obj_loss: 0.0635  noobj_loss: 0.6297  bbox_loss: 0.5288  cls_loss: 0.1652  \n",
      "<<<iteration:[40/657] - total_loss: 4.1572  obj_loss: 0.0507  noobj_loss: 0.5376  bbox_loss: 0.7345  cls_loss: 0.1653  \n",
      "<<<iteration:[60/657] - total_loss: 3.2872  obj_loss: 0.0598  noobj_loss: 0.5549  bbox_loss: 0.5605  cls_loss: 0.1473  \n",
      "<<<iteration:[80/657] - total_loss: 2.3836  obj_loss: 0.0584  noobj_loss: 0.5469  bbox_loss: 0.3765  cls_loss: 0.1695  \n",
      "<<<iteration:[100/657] - total_loss: 2.2280  obj_loss: 0.0637  noobj_loss: 0.5830  bbox_loss: 0.3391  cls_loss: 0.1772  \n",
      "<<<iteration:[120/657] - total_loss: 2.6011  obj_loss: 0.0520  noobj_loss: 0.5618  bbox_loss: 0.4191  cls_loss: 0.1730  \n",
      "<<<iteration:[140/657] - total_loss: 1.7532  obj_loss: 0.0646  noobj_loss: 0.5533  bbox_loss: 0.2543  cls_loss: 0.1402  \n",
      "<<<iteration:[160/657] - total_loss: 1.9231  obj_loss: 0.0475  noobj_loss: 0.6201  bbox_loss: 0.2794  cls_loss: 0.1683  \n",
      "<<<iteration:[180/657] - total_loss: 1.9992  obj_loss: 0.0600  noobj_loss: 0.5618  bbox_loss: 0.2977  cls_loss: 0.1700  \n",
      "<<<iteration:[200/657] - total_loss: 1.5349  obj_loss: 0.0627  noobj_loss: 0.5824  bbox_loss: 0.2015  cls_loss: 0.1735  \n",
      "<<<iteration:[220/657] - total_loss: 3.0740  obj_loss: 0.0676  noobj_loss: 0.5197  bbox_loss: 0.5161  cls_loss: 0.1663  \n",
      "<<<iteration:[240/657] - total_loss: 3.9217  obj_loss: 0.0606  noobj_loss: 0.5728  bbox_loss: 0.6757  cls_loss: 0.1962  \n",
      "<<<iteration:[260/657] - total_loss: 1.9302  obj_loss: 0.0664  noobj_loss: 0.5409  bbox_loss: 0.2860  cls_loss: 0.1634  \n",
      "<<<iteration:[280/657] - total_loss: 3.6132  obj_loss: 0.0572  noobj_loss: 0.5819  bbox_loss: 0.6192  cls_loss: 0.1691  \n",
      "<<<iteration:[300/657] - total_loss: 2.2097  obj_loss: 0.0561  noobj_loss: 0.5239  bbox_loss: 0.3472  cls_loss: 0.1554  \n",
      "<<<iteration:[320/657] - total_loss: 2.4239  obj_loss: 0.0509  noobj_loss: 0.5667  bbox_loss: 0.3863  cls_loss: 0.1582  \n",
      "<<<iteration:[340/657] - total_loss: 2.3023  obj_loss: 0.0664  noobj_loss: 0.5345  bbox_loss: 0.3612  cls_loss: 0.1628  \n",
      "<<<iteration:[360/657] - total_loss: 1.9268  obj_loss: 0.0597  noobj_loss: 0.4985  bbox_loss: 0.2902  cls_loss: 0.1667  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[380/657] - total_loss: 2.9322  obj_loss: 0.0467  noobj_loss: 0.5738  bbox_loss: 0.4878  cls_loss: 0.1596  \n",
      "<<<iteration:[400/657] - total_loss: 2.6774  obj_loss: 0.0664  noobj_loss: 0.5476  bbox_loss: 0.4329  cls_loss: 0.1729  \n",
      "<<<iteration:[420/657] - total_loss: 3.2264  obj_loss: 0.0529  noobj_loss: 0.5761  bbox_loss: 0.5430  cls_loss: 0.1706  \n",
      "<<<iteration:[440/657] - total_loss: 1.5767  obj_loss: 0.0510  noobj_loss: 0.5663  bbox_loss: 0.2170  cls_loss: 0.1575  \n",
      "<<<iteration:[460/657] - total_loss: 2.7730  obj_loss: 0.0638  noobj_loss: 0.5006  bbox_loss: 0.4580  cls_loss: 0.1688  \n",
      "<<<iteration:[480/657] - total_loss: 1.8936  obj_loss: 0.0672  noobj_loss: 0.5825  bbox_loss: 0.2789  cls_loss: 0.1409  \n",
      "<<<iteration:[500/657] - total_loss: 1.6070  obj_loss: 0.0628  noobj_loss: 0.5228  bbox_loss: 0.2280  cls_loss: 0.1429  \n",
      "<<<iteration:[520/657] - total_loss: 3.4337  obj_loss: 0.0615  noobj_loss: 0.5131  bbox_loss: 0.5922  cls_loss: 0.1546  \n",
      "<<<iteration:[540/657] - total_loss: 2.0957  obj_loss: 0.0668  noobj_loss: 0.5129  bbox_loss: 0.3210  cls_loss: 0.1676  \n",
      "<<<iteration:[560/657] - total_loss: 2.1289  obj_loss: 0.0628  noobj_loss: 0.5527  bbox_loss: 0.3271  cls_loss: 0.1541  \n",
      "<<<iteration:[580/657] - total_loss: 1.8574  obj_loss: 0.0518  noobj_loss: 0.5130  bbox_loss: 0.2776  cls_loss: 0.1612  \n",
      "<<<iteration:[600/657] - total_loss: 2.4402  obj_loss: 0.0533  noobj_loss: 0.5436  bbox_loss: 0.3927  cls_loss: 0.1516  \n",
      "<<<iteration:[620/657] - total_loss: 1.5214  obj_loss: 0.0610  noobj_loss: 0.5197  bbox_loss: 0.2095  cls_loss: 0.1529  \n",
      "<<<iteration:[640/657] - total_loss: 1.2360  obj_loss: 0.0594  noobj_loss: 0.5158  bbox_loss: 0.1533  cls_loss: 0.1521  \n",
      "\n",
      "epoch:13/100 - Train Loss: 2.4161, Val Loss: 1.7858\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.9229  obj_loss: 0.0690  noobj_loss: 0.5086  bbox_loss: 0.2865  cls_loss: 0.1669  \n",
      "<<<iteration:[40/657] - total_loss: 1.7764  obj_loss: 0.0577  noobj_loss: 0.5161  bbox_loss: 0.2580  cls_loss: 0.1709  \n",
      "<<<iteration:[60/657] - total_loss: 1.8198  obj_loss: 0.0599  noobj_loss: 0.5275  bbox_loss: 0.2703  cls_loss: 0.1446  \n",
      "<<<iteration:[80/657] - total_loss: 2.0099  obj_loss: 0.0549  noobj_loss: 0.5032  bbox_loss: 0.3138  cls_loss: 0.1342  \n",
      "<<<iteration:[100/657] - total_loss: 2.0330  obj_loss: 0.0648  noobj_loss: 0.4786  bbox_loss: 0.3125  cls_loss: 0.1663  \n",
      "<<<iteration:[120/657] - total_loss: 2.1289  obj_loss: 0.0552  noobj_loss: 0.4828  bbox_loss: 0.3345  cls_loss: 0.1596  \n",
      "<<<iteration:[140/657] - total_loss: 2.1214  obj_loss: 0.0536  noobj_loss: 0.5051  bbox_loss: 0.3335  cls_loss: 0.1478  \n",
      "<<<iteration:[160/657] - total_loss: 1.3247  obj_loss: 0.0651  noobj_loss: 0.4951  bbox_loss: 0.1726  cls_loss: 0.1489  \n",
      "<<<iteration:[180/657] - total_loss: 2.8250  obj_loss: 0.0607  noobj_loss: 0.5586  bbox_loss: 0.4622  cls_loss: 0.1742  \n",
      "<<<iteration:[200/657] - total_loss: 2.2653  obj_loss: 0.0541  noobj_loss: 0.4966  bbox_loss: 0.3627  cls_loss: 0.1491  \n",
      "<<<iteration:[220/657] - total_loss: 2.6263  obj_loss: 0.0689  noobj_loss: 0.5441  bbox_loss: 0.4250  cls_loss: 0.1602  \n",
      "<<<iteration:[240/657] - total_loss: 1.8980  obj_loss: 0.0692  noobj_loss: 0.4930  bbox_loss: 0.2819  cls_loss: 0.1727  \n",
      "<<<iteration:[260/657] - total_loss: 1.4889  obj_loss: 0.0632  noobj_loss: 0.5278  bbox_loss: 0.2017  cls_loss: 0.1534  \n",
      "<<<iteration:[280/657] - total_loss: 2.0817  obj_loss: 0.0651  noobj_loss: 0.4963  bbox_loss: 0.3253  cls_loss: 0.1421  \n",
      "<<<iteration:[300/657] - total_loss: 2.0013  obj_loss: 0.0635  noobj_loss: 0.4946  bbox_loss: 0.3055  cls_loss: 0.1628  \n",
      "<<<iteration:[320/657] - total_loss: 1.2263  obj_loss: 0.0635  noobj_loss: 0.4828  bbox_loss: 0.1494  cls_loss: 0.1746  \n",
      "<<<iteration:[340/657] - total_loss: 1.6468  obj_loss: 0.0606  noobj_loss: 0.5281  bbox_loss: 0.2337  cls_loss: 0.1537  \n",
      "<<<iteration:[360/657] - total_loss: 1.5190  obj_loss: 0.0661  noobj_loss: 0.5281  bbox_loss: 0.2056  cls_loss: 0.1609  \n",
      "<<<iteration:[380/657] - total_loss: 2.0441  obj_loss: 0.0706  noobj_loss: 0.5337  bbox_loss: 0.3102  cls_loss: 0.1559  \n",
      "<<<iteration:[400/657] - total_loss: 2.2367  obj_loss: 0.0590  noobj_loss: 0.4884  bbox_loss: 0.3559  cls_loss: 0.1541  \n",
      "<<<iteration:[420/657] - total_loss: 1.6929  obj_loss: 0.0572  noobj_loss: 0.5023  bbox_loss: 0.2487  cls_loss: 0.1410  \n",
      "<<<iteration:[440/657] - total_loss: 1.5433  obj_loss: 0.0640  noobj_loss: 0.5099  bbox_loss: 0.2140  cls_loss: 0.1545  \n",
      "<<<iteration:[460/657] - total_loss: 2.7516  obj_loss: 0.0705  noobj_loss: 0.5193  bbox_loss: 0.4526  cls_loss: 0.1587  \n",
      "<<<iteration:[480/657] - total_loss: 2.1565  obj_loss: 0.0493  noobj_loss: 0.5045  bbox_loss: 0.3376  cls_loss: 0.1670  \n",
      "<<<iteration:[500/657] - total_loss: 2.8389  obj_loss: 0.0536  noobj_loss: 0.5237  bbox_loss: 0.4690  cls_loss: 0.1786  \n",
      "<<<iteration:[520/657] - total_loss: 1.4674  obj_loss: 0.0726  noobj_loss: 0.4878  bbox_loss: 0.1995  cls_loss: 0.1535  \n",
      "<<<iteration:[540/657] - total_loss: 2.0507  obj_loss: 0.0649  noobj_loss: 0.5251  bbox_loss: 0.3121  cls_loss: 0.1628  \n",
      "<<<iteration:[560/657] - total_loss: 1.3309  obj_loss: 0.0576  noobj_loss: 0.5072  bbox_loss: 0.1710  cls_loss: 0.1645  \n",
      "<<<iteration:[580/657] - total_loss: 2.1033  obj_loss: 0.0640  noobj_loss: 0.5320  bbox_loss: 0.3257  cls_loss: 0.1446  \n",
      "<<<iteration:[600/657] - total_loss: 2.1096  obj_loss: 0.0603  noobj_loss: 0.4859  bbox_loss: 0.3324  cls_loss: 0.1442  \n",
      "<<<iteration:[620/657] - total_loss: 2.3311  obj_loss: 0.0634  noobj_loss: 0.5043  bbox_loss: 0.3679  cls_loss: 0.1762  \n",
      "<<<iteration:[640/657] - total_loss: 2.2787  obj_loss: 0.0552  noobj_loss: 0.5170  bbox_loss: 0.3627  cls_loss: 0.1518  \n",
      "\n",
      "epoch:14/100 - Train Loss: 1.9580, Val Loss: 1.5129\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.5589  obj_loss: 0.0706  noobj_loss: 0.5415  bbox_loss: 0.2159  cls_loss: 0.1380  \n",
      "<<<iteration:[40/657] - total_loss: 2.4681  obj_loss: 0.0650  noobj_loss: 0.4721  bbox_loss: 0.4006  cls_loss: 0.1639  \n",
      "<<<iteration:[60/657] - total_loss: 1.8464  obj_loss: 0.0690  noobj_loss: 0.5090  bbox_loss: 0.2712  cls_loss: 0.1669  \n",
      "<<<iteration:[80/657] - total_loss: 2.1473  obj_loss: 0.0732  noobj_loss: 0.4483  bbox_loss: 0.3384  cls_loss: 0.1581  \n",
      "<<<iteration:[100/657] - total_loss: 0.9684  obj_loss: 0.0652  noobj_loss: 0.5055  bbox_loss: 0.1003  cls_loss: 0.1490  \n",
      "<<<iteration:[120/657] - total_loss: 1.7157  obj_loss: 0.0597  noobj_loss: 0.5011  bbox_loss: 0.2508  cls_loss: 0.1514  \n",
      "<<<iteration:[140/657] - total_loss: 2.4133  obj_loss: 0.0678  noobj_loss: 0.5088  bbox_loss: 0.3884  cls_loss: 0.1492  \n",
      "<<<iteration:[160/657] - total_loss: 1.5332  obj_loss: 0.0641  noobj_loss: 0.4873  bbox_loss: 0.2152  cls_loss: 0.1495  \n",
      "<<<iteration:[180/657] - total_loss: 1.8306  obj_loss: 0.0594  noobj_loss: 0.4861  bbox_loss: 0.2686  cls_loss: 0.1852  \n",
      "<<<iteration:[200/657] - total_loss: 1.4719  obj_loss: 0.0631  noobj_loss: 0.4822  bbox_loss: 0.2001  cls_loss: 0.1673  \n",
      "<<<iteration:[220/657] - total_loss: 1.1819  obj_loss: 0.0722  noobj_loss: 0.4801  bbox_loss: 0.1440  cls_loss: 0.1499  \n",
      "<<<iteration:[240/657] - total_loss: 1.6416  obj_loss: 0.0641  noobj_loss: 0.4559  bbox_loss: 0.2404  cls_loss: 0.1477  \n",
      "<<<iteration:[260/657] - total_loss: 1.4659  obj_loss: 0.0584  noobj_loss: 0.4685  bbox_loss: 0.2042  cls_loss: 0.1521  \n",
      "<<<iteration:[280/657] - total_loss: 1.8132  obj_loss: 0.0564  noobj_loss: 0.4872  bbox_loss: 0.2693  cls_loss: 0.1667  \n",
      "<<<iteration:[300/657] - total_loss: 2.2825  obj_loss: 0.0675  noobj_loss: 0.4964  bbox_loss: 0.3630  cls_loss: 0.1517  \n",
      "<<<iteration:[320/657] - total_loss: 1.1543  obj_loss: 0.0581  noobj_loss: 0.4764  bbox_loss: 0.1389  cls_loss: 0.1634  \n",
      "<<<iteration:[340/657] - total_loss: 2.0113  obj_loss: 0.0648  noobj_loss: 0.4944  bbox_loss: 0.3021  cls_loss: 0.1890  \n",
      "<<<iteration:[360/657] - total_loss: 1.7244  obj_loss: 0.0668  noobj_loss: 0.5161  bbox_loss: 0.2484  cls_loss: 0.1576  \n",
      "<<<iteration:[380/657] - total_loss: 1.2273  obj_loss: 0.0623  noobj_loss: 0.4769  bbox_loss: 0.1542  cls_loss: 0.1555  \n",
      "<<<iteration:[400/657] - total_loss: 1.4043  obj_loss: 0.0705  noobj_loss: 0.4948  bbox_loss: 0.1914  cls_loss: 0.1295  \n",
      "<<<iteration:[420/657] - total_loss: 1.2026  obj_loss: 0.0541  noobj_loss: 0.4850  bbox_loss: 0.1505  cls_loss: 0.1533  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[440/657] - total_loss: 0.9796  obj_loss: 0.0608  noobj_loss: 0.4736  bbox_loss: 0.1049  cls_loss: 0.1574  \n",
      "<<<iteration:[460/657] - total_loss: 1.4140  obj_loss: 0.0628  noobj_loss: 0.4624  bbox_loss: 0.1944  cls_loss: 0.1480  \n",
      "<<<iteration:[480/657] - total_loss: 2.0024  obj_loss: 0.0717  noobj_loss: 0.5279  bbox_loss: 0.3070  cls_loss: 0.1317  \n",
      "<<<iteration:[500/657] - total_loss: 1.7762  obj_loss: 0.0594  noobj_loss: 0.4957  bbox_loss: 0.2623  cls_loss: 0.1571  \n",
      "<<<iteration:[520/657] - total_loss: 1.0988  obj_loss: 0.0576  noobj_loss: 0.4539  bbox_loss: 0.1308  cls_loss: 0.1604  \n",
      "<<<iteration:[540/657] - total_loss: 1.6667  obj_loss: 0.0676  noobj_loss: 0.4709  bbox_loss: 0.2457  cls_loss: 0.1353  \n",
      "<<<iteration:[560/657] - total_loss: 1.7977  obj_loss: 0.0554  noobj_loss: 0.4565  bbox_loss: 0.2706  cls_loss: 0.1609  \n",
      "<<<iteration:[580/657] - total_loss: 1.1135  obj_loss: 0.0751  noobj_loss: 0.4732  bbox_loss: 0.1297  cls_loss: 0.1534  \n",
      "<<<iteration:[600/657] - total_loss: 1.0238  obj_loss: 0.0662  noobj_loss: 0.4629  bbox_loss: 0.1136  cls_loss: 0.1580  \n",
      "<<<iteration:[620/657] - total_loss: 1.3234  obj_loss: 0.0543  noobj_loss: 0.4637  bbox_loss: 0.1749  cls_loss: 0.1625  \n",
      "<<<iteration:[640/657] - total_loss: 1.4668  obj_loss: 0.0627  noobj_loss: 0.4702  bbox_loss: 0.2003  cls_loss: 0.1676  \n",
      "\n",
      "epoch:15/100 - Train Loss: 1.5821, Val Loss: 1.1829\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.2847  obj_loss: 0.0750  noobj_loss: 0.4608  bbox_loss: 0.1643  cls_loss: 0.1579  \n",
      "<<<iteration:[40/657] - total_loss: 1.4272  obj_loss: 0.0659  noobj_loss: 0.4959  bbox_loss: 0.1902  cls_loss: 0.1625  \n",
      "<<<iteration:[60/657] - total_loss: 1.1063  obj_loss: 0.0652  noobj_loss: 0.4583  bbox_loss: 0.1306  cls_loss: 0.1589  \n",
      "<<<iteration:[80/657] - total_loss: 1.2595  obj_loss: 0.0641  noobj_loss: 0.4826  bbox_loss: 0.1607  cls_loss: 0.1505  \n",
      "<<<iteration:[100/657] - total_loss: 1.6237  obj_loss: 0.0685  noobj_loss: 0.4636  bbox_loss: 0.2322  cls_loss: 0.1621  \n",
      "<<<iteration:[120/657] - total_loss: 1.1881  obj_loss: 0.0662  noobj_loss: 0.4648  bbox_loss: 0.1473  cls_loss: 0.1529  \n",
      "<<<iteration:[140/657] - total_loss: 1.5112  obj_loss: 0.0700  noobj_loss: 0.4631  bbox_loss: 0.2102  cls_loss: 0.1587  \n",
      "<<<iteration:[160/657] - total_loss: 1.2994  obj_loss: 0.0611  noobj_loss: 0.4839  bbox_loss: 0.1713  cls_loss: 0.1398  \n",
      "<<<iteration:[180/657] - total_loss: 1.2963  obj_loss: 0.0655  noobj_loss: 0.4796  bbox_loss: 0.1690  cls_loss: 0.1458  \n",
      "<<<iteration:[200/657] - total_loss: 0.9779  obj_loss: 0.0705  noobj_loss: 0.4450  bbox_loss: 0.1080  cls_loss: 0.1448  \n",
      "<<<iteration:[220/657] - total_loss: 1.9215  obj_loss: 0.0661  noobj_loss: 0.4808  bbox_loss: 0.2901  cls_loss: 0.1645  \n",
      "<<<iteration:[240/657] - total_loss: 1.1615  obj_loss: 0.0659  noobj_loss: 0.4783  bbox_loss: 0.1381  cls_loss: 0.1660  \n",
      "<<<iteration:[260/657] - total_loss: 1.4099  obj_loss: 0.0640  noobj_loss: 0.4586  bbox_loss: 0.1879  cls_loss: 0.1771  \n",
      "<<<iteration:[280/657] - total_loss: 1.1720  obj_loss: 0.0712  noobj_loss: 0.4627  bbox_loss: 0.1456  cls_loss: 0.1412  \n",
      "<<<iteration:[300/657] - total_loss: 0.9896  obj_loss: 0.0744  noobj_loss: 0.4106  bbox_loss: 0.1148  cls_loss: 0.1358  \n",
      "<<<iteration:[320/657] - total_loss: 1.5377  obj_loss: 0.0686  noobj_loss: 0.4330  bbox_loss: 0.2142  cls_loss: 0.1815  \n",
      "<<<iteration:[340/657] - total_loss: 1.5688  obj_loss: 0.0840  noobj_loss: 0.5060  bbox_loss: 0.2205  cls_loss: 0.1291  \n",
      "<<<iteration:[360/657] - total_loss: 0.9884  obj_loss: 0.0735  noobj_loss: 0.4558  bbox_loss: 0.1097  cls_loss: 0.1385  \n",
      "<<<iteration:[380/657] - total_loss: 1.2611  obj_loss: 0.0683  noobj_loss: 0.4866  bbox_loss: 0.1576  cls_loss: 0.1615  \n",
      "<<<iteration:[400/657] - total_loss: 2.0193  obj_loss: 0.0679  noobj_loss: 0.4267  bbox_loss: 0.3162  cls_loss: 0.1571  \n",
      "<<<iteration:[420/657] - total_loss: 0.9729  obj_loss: 0.0637  noobj_loss: 0.4405  bbox_loss: 0.1089  cls_loss: 0.1447  \n",
      "<<<iteration:[440/657] - total_loss: 1.0781  obj_loss: 0.0646  noobj_loss: 0.4532  bbox_loss: 0.1282  cls_loss: 0.1458  \n",
      "<<<iteration:[460/657] - total_loss: 0.9580  obj_loss: 0.0740  noobj_loss: 0.4343  bbox_loss: 0.1028  cls_loss: 0.1528  \n",
      "<<<iteration:[480/657] - total_loss: 0.7626  obj_loss: 0.0638  noobj_loss: 0.4055  bbox_loss: 0.0705  cls_loss: 0.1438  \n",
      "<<<iteration:[500/657] - total_loss: 1.0998  obj_loss: 0.0661  noobj_loss: 0.4722  bbox_loss: 0.1322  cls_loss: 0.1366  \n",
      "<<<iteration:[520/657] - total_loss: 1.1432  obj_loss: 0.0571  noobj_loss: 0.5444  bbox_loss: 0.1363  cls_loss: 0.1324  \n",
      "<<<iteration:[540/657] - total_loss: 1.3575  obj_loss: 0.0661  noobj_loss: 0.4783  bbox_loss: 0.1810  cls_loss: 0.1473  \n",
      "<<<iteration:[560/657] - total_loss: 1.5120  obj_loss: 0.0561  noobj_loss: 0.4830  bbox_loss: 0.2127  cls_loss: 0.1511  \n",
      "<<<iteration:[580/657] - total_loss: 1.2583  obj_loss: 0.0673  noobj_loss: 0.4274  bbox_loss: 0.1656  cls_loss: 0.1491  \n",
      "<<<iteration:[600/657] - total_loss: 1.2340  obj_loss: 0.0657  noobj_loss: 0.4867  bbox_loss: 0.1567  cls_loss: 0.1414  \n",
      "<<<iteration:[620/657] - total_loss: 1.3632  obj_loss: 0.0640  noobj_loss: 0.4315  bbox_loss: 0.1878  cls_loss: 0.1444  \n",
      "<<<iteration:[640/657] - total_loss: 1.3221  obj_loss: 0.0577  noobj_loss: 0.4340  bbox_loss: 0.1765  cls_loss: 0.1649  \n",
      "\n",
      "epoch:16/100 - Train Loss: 1.2733, Val Loss: 1.0373\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.1769  obj_loss: 0.0605  noobj_loss: 0.4798  bbox_loss: 0.1469  cls_loss: 0.1422  \n",
      "<<<iteration:[40/657] - total_loss: 1.0264  obj_loss: 0.0666  noobj_loss: 0.4239  bbox_loss: 0.1195  cls_loss: 0.1505  \n",
      "<<<iteration:[60/657] - total_loss: 1.0038  obj_loss: 0.0677  noobj_loss: 0.4163  bbox_loss: 0.1114  cls_loss: 0.1708  \n",
      "<<<iteration:[80/657] - total_loss: 1.3614  obj_loss: 0.0619  noobj_loss: 0.4477  bbox_loss: 0.1844  cls_loss: 0.1537  \n",
      "<<<iteration:[100/657] - total_loss: 0.9939  obj_loss: 0.0739  noobj_loss: 0.4320  bbox_loss: 0.1179  cls_loss: 0.1144  \n",
      "<<<iteration:[120/657] - total_loss: 0.8979  obj_loss: 0.0729  noobj_loss: 0.4301  bbox_loss: 0.0935  cls_loss: 0.1425  \n",
      "<<<iteration:[140/657] - total_loss: 1.0911  obj_loss: 0.0723  noobj_loss: 0.4402  bbox_loss: 0.1310  cls_loss: 0.1438  \n",
      "<<<iteration:[160/657] - total_loss: 1.3736  obj_loss: 0.0675  noobj_loss: 0.4658  bbox_loss: 0.1855  cls_loss: 0.1459  \n",
      "<<<iteration:[180/657] - total_loss: 1.1114  obj_loss: 0.0642  noobj_loss: 0.4048  bbox_loss: 0.1412  cls_loss: 0.1389  \n",
      "<<<iteration:[200/657] - total_loss: 1.1113  obj_loss: 0.0708  noobj_loss: 0.4225  bbox_loss: 0.1327  cls_loss: 0.1656  \n",
      "<<<iteration:[220/657] - total_loss: 0.8708  obj_loss: 0.0669  noobj_loss: 0.4970  bbox_loss: 0.0844  cls_loss: 0.1333  \n",
      "<<<iteration:[240/657] - total_loss: 1.1513  obj_loss: 0.0705  noobj_loss: 0.5514  bbox_loss: 0.1322  cls_loss: 0.1443  \n",
      "<<<iteration:[260/657] - total_loss: 0.8751  obj_loss: 0.0611  noobj_loss: 0.4394  bbox_loss: 0.0894  cls_loss: 0.1475  \n",
      "<<<iteration:[280/657] - total_loss: 0.7344  obj_loss: 0.0684  noobj_loss: 0.4276  bbox_loss: 0.0614  cls_loss: 0.1453  \n",
      "<<<iteration:[300/657] - total_loss: 0.9719  obj_loss: 0.0667  noobj_loss: 0.4795  bbox_loss: 0.1057  cls_loss: 0.1367  \n",
      "<<<iteration:[320/657] - total_loss: 1.1302  obj_loss: 0.0659  noobj_loss: 0.4389  bbox_loss: 0.1411  cls_loss: 0.1393  \n",
      "<<<iteration:[340/657] - total_loss: 0.9495  obj_loss: 0.0672  noobj_loss: 0.4302  bbox_loss: 0.1068  cls_loss: 0.1334  \n",
      "<<<iteration:[360/657] - total_loss: 1.4519  obj_loss: 0.0668  noobj_loss: 0.4039  bbox_loss: 0.2068  cls_loss: 0.1491  \n",
      "<<<iteration:[380/657] - total_loss: 0.8331  obj_loss: 0.0724  noobj_loss: 0.4684  bbox_loss: 0.0750  cls_loss: 0.1516  \n",
      "<<<iteration:[400/657] - total_loss: 1.0440  obj_loss: 0.0660  noobj_loss: 0.4344  bbox_loss: 0.1224  cls_loss: 0.1485  \n",
      "<<<iteration:[420/657] - total_loss: 0.9165  obj_loss: 0.0651  noobj_loss: 0.4215  bbox_loss: 0.0996  cls_loss: 0.1427  \n",
      "<<<iteration:[440/657] - total_loss: 1.2374  obj_loss: 0.0700  noobj_loss: 0.4043  bbox_loss: 0.1582  cls_loss: 0.1740  \n",
      "<<<iteration:[460/657] - total_loss: 1.2133  obj_loss: 0.0698  noobj_loss: 0.4109  bbox_loss: 0.1548  cls_loss: 0.1639  \n",
      "<<<iteration:[480/657] - total_loss: 1.0010  obj_loss: 0.0711  noobj_loss: 0.4615  bbox_loss: 0.1073  cls_loss: 0.1629  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[500/657] - total_loss: 1.0557  obj_loss: 0.0661  noobj_loss: 0.4854  bbox_loss: 0.1213  cls_loss: 0.1405  \n",
      "<<<iteration:[520/657] - total_loss: 1.1454  obj_loss: 0.0693  noobj_loss: 0.4386  bbox_loss: 0.1426  cls_loss: 0.1436  \n",
      "<<<iteration:[540/657] - total_loss: 1.0531  obj_loss: 0.0688  noobj_loss: 0.4343  bbox_loss: 0.1262  cls_loss: 0.1361  \n",
      "<<<iteration:[560/657] - total_loss: 1.2126  obj_loss: 0.0608  noobj_loss: 0.4593  bbox_loss: 0.1526  cls_loss: 0.1593  \n",
      "<<<iteration:[580/657] - total_loss: 0.8932  obj_loss: 0.0766  noobj_loss: 0.4030  bbox_loss: 0.0909  cls_loss: 0.1605  \n",
      "<<<iteration:[600/657] - total_loss: 0.9621  obj_loss: 0.0715  noobj_loss: 0.4226  bbox_loss: 0.1048  cls_loss: 0.1552  \n",
      "<<<iteration:[620/657] - total_loss: 0.9482  obj_loss: 0.0712  noobj_loss: 0.3985  bbox_loss: 0.1082  cls_loss: 0.1367  \n",
      "<<<iteration:[640/657] - total_loss: 0.8863  obj_loss: 0.0620  noobj_loss: 0.4721  bbox_loss: 0.0848  cls_loss: 0.1642  \n",
      "\n",
      "epoch:17/100 - Train Loss: 1.0442, Val Loss: 1.6950\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 1.2437  obj_loss: 0.0760  noobj_loss: 0.4589  bbox_loss: 0.1621  cls_loss: 0.1280  \n",
      "<<<iteration:[40/657] - total_loss: 0.8720  obj_loss: 0.0601  noobj_loss: 0.4002  bbox_loss: 0.0917  cls_loss: 0.1534  \n",
      "<<<iteration:[60/657] - total_loss: 1.0799  obj_loss: 0.0716  noobj_loss: 0.4077  bbox_loss: 0.1309  cls_loss: 0.1498  \n",
      "<<<iteration:[80/657] - total_loss: 0.8770  obj_loss: 0.0655  noobj_loss: 0.4603  bbox_loss: 0.0868  cls_loss: 0.1471  \n",
      "<<<iteration:[100/657] - total_loss: 1.0324  obj_loss: 0.0763  noobj_loss: 0.4506  bbox_loss: 0.1137  cls_loss: 0.1622  \n",
      "<<<iteration:[120/657] - total_loss: 0.8059  obj_loss: 0.0752  noobj_loss: 0.5232  bbox_loss: 0.0646  cls_loss: 0.1462  \n",
      "<<<iteration:[140/657] - total_loss: 1.1243  obj_loss: 0.0594  noobj_loss: 0.4080  bbox_loss: 0.1419  cls_loss: 0.1513  \n",
      "<<<iteration:[160/657] - total_loss: 0.9452  obj_loss: 0.0728  noobj_loss: 0.4406  bbox_loss: 0.1014  cls_loss: 0.1451  \n",
      "<<<iteration:[180/657] - total_loss: 0.9382  obj_loss: 0.0717  noobj_loss: 0.4237  bbox_loss: 0.1044  cls_loss: 0.1325  \n",
      "<<<iteration:[200/657] - total_loss: 0.7987  obj_loss: 0.0724  noobj_loss: 0.4014  bbox_loss: 0.0774  cls_loss: 0.1388  \n",
      "<<<iteration:[220/657] - total_loss: 0.8554  obj_loss: 0.0740  noobj_loss: 0.4161  bbox_loss: 0.0839  cls_loss: 0.1537  \n",
      "<<<iteration:[240/657] - total_loss: 0.9020  obj_loss: 0.0633  noobj_loss: 0.4228  bbox_loss: 0.0970  cls_loss: 0.1423  \n",
      "<<<iteration:[260/657] - total_loss: 0.9567  obj_loss: 0.0671  noobj_loss: 0.4793  bbox_loss: 0.0986  cls_loss: 0.1570  \n",
      "<<<iteration:[280/657] - total_loss: 1.0875  obj_loss: 0.0541  noobj_loss: 0.4249  bbox_loss: 0.1378  cls_loss: 0.1322  \n",
      "<<<iteration:[300/657] - total_loss: 0.7513  obj_loss: 0.0714  noobj_loss: 0.3817  bbox_loss: 0.0701  cls_loss: 0.1387  \n",
      "<<<iteration:[320/657] - total_loss: 1.0588  obj_loss: 0.0572  noobj_loss: 0.4921  bbox_loss: 0.1227  cls_loss: 0.1422  \n",
      "<<<iteration:[340/657] - total_loss: 0.9860  obj_loss: 0.0737  noobj_loss: 0.3846  bbox_loss: 0.1132  cls_loss: 0.1543  \n",
      "<<<iteration:[360/657] - total_loss: 0.9119  obj_loss: 0.0661  noobj_loss: 0.3912  bbox_loss: 0.0987  cls_loss: 0.1568  \n",
      "<<<iteration:[380/657] - total_loss: 0.8790  obj_loss: 0.0663  noobj_loss: 0.4202  bbox_loss: 0.0970  cls_loss: 0.1176  \n",
      "<<<iteration:[400/657] - total_loss: 0.8124  obj_loss: 0.0743  noobj_loss: 0.4345  bbox_loss: 0.0795  cls_loss: 0.1234  \n",
      "<<<iteration:[420/657] - total_loss: 0.9897  obj_loss: 0.0656  noobj_loss: 0.4037  bbox_loss: 0.1141  cls_loss: 0.1519  \n",
      "<<<iteration:[440/657] - total_loss: 0.7641  obj_loss: 0.0649  noobj_loss: 0.3995  bbox_loss: 0.0761  cls_loss: 0.1187  \n",
      "<<<iteration:[460/657] - total_loss: 0.8460  obj_loss: 0.0680  noobj_loss: 0.3887  bbox_loss: 0.0879  cls_loss: 0.1441  \n",
      "<<<iteration:[480/657] - total_loss: 1.0666  obj_loss: 0.0682  noobj_loss: 0.3914  bbox_loss: 0.1351  cls_loss: 0.1272  \n",
      "<<<iteration:[500/657] - total_loss: 0.6538  obj_loss: 0.0718  noobj_loss: 0.3653  bbox_loss: 0.0509  cls_loss: 0.1447  \n",
      "<<<iteration:[520/657] - total_loss: 0.7326  obj_loss: 0.0637  noobj_loss: 0.4191  bbox_loss: 0.0659  cls_loss: 0.1301  \n",
      "<<<iteration:[540/657] - total_loss: 0.9041  obj_loss: 0.0716  noobj_loss: 0.3881  bbox_loss: 0.0937  cls_loss: 0.1700  \n",
      "<<<iteration:[560/657] - total_loss: 0.8121  obj_loss: 0.0604  noobj_loss: 0.4246  bbox_loss: 0.0758  cls_loss: 0.1604  \n",
      "<<<iteration:[580/657] - total_loss: 0.7807  obj_loss: 0.0700  noobj_loss: 0.4424  bbox_loss: 0.0680  cls_loss: 0.1497  \n",
      "<<<iteration:[600/657] - total_loss: 0.8315  obj_loss: 0.0731  noobj_loss: 0.4047  bbox_loss: 0.0806  cls_loss: 0.1529  \n",
      "<<<iteration:[620/657] - total_loss: 0.7186  obj_loss: 0.0686  noobj_loss: 0.4108  bbox_loss: 0.0613  cls_loss: 0.1380  \n",
      "<<<iteration:[640/657] - total_loss: 0.6825  obj_loss: 0.0694  noobj_loss: 0.3723  bbox_loss: 0.0573  cls_loss: 0.1406  \n",
      "\n",
      "epoch:18/100 - Train Loss: 0.8945, Val Loss: 0.9451\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.8641  obj_loss: 0.0770  noobj_loss: 0.4188  bbox_loss: 0.0807  cls_loss: 0.1743  \n",
      "<<<iteration:[40/657] - total_loss: 1.0420  obj_loss: 0.0709  noobj_loss: 0.3845  bbox_loss: 0.1263  cls_loss: 0.1473  \n",
      "<<<iteration:[60/657] - total_loss: 0.8673  obj_loss: 0.0726  noobj_loss: 0.3951  bbox_loss: 0.0939  cls_loss: 0.1278  \n",
      "<<<iteration:[80/657] - total_loss: 0.7757  obj_loss: 0.0700  noobj_loss: 0.4232  bbox_loss: 0.0731  cls_loss: 0.1286  \n",
      "<<<iteration:[100/657] - total_loss: 0.7118  obj_loss: 0.0701  noobj_loss: 0.3854  bbox_loss: 0.0637  cls_loss: 0.1303  \n",
      "<<<iteration:[120/657] - total_loss: 0.7352  obj_loss: 0.0588  noobj_loss: 0.3847  bbox_loss: 0.0674  cls_loss: 0.1469  \n",
      "<<<iteration:[140/657] - total_loss: 0.6775  obj_loss: 0.0685  noobj_loss: 0.3672  bbox_loss: 0.0574  cls_loss: 0.1385  \n",
      "<<<iteration:[160/657] - total_loss: 1.0124  obj_loss: 0.0641  noobj_loss: 0.3818  bbox_loss: 0.1240  cls_loss: 0.1376  \n",
      "<<<iteration:[180/657] - total_loss: 0.7949  obj_loss: 0.0710  noobj_loss: 0.4002  bbox_loss: 0.0774  cls_loss: 0.1368  \n",
      "<<<iteration:[200/657] - total_loss: 0.6749  obj_loss: 0.0658  noobj_loss: 0.3662  bbox_loss: 0.0604  cls_loss: 0.1238  \n",
      "<<<iteration:[220/657] - total_loss: 0.7758  obj_loss: 0.0660  noobj_loss: 0.3797  bbox_loss: 0.0774  cls_loss: 0.1329  \n",
      "<<<iteration:[240/657] - total_loss: 0.7658  obj_loss: 0.0679  noobj_loss: 0.4042  bbox_loss: 0.0675  cls_loss: 0.1583  \n",
      "<<<iteration:[260/657] - total_loss: 1.0014  obj_loss: 0.0790  noobj_loss: 0.3646  bbox_loss: 0.1217  cls_loss: 0.1315  \n",
      "<<<iteration:[280/657] - total_loss: 0.7554  obj_loss: 0.0690  noobj_loss: 0.3966  bbox_loss: 0.0714  cls_loss: 0.1311  \n",
      "<<<iteration:[300/657] - total_loss: 0.8589  obj_loss: 0.0713  noobj_loss: 0.3876  bbox_loss: 0.0930  cls_loss: 0.1286  \n",
      "<<<iteration:[320/657] - total_loss: 1.0203  obj_loss: 0.0726  noobj_loss: 0.3971  bbox_loss: 0.1176  cls_loss: 0.1611  \n",
      "<<<iteration:[340/657] - total_loss: 1.0471  obj_loss: 0.0658  noobj_loss: 0.4844  bbox_loss: 0.1191  cls_loss: 0.1438  \n",
      "<<<iteration:[360/657] - total_loss: 0.7218  obj_loss: 0.0713  noobj_loss: 0.3619  bbox_loss: 0.0658  cls_loss: 0.1407  \n",
      "<<<iteration:[380/657] - total_loss: 0.7499  obj_loss: 0.0702  noobj_loss: 0.3853  bbox_loss: 0.0701  cls_loss: 0.1366  \n",
      "<<<iteration:[400/657] - total_loss: 0.8064  obj_loss: 0.0676  noobj_loss: 0.3878  bbox_loss: 0.0812  cls_loss: 0.1390  \n",
      "<<<iteration:[420/657] - total_loss: 0.7722  obj_loss: 0.0630  noobj_loss: 0.3907  bbox_loss: 0.0692  cls_loss: 0.1680  \n",
      "<<<iteration:[440/657] - total_loss: 0.6842  obj_loss: 0.0723  noobj_loss: 0.3775  bbox_loss: 0.0608  cls_loss: 0.1191  \n",
      "<<<iteration:[460/657] - total_loss: 0.8730  obj_loss: 0.0688  noobj_loss: 0.4643  bbox_loss: 0.0842  cls_loss: 0.1508  \n",
      "<<<iteration:[480/657] - total_loss: 0.7327  obj_loss: 0.0774  noobj_loss: 0.3703  bbox_loss: 0.0684  cls_loss: 0.1282  \n",
      "<<<iteration:[500/657] - total_loss: 0.8297  obj_loss: 0.0776  noobj_loss: 0.3699  bbox_loss: 0.0816  cls_loss: 0.1590  \n",
      "<<<iteration:[520/657] - total_loss: 0.8614  obj_loss: 0.0666  noobj_loss: 0.3971  bbox_loss: 0.0894  cls_loss: 0.1491  \n",
      "<<<iteration:[540/657] - total_loss: 0.7407  obj_loss: 0.0680  noobj_loss: 0.3583  bbox_loss: 0.0731  cls_loss: 0.1282  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[560/657] - total_loss: 1.0200  obj_loss: 0.0711  noobj_loss: 0.4017  bbox_loss: 0.1170  cls_loss: 0.1630  \n",
      "<<<iteration:[580/657] - total_loss: 0.7015  obj_loss: 0.0721  noobj_loss: 0.3681  bbox_loss: 0.0619  cls_loss: 0.1359  \n",
      "<<<iteration:[600/657] - total_loss: 0.7742  obj_loss: 0.0690  noobj_loss: 0.3945  bbox_loss: 0.0753  cls_loss: 0.1313  \n",
      "<<<iteration:[620/657] - total_loss: 0.6702  obj_loss: 0.0637  noobj_loss: 0.3580  bbox_loss: 0.0603  cls_loss: 0.1259  \n",
      "<<<iteration:[640/657] - total_loss: 0.6811  obj_loss: 0.0762  noobj_loss: 0.3520  bbox_loss: 0.0540  cls_loss: 0.1587  \n",
      "\n",
      "epoch:19/100 - Train Loss: 0.8083, Val Loss: 0.6829\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.7259  obj_loss: 0.0806  noobj_loss: 0.3775  bbox_loss: 0.0627  cls_loss: 0.1431  \n",
      "<<<iteration:[40/657] - total_loss: 0.6859  obj_loss: 0.0757  noobj_loss: 0.3830  bbox_loss: 0.0605  cls_loss: 0.1162  \n",
      "<<<iteration:[60/657] - total_loss: 0.6665  obj_loss: 0.0724  noobj_loss: 0.3779  bbox_loss: 0.0528  cls_loss: 0.1413  \n",
      "<<<iteration:[80/657] - total_loss: 0.6412  obj_loss: 0.0764  noobj_loss: 0.3871  bbox_loss: 0.0495  cls_loss: 0.1239  \n",
      "<<<iteration:[100/657] - total_loss: 0.7562  obj_loss: 0.0807  noobj_loss: 0.3843  bbox_loss: 0.0695  cls_loss: 0.1358  \n",
      "<<<iteration:[120/657] - total_loss: 0.8610  obj_loss: 0.0632  noobj_loss: 0.3655  bbox_loss: 0.0955  cls_loss: 0.1376  \n",
      "<<<iteration:[140/657] - total_loss: 0.6403  obj_loss: 0.0665  noobj_loss: 0.3570  bbox_loss: 0.0526  cls_loss: 0.1324  \n",
      "<<<iteration:[160/657] - total_loss: 0.8015  obj_loss: 0.0682  noobj_loss: 0.3284  bbox_loss: 0.0843  cls_loss: 0.1476  \n",
      "<<<iteration:[180/657] - total_loss: 0.7608  obj_loss: 0.0777  noobj_loss: 0.3653  bbox_loss: 0.0747  cls_loss: 0.1271  \n",
      "<<<iteration:[200/657] - total_loss: 0.7116  obj_loss: 0.0735  noobj_loss: 0.3437  bbox_loss: 0.0616  cls_loss: 0.1581  \n",
      "<<<iteration:[220/657] - total_loss: 0.7389  obj_loss: 0.0771  noobj_loss: 0.3707  bbox_loss: 0.0682  cls_loss: 0.1357  \n",
      "<<<iteration:[240/657] - total_loss: 0.9174  obj_loss: 0.0673  noobj_loss: 0.3625  bbox_loss: 0.1064  cls_loss: 0.1366  \n",
      "<<<iteration:[260/657] - total_loss: 0.6821  obj_loss: 0.0698  noobj_loss: 0.3857  bbox_loss: 0.0574  cls_loss: 0.1325  \n",
      "<<<iteration:[280/657] - total_loss: 0.7159  obj_loss: 0.0671  noobj_loss: 0.3451  bbox_loss: 0.0672  cls_loss: 0.1403  \n",
      "<<<iteration:[300/657] - total_loss: 0.6462  obj_loss: 0.0697  noobj_loss: 0.3107  bbox_loss: 0.0571  cls_loss: 0.1359  \n",
      "<<<iteration:[320/657] - total_loss: 0.7331  obj_loss: 0.0802  noobj_loss: 0.4058  bbox_loss: 0.0591  cls_loss: 0.1545  \n",
      "<<<iteration:[340/657] - total_loss: 0.8515  obj_loss: 0.0696  noobj_loss: 0.3626  bbox_loss: 0.0908  cls_loss: 0.1466  \n",
      "<<<iteration:[360/657] - total_loss: 0.6669  obj_loss: 0.0742  noobj_loss: 0.3719  bbox_loss: 0.0542  cls_loss: 0.1358  \n",
      "<<<iteration:[380/657] - total_loss: 0.6662  obj_loss: 0.0722  noobj_loss: 0.3462  bbox_loss: 0.0570  cls_loss: 0.1360  \n",
      "<<<iteration:[400/657] - total_loss: 0.6008  obj_loss: 0.0839  noobj_loss: 0.3277  bbox_loss: 0.0438  cls_loss: 0.1340  \n",
      "<<<iteration:[420/657] - total_loss: 0.6654  obj_loss: 0.0750  noobj_loss: 0.3517  bbox_loss: 0.0558  cls_loss: 0.1358  \n",
      "<<<iteration:[440/657] - total_loss: 0.6381  obj_loss: 0.0779  noobj_loss: 0.3357  bbox_loss: 0.0497  cls_loss: 0.1437  \n",
      "<<<iteration:[460/657] - total_loss: 0.7595  obj_loss: 0.0716  noobj_loss: 0.3673  bbox_loss: 0.0718  cls_loss: 0.1451  \n",
      "<<<iteration:[480/657] - total_loss: 0.6807  obj_loss: 0.0756  noobj_loss: 0.3454  bbox_loss: 0.0636  cls_loss: 0.1142  \n",
      "<<<iteration:[500/657] - total_loss: 0.7384  obj_loss: 0.0693  noobj_loss: 0.3432  bbox_loss: 0.0741  cls_loss: 0.1268  \n",
      "<<<iteration:[520/657] - total_loss: 0.6602  obj_loss: 0.0739  noobj_loss: 0.3511  bbox_loss: 0.0537  cls_loss: 0.1421  \n",
      "<<<iteration:[540/657] - total_loss: 0.7567  obj_loss: 0.0729  noobj_loss: 0.3441  bbox_loss: 0.0757  cls_loss: 0.1335  \n",
      "<<<iteration:[560/657] - total_loss: 0.6846  obj_loss: 0.0734  noobj_loss: 0.3448  bbox_loss: 0.0648  cls_loss: 0.1149  \n",
      "<<<iteration:[580/657] - total_loss: 0.8199  obj_loss: 0.0817  noobj_loss: 0.3291  bbox_loss: 0.0887  cls_loss: 0.1303  \n",
      "<<<iteration:[600/657] - total_loss: 0.6792  obj_loss: 0.0818  noobj_loss: 0.3637  bbox_loss: 0.0527  cls_loss: 0.1519  \n",
      "<<<iteration:[620/657] - total_loss: 0.7779  obj_loss: 0.0755  noobj_loss: 0.3750  bbox_loss: 0.0780  cls_loss: 0.1249  \n",
      "<<<iteration:[640/657] - total_loss: 0.8554  obj_loss: 0.0657  noobj_loss: 0.3482  bbox_loss: 0.0953  cls_loss: 0.1390  \n",
      "\n",
      "epoch:20/100 - Train Loss: 0.7210, Val Loss: 0.6425\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.7852  obj_loss: 0.0811  noobj_loss: 0.3652  bbox_loss: 0.0777  cls_loss: 0.1329  \n",
      "<<<iteration:[40/657] - total_loss: 0.6283  obj_loss: 0.0735  noobj_loss: 0.3627  bbox_loss: 0.0506  cls_loss: 0.1207  \n",
      "<<<iteration:[60/657] - total_loss: 0.7113  obj_loss: 0.0648  noobj_loss: 0.3356  bbox_loss: 0.0668  cls_loss: 0.1446  \n",
      "<<<iteration:[80/657] - total_loss: 0.7332  obj_loss: 0.0653  noobj_loss: 0.3414  bbox_loss: 0.0747  cls_loss: 0.1236  \n",
      "<<<iteration:[100/657] - total_loss: 0.6444  obj_loss: 0.0716  noobj_loss: 0.3371  bbox_loss: 0.0541  cls_loss: 0.1339  \n",
      "<<<iteration:[120/657] - total_loss: 0.6603  obj_loss: 0.0868  noobj_loss: 0.3266  bbox_loss: 0.0532  cls_loss: 0.1440  \n",
      "<<<iteration:[140/657] - total_loss: 0.7368  obj_loss: 0.0675  noobj_loss: 0.3295  bbox_loss: 0.0729  cls_loss: 0.1399  \n",
      "<<<iteration:[160/657] - total_loss: 0.6618  obj_loss: 0.0686  noobj_loss: 0.3316  bbox_loss: 0.0625  cls_loss: 0.1148  \n",
      "<<<iteration:[180/657] - total_loss: 0.7559  obj_loss: 0.0681  noobj_loss: 0.3614  bbox_loss: 0.0789  cls_loss: 0.1125  \n",
      "<<<iteration:[200/657] - total_loss: 0.7428  obj_loss: 0.0790  noobj_loss: 0.3705  bbox_loss: 0.0670  cls_loss: 0.1436  \n",
      "<<<iteration:[220/657] - total_loss: 0.6363  obj_loss: 0.0986  noobj_loss: 0.3356  bbox_loss: 0.0491  cls_loss: 0.1243  \n",
      "<<<iteration:[240/657] - total_loss: 0.9461  obj_loss: 0.0785  noobj_loss: 0.3382  bbox_loss: 0.1099  cls_loss: 0.1489  \n",
      "<<<iteration:[260/657] - total_loss: 0.6626  obj_loss: 0.0737  noobj_loss: 0.3459  bbox_loss: 0.0538  cls_loss: 0.1469  \n",
      "<<<iteration:[280/657] - total_loss: 0.7979  obj_loss: 0.0718  noobj_loss: 0.3273  bbox_loss: 0.0815  cls_loss: 0.1551  \n",
      "<<<iteration:[300/657] - total_loss: 0.6854  obj_loss: 0.0727  noobj_loss: 0.3577  bbox_loss: 0.0595  cls_loss: 0.1364  \n",
      "<<<iteration:[320/657] - total_loss: 0.7917  obj_loss: 0.0739  noobj_loss: 0.3408  bbox_loss: 0.0812  cls_loss: 0.1416  \n",
      "<<<iteration:[340/657] - total_loss: 0.7386  obj_loss: 0.0856  noobj_loss: 0.3193  bbox_loss: 0.0726  cls_loss: 0.1302  \n",
      "<<<iteration:[360/657] - total_loss: 0.6651  obj_loss: 0.0698  noobj_loss: 0.3108  bbox_loss: 0.0620  cls_loss: 0.1298  \n",
      "<<<iteration:[380/657] - total_loss: 0.7072  obj_loss: 0.0719  noobj_loss: 0.3300  bbox_loss: 0.0670  cls_loss: 0.1352  \n",
      "<<<iteration:[400/657] - total_loss: 0.7611  obj_loss: 0.0734  noobj_loss: 0.3274  bbox_loss: 0.0780  cls_loss: 0.1341  \n",
      "<<<iteration:[420/657] - total_loss: 0.7710  obj_loss: 0.0715  noobj_loss: 0.3062  bbox_loss: 0.0817  cls_loss: 0.1379  \n",
      "<<<iteration:[440/657] - total_loss: 0.6794  obj_loss: 0.0635  noobj_loss: 0.3032  bbox_loss: 0.0657  cls_loss: 0.1359  \n",
      "<<<iteration:[460/657] - total_loss: 0.6801  obj_loss: 0.0761  noobj_loss: 0.3188  bbox_loss: 0.0612  cls_loss: 0.1383  \n",
      "<<<iteration:[480/657] - total_loss: 0.6951  obj_loss: 0.0738  noobj_loss: 0.3473  bbox_loss: 0.0664  cls_loss: 0.1158  \n",
      "<<<iteration:[500/657] - total_loss: 0.6285  obj_loss: 0.0870  noobj_loss: 0.3361  bbox_loss: 0.0481  cls_loss: 0.1331  \n",
      "<<<iteration:[520/657] - total_loss: 0.8253  obj_loss: 0.0624  noobj_loss: 0.3207  bbox_loss: 0.0912  cls_loss: 0.1465  \n",
      "<<<iteration:[540/657] - total_loss: 0.5950  obj_loss: 0.0780  noobj_loss: 0.3147  bbox_loss: 0.0505  cls_loss: 0.1073  \n",
      "<<<iteration:[560/657] - total_loss: 0.6579  obj_loss: 0.0748  noobj_loss: 0.3289  bbox_loss: 0.0499  cls_loss: 0.1693  \n",
      "<<<iteration:[580/657] - total_loss: 0.8301  obj_loss: 0.0719  noobj_loss: 0.3441  bbox_loss: 0.0901  cls_loss: 0.1354  \n",
      "<<<iteration:[600/657] - total_loss: 0.8916  obj_loss: 0.0689  noobj_loss: 0.3170  bbox_loss: 0.1038  cls_loss: 0.1450  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[620/657] - total_loss: 0.6221  obj_loss: 0.0749  noobj_loss: 0.3103  bbox_loss: 0.0501  cls_loss: 0.1415  \n",
      "<<<iteration:[640/657] - total_loss: 0.6078  obj_loss: 0.0783  noobj_loss: 0.3182  bbox_loss: 0.0477  cls_loss: 0.1319  \n",
      "\n",
      "epoch:21/100 - Train Loss: 0.7142, Val Loss: 0.6298\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.6942  obj_loss: 0.0815  noobj_loss: 0.3455  bbox_loss: 0.0569  cls_loss: 0.1553  \n",
      "<<<iteration:[40/657] - total_loss: 0.6138  obj_loss: 0.0764  noobj_loss: 0.3238  bbox_loss: 0.0493  cls_loss: 0.1290  \n",
      "<<<iteration:[60/657] - total_loss: 0.9370  obj_loss: 0.0766  noobj_loss: 0.3200  bbox_loss: 0.1147  cls_loss: 0.1272  \n",
      "<<<iteration:[80/657] - total_loss: 0.7216  obj_loss: 0.0649  noobj_loss: 0.3647  bbox_loss: 0.0705  cls_loss: 0.1221  \n",
      "<<<iteration:[100/657] - total_loss: 0.6081  obj_loss: 0.0768  noobj_loss: 0.2956  bbox_loss: 0.0492  cls_loss: 0.1375  \n",
      "<<<iteration:[120/657] - total_loss: 0.6100  obj_loss: 0.0721  noobj_loss: 0.3085  bbox_loss: 0.0478  cls_loss: 0.1448  \n",
      "<<<iteration:[140/657] - total_loss: 0.7975  obj_loss: 0.0648  noobj_loss: 0.3383  bbox_loss: 0.0851  cls_loss: 0.1379  \n",
      "<<<iteration:[160/657] - total_loss: 0.5924  obj_loss: 0.0689  noobj_loss: 0.3083  bbox_loss: 0.0491  cls_loss: 0.1241  \n",
      "<<<iteration:[180/657] - total_loss: 0.6151  obj_loss: 0.0835  noobj_loss: 0.2984  bbox_loss: 0.0535  cls_loss: 0.1148  \n",
      "<<<iteration:[200/657] - total_loss: 0.7260  obj_loss: 0.0811  noobj_loss: 0.3223  bbox_loss: 0.0726  cls_loss: 0.1209  \n",
      "<<<iteration:[220/657] - total_loss: 0.6531  obj_loss: 0.0658  noobj_loss: 0.3308  bbox_loss: 0.0575  cls_loss: 0.1343  \n",
      "<<<iteration:[240/657] - total_loss: 0.8359  obj_loss: 0.0754  noobj_loss: 0.3254  bbox_loss: 0.0884  cls_loss: 0.1561  \n",
      "<<<iteration:[260/657] - total_loss: 0.6134  obj_loss: 0.0704  noobj_loss: 0.3072  bbox_loss: 0.0519  cls_loss: 0.1300  \n",
      "<<<iteration:[280/657] - total_loss: 0.5933  obj_loss: 0.0709  noobj_loss: 0.3067  bbox_loss: 0.0497  cls_loss: 0.1204  \n",
      "<<<iteration:[300/657] - total_loss: 0.7154  obj_loss: 0.0758  noobj_loss: 0.3023  bbox_loss: 0.0728  cls_loss: 0.1245  \n",
      "<<<iteration:[320/657] - total_loss: 0.6237  obj_loss: 0.0772  noobj_loss: 0.3374  bbox_loss: 0.0510  cls_loss: 0.1228  \n",
      "<<<iteration:[340/657] - total_loss: 0.6322  obj_loss: 0.0741  noobj_loss: 0.3225  bbox_loss: 0.0540  cls_loss: 0.1268  \n",
      "<<<iteration:[360/657] - total_loss: 0.8140  obj_loss: 0.0639  noobj_loss: 0.3064  bbox_loss: 0.0928  cls_loss: 0.1330  \n",
      "<<<iteration:[380/657] - total_loss: 0.6962  obj_loss: 0.0680  noobj_loss: 0.3198  bbox_loss: 0.0669  cls_loss: 0.1337  \n",
      "<<<iteration:[400/657] - total_loss: 0.5936  obj_loss: 0.0774  noobj_loss: 0.3040  bbox_loss: 0.0467  cls_loss: 0.1309  \n",
      "<<<iteration:[420/657] - total_loss: 0.6526  obj_loss: 0.0804  noobj_loss: 0.3066  bbox_loss: 0.0562  cls_loss: 0.1379  \n",
      "<<<iteration:[440/657] - total_loss: 0.7930  obj_loss: 0.0751  noobj_loss: 0.3081  bbox_loss: 0.0823  cls_loss: 0.1524  \n",
      "<<<iteration:[460/657] - total_loss: 0.6373  obj_loss: 0.0824  noobj_loss: 0.3186  bbox_loss: 0.0571  cls_loss: 0.1100  \n",
      "<<<iteration:[480/657] - total_loss: 0.6754  obj_loss: 0.0786  noobj_loss: 0.3146  bbox_loss: 0.0610  cls_loss: 0.1348  \n",
      "<<<iteration:[500/657] - total_loss: 0.6232  obj_loss: 0.0758  noobj_loss: 0.3156  bbox_loss: 0.0532  cls_loss: 0.1237  \n",
      "<<<iteration:[520/657] - total_loss: 0.6554  obj_loss: 0.0892  noobj_loss: 0.3225  bbox_loss: 0.0558  cls_loss: 0.1258  \n",
      "<<<iteration:[540/657] - total_loss: 0.6123  obj_loss: 0.0712  noobj_loss: 0.3090  bbox_loss: 0.0505  cls_loss: 0.1340  \n",
      "<<<iteration:[560/657] - total_loss: 0.5727  obj_loss: 0.0796  noobj_loss: 0.3156  bbox_loss: 0.0449  cls_loss: 0.1110  \n",
      "<<<iteration:[580/657] - total_loss: 0.6303  obj_loss: 0.0742  noobj_loss: 0.3156  bbox_loss: 0.0544  cls_loss: 0.1263  \n",
      "<<<iteration:[600/657] - total_loss: 0.6299  obj_loss: 0.0924  noobj_loss: 0.2987  bbox_loss: 0.0493  cls_loss: 0.1417  \n",
      "<<<iteration:[620/657] - total_loss: 0.6023  obj_loss: 0.0784  noobj_loss: 0.3193  bbox_loss: 0.0507  cls_loss: 0.1105  \n",
      "<<<iteration:[640/657] - total_loss: 0.6986  obj_loss: 0.0752  noobj_loss: 0.3125  bbox_loss: 0.0655  cls_loss: 0.1395  \n",
      "\n",
      "epoch:22/100 - Train Loss: 0.6744, Val Loss: 0.5948\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.6992  obj_loss: 0.0788  noobj_loss: 0.3044  bbox_loss: 0.0628  cls_loss: 0.1540  \n",
      "<<<iteration:[40/657] - total_loss: 0.5950  obj_loss: 0.0850  noobj_loss: 0.3018  bbox_loss: 0.0503  cls_loss: 0.1075  \n",
      "<<<iteration:[60/657] - total_loss: 0.6575  obj_loss: 0.0681  noobj_loss: 0.3148  bbox_loss: 0.0630  cls_loss: 0.1172  \n",
      "<<<iteration:[80/657] - total_loss: 0.6224  obj_loss: 0.0745  noobj_loss: 0.2948  bbox_loss: 0.0555  cls_loss: 0.1233  \n",
      "<<<iteration:[100/657] - total_loss: 0.5752  obj_loss: 0.0751  noobj_loss: 0.3022  bbox_loss: 0.0474  cls_loss: 0.1122  \n",
      "<<<iteration:[120/657] - total_loss: 0.7240  obj_loss: 0.0704  noobj_loss: 0.3081  bbox_loss: 0.0750  cls_loss: 0.1244  \n",
      "<<<iteration:[140/657] - total_loss: 0.6549  obj_loss: 0.0776  noobj_loss: 0.3437  bbox_loss: 0.0551  cls_loss: 0.1301  \n",
      "<<<iteration:[160/657] - total_loss: 0.6161  obj_loss: 0.0785  noobj_loss: 0.2924  bbox_loss: 0.0499  cls_loss: 0.1419  \n",
      "<<<iteration:[180/657] - total_loss: 0.5752  obj_loss: 0.0896  noobj_loss: 0.3059  bbox_loss: 0.0413  cls_loss: 0.1263  \n",
      "<<<iteration:[200/657] - total_loss: 0.6153  obj_loss: 0.0825  noobj_loss: 0.3267  bbox_loss: 0.0479  cls_loss: 0.1299  \n",
      "<<<iteration:[220/657] - total_loss: 0.5996  obj_loss: 0.0835  noobj_loss: 0.3027  bbox_loss: 0.0441  cls_loss: 0.1441  \n",
      "<<<iteration:[240/657] - total_loss: 0.5692  obj_loss: 0.0820  noobj_loss: 0.2919  bbox_loss: 0.0455  cls_loss: 0.1137  \n",
      "<<<iteration:[260/657] - total_loss: 0.6978  obj_loss: 0.0763  noobj_loss: 0.3070  bbox_loss: 0.0708  cls_loss: 0.1140  \n",
      "<<<iteration:[280/657] - total_loss: 0.6709  obj_loss: 0.0783  noobj_loss: 0.3451  bbox_loss: 0.0582  cls_loss: 0.1290  \n",
      "<<<iteration:[300/657] - total_loss: 0.6012  obj_loss: 0.0784  noobj_loss: 0.3135  bbox_loss: 0.0484  cls_loss: 0.1243  \n",
      "<<<iteration:[320/657] - total_loss: 0.6542  obj_loss: 0.0817  noobj_loss: 0.3202  bbox_loss: 0.0591  cls_loss: 0.1169  \n",
      "<<<iteration:[340/657] - total_loss: 0.6887  obj_loss: 0.0940  noobj_loss: 0.3343  bbox_loss: 0.0557  cls_loss: 0.1489  \n",
      "<<<iteration:[360/657] - total_loss: 0.7008  obj_loss: 0.0787  noobj_loss: 0.3046  bbox_loss: 0.0651  cls_loss: 0.1441  \n",
      "<<<iteration:[380/657] - total_loss: 0.6015  obj_loss: 0.0729  noobj_loss: 0.3087  bbox_loss: 0.0521  cls_loss: 0.1136  \n",
      "<<<iteration:[400/657] - total_loss: 0.8081  obj_loss: 0.0679  noobj_loss: 0.3194  bbox_loss: 0.0909  cls_loss: 0.1258  \n",
      "<<<iteration:[420/657] - total_loss: 0.7412  obj_loss: 0.0777  noobj_loss: 0.2896  bbox_loss: 0.0774  cls_loss: 0.1318  \n",
      "<<<iteration:[440/657] - total_loss: 0.6029  obj_loss: 0.0812  noobj_loss: 0.3000  bbox_loss: 0.0499  cls_loss: 0.1219  \n",
      "<<<iteration:[460/657] - total_loss: 0.5850  obj_loss: 0.0752  noobj_loss: 0.2924  bbox_loss: 0.0461  cls_loss: 0.1332  \n",
      "<<<iteration:[480/657] - total_loss: 0.8310  obj_loss: 0.0796  noobj_loss: 0.2907  bbox_loss: 0.0949  cls_loss: 0.1316  \n",
      "<<<iteration:[500/657] - total_loss: 0.6111  obj_loss: 0.0774  noobj_loss: 0.2989  bbox_loss: 0.0482  cls_loss: 0.1435  \n",
      "<<<iteration:[520/657] - total_loss: 0.7159  obj_loss: 0.0700  noobj_loss: 0.3009  bbox_loss: 0.0737  cls_loss: 0.1269  \n",
      "<<<iteration:[540/657] - total_loss: 0.5966  obj_loss: 0.0780  noobj_loss: 0.3116  bbox_loss: 0.0483  cls_loss: 0.1212  \n",
      "<<<iteration:[560/657] - total_loss: 1.1624  obj_loss: 0.0598  noobj_loss: 0.3014  bbox_loss: 0.1665  cls_loss: 0.1195  \n",
      "<<<iteration:[580/657] - total_loss: 0.6813  obj_loss: 0.0720  noobj_loss: 0.3033  bbox_loss: 0.0654  cls_loss: 0.1304  \n",
      "<<<iteration:[600/657] - total_loss: 0.6033  obj_loss: 0.0868  noobj_loss: 0.2953  bbox_loss: 0.0495  cls_loss: 0.1214  \n",
      "<<<iteration:[620/657] - total_loss: 0.7130  obj_loss: 0.0801  noobj_loss: 0.2940  bbox_loss: 0.0705  cls_loss: 0.1332  \n",
      "<<<iteration:[640/657] - total_loss: 0.5908  obj_loss: 0.0846  noobj_loss: 0.2862  bbox_loss: 0.0472  cls_loss: 0.1272  \n",
      "\n",
      "epoch:23/100 - Train Loss: 0.6673, Val Loss: 0.6504\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.8322  obj_loss: 0.0896  noobj_loss: 0.3212  bbox_loss: 0.0866  cls_loss: 0.1491  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[40/657] - total_loss: 0.6024  obj_loss: 0.0692  noobj_loss: 0.3186  bbox_loss: 0.0468  cls_loss: 0.1398  \n",
      "<<<iteration:[60/657] - total_loss: 0.5630  obj_loss: 0.0803  noobj_loss: 0.2826  bbox_loss: 0.0420  cls_loss: 0.1313  \n",
      "<<<iteration:[80/657] - total_loss: 0.6580  obj_loss: 0.0817  noobj_loss: 0.3268  bbox_loss: 0.0569  cls_loss: 0.1283  \n",
      "<<<iteration:[100/657] - total_loss: 0.6468  obj_loss: 0.0832  noobj_loss: 0.3027  bbox_loss: 0.0582  cls_loss: 0.1211  \n",
      "<<<iteration:[120/657] - total_loss: 0.7837  obj_loss: 0.0744  noobj_loss: 0.3804  bbox_loss: 0.0794  cls_loss: 0.1223  \n",
      "<<<iteration:[140/657] - total_loss: 0.5926  obj_loss: 0.0862  noobj_loss: 0.2996  bbox_loss: 0.0455  cls_loss: 0.1293  \n",
      "<<<iteration:[160/657] - total_loss: 0.5876  obj_loss: 0.0771  noobj_loss: 0.2881  bbox_loss: 0.0479  cls_loss: 0.1270  \n",
      "<<<iteration:[180/657] - total_loss: 0.5656  obj_loss: 0.0863  noobj_loss: 0.2719  bbox_loss: 0.0444  cls_loss: 0.1215  \n",
      "<<<iteration:[200/657] - total_loss: 0.6565  obj_loss: 0.0733  noobj_loss: 0.2908  bbox_loss: 0.0643  cls_loss: 0.1161  \n",
      "<<<iteration:[220/657] - total_loss: 0.7577  obj_loss: 0.0831  noobj_loss: 0.3048  bbox_loss: 0.0805  cls_loss: 0.1198  \n",
      "<<<iteration:[240/657] - total_loss: 0.5774  obj_loss: 0.0757  noobj_loss: 0.2698  bbox_loss: 0.0513  cls_loss: 0.1104  \n",
      "<<<iteration:[260/657] - total_loss: 0.6082  obj_loss: 0.0850  noobj_loss: 0.2753  bbox_loss: 0.0528  cls_loss: 0.1214  \n",
      "<<<iteration:[280/657] - total_loss: 0.6098  obj_loss: 0.0805  noobj_loss: 0.2775  bbox_loss: 0.0542  cls_loss: 0.1198  \n",
      "<<<iteration:[300/657] - total_loss: 0.5987  obj_loss: 0.0809  noobj_loss: 0.2743  bbox_loss: 0.0452  cls_loss: 0.1548  \n",
      "<<<iteration:[320/657] - total_loss: 0.6180  obj_loss: 0.0807  noobj_loss: 0.2905  bbox_loss: 0.0539  cls_loss: 0.1223  \n",
      "<<<iteration:[340/657] - total_loss: 0.6097  obj_loss: 0.0763  noobj_loss: 0.2917  bbox_loss: 0.0555  cls_loss: 0.1103  \n",
      "<<<iteration:[360/657] - total_loss: 0.5902  obj_loss: 0.0817  noobj_loss: 0.2734  bbox_loss: 0.0473  cls_loss: 0.1350  \n",
      "<<<iteration:[380/657] - total_loss: 0.6483  obj_loss: 0.0765  noobj_loss: 0.2967  bbox_loss: 0.0586  cls_loss: 0.1304  \n",
      "<<<iteration:[400/657] - total_loss: 0.5929  obj_loss: 0.0708  noobj_loss: 0.2962  bbox_loss: 0.0545  cls_loss: 0.1014  \n",
      "<<<iteration:[420/657] - total_loss: 0.5627  obj_loss: 0.0773  noobj_loss: 0.2761  bbox_loss: 0.0456  cls_loss: 0.1192  \n",
      "<<<iteration:[440/657] - total_loss: 0.5568  obj_loss: 0.0799  noobj_loss: 0.2699  bbox_loss: 0.0451  cls_loss: 0.1163  \n",
      "<<<iteration:[460/657] - total_loss: 0.5934  obj_loss: 0.0796  noobj_loss: 0.2765  bbox_loss: 0.0482  cls_loss: 0.1347  \n",
      "<<<iteration:[480/657] - total_loss: 0.6059  obj_loss: 0.0761  noobj_loss: 0.2796  bbox_loss: 0.0561  cls_loss: 0.1097  \n",
      "<<<iteration:[500/657] - total_loss: 0.5960  obj_loss: 0.0788  noobj_loss: 0.2863  bbox_loss: 0.0509  cls_loss: 0.1196  \n",
      "<<<iteration:[520/657] - total_loss: 0.5700  obj_loss: 0.0800  noobj_loss: 0.2816  bbox_loss: 0.0458  cls_loss: 0.1201  \n",
      "<<<iteration:[540/657] - total_loss: 0.7009  obj_loss: 0.0770  noobj_loss: 0.2820  bbox_loss: 0.0711  cls_loss: 0.1273  \n",
      "<<<iteration:[560/657] - total_loss: 0.6206  obj_loss: 0.0696  noobj_loss: 0.2789  bbox_loss: 0.0597  cls_loss: 0.1128  \n",
      "<<<iteration:[580/657] - total_loss: 0.5902  obj_loss: 0.0793  noobj_loss: 0.2936  bbox_loss: 0.0475  cls_loss: 0.1266  \n",
      "<<<iteration:[600/657] - total_loss: 0.6236  obj_loss: 0.0747  noobj_loss: 0.2806  bbox_loss: 0.0598  cls_loss: 0.1096  \n",
      "<<<iteration:[620/657] - total_loss: 0.5805  obj_loss: 0.0777  noobj_loss: 0.2743  bbox_loss: 0.0476  cls_loss: 0.1279  \n",
      "<<<iteration:[640/657] - total_loss: 0.8403  obj_loss: 0.0703  noobj_loss: 0.2868  bbox_loss: 0.0993  cls_loss: 0.1300  \n",
      "\n",
      "epoch:24/100 - Train Loss: 0.6293, Val Loss: 0.7827\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.6323  obj_loss: 0.0797  noobj_loss: 0.2821  bbox_loss: 0.0536  cls_loss: 0.1436  \n",
      "<<<iteration:[40/657] - total_loss: 0.5501  obj_loss: 0.0761  noobj_loss: 0.2861  bbox_loss: 0.0456  cls_loss: 0.1031  \n",
      "<<<iteration:[60/657] - total_loss: 0.5056  obj_loss: 0.0879  noobj_loss: 0.2583  bbox_loss: 0.0373  cls_loss: 0.1021  \n",
      "<<<iteration:[80/657] - total_loss: 0.6401  obj_loss: 0.0834  noobj_loss: 0.2817  bbox_loss: 0.0567  cls_loss: 0.1324  \n",
      "<<<iteration:[100/657] - total_loss: 0.5541  obj_loss: 0.0735  noobj_loss: 0.2766  bbox_loss: 0.0458  cls_loss: 0.1131  \n",
      "<<<iteration:[120/657] - total_loss: 0.5847  obj_loss: 0.0718  noobj_loss: 0.2700  bbox_loss: 0.0511  cls_loss: 0.1222  \n",
      "<<<iteration:[140/657] - total_loss: 0.6383  obj_loss: 0.0826  noobj_loss: 0.2923  bbox_loss: 0.0605  cls_loss: 0.1070  \n",
      "<<<iteration:[160/657] - total_loss: 0.6756  obj_loss: 0.0845  noobj_loss: 0.3424  bbox_loss: 0.0610  cls_loss: 0.1148  \n",
      "<<<iteration:[180/657] - total_loss: 0.5545  obj_loss: 0.0897  noobj_loss: 0.2873  bbox_loss: 0.0396  cls_loss: 0.1231  \n",
      "<<<iteration:[200/657] - total_loss: 0.5549  obj_loss: 0.0897  noobj_loss: 0.3109  bbox_loss: 0.0415  cls_loss: 0.1020  \n",
      "<<<iteration:[220/657] - total_loss: 0.6451  obj_loss: 0.0777  noobj_loss: 0.2545  bbox_loss: 0.0595  cls_loss: 0.1425  \n",
      "<<<iteration:[240/657] - total_loss: 0.5491  obj_loss: 0.0810  noobj_loss: 0.2902  bbox_loss: 0.0421  cls_loss: 0.1124  \n",
      "<<<iteration:[260/657] - total_loss: 0.5764  obj_loss: 0.0839  noobj_loss: 0.2733  bbox_loss: 0.0473  cls_loss: 0.1195  \n",
      "<<<iteration:[280/657] - total_loss: 0.5808  obj_loss: 0.0740  noobj_loss: 0.2847  bbox_loss: 0.0504  cls_loss: 0.1125  \n",
      "<<<iteration:[300/657] - total_loss: 0.5804  obj_loss: 0.0719  noobj_loss: 0.2892  bbox_loss: 0.0478  cls_loss: 0.1248  \n",
      "<<<iteration:[320/657] - total_loss: 0.7408  obj_loss: 0.0759  noobj_loss: 0.2916  bbox_loss: 0.0775  cls_loss: 0.1316  \n",
      "<<<iteration:[340/657] - total_loss: 0.5656  obj_loss: 0.0780  noobj_loss: 0.2865  bbox_loss: 0.0446  cls_loss: 0.1213  \n",
      "<<<iteration:[360/657] - total_loss: 0.5681  obj_loss: 0.0932  noobj_loss: 0.2805  bbox_loss: 0.0413  cls_loss: 0.1281  \n",
      "<<<iteration:[380/657] - total_loss: 0.5870  obj_loss: 0.0783  noobj_loss: 0.2678  bbox_loss: 0.0488  cls_loss: 0.1309  \n",
      "<<<iteration:[400/657] - total_loss: 0.6646  obj_loss: 0.0849  noobj_loss: 0.2816  bbox_loss: 0.0638  cls_loss: 0.1199  \n",
      "<<<iteration:[420/657] - total_loss: 0.6111  obj_loss: 0.0887  noobj_loss: 0.2721  bbox_loss: 0.0502  cls_loss: 0.1353  \n",
      "<<<iteration:[440/657] - total_loss: 0.6074  obj_loss: 0.0805  noobj_loss: 0.2910  bbox_loss: 0.0490  cls_loss: 0.1363  \n",
      "<<<iteration:[460/657] - total_loss: 0.5708  obj_loss: 0.0873  noobj_loss: 0.2591  bbox_loss: 0.0452  cls_loss: 0.1280  \n",
      "<<<iteration:[480/657] - total_loss: 0.5452  obj_loss: 0.0820  noobj_loss: 0.2627  bbox_loss: 0.0440  cls_loss: 0.1119  \n",
      "<<<iteration:[500/657] - total_loss: 0.5260  obj_loss: 0.0791  noobj_loss: 0.2654  bbox_loss: 0.0393  cls_loss: 0.1175  \n",
      "<<<iteration:[520/657] - total_loss: 0.8804  obj_loss: 0.0674  noobj_loss: 0.2730  bbox_loss: 0.1082  cls_loss: 0.1356  \n",
      "<<<iteration:[540/657] - total_loss: 0.5444  obj_loss: 0.0801  noobj_loss: 0.2568  bbox_loss: 0.0466  cls_loss: 0.1030  \n",
      "<<<iteration:[560/657] - total_loss: 0.6141  obj_loss: 0.0845  noobj_loss: 0.2588  bbox_loss: 0.0551  cls_loss: 0.1248  \n",
      "<<<iteration:[580/657] - total_loss: 0.7743  obj_loss: 0.0768  noobj_loss: 0.3028  bbox_loss: 0.0816  cls_loss: 0.1381  \n",
      "<<<iteration:[600/657] - total_loss: 0.6908  obj_loss: 0.0885  noobj_loss: 0.2604  bbox_loss: 0.0713  cls_loss: 0.1155  \n",
      "<<<iteration:[620/657] - total_loss: 0.5497  obj_loss: 0.0837  noobj_loss: 0.2691  bbox_loss: 0.0433  cls_loss: 0.1150  \n",
      "<<<iteration:[640/657] - total_loss: 0.5472  obj_loss: 0.0690  noobj_loss: 0.2574  bbox_loss: 0.0457  cls_loss: 0.1208  \n",
      "\n",
      "epoch:25/100 - Train Loss: 0.6105, Val Loss: 0.6181\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.6703  obj_loss: 0.0790  noobj_loss: 0.2963  bbox_loss: 0.0623  cls_loss: 0.1316  \n",
      "<<<iteration:[40/657] - total_loss: 0.5432  obj_loss: 0.0811  noobj_loss: 0.2611  bbox_loss: 0.0421  cls_loss: 0.1210  \n",
      "<<<iteration:[60/657] - total_loss: 0.8428  obj_loss: 0.0826  noobj_loss: 0.2816  bbox_loss: 0.0990  cls_loss: 0.1242  \n",
      "<<<iteration:[80/657] - total_loss: 0.5654  obj_loss: 0.0769  noobj_loss: 0.2813  bbox_loss: 0.0490  cls_loss: 0.1028  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[100/657] - total_loss: 0.6120  obj_loss: 0.0852  noobj_loss: 0.2909  bbox_loss: 0.0495  cls_loss: 0.1338  \n",
      "<<<iteration:[120/657] - total_loss: 0.6228  obj_loss: 0.0750  noobj_loss: 0.2724  bbox_loss: 0.0612  cls_loss: 0.1057  \n",
      "<<<iteration:[140/657] - total_loss: 0.5103  obj_loss: 0.0741  noobj_loss: 0.2514  bbox_loss: 0.0396  cls_loss: 0.1124  \n",
      "<<<iteration:[160/657] - total_loss: 0.5906  obj_loss: 0.0795  noobj_loss: 0.2618  bbox_loss: 0.0515  cls_loss: 0.1227  \n",
      "<<<iteration:[180/657] - total_loss: 0.5498  obj_loss: 0.0935  noobj_loss: 0.2637  bbox_loss: 0.0420  cls_loss: 0.1143  \n",
      "<<<iteration:[200/657] - total_loss: 0.5831  obj_loss: 0.0834  noobj_loss: 0.2724  bbox_loss: 0.0495  cls_loss: 0.1161  \n",
      "<<<iteration:[220/657] - total_loss: 0.6256  obj_loss: 0.0764  noobj_loss: 0.2791  bbox_loss: 0.0590  cls_loss: 0.1146  \n",
      "<<<iteration:[240/657] - total_loss: 0.5732  obj_loss: 0.0887  noobj_loss: 0.2985  bbox_loss: 0.0462  cls_loss: 0.1045  \n",
      "<<<iteration:[260/657] - total_loss: 0.5654  obj_loss: 0.0800  noobj_loss: 0.2639  bbox_loss: 0.0493  cls_loss: 0.1068  \n",
      "<<<iteration:[280/657] - total_loss: 0.5679  obj_loss: 0.0781  noobj_loss: 0.3187  bbox_loss: 0.0413  cls_loss: 0.1239  \n",
      "<<<iteration:[300/657] - total_loss: 0.5717  obj_loss: 0.0782  noobj_loss: 0.2781  bbox_loss: 0.0467  cls_loss: 0.1212  \n",
      "<<<iteration:[320/657] - total_loss: 0.5481  obj_loss: 0.0759  noobj_loss: 0.2567  bbox_loss: 0.0449  cls_loss: 0.1192  \n",
      "<<<iteration:[340/657] - total_loss: 0.5962  obj_loss: 0.0762  noobj_loss: 0.2592  bbox_loss: 0.0525  cls_loss: 0.1276  \n",
      "<<<iteration:[360/657] - total_loss: 0.5784  obj_loss: 0.0830  noobj_loss: 0.2764  bbox_loss: 0.0488  cls_loss: 0.1132  \n",
      "<<<iteration:[380/657] - total_loss: 0.5655  obj_loss: 0.0786  noobj_loss: 0.2646  bbox_loss: 0.0461  cls_loss: 0.1239  \n",
      "<<<iteration:[400/657] - total_loss: 0.6124  obj_loss: 0.0857  noobj_loss: 0.2632  bbox_loss: 0.0598  cls_loss: 0.0961  \n",
      "<<<iteration:[420/657] - total_loss: 0.5916  obj_loss: 0.0772  noobj_loss: 0.2947  bbox_loss: 0.0442  cls_loss: 0.1458  \n",
      "<<<iteration:[440/657] - total_loss: 0.8053  obj_loss: 0.0688  noobj_loss: 0.2756  bbox_loss: 0.0932  cls_loss: 0.1324  \n",
      "<<<iteration:[460/657] - total_loss: 0.5841  obj_loss: 0.0802  noobj_loss: 0.2794  bbox_loss: 0.0481  cls_loss: 0.1237  \n",
      "<<<iteration:[480/657] - total_loss: 0.5982  obj_loss: 0.0891  noobj_loss: 0.2737  bbox_loss: 0.0502  cls_loss: 0.1214  \n",
      "<<<iteration:[500/657] - total_loss: 0.7429  obj_loss: 0.0916  noobj_loss: 0.2661  bbox_loss: 0.0756  cls_loss: 0.1401  \n",
      "<<<iteration:[520/657] - total_loss: 0.6044  obj_loss: 0.0857  noobj_loss: 0.2668  bbox_loss: 0.0524  cls_loss: 0.1232  \n",
      "<<<iteration:[540/657] - total_loss: 0.5438  obj_loss: 0.0798  noobj_loss: 0.2756  bbox_loss: 0.0417  cls_loss: 0.1179  \n",
      "<<<iteration:[560/657] - total_loss: 0.5128  obj_loss: 0.0900  noobj_loss: 0.2681  bbox_loss: 0.0387  cls_loss: 0.0952  \n",
      "<<<iteration:[580/657] - total_loss: 0.5441  obj_loss: 0.0841  noobj_loss: 0.2691  bbox_loss: 0.0360  cls_loss: 0.1453  \n",
      "<<<iteration:[600/657] - total_loss: 0.5405  obj_loss: 0.0921  noobj_loss: 0.2545  bbox_loss: 0.0415  cls_loss: 0.1136  \n",
      "<<<iteration:[620/657] - total_loss: 0.5800  obj_loss: 0.0777  noobj_loss: 0.2715  bbox_loss: 0.0505  cls_loss: 0.1139  \n",
      "<<<iteration:[640/657] - total_loss: 0.5819  obj_loss: 0.0800  noobj_loss: 0.2698  bbox_loss: 0.0465  cls_loss: 0.1344  \n",
      "\n",
      "epoch:26/100 - Train Loss: 0.5961, Val Loss: 0.7127\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5950  obj_loss: 0.0851  noobj_loss: 0.2889  bbox_loss: 0.0484  cls_loss: 0.1236  \n",
      "<<<iteration:[40/657] - total_loss: 0.5432  obj_loss: 0.0766  noobj_loss: 0.2633  bbox_loss: 0.0449  cls_loss: 0.1106  \n",
      "<<<iteration:[60/657] - total_loss: 0.6212  obj_loss: 0.0806  noobj_loss: 0.2582  bbox_loss: 0.0603  cls_loss: 0.1100  \n",
      "<<<iteration:[80/657] - total_loss: 0.5803  obj_loss: 0.0809  noobj_loss: 0.3208  bbox_loss: 0.0428  cls_loss: 0.1249  \n",
      "<<<iteration:[100/657] - total_loss: 0.5972  obj_loss: 0.0779  noobj_loss: 0.2489  bbox_loss: 0.0559  cls_loss: 0.1153  \n",
      "<<<iteration:[120/657] - total_loss: 0.5506  obj_loss: 0.0811  noobj_loss: 0.2730  bbox_loss: 0.0431  cls_loss: 0.1174  \n",
      "<<<iteration:[140/657] - total_loss: 0.6369  obj_loss: 0.0807  noobj_loss: 0.2542  bbox_loss: 0.0543  cls_loss: 0.1574  \n",
      "<<<iteration:[160/657] - total_loss: 0.5559  obj_loss: 0.0894  noobj_loss: 0.2464  bbox_loss: 0.0452  cls_loss: 0.1174  \n",
      "<<<iteration:[180/657] - total_loss: 0.4959  obj_loss: 0.0963  noobj_loss: 0.2598  bbox_loss: 0.0330  cls_loss: 0.1048  \n",
      "<<<iteration:[200/657] - total_loss: 0.5591  obj_loss: 0.0875  noobj_loss: 0.2875  bbox_loss: 0.0415  cls_loss: 0.1204  \n",
      "<<<iteration:[220/657] - total_loss: 0.5883  obj_loss: 0.0784  noobj_loss: 0.2550  bbox_loss: 0.0548  cls_loss: 0.1085  \n",
      "<<<iteration:[240/657] - total_loss: 0.6100  obj_loss: 0.0725  noobj_loss: 0.2503  bbox_loss: 0.0534  cls_loss: 0.1451  \n",
      "<<<iteration:[260/657] - total_loss: 0.5903  obj_loss: 0.0802  noobj_loss: 0.2703  bbox_loss: 0.0530  cls_loss: 0.1099  \n",
      "<<<iteration:[280/657] - total_loss: 0.5558  obj_loss: 0.0796  noobj_loss: 0.2650  bbox_loss: 0.0471  cls_loss: 0.1080  \n",
      "<<<iteration:[300/657] - total_loss: 0.5711  obj_loss: 0.0789  noobj_loss: 0.2722  bbox_loss: 0.0427  cls_loss: 0.1427  \n",
      "<<<iteration:[320/657] - total_loss: 0.5978  obj_loss: 0.0801  noobj_loss: 0.2677  bbox_loss: 0.0543  cls_loss: 0.1125  \n",
      "<<<iteration:[340/657] - total_loss: 0.5512  obj_loss: 0.0846  noobj_loss: 0.2429  bbox_loss: 0.0424  cls_loss: 0.1331  \n",
      "<<<iteration:[360/657] - total_loss: 0.5541  obj_loss: 0.0860  noobj_loss: 0.2685  bbox_loss: 0.0470  cls_loss: 0.0990  \n",
      "<<<iteration:[380/657] - total_loss: 0.4905  obj_loss: 0.0863  noobj_loss: 0.2577  bbox_loss: 0.0340  cls_loss: 0.1055  \n",
      "<<<iteration:[400/657] - total_loss: 0.7253  obj_loss: 0.0820  noobj_loss: 0.2466  bbox_loss: 0.0826  cls_loss: 0.1072  \n",
      "<<<iteration:[420/657] - total_loss: 0.5733  obj_loss: 0.0754  noobj_loss: 0.2533  bbox_loss: 0.0525  cls_loss: 0.1085  \n",
      "<<<iteration:[440/657] - total_loss: 0.6342  obj_loss: 0.0829  noobj_loss: 0.2658  bbox_loss: 0.0586  cls_loss: 0.1251  \n",
      "<<<iteration:[460/657] - total_loss: 0.5777  obj_loss: 0.0807  noobj_loss: 0.2539  bbox_loss: 0.0477  cls_loss: 0.1316  \n",
      "<<<iteration:[480/657] - total_loss: 0.5119  obj_loss: 0.0833  noobj_loss: 0.2547  bbox_loss: 0.0409  cls_loss: 0.0968  \n",
      "<<<iteration:[500/657] - total_loss: 0.5525  obj_loss: 0.0876  noobj_loss: 0.2390  bbox_loss: 0.0435  cls_loss: 0.1279  \n",
      "<<<iteration:[520/657] - total_loss: 0.6371  obj_loss: 0.0760  noobj_loss: 0.2633  bbox_loss: 0.0638  cls_loss: 0.1101  \n",
      "<<<iteration:[540/657] - total_loss: 0.6029  obj_loss: 0.0874  noobj_loss: 0.2484  bbox_loss: 0.0537  cls_loss: 0.1228  \n",
      "<<<iteration:[560/657] - total_loss: 0.5120  obj_loss: 0.0823  noobj_loss: 0.2680  bbox_loss: 0.0369  cls_loss: 0.1110  \n",
      "<<<iteration:[580/657] - total_loss: 0.6390  obj_loss: 0.0800  noobj_loss: 0.2534  bbox_loss: 0.0609  cls_loss: 0.1278  \n",
      "<<<iteration:[600/657] - total_loss: 0.5683  obj_loss: 0.0792  noobj_loss: 0.2841  bbox_loss: 0.0492  cls_loss: 0.1011  \n",
      "<<<iteration:[620/657] - total_loss: 0.5027  obj_loss: 0.0874  noobj_loss: 0.2404  bbox_loss: 0.0382  cls_loss: 0.1041  \n",
      "<<<iteration:[640/657] - total_loss: 0.5653  obj_loss: 0.0953  noobj_loss: 0.2939  bbox_loss: 0.0407  cls_loss: 0.1196  \n",
      "\n",
      "epoch:27/100 - Train Loss: 0.5759, Val Loss: 0.5568\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.7475  obj_loss: 0.0894  noobj_loss: 0.3187  bbox_loss: 0.0752  cls_loss: 0.1227  \n",
      "<<<iteration:[40/657] - total_loss: 0.5431  obj_loss: 0.0814  noobj_loss: 0.2432  bbox_loss: 0.0429  cls_loss: 0.1257  \n",
      "<<<iteration:[60/657] - total_loss: 0.6473  obj_loss: 0.0859  noobj_loss: 0.2614  bbox_loss: 0.0636  cls_loss: 0.1125  \n",
      "<<<iteration:[80/657] - total_loss: 0.5290  obj_loss: 0.0711  noobj_loss: 0.2405  bbox_loss: 0.0427  cls_loss: 0.1244  \n",
      "<<<iteration:[100/657] - total_loss: 0.5314  obj_loss: 0.0710  noobj_loss: 0.2401  bbox_loss: 0.0474  cls_loss: 0.1033  \n",
      "<<<iteration:[120/657] - total_loss: 0.5412  obj_loss: 0.0694  noobj_loss: 0.2519  bbox_loss: 0.0476  cls_loss: 0.1077  \n",
      "<<<iteration:[140/657] - total_loss: 0.5127  obj_loss: 0.0844  noobj_loss: 0.2409  bbox_loss: 0.0386  cls_loss: 0.1147  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[160/657] - total_loss: 0.5568  obj_loss: 0.0783  noobj_loss: 0.2346  bbox_loss: 0.0486  cls_loss: 0.1183  \n",
      "<<<iteration:[180/657] - total_loss: 0.5888  obj_loss: 0.0844  noobj_loss: 0.2688  bbox_loss: 0.0533  cls_loss: 0.1038  \n",
      "<<<iteration:[200/657] - total_loss: 0.5456  obj_loss: 0.0935  noobj_loss: 0.2557  bbox_loss: 0.0409  cls_loss: 0.1199  \n",
      "<<<iteration:[220/657] - total_loss: 0.5246  obj_loss: 0.0834  noobj_loss: 0.2728  bbox_loss: 0.0406  cls_loss: 0.1017  \n",
      "<<<iteration:[240/657] - total_loss: 0.5211  obj_loss: 0.0917  noobj_loss: 0.2459  bbox_loss: 0.0414  cls_loss: 0.0994  \n",
      "<<<iteration:[260/657] - total_loss: 0.6024  obj_loss: 0.0741  noobj_loss: 0.2653  bbox_loss: 0.0555  cls_loss: 0.1181  \n",
      "<<<iteration:[280/657] - total_loss: 0.5543  obj_loss: 0.0782  noobj_loss: 0.2865  bbox_loss: 0.0403  cls_loss: 0.1312  \n",
      "<<<iteration:[300/657] - total_loss: 0.6464  obj_loss: 0.0790  noobj_loss: 0.3035  bbox_loss: 0.0614  cls_loss: 0.1086  \n",
      "<<<iteration:[320/657] - total_loss: 0.5695  obj_loss: 0.0903  noobj_loss: 0.2384  bbox_loss: 0.0478  cls_loss: 0.1211  \n",
      "<<<iteration:[340/657] - total_loss: 0.6016  obj_loss: 0.0883  noobj_loss: 0.2557  bbox_loss: 0.0531  cls_loss: 0.1198  \n",
      "<<<iteration:[360/657] - total_loss: 0.5144  obj_loss: 0.0870  noobj_loss: 0.2432  bbox_loss: 0.0378  cls_loss: 0.1168  \n",
      "<<<iteration:[380/657] - total_loss: 0.5304  obj_loss: 0.0808  noobj_loss: 0.2431  bbox_loss: 0.0414  cls_loss: 0.1210  \n",
      "<<<iteration:[400/657] - total_loss: 0.5624  obj_loss: 0.0945  noobj_loss: 0.2539  bbox_loss: 0.0427  cls_loss: 0.1272  \n",
      "<<<iteration:[420/657] - total_loss: 0.5242  obj_loss: 0.0738  noobj_loss: 0.2468  bbox_loss: 0.0447  cls_loss: 0.1033  \n",
      "<<<iteration:[440/657] - total_loss: 0.5913  obj_loss: 0.0874  noobj_loss: 0.2387  bbox_loss: 0.0507  cls_loss: 0.1311  \n",
      "<<<iteration:[460/657] - total_loss: 0.5708  obj_loss: 0.0859  noobj_loss: 0.2665  bbox_loss: 0.0490  cls_loss: 0.1066  \n",
      "<<<iteration:[480/657] - total_loss: 0.6217  obj_loss: 0.0978  noobj_loss: 0.2498  bbox_loss: 0.0556  cls_loss: 0.1212  \n",
      "<<<iteration:[500/657] - total_loss: 0.6089  obj_loss: 0.0762  noobj_loss: 0.2561  bbox_loss: 0.0599  cls_loss: 0.1054  \n",
      "<<<iteration:[520/657] - total_loss: 0.5330  obj_loss: 0.0854  noobj_loss: 0.2413  bbox_loss: 0.0421  cls_loss: 0.1164  \n",
      "<<<iteration:[540/657] - total_loss: 0.5870  obj_loss: 0.0775  noobj_loss: 0.2628  bbox_loss: 0.0557  cls_loss: 0.0994  \n",
      "<<<iteration:[560/657] - total_loss: 0.5345  obj_loss: 0.0857  noobj_loss: 0.2323  bbox_loss: 0.0402  cls_loss: 0.1317  \n",
      "<<<iteration:[580/657] - total_loss: 0.5035  obj_loss: 0.0753  noobj_loss: 0.2359  bbox_loss: 0.0401  cls_loss: 0.1096  \n",
      "<<<iteration:[600/657] - total_loss: 0.5421  obj_loss: 0.0754  noobj_loss: 0.2307  bbox_loss: 0.0440  cls_loss: 0.1313  \n",
      "<<<iteration:[620/657] - total_loss: 0.5145  obj_loss: 0.0892  noobj_loss: 0.2453  bbox_loss: 0.0366  cls_loss: 0.1197  \n",
      "<<<iteration:[640/657] - total_loss: 0.5847  obj_loss: 0.0806  noobj_loss: 0.2696  bbox_loss: 0.0493  cls_loss: 0.1228  \n",
      "\n",
      "epoch:28/100 - Train Loss: 0.5636, Val Loss: 0.5367\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5111  obj_loss: 0.0894  noobj_loss: 0.2566  bbox_loss: 0.0358  cls_loss: 0.1144  \n",
      "<<<iteration:[40/657] - total_loss: 0.6110  obj_loss: 0.0756  noobj_loss: 0.2665  bbox_loss: 0.0539  cls_loss: 0.1328  \n",
      "<<<iteration:[60/657] - total_loss: 0.5398  obj_loss: 0.0893  noobj_loss: 0.2477  bbox_loss: 0.0413  cls_loss: 0.1199  \n",
      "<<<iteration:[80/657] - total_loss: 0.5918  obj_loss: 0.0851  noobj_loss: 0.2559  bbox_loss: 0.0532  cls_loss: 0.1126  \n",
      "<<<iteration:[100/657] - total_loss: 0.5961  obj_loss: 0.0856  noobj_loss: 0.2514  bbox_loss: 0.0508  cls_loss: 0.1305  \n",
      "<<<iteration:[120/657] - total_loss: 0.5618  obj_loss: 0.0804  noobj_loss: 0.2484  bbox_loss: 0.0491  cls_loss: 0.1115  \n",
      "<<<iteration:[140/657] - total_loss: 0.5198  obj_loss: 0.0791  noobj_loss: 0.2471  bbox_loss: 0.0388  cls_loss: 0.1234  \n",
      "<<<iteration:[160/657] - total_loss: 0.5569  obj_loss: 0.0908  noobj_loss: 0.2756  bbox_loss: 0.0428  cls_loss: 0.1142  \n",
      "<<<iteration:[180/657] - total_loss: 0.5481  obj_loss: 0.0833  noobj_loss: 0.2436  bbox_loss: 0.0472  cls_loss: 0.1072  \n",
      "<<<iteration:[200/657] - total_loss: 0.5757  obj_loss: 0.0970  noobj_loss: 0.2448  bbox_loss: 0.0414  cls_loss: 0.1493  \n",
      "<<<iteration:[220/657] - total_loss: 0.5449  obj_loss: 0.0749  noobj_loss: 0.2444  bbox_loss: 0.0425  cls_loss: 0.1351  \n",
      "<<<iteration:[240/657] - total_loss: 0.5153  obj_loss: 0.0823  noobj_loss: 0.2426  bbox_loss: 0.0379  cls_loss: 0.1221  \n",
      "<<<iteration:[260/657] - total_loss: 0.7262  obj_loss: 0.0912  noobj_loss: 0.2553  bbox_loss: 0.0745  cls_loss: 0.1346  \n",
      "<<<iteration:[280/657] - total_loss: 0.5179  obj_loss: 0.0844  noobj_loss: 0.2446  bbox_loss: 0.0399  cls_loss: 0.1117  \n",
      "<<<iteration:[300/657] - total_loss: 0.6503  obj_loss: 0.0865  noobj_loss: 0.2459  bbox_loss: 0.0660  cls_loss: 0.1109  \n",
      "<<<iteration:[320/657] - total_loss: 0.5196  obj_loss: 0.0910  noobj_loss: 0.2413  bbox_loss: 0.0389  cls_loss: 0.1136  \n",
      "<<<iteration:[340/657] - total_loss: 0.4892  obj_loss: 0.0892  noobj_loss: 0.2446  bbox_loss: 0.0325  cls_loss: 0.1149  \n",
      "<<<iteration:[360/657] - total_loss: 0.5391  obj_loss: 0.0896  noobj_loss: 0.2470  bbox_loss: 0.0401  cls_loss: 0.1257  \n",
      "<<<iteration:[380/657] - total_loss: 0.5085  obj_loss: 0.0913  noobj_loss: 0.2433  bbox_loss: 0.0354  cls_loss: 0.1183  \n",
      "<<<iteration:[400/657] - total_loss: 0.5312  obj_loss: 0.0850  noobj_loss: 0.2305  bbox_loss: 0.0428  cls_loss: 0.1169  \n",
      "<<<iteration:[420/657] - total_loss: 0.5449  obj_loss: 0.0773  noobj_loss: 0.2480  bbox_loss: 0.0448  cls_loss: 0.1194  \n",
      "<<<iteration:[440/657] - total_loss: 0.5120  obj_loss: 0.0813  noobj_loss: 0.2529  bbox_loss: 0.0418  cls_loss: 0.0954  \n",
      "<<<iteration:[460/657] - total_loss: 0.4905  obj_loss: 0.0847  noobj_loss: 0.2323  bbox_loss: 0.0375  cls_loss: 0.1021  \n",
      "<<<iteration:[480/657] - total_loss: 0.5154  obj_loss: 0.0831  noobj_loss: 0.2476  bbox_loss: 0.0418  cls_loss: 0.0996  \n",
      "<<<iteration:[500/657] - total_loss: 0.5247  obj_loss: 0.0867  noobj_loss: 0.2414  bbox_loss: 0.0420  cls_loss: 0.1074  \n",
      "<<<iteration:[520/657] - total_loss: 0.5900  obj_loss: 0.0873  noobj_loss: 0.2454  bbox_loss: 0.0577  cls_loss: 0.0917  \n",
      "<<<iteration:[540/657] - total_loss: 0.5075  obj_loss: 0.0812  noobj_loss: 0.2413  bbox_loss: 0.0401  cls_loss: 0.1052  \n",
      "<<<iteration:[560/657] - total_loss: 0.5343  obj_loss: 0.0957  noobj_loss: 0.2423  bbox_loss: 0.0415  cls_loss: 0.1099  \n",
      "<<<iteration:[580/657] - total_loss: 0.5612  obj_loss: 0.0854  noobj_loss: 0.2495  bbox_loss: 0.0469  cls_loss: 0.1164  \n",
      "<<<iteration:[600/657] - total_loss: 0.6010  obj_loss: 0.0763  noobj_loss: 0.2469  bbox_loss: 0.0569  cls_loss: 0.1165  \n",
      "<<<iteration:[620/657] - total_loss: 0.4917  obj_loss: 0.0837  noobj_loss: 0.2283  bbox_loss: 0.0338  cls_loss: 0.1248  \n",
      "<<<iteration:[640/657] - total_loss: 0.5348  obj_loss: 0.0884  noobj_loss: 0.2305  bbox_loss: 0.0399  cls_loss: 0.1319  \n",
      "\n",
      "epoch:29/100 - Train Loss: 0.5465, Val Loss: 0.5437\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5569  obj_loss: 0.0855  noobj_loss: 0.2602  bbox_loss: 0.0445  cls_loss: 0.1190  \n",
      "<<<iteration:[40/657] - total_loss: 0.5568  obj_loss: 0.0781  noobj_loss: 0.2479  bbox_loss: 0.0515  cls_loss: 0.0973  \n",
      "<<<iteration:[60/657] - total_loss: 0.5356  obj_loss: 0.1003  noobj_loss: 0.2545  bbox_loss: 0.0375  cls_loss: 0.1206  \n",
      "<<<iteration:[80/657] - total_loss: 0.5404  obj_loss: 0.0753  noobj_loss: 0.2399  bbox_loss: 0.0438  cls_loss: 0.1259  \n",
      "<<<iteration:[100/657] - total_loss: 0.5294  obj_loss: 0.0825  noobj_loss: 0.2484  bbox_loss: 0.0375  cls_loss: 0.1354  \n",
      "<<<iteration:[120/657] - total_loss: 0.5007  obj_loss: 0.0843  noobj_loss: 0.2474  bbox_loss: 0.0380  cls_loss: 0.1026  \n",
      "<<<iteration:[140/657] - total_loss: 0.7306  obj_loss: 0.0729  noobj_loss: 0.2383  bbox_loss: 0.0852  cls_loss: 0.1129  \n",
      "<<<iteration:[160/657] - total_loss: 0.5496  obj_loss: 0.0767  noobj_loss: 0.2423  bbox_loss: 0.0464  cls_loss: 0.1199  \n",
      "<<<iteration:[180/657] - total_loss: 0.5809  obj_loss: 0.0907  noobj_loss: 0.2559  bbox_loss: 0.0479  cls_loss: 0.1229  \n",
      "<<<iteration:[200/657] - total_loss: 0.5741  obj_loss: 0.0826  noobj_loss: 0.2475  bbox_loss: 0.0501  cls_loss: 0.1175  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[220/657] - total_loss: 0.5151  obj_loss: 0.0858  noobj_loss: 0.2375  bbox_loss: 0.0400  cls_loss: 0.1106  \n",
      "<<<iteration:[240/657] - total_loss: 0.7508  obj_loss: 0.0900  noobj_loss: 0.2429  bbox_loss: 0.0840  cls_loss: 0.1193  \n",
      "<<<iteration:[260/657] - total_loss: 0.5095  obj_loss: 0.0806  noobj_loss: 0.2542  bbox_loss: 0.0376  cls_loss: 0.1136  \n",
      "<<<iteration:[280/657] - total_loss: 0.5266  obj_loss: 0.0983  noobj_loss: 0.2302  bbox_loss: 0.0420  cls_loss: 0.1031  \n",
      "<<<iteration:[300/657] - total_loss: 0.5007  obj_loss: 0.0892  noobj_loss: 0.2389  bbox_loss: 0.0390  cls_loss: 0.0973  \n",
      "<<<iteration:[320/657] - total_loss: 0.5152  obj_loss: 0.0970  noobj_loss: 0.2245  bbox_loss: 0.0384  cls_loss: 0.1141  \n",
      "<<<iteration:[340/657] - total_loss: 0.5502  obj_loss: 0.0873  noobj_loss: 0.2422  bbox_loss: 0.0451  cls_loss: 0.1162  \n",
      "<<<iteration:[360/657] - total_loss: 0.5131  obj_loss: 0.0884  noobj_loss: 0.2309  bbox_loss: 0.0411  cls_loss: 0.1038  \n",
      "<<<iteration:[380/657] - total_loss: 0.4873  obj_loss: 0.0866  noobj_loss: 0.2291  bbox_loss: 0.0364  cls_loss: 0.1041  \n",
      "<<<iteration:[400/657] - total_loss: 0.4893  obj_loss: 0.0835  noobj_loss: 0.2214  bbox_loss: 0.0383  cls_loss: 0.1036  \n",
      "<<<iteration:[420/657] - total_loss: 0.4983  obj_loss: 0.0850  noobj_loss: 0.2288  bbox_loss: 0.0386  cls_loss: 0.1058  \n",
      "<<<iteration:[440/657] - total_loss: 0.6298  obj_loss: 0.0889  noobj_loss: 0.2378  bbox_loss: 0.0605  cls_loss: 0.1194  \n",
      "<<<iteration:[460/657] - total_loss: 0.5218  obj_loss: 0.0841  noobj_loss: 0.2351  bbox_loss: 0.0392  cls_loss: 0.1242  \n",
      "<<<iteration:[480/657] - total_loss: 0.4891  obj_loss: 0.0863  noobj_loss: 0.2379  bbox_loss: 0.0375  cls_loss: 0.0965  \n",
      "<<<iteration:[500/657] - total_loss: 0.5216  obj_loss: 0.0756  noobj_loss: 0.2631  bbox_loss: 0.0412  cls_loss: 0.1085  \n",
      "<<<iteration:[520/657] - total_loss: 0.5867  obj_loss: 0.0899  noobj_loss: 0.2422  bbox_loss: 0.0520  cls_loss: 0.1154  \n",
      "<<<iteration:[540/657] - total_loss: 0.4880  obj_loss: 0.1002  noobj_loss: 0.2199  bbox_loss: 0.0343  cls_loss: 0.1063  \n",
      "<<<iteration:[560/657] - total_loss: 0.4922  obj_loss: 0.0715  noobj_loss: 0.2373  bbox_loss: 0.0384  cls_loss: 0.1098  \n",
      "<<<iteration:[580/657] - total_loss: 0.4781  obj_loss: 0.0779  noobj_loss: 0.2298  bbox_loss: 0.0378  cls_loss: 0.0965  \n",
      "<<<iteration:[600/657] - total_loss: 0.5150  obj_loss: 0.0907  noobj_loss: 0.2313  bbox_loss: 0.0400  cls_loss: 0.1088  \n",
      "<<<iteration:[620/657] - total_loss: 0.5517  obj_loss: 0.0923  noobj_loss: 0.2273  bbox_loss: 0.0416  cls_loss: 0.1378  \n",
      "<<<iteration:[640/657] - total_loss: 0.5026  obj_loss: 0.0913  noobj_loss: 0.2193  bbox_loss: 0.0335  cls_loss: 0.1343  \n",
      "\n",
      "epoch:30/100 - Train Loss: 0.5392, Val Loss: 0.5728\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5408  obj_loss: 0.0936  noobj_loss: 0.2433  bbox_loss: 0.0428  cls_loss: 0.1114  \n",
      "<<<iteration:[40/657] - total_loss: 0.5053  obj_loss: 0.0786  noobj_loss: 0.2572  bbox_loss: 0.0382  cls_loss: 0.1069  \n",
      "<<<iteration:[60/657] - total_loss: 0.5215  obj_loss: 0.0968  noobj_loss: 0.2315  bbox_loss: 0.0398  cls_loss: 0.1098  \n",
      "<<<iteration:[80/657] - total_loss: 0.4991  obj_loss: 0.0889  noobj_loss: 0.2274  bbox_loss: 0.0381  cls_loss: 0.1062  \n",
      "<<<iteration:[100/657] - total_loss: 0.5266  obj_loss: 0.0860  noobj_loss: 0.2393  bbox_loss: 0.0416  cls_loss: 0.1130  \n",
      "<<<iteration:[120/657] - total_loss: 0.5741  obj_loss: 0.0784  noobj_loss: 0.2372  bbox_loss: 0.0519  cls_loss: 0.1178  \n",
      "<<<iteration:[140/657] - total_loss: 0.5076  obj_loss: 0.0854  noobj_loss: 0.2328  bbox_loss: 0.0362  cls_loss: 0.1250  \n",
      "<<<iteration:[160/657] - total_loss: 0.6076  obj_loss: 0.0876  noobj_loss: 0.2247  bbox_loss: 0.0566  cls_loss: 0.1246  \n",
      "<<<iteration:[180/657] - total_loss: 0.5194  obj_loss: 0.0871  noobj_loss: 0.2312  bbox_loss: 0.0411  cls_loss: 0.1111  \n",
      "<<<iteration:[200/657] - total_loss: 0.5908  obj_loss: 0.0882  noobj_loss: 0.2271  bbox_loss: 0.0577  cls_loss: 0.1006  \n",
      "<<<iteration:[220/657] - total_loss: 0.5397  obj_loss: 0.0946  noobj_loss: 0.2469  bbox_loss: 0.0432  cls_loss: 0.1059  \n",
      "<<<iteration:[240/657] - total_loss: 0.4858  obj_loss: 0.1005  noobj_loss: 0.2257  bbox_loss: 0.0333  cls_loss: 0.1057  \n",
      "<<<iteration:[260/657] - total_loss: 0.5158  obj_loss: 0.0864  noobj_loss: 0.2253  bbox_loss: 0.0413  cls_loss: 0.1100  \n",
      "<<<iteration:[280/657] - total_loss: 0.5165  obj_loss: 0.0902  noobj_loss: 0.2200  bbox_loss: 0.0411  cls_loss: 0.1107  \n",
      "<<<iteration:[300/657] - total_loss: 0.4948  obj_loss: 0.0851  noobj_loss: 0.2309  bbox_loss: 0.0364  cls_loss: 0.1125  \n",
      "<<<iteration:[320/657] - total_loss: 0.5043  obj_loss: 0.0966  noobj_loss: 0.2280  bbox_loss: 0.0350  cls_loss: 0.1189  \n",
      "<<<iteration:[340/657] - total_loss: 0.5133  obj_loss: 0.0826  noobj_loss: 0.2505  bbox_loss: 0.0409  cls_loss: 0.1011  \n",
      "<<<iteration:[360/657] - total_loss: 0.6481  obj_loss: 0.0774  noobj_loss: 0.2301  bbox_loss: 0.0691  cls_loss: 0.1104  \n",
      "<<<iteration:[380/657] - total_loss: 0.5057  obj_loss: 0.0866  noobj_loss: 0.2259  bbox_loss: 0.0418  cls_loss: 0.0970  \n",
      "<<<iteration:[400/657] - total_loss: 0.4980  obj_loss: 0.0907  noobj_loss: 0.2197  bbox_loss: 0.0352  cls_loss: 0.1216  \n",
      "<<<iteration:[420/657] - total_loss: 0.4841  obj_loss: 0.0907  noobj_loss: 0.2239  bbox_loss: 0.0335  cls_loss: 0.1140  \n",
      "<<<iteration:[440/657] - total_loss: 0.4889  obj_loss: 0.0942  noobj_loss: 0.2300  bbox_loss: 0.0361  cls_loss: 0.0994  \n",
      "<<<iteration:[460/657] - total_loss: 0.5027  obj_loss: 0.0871  noobj_loss: 0.2304  bbox_loss: 0.0412  cls_loss: 0.0945  \n",
      "<<<iteration:[480/657] - total_loss: 0.5030  obj_loss: 0.0891  noobj_loss: 0.2347  bbox_loss: 0.0395  cls_loss: 0.0990  \n",
      "<<<iteration:[500/657] - total_loss: 0.4877  obj_loss: 0.0853  noobj_loss: 0.2265  bbox_loss: 0.0363  cls_loss: 0.1076  \n",
      "<<<iteration:[520/657] - total_loss: 0.4921  obj_loss: 0.0832  noobj_loss: 0.2267  bbox_loss: 0.0394  cls_loss: 0.0985  \n",
      "<<<iteration:[540/657] - total_loss: 0.5603  obj_loss: 0.0863  noobj_loss: 0.2384  bbox_loss: 0.0460  cls_loss: 0.1246  \n",
      "<<<iteration:[560/657] - total_loss: 0.5288  obj_loss: 0.0897  noobj_loss: 0.2268  bbox_loss: 0.0440  cls_loss: 0.1059  \n",
      "<<<iteration:[580/657] - total_loss: 0.5609  obj_loss: 0.0917  noobj_loss: 0.2435  bbox_loss: 0.0461  cls_loss: 0.1170  \n",
      "<<<iteration:[600/657] - total_loss: 0.5164  obj_loss: 0.0857  noobj_loss: 0.2365  bbox_loss: 0.0422  cls_loss: 0.1016  \n",
      "<<<iteration:[620/657] - total_loss: 0.4982  obj_loss: 0.0845  noobj_loss: 0.2386  bbox_loss: 0.0340  cls_loss: 0.1241  \n",
      "<<<iteration:[640/657] - total_loss: 0.4930  obj_loss: 0.0924  noobj_loss: 0.2241  bbox_loss: 0.0360  cls_loss: 0.1084  \n",
      "\n",
      "epoch:31/100 - Train Loss: 0.5216, Val Loss: 0.4966\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5133  obj_loss: 0.0950  noobj_loss: 0.2255  bbox_loss: 0.0391  cls_loss: 0.1098  \n",
      "<<<iteration:[40/657] - total_loss: 0.6995  obj_loss: 0.0756  noobj_loss: 0.2322  bbox_loss: 0.0803  cls_loss: 0.1063  \n",
      "<<<iteration:[60/657] - total_loss: 0.5282  obj_loss: 0.0915  noobj_loss: 0.2316  bbox_loss: 0.0430  cls_loss: 0.1061  \n",
      "<<<iteration:[80/657] - total_loss: 0.4877  obj_loss: 0.0925  noobj_loss: 0.2305  bbox_loss: 0.0351  cls_loss: 0.1045  \n",
      "<<<iteration:[100/657] - total_loss: 0.5200  obj_loss: 0.0843  noobj_loss: 0.2247  bbox_loss: 0.0405  cls_loss: 0.1210  \n",
      "<<<iteration:[120/657] - total_loss: 0.5554  obj_loss: 0.0840  noobj_loss: 0.2353  bbox_loss: 0.0459  cls_loss: 0.1242  \n",
      "<<<iteration:[140/657] - total_loss: 0.4938  obj_loss: 0.0875  noobj_loss: 0.2039  bbox_loss: 0.0381  cls_loss: 0.1138  \n",
      "<<<iteration:[160/657] - total_loss: 0.5060  obj_loss: 0.0848  noobj_loss: 0.2418  bbox_loss: 0.0384  cls_loss: 0.1083  \n",
      "<<<iteration:[180/657] - total_loss: 0.4960  obj_loss: 0.0839  noobj_loss: 0.2135  bbox_loss: 0.0352  cls_loss: 0.1292  \n",
      "<<<iteration:[200/657] - total_loss: 0.6060  obj_loss: 0.0813  noobj_loss: 0.2339  bbox_loss: 0.0584  cls_loss: 0.1157  \n",
      "<<<iteration:[220/657] - total_loss: 0.5553  obj_loss: 0.0916  noobj_loss: 0.2212  bbox_loss: 0.0492  cls_loss: 0.1070  \n",
      "<<<iteration:[240/657] - total_loss: 0.5204  obj_loss: 0.0903  noobj_loss: 0.2205  bbox_loss: 0.0403  cls_loss: 0.1183  \n",
      "<<<iteration:[260/657] - total_loss: 0.5288  obj_loss: 0.0855  noobj_loss: 0.2445  bbox_loss: 0.0434  cls_loss: 0.1041  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[280/657] - total_loss: 0.4844  obj_loss: 0.0898  noobj_loss: 0.2233  bbox_loss: 0.0340  cls_loss: 0.1129  \n",
      "<<<iteration:[300/657] - total_loss: 0.5423  obj_loss: 0.0868  noobj_loss: 0.2104  bbox_loss: 0.0456  cls_loss: 0.1221  \n",
      "<<<iteration:[320/657] - total_loss: 0.4761  obj_loss: 0.0872  noobj_loss: 0.2296  bbox_loss: 0.0353  cls_loss: 0.0975  \n",
      "<<<iteration:[340/657] - total_loss: 0.4880  obj_loss: 0.0908  noobj_loss: 0.2190  bbox_loss: 0.0348  cls_loss: 0.1137  \n",
      "<<<iteration:[360/657] - total_loss: 0.5420  obj_loss: 0.0831  noobj_loss: 0.2446  bbox_loss: 0.0472  cls_loss: 0.1005  \n",
      "<<<iteration:[380/657] - total_loss: 0.4768  obj_loss: 0.0873  noobj_loss: 0.2172  bbox_loss: 0.0336  cls_loss: 0.1128  \n",
      "<<<iteration:[400/657] - total_loss: 0.6106  obj_loss: 0.0914  noobj_loss: 0.2423  bbox_loss: 0.0557  cls_loss: 0.1193  \n",
      "<<<iteration:[420/657] - total_loss: 0.5041  obj_loss: 0.0886  noobj_loss: 0.2353  bbox_loss: 0.0398  cls_loss: 0.0987  \n",
      "<<<iteration:[440/657] - total_loss: 0.4700  obj_loss: 0.0943  noobj_loss: 0.2308  bbox_loss: 0.0319  cls_loss: 0.1010  \n",
      "<<<iteration:[460/657] - total_loss: 0.5599  obj_loss: 0.0882  noobj_loss: 0.2131  bbox_loss: 0.0504  cls_loss: 0.1134  \n",
      "<<<iteration:[480/657] - total_loss: 0.5131  obj_loss: 0.0869  noobj_loss: 0.2157  bbox_loss: 0.0411  cls_loss: 0.1128  \n",
      "<<<iteration:[500/657] - total_loss: 0.5262  obj_loss: 0.0885  noobj_loss: 0.2303  bbox_loss: 0.0398  cls_loss: 0.1237  \n",
      "<<<iteration:[520/657] - total_loss: 0.6047  obj_loss: 0.0737  noobj_loss: 0.2146  bbox_loss: 0.0638  cls_loss: 0.1046  \n",
      "<<<iteration:[540/657] - total_loss: 0.5030  obj_loss: 0.0921  noobj_loss: 0.2241  bbox_loss: 0.0378  cls_loss: 0.1101  \n",
      "<<<iteration:[560/657] - total_loss: 0.5025  obj_loss: 0.0823  noobj_loss: 0.2270  bbox_loss: 0.0384  cls_loss: 0.1146  \n",
      "<<<iteration:[580/657] - total_loss: 0.4850  obj_loss: 0.0896  noobj_loss: 0.2203  bbox_loss: 0.0365  cls_loss: 0.1025  \n",
      "<<<iteration:[600/657] - total_loss: 0.4924  obj_loss: 0.0856  noobj_loss: 0.2308  bbox_loss: 0.0391  cls_loss: 0.0957  \n",
      "<<<iteration:[620/657] - total_loss: 0.4999  obj_loss: 0.1058  noobj_loss: 0.2140  bbox_loss: 0.0357  cls_loss: 0.1085  \n",
      "<<<iteration:[640/657] - total_loss: 0.5438  obj_loss: 0.0926  noobj_loss: 0.2112  bbox_loss: 0.0463  cls_loss: 0.1140  \n",
      "\n",
      "epoch:32/100 - Train Loss: 0.5282, Val Loss: 0.5310\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.6303  obj_loss: 0.0910  noobj_loss: 0.2263  bbox_loss: 0.0632  cls_loss: 0.1100  \n",
      "<<<iteration:[40/657] - total_loss: 0.4705  obj_loss: 0.0875  noobj_loss: 0.2193  bbox_loss: 0.0344  cls_loss: 0.1012  \n",
      "<<<iteration:[60/657] - total_loss: 0.4926  obj_loss: 0.0872  noobj_loss: 0.2156  bbox_loss: 0.0380  cls_loss: 0.1077  \n",
      "<<<iteration:[80/657] - total_loss: 0.5015  obj_loss: 0.0885  noobj_loss: 0.2264  bbox_loss: 0.0372  cls_loss: 0.1140  \n",
      "<<<iteration:[100/657] - total_loss: 0.4802  obj_loss: 0.0825  noobj_loss: 0.2178  bbox_loss: 0.0349  cls_loss: 0.1142  \n",
      "<<<iteration:[120/657] - total_loss: 0.4792  obj_loss: 0.0888  noobj_loss: 0.2244  bbox_loss: 0.0338  cls_loss: 0.1094  \n",
      "<<<iteration:[140/657] - total_loss: 0.4774  obj_loss: 0.0960  noobj_loss: 0.2110  bbox_loss: 0.0333  cls_loss: 0.1093  \n",
      "<<<iteration:[160/657] - total_loss: 0.5610  obj_loss: 0.0897  noobj_loss: 0.2572  bbox_loss: 0.0448  cls_loss: 0.1189  \n",
      "<<<iteration:[180/657] - total_loss: 0.5091  obj_loss: 0.0836  noobj_loss: 0.2167  bbox_loss: 0.0432  cls_loss: 0.1013  \n",
      "<<<iteration:[200/657] - total_loss: 0.5219  obj_loss: 0.0914  noobj_loss: 0.2487  bbox_loss: 0.0385  cls_loss: 0.1135  \n",
      "<<<iteration:[220/657] - total_loss: 0.5225  obj_loss: 0.0978  noobj_loss: 0.2164  bbox_loss: 0.0430  cls_loss: 0.1015  \n",
      "<<<iteration:[240/657] - total_loss: 0.4889  obj_loss: 0.0909  noobj_loss: 0.2116  bbox_loss: 0.0361  cls_loss: 0.1116  \n",
      "<<<iteration:[260/657] - total_loss: 0.4947  obj_loss: 0.0855  noobj_loss: 0.2223  bbox_loss: 0.0400  cls_loss: 0.0982  \n",
      "<<<iteration:[280/657] - total_loss: 0.5101  obj_loss: 0.0916  noobj_loss: 0.2192  bbox_loss: 0.0382  cls_loss: 0.1178  \n",
      "<<<iteration:[300/657] - total_loss: 0.5063  obj_loss: 0.0907  noobj_loss: 0.2204  bbox_loss: 0.0385  cls_loss: 0.1128  \n",
      "<<<iteration:[320/657] - total_loss: 0.5459  obj_loss: 0.0889  noobj_loss: 0.2124  bbox_loss: 0.0479  cls_loss: 0.1111  \n",
      "<<<iteration:[340/657] - total_loss: 0.5132  obj_loss: 0.0946  noobj_loss: 0.2301  bbox_loss: 0.0360  cls_loss: 0.1234  \n",
      "<<<iteration:[360/657] - total_loss: 0.5089  obj_loss: 0.0840  noobj_loss: 0.2197  bbox_loss: 0.0363  cls_loss: 0.1338  \n",
      "<<<iteration:[380/657] - total_loss: 0.5166  obj_loss: 0.0992  noobj_loss: 0.2231  bbox_loss: 0.0372  cls_loss: 0.1201  \n",
      "<<<iteration:[400/657] - total_loss: 0.5033  obj_loss: 0.0917  noobj_loss: 0.2210  bbox_loss: 0.0428  cls_loss: 0.0872  \n",
      "<<<iteration:[420/657] - total_loss: 0.5979  obj_loss: 0.0822  noobj_loss: 0.2203  bbox_loss: 0.0565  cls_loss: 0.1229  \n",
      "<<<iteration:[440/657] - total_loss: 0.6365  obj_loss: 0.0787  noobj_loss: 0.2127  bbox_loss: 0.0675  cls_loss: 0.1140  \n",
      "<<<iteration:[460/657] - total_loss: 0.4937  obj_loss: 0.0911  noobj_loss: 0.2136  bbox_loss: 0.0391  cls_loss: 0.1002  \n",
      "<<<iteration:[480/657] - total_loss: 0.5023  obj_loss: 0.0880  noobj_loss: 0.2111  bbox_loss: 0.0369  cls_loss: 0.1240  \n",
      "<<<iteration:[500/657] - total_loss: 0.4885  obj_loss: 0.0857  noobj_loss: 0.2138  bbox_loss: 0.0402  cls_loss: 0.0950  \n",
      "<<<iteration:[520/657] - total_loss: 0.5169  obj_loss: 0.0860  noobj_loss: 0.2164  bbox_loss: 0.0432  cls_loss: 0.1069  \n",
      "<<<iteration:[540/657] - total_loss: 2.5021  obj_loss: 0.0391  noobj_loss: 0.2186  bbox_loss: 0.4507  cls_loss: 0.1001  \n",
      "<<<iteration:[560/657] - total_loss: 0.9734  obj_loss: 0.0551  noobj_loss: 0.2057  bbox_loss: 0.1437  cls_loss: 0.0971  \n",
      "<<<iteration:[580/657] - total_loss: 0.6851  obj_loss: 0.0668  noobj_loss: 0.2194  bbox_loss: 0.0832  cls_loss: 0.0925  \n",
      "<<<iteration:[600/657] - total_loss: 0.5769  obj_loss: 0.0745  noobj_loss: 0.2124  bbox_loss: 0.0580  cls_loss: 0.1061  \n",
      "<<<iteration:[620/657] - total_loss: 0.5387  obj_loss: 0.0843  noobj_loss: 0.2200  bbox_loss: 0.0471  cls_loss: 0.1091  \n",
      "<<<iteration:[640/657] - total_loss: 0.5374  obj_loss: 0.0874  noobj_loss: 0.2230  bbox_loss: 0.0488  cls_loss: 0.0944  \n",
      "\n",
      "epoch:33/100 - Train Loss: 0.5998, Val Loss: 0.5369\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5432  obj_loss: 0.0868  noobj_loss: 0.2223  bbox_loss: 0.0471  cls_loss: 0.1100  \n",
      "<<<iteration:[40/657] - total_loss: 0.4855  obj_loss: 0.0846  noobj_loss: 0.2098  bbox_loss: 0.0391  cls_loss: 0.1003  \n",
      "<<<iteration:[60/657] - total_loss: 0.5528  obj_loss: 0.0887  noobj_loss: 0.2133  bbox_loss: 0.0498  cls_loss: 0.1082  \n",
      "<<<iteration:[80/657] - total_loss: 0.5934  obj_loss: 0.0765  noobj_loss: 0.2198  bbox_loss: 0.0579  cls_loss: 0.1174  \n",
      "<<<iteration:[100/657] - total_loss: 0.4896  obj_loss: 0.0824  noobj_loss: 0.2009  bbox_loss: 0.0416  cls_loss: 0.0987  \n",
      "<<<iteration:[120/657] - total_loss: 0.5203  obj_loss: 0.0869  noobj_loss: 0.2201  bbox_loss: 0.0426  cls_loss: 0.1104  \n",
      "<<<iteration:[140/657] - total_loss: 0.4735  obj_loss: 0.0904  noobj_loss: 0.2130  bbox_loss: 0.0339  cls_loss: 0.1073  \n",
      "<<<iteration:[160/657] - total_loss: 0.5191  obj_loss: 0.0927  noobj_loss: 0.2275  bbox_loss: 0.0394  cls_loss: 0.1159  \n",
      "<<<iteration:[180/657] - total_loss: 0.5216  obj_loss: 0.0898  noobj_loss: 0.2163  bbox_loss: 0.0401  cls_loss: 0.1234  \n",
      "<<<iteration:[200/657] - total_loss: 0.5217  obj_loss: 0.0821  noobj_loss: 0.2369  bbox_loss: 0.0422  cls_loss: 0.1099  \n",
      "<<<iteration:[220/657] - total_loss: 0.4726  obj_loss: 0.0806  noobj_loss: 0.2101  bbox_loss: 0.0364  cls_loss: 0.1049  \n",
      "<<<iteration:[240/657] - total_loss: 0.4872  obj_loss: 0.0799  noobj_loss: 0.2114  bbox_loss: 0.0393  cls_loss: 0.1050  \n",
      "<<<iteration:[260/657] - total_loss: 0.4914  obj_loss: 0.0835  noobj_loss: 0.2124  bbox_loss: 0.0350  cls_loss: 0.1266  \n",
      "<<<iteration:[280/657] - total_loss: 0.5005  obj_loss: 0.0861  noobj_loss: 0.2271  bbox_loss: 0.0360  cls_loss: 0.1208  \n",
      "<<<iteration:[300/657] - total_loss: 0.4750  obj_loss: 0.0960  noobj_loss: 0.2170  bbox_loss: 0.0333  cls_loss: 0.1042  \n",
      "<<<iteration:[320/657] - total_loss: 0.5065  obj_loss: 0.1027  noobj_loss: 0.2094  bbox_loss: 0.0359  cls_loss: 0.1194  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[340/657] - total_loss: 0.5445  obj_loss: 0.0893  noobj_loss: 0.2086  bbox_loss: 0.0475  cls_loss: 0.1131  \n",
      "<<<iteration:[360/657] - total_loss: 0.5344  obj_loss: 0.0959  noobj_loss: 0.2126  bbox_loss: 0.0447  cls_loss: 0.1087  \n",
      "<<<iteration:[380/657] - total_loss: 0.5508  obj_loss: 0.0948  noobj_loss: 0.2177  bbox_loss: 0.0438  cls_loss: 0.1282  \n",
      "<<<iteration:[400/657] - total_loss: 0.4897  obj_loss: 0.0923  noobj_loss: 0.2200  bbox_loss: 0.0352  cls_loss: 0.1112  \n",
      "<<<iteration:[420/657] - total_loss: 0.5245  obj_loss: 0.0906  noobj_loss: 0.2208  bbox_loss: 0.0474  cls_loss: 0.0866  \n",
      "<<<iteration:[440/657] - total_loss: 0.5072  obj_loss: 0.1016  noobj_loss: 0.2138  bbox_loss: 0.0365  cls_loss: 0.1162  \n",
      "<<<iteration:[460/657] - total_loss: 0.4882  obj_loss: 0.0956  noobj_loss: 0.2049  bbox_loss: 0.0375  cls_loss: 0.1027  \n",
      "<<<iteration:[480/657] - total_loss: 0.4948  obj_loss: 0.0918  noobj_loss: 0.2268  bbox_loss: 0.0387  cls_loss: 0.0962  \n",
      "<<<iteration:[500/657] - total_loss: 0.4798  obj_loss: 0.0922  noobj_loss: 0.2214  bbox_loss: 0.0364  cls_loss: 0.0947  \n",
      "<<<iteration:[520/657] - total_loss: 0.4779  obj_loss: 0.0960  noobj_loss: 0.2095  bbox_loss: 0.0346  cls_loss: 0.1041  \n",
      "<<<iteration:[540/657] - total_loss: 0.4723  obj_loss: 0.0951  noobj_loss: 0.2076  bbox_loss: 0.0358  cls_loss: 0.0942  \n",
      "<<<iteration:[560/657] - total_loss: 0.4939  obj_loss: 0.0821  noobj_loss: 0.2119  bbox_loss: 0.0399  cls_loss: 0.1066  \n",
      "<<<iteration:[580/657] - total_loss: 0.4472  obj_loss: 0.0927  noobj_loss: 0.2031  bbox_loss: 0.0313  cls_loss: 0.0966  \n",
      "<<<iteration:[600/657] - total_loss: 0.5536  obj_loss: 0.0856  noobj_loss: 0.2079  bbox_loss: 0.0551  cls_loss: 0.0885  \n",
      "<<<iteration:[620/657] - total_loss: 0.4369  obj_loss: 0.0850  noobj_loss: 0.2082  bbox_loss: 0.0303  cls_loss: 0.0964  \n",
      "<<<iteration:[640/657] - total_loss: 0.4631  obj_loss: 0.0869  noobj_loss: 0.2168  bbox_loss: 0.0367  cls_loss: 0.0843  \n",
      "\n",
      "epoch:34/100 - Train Loss: 0.5083, Val Loss: 0.6225\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5334  obj_loss: 0.0913  noobj_loss: 0.2157  bbox_loss: 0.0427  cls_loss: 0.1205  \n",
      "<<<iteration:[40/657] - total_loss: 0.4826  obj_loss: 0.0938  noobj_loss: 0.2189  bbox_loss: 0.0366  cls_loss: 0.0961  \n",
      "<<<iteration:[60/657] - total_loss: 0.4482  obj_loss: 0.0869  noobj_loss: 0.2045  bbox_loss: 0.0314  cls_loss: 0.1022  \n",
      "<<<iteration:[80/657] - total_loss: 0.4814  obj_loss: 0.0950  noobj_loss: 0.2124  bbox_loss: 0.0318  cls_loss: 0.1211  \n",
      "<<<iteration:[100/657] - total_loss: 0.4818  obj_loss: 0.0973  noobj_loss: 0.2122  bbox_loss: 0.0334  cls_loss: 0.1114  \n",
      "<<<iteration:[120/657] - total_loss: 0.5464  obj_loss: 0.0844  noobj_loss: 0.2088  bbox_loss: 0.0501  cls_loss: 0.1070  \n",
      "<<<iteration:[140/657] - total_loss: 0.5359  obj_loss: 0.0930  noobj_loss: 0.2082  bbox_loss: 0.0468  cls_loss: 0.1048  \n",
      "<<<iteration:[160/657] - total_loss: 0.5036  obj_loss: 0.0915  noobj_loss: 0.2112  bbox_loss: 0.0416  cls_loss: 0.0985  \n",
      "<<<iteration:[180/657] - total_loss: 0.4875  obj_loss: 0.0957  noobj_loss: 0.2245  bbox_loss: 0.0345  cls_loss: 0.1070  \n",
      "<<<iteration:[200/657] - total_loss: 0.4683  obj_loss: 0.0895  noobj_loss: 0.2027  bbox_loss: 0.0344  cls_loss: 0.1054  \n",
      "<<<iteration:[220/657] - total_loss: 0.4802  obj_loss: 0.0909  noobj_loss: 0.2054  bbox_loss: 0.0350  cls_loss: 0.1118  \n",
      "<<<iteration:[240/657] - total_loss: 0.4891  obj_loss: 0.0822  noobj_loss: 0.2110  bbox_loss: 0.0379  cls_loss: 0.1117  \n",
      "<<<iteration:[260/657] - total_loss: 0.5045  obj_loss: 0.0895  noobj_loss: 0.2015  bbox_loss: 0.0427  cls_loss: 0.1007  \n",
      "<<<iteration:[280/657] - total_loss: 0.5605  obj_loss: 0.0910  noobj_loss: 0.1956  bbox_loss: 0.0522  cls_loss: 0.1107  \n",
      "<<<iteration:[300/657] - total_loss: 0.4664  obj_loss: 0.0854  noobj_loss: 0.2240  bbox_loss: 0.0343  cls_loss: 0.0973  \n",
      "<<<iteration:[320/657] - total_loss: 0.4830  obj_loss: 0.1015  noobj_loss: 0.2000  bbox_loss: 0.0344  cls_loss: 0.1096  \n",
      "<<<iteration:[340/657] - total_loss: 0.4867  obj_loss: 0.0947  noobj_loss: 0.2029  bbox_loss: 0.0328  cls_loss: 0.1266  \n",
      "<<<iteration:[360/657] - total_loss: 0.5096  obj_loss: 0.0934  noobj_loss: 0.2058  bbox_loss: 0.0392  cls_loss: 0.1172  \n",
      "<<<iteration:[380/657] - total_loss: 0.4514  obj_loss: 0.0956  noobj_loss: 0.2044  bbox_loss: 0.0334  cls_loss: 0.0866  \n",
      "<<<iteration:[400/657] - total_loss: 0.4475  obj_loss: 0.0904  noobj_loss: 0.2243  bbox_loss: 0.0305  cls_loss: 0.0922  \n",
      "<<<iteration:[420/657] - total_loss: 0.4688  obj_loss: 0.0943  noobj_loss: 0.2039  bbox_loss: 0.0347  cls_loss: 0.0988  \n",
      "<<<iteration:[440/657] - total_loss: 0.5073  obj_loss: 0.0882  noobj_loss: 0.2187  bbox_loss: 0.0407  cls_loss: 0.1062  \n",
      "<<<iteration:[460/657] - total_loss: 0.4849  obj_loss: 0.0820  noobj_loss: 0.2128  bbox_loss: 0.0386  cls_loss: 0.1034  \n",
      "<<<iteration:[480/657] - total_loss: 0.4662  obj_loss: 0.0799  noobj_loss: 0.1991  bbox_loss: 0.0373  cls_loss: 0.1002  \n",
      "<<<iteration:[500/657] - total_loss: 0.4492  obj_loss: 0.0942  noobj_loss: 0.2105  bbox_loss: 0.0317  cls_loss: 0.0913  \n",
      "<<<iteration:[520/657] - total_loss: 0.5011  obj_loss: 0.0879  noobj_loss: 0.2082  bbox_loss: 0.0431  cls_loss: 0.0936  \n",
      "<<<iteration:[540/657] - total_loss: 0.5005  obj_loss: 0.0969  noobj_loss: 0.2212  bbox_loss: 0.0355  cls_loss: 0.1157  \n",
      "<<<iteration:[560/657] - total_loss: 0.4899  obj_loss: 0.0903  noobj_loss: 0.2074  bbox_loss: 0.0383  cls_loss: 0.1043  \n",
      "<<<iteration:[580/657] - total_loss: 0.4765  obj_loss: 0.0794  noobj_loss: 0.1984  bbox_loss: 0.0348  cls_loss: 0.1239  \n",
      "<<<iteration:[600/657] - total_loss: 0.4569  obj_loss: 0.0965  noobj_loss: 0.2035  bbox_loss: 0.0308  cls_loss: 0.1045  \n",
      "<<<iteration:[620/657] - total_loss: 0.5053  obj_loss: 0.0919  noobj_loss: 0.2136  bbox_loss: 0.0377  cls_loss: 0.1183  \n",
      "<<<iteration:[640/657] - total_loss: 0.4834  obj_loss: 0.0868  noobj_loss: 0.2042  bbox_loss: 0.0387  cls_loss: 0.1011  \n",
      "\n",
      "epoch:35/100 - Train Loss: 0.4884, Val Loss: 0.4848\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5033  obj_loss: 0.0902  noobj_loss: 0.2269  bbox_loss: 0.0359  cls_loss: 0.1203  \n",
      "<<<iteration:[40/657] - total_loss: 0.5349  obj_loss: 0.0831  noobj_loss: 0.2366  bbox_loss: 0.0447  cls_loss: 0.1101  \n",
      "<<<iteration:[60/657] - total_loss: 0.4568  obj_loss: 0.0947  noobj_loss: 0.2055  bbox_loss: 0.0336  cls_loss: 0.0912  \n",
      "<<<iteration:[80/657] - total_loss: 0.5294  obj_loss: 0.0957  noobj_loss: 0.1989  bbox_loss: 0.0470  cls_loss: 0.0991  \n",
      "<<<iteration:[100/657] - total_loss: 0.5431  obj_loss: 0.0869  noobj_loss: 0.2042  bbox_loss: 0.0484  cls_loss: 0.1118  \n",
      "<<<iteration:[120/657] - total_loss: 0.5032  obj_loss: 0.0889  noobj_loss: 0.2231  bbox_loss: 0.0374  cls_loss: 0.1156  \n",
      "<<<iteration:[140/657] - total_loss: 0.4933  obj_loss: 0.0909  noobj_loss: 0.2049  bbox_loss: 0.0368  cls_loss: 0.1160  \n",
      "<<<iteration:[160/657] - total_loss: 0.4501  obj_loss: 0.1006  noobj_loss: 0.2007  bbox_loss: 0.0313  cls_loss: 0.0928  \n",
      "<<<iteration:[180/657] - total_loss: 0.4618  obj_loss: 0.0981  noobj_loss: 0.2020  bbox_loss: 0.0347  cls_loss: 0.0894  \n",
      "<<<iteration:[200/657] - total_loss: 0.5101  obj_loss: 0.1021  noobj_loss: 0.1999  bbox_loss: 0.0419  cls_loss: 0.0985  \n",
      "<<<iteration:[220/657] - total_loss: 0.4490  obj_loss: 0.0997  noobj_loss: 0.2027  bbox_loss: 0.0288  cls_loss: 0.1039  \n",
      "<<<iteration:[240/657] - total_loss: 0.5131  obj_loss: 0.0778  noobj_loss: 0.2096  bbox_loss: 0.0426  cls_loss: 0.1178  \n",
      "<<<iteration:[260/657] - total_loss: 0.4865  obj_loss: 0.0954  noobj_loss: 0.1953  bbox_loss: 0.0374  cls_loss: 0.1065  \n",
      "<<<iteration:[280/657] - total_loss: 0.5562  obj_loss: 0.0941  noobj_loss: 0.2033  bbox_loss: 0.0498  cls_loss: 0.1114  \n",
      "<<<iteration:[300/657] - total_loss: 0.5078  obj_loss: 0.0929  noobj_loss: 0.2064  bbox_loss: 0.0371  cls_loss: 0.1262  \n",
      "<<<iteration:[320/657] - total_loss: 0.4894  obj_loss: 0.1019  noobj_loss: 0.2115  bbox_loss: 0.0350  cls_loss: 0.1069  \n",
      "<<<iteration:[340/657] - total_loss: 0.4425  obj_loss: 0.0829  noobj_loss: 0.2022  bbox_loss: 0.0318  cls_loss: 0.0996  \n",
      "<<<iteration:[360/657] - total_loss: 0.4747  obj_loss: 0.1037  noobj_loss: 0.1987  bbox_loss: 0.0324  cls_loss: 0.1097  \n",
      "<<<iteration:[380/657] - total_loss: 0.4776  obj_loss: 0.0938  noobj_loss: 0.2051  bbox_loss: 0.0360  cls_loss: 0.1013  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[400/657] - total_loss: 0.4752  obj_loss: 0.0912  noobj_loss: 0.1998  bbox_loss: 0.0340  cls_loss: 0.1142  \n",
      "<<<iteration:[420/657] - total_loss: 0.4891  obj_loss: 0.0863  noobj_loss: 0.2143  bbox_loss: 0.0373  cls_loss: 0.1091  \n",
      "<<<iteration:[440/657] - total_loss: 0.5262  obj_loss: 0.0897  noobj_loss: 0.2098  bbox_loss: 0.0446  cls_loss: 0.1083  \n",
      "<<<iteration:[460/657] - total_loss: 0.5195  obj_loss: 0.0928  noobj_loss: 0.2675  bbox_loss: 0.0374  cls_loss: 0.1057  \n",
      "<<<iteration:[480/657] - total_loss: 0.4901  obj_loss: 0.0851  noobj_loss: 0.1972  bbox_loss: 0.0417  cls_loss: 0.0978  \n",
      "<<<iteration:[500/657] - total_loss: 0.4638  obj_loss: 0.0878  noobj_loss: 0.2109  bbox_loss: 0.0332  cls_loss: 0.1047  \n",
      "<<<iteration:[520/657] - total_loss: 0.4709  obj_loss: 0.0841  noobj_loss: 0.2042  bbox_loss: 0.0373  cls_loss: 0.0980  \n",
      "<<<iteration:[540/657] - total_loss: 0.5103  obj_loss: 0.0854  noobj_loss: 0.2061  bbox_loss: 0.0425  cls_loss: 0.1092  \n",
      "<<<iteration:[560/657] - total_loss: 0.4424  obj_loss: 0.0836  noobj_loss: 0.2321  bbox_loss: 0.0330  cls_loss: 0.0777  \n",
      "<<<iteration:[580/657] - total_loss: 0.5558  obj_loss: 0.0955  noobj_loss: 0.2044  bbox_loss: 0.0462  cls_loss: 0.1271  \n",
      "<<<iteration:[600/657] - total_loss: 0.4782  obj_loss: 0.1044  noobj_loss: 0.2008  bbox_loss: 0.0337  cls_loss: 0.1051  \n",
      "<<<iteration:[620/657] - total_loss: 0.4875  obj_loss: 0.0965  noobj_loss: 0.1927  bbox_loss: 0.0398  cls_loss: 0.0958  \n",
      "<<<iteration:[640/657] - total_loss: 0.4908  obj_loss: 0.0952  noobj_loss: 0.2004  bbox_loss: 0.0384  cls_loss: 0.1035  \n",
      "\n",
      "epoch:36/100 - Train Loss: 0.4910, Val Loss: 0.4760\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5617  obj_loss: 0.0916  noobj_loss: 0.2089  bbox_loss: 0.0497  cls_loss: 0.1172  \n",
      "<<<iteration:[40/657] - total_loss: 0.4646  obj_loss: 0.0933  noobj_loss: 0.2016  bbox_loss: 0.0331  cls_loss: 0.1049  \n",
      "<<<iteration:[60/657] - total_loss: 0.4565  obj_loss: 0.0937  noobj_loss: 0.1987  bbox_loss: 0.0325  cls_loss: 0.1011  \n",
      "<<<iteration:[80/657] - total_loss: 0.4674  obj_loss: 0.1059  noobj_loss: 0.2069  bbox_loss: 0.0321  cls_loss: 0.0976  \n",
      "<<<iteration:[100/657] - total_loss: 0.4522  obj_loss: 0.0968  noobj_loss: 0.2040  bbox_loss: 0.0296  cls_loss: 0.1052  \n",
      "<<<iteration:[120/657] - total_loss: 0.4577  obj_loss: 0.0801  noobj_loss: 0.2217  bbox_loss: 0.0320  cls_loss: 0.1068  \n",
      "<<<iteration:[140/657] - total_loss: 0.4463  obj_loss: 0.0983  noobj_loss: 0.2077  bbox_loss: 0.0309  cls_loss: 0.0897  \n",
      "<<<iteration:[160/657] - total_loss: 0.4561  obj_loss: 0.0923  noobj_loss: 0.1962  bbox_loss: 0.0311  cls_loss: 0.1105  \n",
      "<<<iteration:[180/657] - total_loss: 0.4477  obj_loss: 0.1055  noobj_loss: 0.1868  bbox_loss: 0.0289  cls_loss: 0.1042  \n",
      "<<<iteration:[200/657] - total_loss: 0.4731  obj_loss: 0.0876  noobj_loss: 0.2192  bbox_loss: 0.0318  cls_loss: 0.1169  \n",
      "<<<iteration:[220/657] - total_loss: 0.4882  obj_loss: 0.0933  noobj_loss: 0.1944  bbox_loss: 0.0390  cls_loss: 0.1027  \n",
      "<<<iteration:[240/657] - total_loss: 0.5018  obj_loss: 0.0974  noobj_loss: 0.1893  bbox_loss: 0.0418  cls_loss: 0.1008  \n",
      "<<<iteration:[260/657] - total_loss: 0.4364  obj_loss: 0.0919  noobj_loss: 0.1999  bbox_loss: 0.0291  cls_loss: 0.0989  \n",
      "<<<iteration:[280/657] - total_loss: 0.4703  obj_loss: 0.1000  noobj_loss: 0.1985  bbox_loss: 0.0335  cls_loss: 0.1035  \n",
      "<<<iteration:[300/657] - total_loss: 0.4252  obj_loss: 0.0887  noobj_loss: 0.1963  bbox_loss: 0.0320  cls_loss: 0.0782  \n",
      "<<<iteration:[320/657] - total_loss: 0.4576  obj_loss: 0.0923  noobj_loss: 0.1941  bbox_loss: 0.0320  cls_loss: 0.1085  \n",
      "<<<iteration:[340/657] - total_loss: 0.5026  obj_loss: 0.0990  noobj_loss: 0.1928  bbox_loss: 0.0368  cls_loss: 0.1232  \n",
      "<<<iteration:[360/657] - total_loss: 0.4689  obj_loss: 0.0879  noobj_loss: 0.1904  bbox_loss: 0.0360  cls_loss: 0.1058  \n",
      "<<<iteration:[380/657] - total_loss: 0.4809  obj_loss: 0.0863  noobj_loss: 0.2037  bbox_loss: 0.0382  cls_loss: 0.1017  \n",
      "<<<iteration:[400/657] - total_loss: 0.4522  obj_loss: 0.0855  noobj_loss: 0.1980  bbox_loss: 0.0334  cls_loss: 0.1005  \n",
      "<<<iteration:[420/657] - total_loss: 0.4688  obj_loss: 0.0940  noobj_loss: 0.2151  bbox_loss: 0.0321  cls_loss: 0.1066  \n",
      "<<<iteration:[440/657] - total_loss: 0.5356  obj_loss: 0.0947  noobj_loss: 0.2077  bbox_loss: 0.0461  cls_loss: 0.1064  \n",
      "<<<iteration:[460/657] - total_loss: 0.4529  obj_loss: 0.0845  noobj_loss: 0.1936  bbox_loss: 0.0322  cls_loss: 0.1108  \n",
      "<<<iteration:[480/657] - total_loss: 0.4856  obj_loss: 0.1006  noobj_loss: 0.1969  bbox_loss: 0.0342  cls_loss: 0.1156  \n",
      "<<<iteration:[500/657] - total_loss: 0.4544  obj_loss: 0.0948  noobj_loss: 0.2026  bbox_loss: 0.0327  cls_loss: 0.0946  \n",
      "<<<iteration:[520/657] - total_loss: 0.4372  obj_loss: 0.0939  noobj_loss: 0.1842  bbox_loss: 0.0304  cls_loss: 0.0992  \n",
      "<<<iteration:[540/657] - total_loss: 0.4762  obj_loss: 0.0933  noobj_loss: 0.1968  bbox_loss: 0.0355  cls_loss: 0.1069  \n",
      "<<<iteration:[560/657] - total_loss: 0.4819  obj_loss: 0.0925  noobj_loss: 0.2037  bbox_loss: 0.0358  cls_loss: 0.1086  \n",
      "<<<iteration:[580/657] - total_loss: 0.4613  obj_loss: 0.0913  noobj_loss: 0.1968  bbox_loss: 0.0367  cls_loss: 0.0882  \n",
      "<<<iteration:[600/657] - total_loss: 0.4514  obj_loss: 0.0926  noobj_loss: 0.2009  bbox_loss: 0.0318  cls_loss: 0.0995  \n",
      "<<<iteration:[620/657] - total_loss: 0.4915  obj_loss: 0.0956  noobj_loss: 0.1951  bbox_loss: 0.0385  cls_loss: 0.1060  \n",
      "<<<iteration:[640/657] - total_loss: 0.4359  obj_loss: 0.0855  noobj_loss: 0.2025  bbox_loss: 0.0323  cls_loss: 0.0875  \n",
      "\n",
      "epoch:37/100 - Train Loss: 0.4700, Val Loss: 0.5000\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4971  obj_loss: 0.0896  noobj_loss: 0.2195  bbox_loss: 0.0354  cls_loss: 0.1206  \n",
      "<<<iteration:[40/657] - total_loss: 0.4889  obj_loss: 0.1109  noobj_loss: 0.1992  bbox_loss: 0.0378  cls_loss: 0.0896  \n",
      "<<<iteration:[60/657] - total_loss: 0.4986  obj_loss: 0.0948  noobj_loss: 0.2155  bbox_loss: 0.0399  cls_loss: 0.0967  \n",
      "<<<iteration:[80/657] - total_loss: 0.5320  obj_loss: 0.0824  noobj_loss: 0.1945  bbox_loss: 0.0486  cls_loss: 0.1091  \n",
      "<<<iteration:[100/657] - total_loss: 0.4783  obj_loss: 0.0940  noobj_loss: 0.1966  bbox_loss: 0.0372  cls_loss: 0.1000  \n",
      "<<<iteration:[120/657] - total_loss: 0.4718  obj_loss: 0.0858  noobj_loss: 0.1958  bbox_loss: 0.0385  cls_loss: 0.0955  \n",
      "<<<iteration:[140/657] - total_loss: 0.4756  obj_loss: 0.0893  noobj_loss: 0.1990  bbox_loss: 0.0344  cls_loss: 0.1147  \n",
      "<<<iteration:[160/657] - total_loss: 0.4493  obj_loss: 0.0927  noobj_loss: 0.2166  bbox_loss: 0.0326  cls_loss: 0.0852  \n",
      "<<<iteration:[180/657] - total_loss: 0.4922  obj_loss: 0.0933  noobj_loss: 0.1931  bbox_loss: 0.0376  cls_loss: 0.1143  \n",
      "<<<iteration:[200/657] - total_loss: 0.4756  obj_loss: 0.0916  noobj_loss: 0.1935  bbox_loss: 0.0362  cls_loss: 0.1061  \n",
      "<<<iteration:[220/657] - total_loss: 0.4643  obj_loss: 0.0966  noobj_loss: 0.1869  bbox_loss: 0.0330  cls_loss: 0.1094  \n",
      "<<<iteration:[240/657] - total_loss: 0.4543  obj_loss: 0.0844  noobj_loss: 0.2104  bbox_loss: 0.0332  cls_loss: 0.0987  \n",
      "<<<iteration:[260/657] - total_loss: 0.5124  obj_loss: 0.0895  noobj_loss: 0.2116  bbox_loss: 0.0373  cls_loss: 0.1308  \n",
      "<<<iteration:[280/657] - total_loss: 0.4492  obj_loss: 0.0956  noobj_loss: 0.1953  bbox_loss: 0.0284  cls_loss: 0.1140  \n",
      "<<<iteration:[300/657] - total_loss: 0.4678  obj_loss: 0.1012  noobj_loss: 0.2109  bbox_loss: 0.0330  cls_loss: 0.0961  \n",
      "<<<iteration:[320/657] - total_loss: 0.4989  obj_loss: 0.0904  noobj_loss: 0.2002  bbox_loss: 0.0397  cls_loss: 0.1100  \n",
      "<<<iteration:[340/657] - total_loss: 0.5120  obj_loss: 0.0902  noobj_loss: 0.1951  bbox_loss: 0.0450  cls_loss: 0.0995  \n",
      "<<<iteration:[360/657] - total_loss: 0.4408  obj_loss: 0.0878  noobj_loss: 0.1904  bbox_loss: 0.0323  cls_loss: 0.0964  \n",
      "<<<iteration:[380/657] - total_loss: 0.4486  obj_loss: 0.1036  noobj_loss: 0.1950  bbox_loss: 0.0301  cls_loss: 0.0970  \n",
      "<<<iteration:[400/657] - total_loss: 0.4472  obj_loss: 0.0901  noobj_loss: 0.2100  bbox_loss: 0.0340  cls_loss: 0.0821  \n",
      "<<<iteration:[420/657] - total_loss: 0.4337  obj_loss: 0.0902  noobj_loss: 0.2056  bbox_loss: 0.0303  cls_loss: 0.0892  \n",
      "<<<iteration:[440/657] - total_loss: 0.4522  obj_loss: 0.0964  noobj_loss: 0.2106  bbox_loss: 0.0306  cls_loss: 0.0975  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[460/657] - total_loss: 0.4508  obj_loss: 0.0882  noobj_loss: 0.2135  bbox_loss: 0.0318  cls_loss: 0.0968  \n",
      "<<<iteration:[480/657] - total_loss: 0.4458  obj_loss: 0.1058  noobj_loss: 0.1853  bbox_loss: 0.0307  cls_loss: 0.0936  \n",
      "<<<iteration:[500/657] - total_loss: 0.4775  obj_loss: 0.0916  noobj_loss: 0.1895  bbox_loss: 0.0359  cls_loss: 0.1117  \n",
      "<<<iteration:[520/657] - total_loss: 0.4531  obj_loss: 0.0932  noobj_loss: 0.1911  bbox_loss: 0.0341  cls_loss: 0.0936  \n",
      "<<<iteration:[540/657] - total_loss: 0.4667  obj_loss: 0.1046  noobj_loss: 0.1883  bbox_loss: 0.0314  cls_loss: 0.1111  \n",
      "<<<iteration:[560/657] - total_loss: 0.4379  obj_loss: 0.0850  noobj_loss: 0.1987  bbox_loss: 0.0314  cls_loss: 0.0967  \n",
      "<<<iteration:[580/657] - total_loss: 0.4724  obj_loss: 0.1016  noobj_loss: 0.1895  bbox_loss: 0.0375  cls_loss: 0.0883  \n",
      "<<<iteration:[600/657] - total_loss: 0.4502  obj_loss: 0.1043  noobj_loss: 0.1957  bbox_loss: 0.0294  cls_loss: 0.1012  \n",
      "<<<iteration:[620/657] - total_loss: 0.4229  obj_loss: 0.0857  noobj_loss: 0.1935  bbox_loss: 0.0304  cls_loss: 0.0885  \n",
      "<<<iteration:[640/657] - total_loss: 0.4327  obj_loss: 0.0983  noobj_loss: 0.1923  bbox_loss: 0.0288  cls_loss: 0.0943  \n",
      "\n",
      "epoch:38/100 - Train Loss: 0.4661, Val Loss: 0.4751\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4754  obj_loss: 0.1120  noobj_loss: 0.2013  bbox_loss: 0.0320  cls_loss: 0.1027  \n",
      "<<<iteration:[40/657] - total_loss: 0.4917  obj_loss: 0.0916  noobj_loss: 0.1969  bbox_loss: 0.0399  cls_loss: 0.1024  \n",
      "<<<iteration:[60/657] - total_loss: 0.4572  obj_loss: 0.0906  noobj_loss: 0.2011  bbox_loss: 0.0316  cls_loss: 0.1080  \n",
      "<<<iteration:[80/657] - total_loss: 0.4793  obj_loss: 0.0988  noobj_loss: 0.1922  bbox_loss: 0.0377  cls_loss: 0.0958  \n",
      "<<<iteration:[100/657] - total_loss: 0.4347  obj_loss: 0.1059  noobj_loss: 0.1893  bbox_loss: 0.0295  cls_loss: 0.0866  \n",
      "<<<iteration:[120/657] - total_loss: 0.4827  obj_loss: 0.1024  noobj_loss: 0.2049  bbox_loss: 0.0322  cls_loss: 0.1167  \n",
      "<<<iteration:[140/657] - total_loss: 0.4466  obj_loss: 0.0863  noobj_loss: 0.1965  bbox_loss: 0.0323  cls_loss: 0.1004  \n",
      "<<<iteration:[160/657] - total_loss: 0.4771  obj_loss: 0.0916  noobj_loss: 0.1929  bbox_loss: 0.0371  cls_loss: 0.1038  \n",
      "<<<iteration:[180/657] - total_loss: 0.4667  obj_loss: 0.0970  noobj_loss: 0.1997  bbox_loss: 0.0334  cls_loss: 0.1029  \n",
      "<<<iteration:[200/657] - total_loss: 0.4211  obj_loss: 0.0889  noobj_loss: 0.1931  bbox_loss: 0.0278  cls_loss: 0.0966  \n",
      "<<<iteration:[220/657] - total_loss: 0.4677  obj_loss: 0.0965  noobj_loss: 0.1870  bbox_loss: 0.0355  cls_loss: 0.1000  \n",
      "<<<iteration:[240/657] - total_loss: 0.4513  obj_loss: 0.0992  noobj_loss: 0.1826  bbox_loss: 0.0313  cls_loss: 0.1042  \n",
      "<<<iteration:[260/657] - total_loss: 0.4760  obj_loss: 0.0827  noobj_loss: 0.2059  bbox_loss: 0.0337  cls_loss: 0.1216  \n",
      "<<<iteration:[280/657] - total_loss: 0.4627  obj_loss: 0.0871  noobj_loss: 0.1887  bbox_loss: 0.0355  cls_loss: 0.1036  \n",
      "<<<iteration:[300/657] - total_loss: 0.4328  obj_loss: 0.0962  noobj_loss: 0.1933  bbox_loss: 0.0304  cls_loss: 0.0882  \n",
      "<<<iteration:[320/657] - total_loss: 0.4194  obj_loss: 0.1052  noobj_loss: 0.1903  bbox_loss: 0.0255  cls_loss: 0.0914  \n",
      "<<<iteration:[340/657] - total_loss: 0.4642  obj_loss: 0.0854  noobj_loss: 0.1901  bbox_loss: 0.0369  cls_loss: 0.0991  \n",
      "<<<iteration:[360/657] - total_loss: 0.5036  obj_loss: 0.0887  noobj_loss: 0.2061  bbox_loss: 0.0439  cls_loss: 0.0924  \n",
      "<<<iteration:[380/657] - total_loss: 0.4607  obj_loss: 0.1004  noobj_loss: 0.1887  bbox_loss: 0.0284  cls_loss: 0.1240  \n",
      "<<<iteration:[400/657] - total_loss: 0.4641  obj_loss: 0.0959  noobj_loss: 0.1864  bbox_loss: 0.0330  cls_loss: 0.1100  \n",
      "<<<iteration:[420/657] - total_loss: 0.4318  obj_loss: 0.0973  noobj_loss: 0.1972  bbox_loss: 0.0292  cls_loss: 0.0901  \n",
      "<<<iteration:[440/657] - total_loss: 0.4415  obj_loss: 0.0839  noobj_loss: 0.1848  bbox_loss: 0.0344  cls_loss: 0.0933  \n",
      "<<<iteration:[460/657] - total_loss: 0.4326  obj_loss: 0.0942  noobj_loss: 0.1842  bbox_loss: 0.0311  cls_loss: 0.0908  \n",
      "<<<iteration:[480/657] - total_loss: 0.4573  obj_loss: 0.1044  noobj_loss: 0.1869  bbox_loss: 0.0326  cls_loss: 0.0964  \n",
      "<<<iteration:[500/657] - total_loss: 0.4503  obj_loss: 0.0929  noobj_loss: 0.2039  bbox_loss: 0.0332  cls_loss: 0.0894  \n",
      "<<<iteration:[520/657] - total_loss: 0.4334  obj_loss: 0.1003  noobj_loss: 0.1962  bbox_loss: 0.0266  cls_loss: 0.1019  \n",
      "<<<iteration:[540/657] - total_loss: 0.6238  obj_loss: 0.0825  noobj_loss: 0.2149  bbox_loss: 0.0652  cls_loss: 0.1080  \n",
      "<<<iteration:[560/657] - total_loss: 0.4458  obj_loss: 0.0930  noobj_loss: 0.1948  bbox_loss: 0.0351  cls_loss: 0.0798  \n",
      "<<<iteration:[580/657] - total_loss: 0.4629  obj_loss: 0.0990  noobj_loss: 0.1995  bbox_loss: 0.0333  cls_loss: 0.0976  \n",
      "<<<iteration:[600/657] - total_loss: 0.4951  obj_loss: 0.0894  noobj_loss: 0.2140  bbox_loss: 0.0413  cls_loss: 0.0921  \n",
      "<<<iteration:[620/657] - total_loss: 0.4943  obj_loss: 0.0907  noobj_loss: 0.1858  bbox_loss: 0.0379  cls_loss: 0.1214  \n",
      "<<<iteration:[640/657] - total_loss: 0.4419  obj_loss: 0.0938  noobj_loss: 0.1881  bbox_loss: 0.0299  cls_loss: 0.1046  \n",
      "\n",
      "epoch:39/100 - Train Loss: 0.4635, Val Loss: 0.4757\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4702  obj_loss: 0.0979  noobj_loss: 0.2013  bbox_loss: 0.0373  cls_loss: 0.0853  \n",
      "<<<iteration:[40/657] - total_loss: 0.4271  obj_loss: 0.0939  noobj_loss: 0.1912  bbox_loss: 0.0281  cls_loss: 0.0970  \n",
      "<<<iteration:[60/657] - total_loss: 0.4723  obj_loss: 0.0906  noobj_loss: 0.1961  bbox_loss: 0.0356  cls_loss: 0.1055  \n",
      "<<<iteration:[80/657] - total_loss: 0.4552  obj_loss: 0.1032  noobj_loss: 0.1896  bbox_loss: 0.0321  cls_loss: 0.0967  \n",
      "<<<iteration:[100/657] - total_loss: 0.4538  obj_loss: 0.0999  noobj_loss: 0.1956  bbox_loss: 0.0312  cls_loss: 0.1003  \n",
      "<<<iteration:[120/657] - total_loss: 0.5315  obj_loss: 0.1020  noobj_loss: 0.1947  bbox_loss: 0.0450  cls_loss: 0.1072  \n",
      "<<<iteration:[140/657] - total_loss: 0.4376  obj_loss: 0.0908  noobj_loss: 0.1907  bbox_loss: 0.0316  cls_loss: 0.0933  \n",
      "<<<iteration:[160/657] - total_loss: 0.4404  obj_loss: 0.1055  noobj_loss: 0.1803  bbox_loss: 0.0296  cls_loss: 0.0967  \n",
      "<<<iteration:[180/657] - total_loss: 0.4617  obj_loss: 0.0974  noobj_loss: 0.1868  bbox_loss: 0.0330  cls_loss: 0.1059  \n",
      "<<<iteration:[200/657] - total_loss: 0.4205  obj_loss: 0.0885  noobj_loss: 0.1819  bbox_loss: 0.0297  cls_loss: 0.0925  \n",
      "<<<iteration:[220/657] - total_loss: 0.4223  obj_loss: 0.0976  noobj_loss: 0.1917  bbox_loss: 0.0280  cls_loss: 0.0887  \n",
      "<<<iteration:[240/657] - total_loss: 0.4336  obj_loss: 0.0907  noobj_loss: 0.1880  bbox_loss: 0.0312  cls_loss: 0.0930  \n",
      "<<<iteration:[260/657] - total_loss: 0.4510  obj_loss: 0.1006  noobj_loss: 0.1939  bbox_loss: 0.0322  cls_loss: 0.0922  \n",
      "<<<iteration:[280/657] - total_loss: 0.4650  obj_loss: 0.0901  noobj_loss: 0.1805  bbox_loss: 0.0325  cls_loss: 0.1224  \n",
      "<<<iteration:[300/657] - total_loss: 0.4976  obj_loss: 0.0886  noobj_loss: 0.2001  bbox_loss: 0.0413  cls_loss: 0.1026  \n",
      "<<<iteration:[320/657] - total_loss: 0.4715  obj_loss: 0.1030  noobj_loss: 0.1880  bbox_loss: 0.0345  cls_loss: 0.1021  \n",
      "<<<iteration:[340/657] - total_loss: 0.4595  obj_loss: 0.0986  noobj_loss: 0.1813  bbox_loss: 0.0325  cls_loss: 0.1079  \n",
      "<<<iteration:[360/657] - total_loss: 0.4435  obj_loss: 0.0872  noobj_loss: 0.1828  bbox_loss: 0.0318  cls_loss: 0.1060  \n",
      "<<<iteration:[380/657] - total_loss: 0.4423  obj_loss: 0.0965  noobj_loss: 0.1945  bbox_loss: 0.0301  cls_loss: 0.0982  \n",
      "<<<iteration:[400/657] - total_loss: 0.4391  obj_loss: 0.0941  noobj_loss: 0.1931  bbox_loss: 0.0312  cls_loss: 0.0927  \n",
      "<<<iteration:[420/657] - total_loss: 0.4963  obj_loss: 0.0998  noobj_loss: 0.1948  bbox_loss: 0.0395  cls_loss: 0.1016  \n",
      "<<<iteration:[440/657] - total_loss: 0.4751  obj_loss: 0.0853  noobj_loss: 0.1984  bbox_loss: 0.0374  cls_loss: 0.1038  \n",
      "<<<iteration:[460/657] - total_loss: 0.4132  obj_loss: 0.0978  noobj_loss: 0.1822  bbox_loss: 0.0301  cls_loss: 0.0740  \n",
      "<<<iteration:[480/657] - total_loss: 0.4778  obj_loss: 0.1000  noobj_loss: 0.2079  bbox_loss: 0.0343  cls_loss: 0.1023  \n",
      "<<<iteration:[500/657] - total_loss: 0.4535  obj_loss: 0.0997  noobj_loss: 0.1943  bbox_loss: 0.0343  cls_loss: 0.0852  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[520/657] - total_loss: 0.4536  obj_loss: 0.1021  noobj_loss: 0.1805  bbox_loss: 0.0276  cls_loss: 0.1234  \n",
      "<<<iteration:[540/657] - total_loss: 0.4500  obj_loss: 0.0948  noobj_loss: 0.1910  bbox_loss: 0.0325  cls_loss: 0.0970  \n",
      "<<<iteration:[560/657] - total_loss: 0.4855  obj_loss: 0.0875  noobj_loss: 0.1998  bbox_loss: 0.0380  cls_loss: 0.1081  \n",
      "<<<iteration:[580/657] - total_loss: 0.4787  obj_loss: 0.0942  noobj_loss: 0.1883  bbox_loss: 0.0372  cls_loss: 0.1042  \n",
      "<<<iteration:[600/657] - total_loss: 0.4405  obj_loss: 0.0949  noobj_loss: 0.1847  bbox_loss: 0.0282  cls_loss: 0.1121  \n",
      "<<<iteration:[620/657] - total_loss: 0.4324  obj_loss: 0.1010  noobj_loss: 0.1984  bbox_loss: 0.0287  cls_loss: 0.0886  \n",
      "<<<iteration:[640/657] - total_loss: 0.4112  obj_loss: 0.0861  noobj_loss: 0.1788  bbox_loss: 0.0297  cls_loss: 0.0872  \n",
      "\n",
      "epoch:40/100 - Train Loss: 0.4547, Val Loss: 0.4777\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4570  obj_loss: 0.1036  noobj_loss: 0.1933  bbox_loss: 0.0316  cls_loss: 0.0985  \n",
      "<<<iteration:[40/657] - total_loss: 0.4747  obj_loss: 0.0959  noobj_loss: 0.1974  bbox_loss: 0.0332  cls_loss: 0.1140  \n",
      "<<<iteration:[60/657] - total_loss: 0.4531  obj_loss: 0.0988  noobj_loss: 0.1840  bbox_loss: 0.0306  cls_loss: 0.1094  \n",
      "<<<iteration:[80/657] - total_loss: 0.4936  obj_loss: 0.0966  noobj_loss: 0.1760  bbox_loss: 0.0397  cls_loss: 0.1107  \n",
      "<<<iteration:[100/657] - total_loss: 0.4802  obj_loss: 0.0980  noobj_loss: 0.1830  bbox_loss: 0.0362  cls_loss: 0.1097  \n",
      "<<<iteration:[120/657] - total_loss: 0.4622  obj_loss: 0.0979  noobj_loss: 0.2054  bbox_loss: 0.0327  cls_loss: 0.0979  \n",
      "<<<iteration:[140/657] - total_loss: 0.4425  obj_loss: 0.0970  noobj_loss: 0.1825  bbox_loss: 0.0320  cls_loss: 0.0940  \n",
      "<<<iteration:[160/657] - total_loss: 0.4461  obj_loss: 0.0940  noobj_loss: 0.1789  bbox_loss: 0.0336  cls_loss: 0.0945  \n",
      "<<<iteration:[180/657] - total_loss: 0.4311  obj_loss: 0.0906  noobj_loss: 0.1807  bbox_loss: 0.0305  cls_loss: 0.0975  \n",
      "<<<iteration:[200/657] - total_loss: 0.4902  obj_loss: 0.0980  noobj_loss: 0.2028  bbox_loss: 0.0389  cls_loss: 0.0962  \n",
      "<<<iteration:[220/657] - total_loss: 0.4375  obj_loss: 0.1056  noobj_loss: 0.1918  bbox_loss: 0.0305  cls_loss: 0.0833  \n",
      "<<<iteration:[240/657] - total_loss: 0.4122  obj_loss: 0.0974  noobj_loss: 0.1937  bbox_loss: 0.0293  cls_loss: 0.0714  \n",
      "<<<iteration:[260/657] - total_loss: 0.4631  obj_loss: 0.0963  noobj_loss: 0.1834  bbox_loss: 0.0362  cls_loss: 0.0939  \n",
      "<<<iteration:[280/657] - total_loss: 0.4277  obj_loss: 0.0997  noobj_loss: 0.1851  bbox_loss: 0.0296  cls_loss: 0.0874  \n",
      "<<<iteration:[300/657] - total_loss: 0.4606  obj_loss: 0.1053  noobj_loss: 0.2031  bbox_loss: 0.0314  cls_loss: 0.0967  \n",
      "<<<iteration:[320/657] - total_loss: 0.4829  obj_loss: 0.1000  noobj_loss: 0.1802  bbox_loss: 0.0331  cls_loss: 0.1272  \n",
      "<<<iteration:[340/657] - total_loss: 0.4396  obj_loss: 0.0918  noobj_loss: 0.1878  bbox_loss: 0.0312  cls_loss: 0.0977  \n",
      "<<<iteration:[360/657] - total_loss: 0.4809  obj_loss: 0.0823  noobj_loss: 0.1821  bbox_loss: 0.0413  cls_loss: 0.1009  \n",
      "<<<iteration:[380/657] - total_loss: 0.4124  obj_loss: 0.0913  noobj_loss: 0.1798  bbox_loss: 0.0286  cls_loss: 0.0885  \n",
      "<<<iteration:[400/657] - total_loss: 0.4663  obj_loss: 0.1067  noobj_loss: 0.1795  bbox_loss: 0.0337  cls_loss: 0.1016  \n",
      "<<<iteration:[420/657] - total_loss: 0.4799  obj_loss: 0.0930  noobj_loss: 0.2088  bbox_loss: 0.0360  cls_loss: 0.1024  \n",
      "<<<iteration:[440/657] - total_loss: 0.4477  obj_loss: 0.0859  noobj_loss: 0.1990  bbox_loss: 0.0318  cls_loss: 0.1030  \n",
      "<<<iteration:[460/657] - total_loss: 0.4495  obj_loss: 0.0957  noobj_loss: 0.1800  bbox_loss: 0.0309  cls_loss: 0.1091  \n",
      "<<<iteration:[480/657] - total_loss: 0.4618  obj_loss: 0.0936  noobj_loss: 0.1818  bbox_loss: 0.0367  cls_loss: 0.0938  \n",
      "<<<iteration:[500/657] - total_loss: 0.4805  obj_loss: 0.0956  noobj_loss: 0.1810  bbox_loss: 0.0374  cls_loss: 0.1075  \n",
      "<<<iteration:[520/657] - total_loss: 0.4371  obj_loss: 0.0923  noobj_loss: 0.1916  bbox_loss: 0.0307  cls_loss: 0.0954  \n",
      "<<<iteration:[540/657] - total_loss: 0.4367  obj_loss: 0.0887  noobj_loss: 0.1967  bbox_loss: 0.0319  cls_loss: 0.0902  \n",
      "<<<iteration:[560/657] - total_loss: 0.4326  obj_loss: 0.0963  noobj_loss: 0.1897  bbox_loss: 0.0302  cls_loss: 0.0907  \n",
      "<<<iteration:[580/657] - total_loss: 0.4411  obj_loss: 0.0976  noobj_loss: 0.1862  bbox_loss: 0.0285  cls_loss: 0.1078  \n",
      "<<<iteration:[600/657] - total_loss: 0.4411  obj_loss: 0.0874  noobj_loss: 0.1882  bbox_loss: 0.0326  cls_loss: 0.0967  \n",
      "<<<iteration:[620/657] - total_loss: 0.4444  obj_loss: 0.0886  noobj_loss: 0.1831  bbox_loss: 0.0358  cls_loss: 0.0852  \n",
      "<<<iteration:[640/657] - total_loss: 0.4505  obj_loss: 0.0968  noobj_loss: 0.1845  bbox_loss: 0.0314  cls_loss: 0.1045  \n",
      "\n",
      "epoch:41/100 - Train Loss: 0.4530, Val Loss: 0.4672\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4855  obj_loss: 0.0991  noobj_loss: 0.1953  bbox_loss: 0.0355  cls_loss: 0.1110  \n",
      "<<<iteration:[40/657] - total_loss: 0.4513  obj_loss: 0.0861  noobj_loss: 0.1841  bbox_loss: 0.0356  cls_loss: 0.0953  \n",
      "<<<iteration:[60/657] - total_loss: 0.4413  obj_loss: 0.0997  noobj_loss: 0.1852  bbox_loss: 0.0288  cls_loss: 0.1052  \n",
      "<<<iteration:[80/657] - total_loss: 0.4212  obj_loss: 0.0976  noobj_loss: 0.1856  bbox_loss: 0.0298  cls_loss: 0.0819  \n",
      "<<<iteration:[100/657] - total_loss: 0.4344  obj_loss: 0.0918  noobj_loss: 0.1806  bbox_loss: 0.0327  cls_loss: 0.0889  \n",
      "<<<iteration:[120/657] - total_loss: 0.4388  obj_loss: 0.0994  noobj_loss: 0.1869  bbox_loss: 0.0321  cls_loss: 0.0855  \n",
      "<<<iteration:[140/657] - total_loss: 0.4448  obj_loss: 0.1056  noobj_loss: 0.1844  bbox_loss: 0.0326  cls_loss: 0.0841  \n",
      "<<<iteration:[160/657] - total_loss: 0.4361  obj_loss: 0.0896  noobj_loss: 0.1905  bbox_loss: 0.0332  cls_loss: 0.0853  \n",
      "<<<iteration:[180/657] - total_loss: 0.4566  obj_loss: 0.1024  noobj_loss: 0.1816  bbox_loss: 0.0338  cls_loss: 0.0946  \n",
      "<<<iteration:[200/657] - total_loss: 0.4416  obj_loss: 0.0966  noobj_loss: 0.1794  bbox_loss: 0.0342  cls_loss: 0.0844  \n",
      "<<<iteration:[220/657] - total_loss: 0.4672  obj_loss: 0.0928  noobj_loss: 0.1947  bbox_loss: 0.0337  cls_loss: 0.1087  \n",
      "<<<iteration:[240/657] - total_loss: 0.4386  obj_loss: 0.0908  noobj_loss: 0.1796  bbox_loss: 0.0310  cls_loss: 0.1031  \n",
      "<<<iteration:[260/657] - total_loss: 0.4456  obj_loss: 0.0885  noobj_loss: 0.1778  bbox_loss: 0.0322  cls_loss: 0.1074  \n",
      "<<<iteration:[280/657] - total_loss: 0.4403  obj_loss: 0.0917  noobj_loss: 0.1801  bbox_loss: 0.0328  cls_loss: 0.0947  \n",
      "<<<iteration:[300/657] - total_loss: 0.4210  obj_loss: 0.1034  noobj_loss: 0.1777  bbox_loss: 0.0279  cls_loss: 0.0895  \n",
      "<<<iteration:[320/657] - total_loss: 0.4249  obj_loss: 0.0929  noobj_loss: 0.1787  bbox_loss: 0.0305  cls_loss: 0.0901  \n",
      "<<<iteration:[340/657] - total_loss: 0.4253  obj_loss: 0.0942  noobj_loss: 0.1871  bbox_loss: 0.0289  cls_loss: 0.0928  \n",
      "<<<iteration:[360/657] - total_loss: 0.4441  obj_loss: 0.0955  noobj_loss: 0.1755  bbox_loss: 0.0338  cls_loss: 0.0918  \n",
      "<<<iteration:[380/657] - total_loss: 0.4415  obj_loss: 0.1116  noobj_loss: 0.1862  bbox_loss: 0.0302  cls_loss: 0.0859  \n",
      "<<<iteration:[400/657] - total_loss: 0.4304  obj_loss: 0.0913  noobj_loss: 0.1770  bbox_loss: 0.0311  cls_loss: 0.0950  \n",
      "<<<iteration:[420/657] - total_loss: 0.4457  obj_loss: 0.0989  noobj_loss: 0.1827  bbox_loss: 0.0316  cls_loss: 0.0973  \n",
      "<<<iteration:[440/657] - total_loss: 0.4267  obj_loss: 0.0901  noobj_loss: 0.1818  bbox_loss: 0.0290  cls_loss: 0.1005  \n",
      "<<<iteration:[460/657] - total_loss: 0.6040  obj_loss: 0.0884  noobj_loss: 0.1808  bbox_loss: 0.0610  cls_loss: 0.1201  \n",
      "<<<iteration:[480/657] - total_loss: 0.4630  obj_loss: 0.0936  noobj_loss: 0.1881  bbox_loss: 0.0339  cls_loss: 0.1056  \n",
      "<<<iteration:[500/657] - total_loss: 0.4802  obj_loss: 0.0916  noobj_loss: 0.1854  bbox_loss: 0.0369  cls_loss: 0.1113  \n",
      "<<<iteration:[520/657] - total_loss: 0.4804  obj_loss: 0.1074  noobj_loss: 0.1824  bbox_loss: 0.0365  cls_loss: 0.0994  \n",
      "<<<iteration:[540/657] - total_loss: 0.4324  obj_loss: 0.0930  noobj_loss: 0.1901  bbox_loss: 0.0305  cls_loss: 0.0918  \n",
      "<<<iteration:[560/657] - total_loss: 0.4482  obj_loss: 0.1074  noobj_loss: 0.1775  bbox_loss: 0.0326  cls_loss: 0.0889  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[580/657] - total_loss: 0.4221  obj_loss: 0.0961  noobj_loss: 0.1680  bbox_loss: 0.0301  cls_loss: 0.0913  \n",
      "<<<iteration:[600/657] - total_loss: 0.4674  obj_loss: 0.0769  noobj_loss: 0.1751  bbox_loss: 0.0384  cls_loss: 0.1110  \n",
      "<<<iteration:[620/657] - total_loss: 0.4289  obj_loss: 0.0995  noobj_loss: 0.1813  bbox_loss: 0.0319  cls_loss: 0.0794  \n",
      "<<<iteration:[640/657] - total_loss: 0.4492  obj_loss: 0.1009  noobj_loss: 0.1867  bbox_loss: 0.0321  cls_loss: 0.0946  \n",
      "\n",
      "epoch:42/100 - Train Loss: 0.4489, Val Loss: 0.5004\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4889  obj_loss: 0.0990  noobj_loss: 0.1889  bbox_loss: 0.0376  cls_loss: 0.1078  \n",
      "<<<iteration:[40/657] - total_loss: 0.4604  obj_loss: 0.1081  noobj_loss: 0.1745  bbox_loss: 0.0330  cls_loss: 0.1003  \n",
      "<<<iteration:[60/657] - total_loss: 0.6674  obj_loss: 0.0862  noobj_loss: 0.1930  bbox_loss: 0.0749  cls_loss: 0.1103  \n",
      "<<<iteration:[80/657] - total_loss: 0.5192  obj_loss: 0.0749  noobj_loss: 0.1770  bbox_loss: 0.0519  cls_loss: 0.0961  \n",
      "<<<iteration:[100/657] - total_loss: 0.4482  obj_loss: 0.0860  noobj_loss: 0.1759  bbox_loss: 0.0350  cls_loss: 0.0992  \n",
      "<<<iteration:[120/657] - total_loss: 0.5077  obj_loss: 0.0943  noobj_loss: 0.1873  bbox_loss: 0.0457  cls_loss: 0.0914  \n",
      "<<<iteration:[140/657] - total_loss: 0.4512  obj_loss: 0.0955  noobj_loss: 0.1740  bbox_loss: 0.0344  cls_loss: 0.0969  \n",
      "<<<iteration:[160/657] - total_loss: 0.4436  obj_loss: 0.1055  noobj_loss: 0.1777  bbox_loss: 0.0339  cls_loss: 0.0797  \n",
      "<<<iteration:[180/657] - total_loss: 0.4449  obj_loss: 0.1025  noobj_loss: 0.1851  bbox_loss: 0.0282  cls_loss: 0.1089  \n",
      "<<<iteration:[200/657] - total_loss: 0.4381  obj_loss: 0.1046  noobj_loss: 0.1812  bbox_loss: 0.0287  cls_loss: 0.0995  \n",
      "<<<iteration:[220/657] - total_loss: 0.4564  obj_loss: 0.1044  noobj_loss: 0.1783  bbox_loss: 0.0348  cls_loss: 0.0887  \n",
      "<<<iteration:[240/657] - total_loss: 0.4406  obj_loss: 0.1017  noobj_loss: 0.1748  bbox_loss: 0.0306  cls_loss: 0.0986  \n",
      "<<<iteration:[260/657] - total_loss: 0.4286  obj_loss: 0.0945  noobj_loss: 0.1801  bbox_loss: 0.0320  cls_loss: 0.0839  \n",
      "<<<iteration:[280/657] - total_loss: 0.4557  obj_loss: 0.1013  noobj_loss: 0.1814  bbox_loss: 0.0317  cls_loss: 0.1051  \n",
      "<<<iteration:[300/657] - total_loss: 0.4285  obj_loss: 0.1083  noobj_loss: 0.1763  bbox_loss: 0.0285  cls_loss: 0.0898  \n",
      "<<<iteration:[320/657] - total_loss: 0.4722  obj_loss: 0.0961  noobj_loss: 0.1746  bbox_loss: 0.0395  cls_loss: 0.0911  \n",
      "<<<iteration:[340/657] - total_loss: 0.4475  obj_loss: 0.0920  noobj_loss: 0.1714  bbox_loss: 0.0355  cls_loss: 0.0922  \n",
      "<<<iteration:[360/657] - total_loss: 0.4097  obj_loss: 0.0929  noobj_loss: 0.1764  bbox_loss: 0.0308  cls_loss: 0.0746  \n",
      "<<<iteration:[380/657] - total_loss: 0.4402  obj_loss: 0.0944  noobj_loss: 0.1845  bbox_loss: 0.0312  cls_loss: 0.0978  \n",
      "<<<iteration:[400/657] - total_loss: 0.4394  obj_loss: 0.0960  noobj_loss: 0.1786  bbox_loss: 0.0321  cls_loss: 0.0937  \n",
      "<<<iteration:[420/657] - total_loss: 0.4743  obj_loss: 0.1050  noobj_loss: 0.1933  bbox_loss: 0.0321  cls_loss: 0.1122  \n",
      "<<<iteration:[440/657] - total_loss: 0.4418  obj_loss: 0.0962  noobj_loss: 0.1865  bbox_loss: 0.0313  cls_loss: 0.0960  \n",
      "<<<iteration:[460/657] - total_loss: 0.4210  obj_loss: 0.0969  noobj_loss: 0.1759  bbox_loss: 0.0301  cls_loss: 0.0857  \n",
      "<<<iteration:[480/657] - total_loss: 0.4426  obj_loss: 0.1057  noobj_loss: 0.1789  bbox_loss: 0.0304  cls_loss: 0.0957  \n",
      "<<<iteration:[500/657] - total_loss: 0.4174  obj_loss: 0.0911  noobj_loss: 0.1782  bbox_loss: 0.0296  cls_loss: 0.0890  \n",
      "<<<iteration:[520/657] - total_loss: 0.4559  obj_loss: 0.0955  noobj_loss: 0.1748  bbox_loss: 0.0303  cls_loss: 0.1214  \n",
      "<<<iteration:[540/657] - total_loss: 0.4877  obj_loss: 0.0829  noobj_loss: 0.1859  bbox_loss: 0.0466  cls_loss: 0.0787  \n",
      "<<<iteration:[560/657] - total_loss: 0.4553  obj_loss: 0.0975  noobj_loss: 0.1734  bbox_loss: 0.0354  cls_loss: 0.0940  \n",
      "<<<iteration:[580/657] - total_loss: 0.4945  obj_loss: 0.0912  noobj_loss: 0.1795  bbox_loss: 0.0435  cls_loss: 0.0962  \n",
      "<<<iteration:[600/657] - total_loss: 0.4662  obj_loss: 0.0813  noobj_loss: 0.1763  bbox_loss: 0.0386  cls_loss: 0.1037  \n",
      "<<<iteration:[620/657] - total_loss: 0.4622  obj_loss: 0.0955  noobj_loss: 0.1863  bbox_loss: 0.0329  cls_loss: 0.1091  \n",
      "<<<iteration:[640/657] - total_loss: 0.4302  obj_loss: 0.0952  noobj_loss: 0.1761  bbox_loss: 0.0301  cls_loss: 0.0962  \n",
      "\n",
      "epoch:43/100 - Train Loss: 0.4594, Val Loss: 0.4526\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4346  obj_loss: 0.0949  noobj_loss: 0.1834  bbox_loss: 0.0309  cls_loss: 0.0937  \n",
      "<<<iteration:[40/657] - total_loss: 0.4531  obj_loss: 0.1027  noobj_loss: 0.1818  bbox_loss: 0.0316  cls_loss: 0.1014  \n",
      "<<<iteration:[60/657] - total_loss: 0.4370  obj_loss: 0.1048  noobj_loss: 0.1809  bbox_loss: 0.0297  cls_loss: 0.0935  \n",
      "<<<iteration:[80/657] - total_loss: 0.4407  obj_loss: 0.0991  noobj_loss: 0.1732  bbox_loss: 0.0313  cls_loss: 0.0984  \n",
      "<<<iteration:[100/657] - total_loss: 0.4505  obj_loss: 0.1101  noobj_loss: 0.1820  bbox_loss: 0.0309  cls_loss: 0.0949  \n",
      "<<<iteration:[120/657] - total_loss: 0.4458  obj_loss: 0.1011  noobj_loss: 0.1846  bbox_loss: 0.0301  cls_loss: 0.1017  \n",
      "<<<iteration:[140/657] - total_loss: 0.4461  obj_loss: 0.0967  noobj_loss: 0.1701  bbox_loss: 0.0323  cls_loss: 0.1028  \n",
      "<<<iteration:[160/657] - total_loss: 0.4406  obj_loss: 0.0974  noobj_loss: 0.1834  bbox_loss: 0.0296  cls_loss: 0.1033  \n",
      "<<<iteration:[180/657] - total_loss: 0.4129  obj_loss: 0.0991  noobj_loss: 0.1733  bbox_loss: 0.0275  cls_loss: 0.0895  \n",
      "<<<iteration:[200/657] - total_loss: 0.4198  obj_loss: 0.0949  noobj_loss: 0.1767  bbox_loss: 0.0288  cls_loss: 0.0928  \n",
      "<<<iteration:[220/657] - total_loss: 0.4310  obj_loss: 0.1037  noobj_loss: 0.1713  bbox_loss: 0.0281  cls_loss: 0.1013  \n",
      "<<<iteration:[240/657] - total_loss: 0.4740  obj_loss: 0.0852  noobj_loss: 0.1784  bbox_loss: 0.0418  cls_loss: 0.0909  \n",
      "<<<iteration:[260/657] - total_loss: 0.7517  obj_loss: 0.0866  noobj_loss: 0.1746  bbox_loss: 0.1006  cls_loss: 0.0748  \n",
      "<<<iteration:[280/657] - total_loss: 0.6272  obj_loss: 0.0819  noobj_loss: 0.1807  bbox_loss: 0.0733  cls_loss: 0.0884  \n",
      "<<<iteration:[300/657] - total_loss: 0.4508  obj_loss: 0.0981  noobj_loss: 0.1704  bbox_loss: 0.0352  cls_loss: 0.0917  \n",
      "<<<iteration:[320/657] - total_loss: 0.4097  obj_loss: 0.0975  noobj_loss: 0.1787  bbox_loss: 0.0300  cls_loss: 0.0729  \n",
      "<<<iteration:[340/657] - total_loss: 0.4916  obj_loss: 0.0921  noobj_loss: 0.1822  bbox_loss: 0.0429  cls_loss: 0.0937  \n",
      "<<<iteration:[360/657] - total_loss: 0.4536  obj_loss: 0.0931  noobj_loss: 0.1789  bbox_loss: 0.0345  cls_loss: 0.0986  \n",
      "<<<iteration:[380/657] - total_loss: 0.4234  obj_loss: 0.0872  noobj_loss: 0.1697  bbox_loss: 0.0296  cls_loss: 0.1037  \n",
      "<<<iteration:[400/657] - total_loss: 0.4122  obj_loss: 0.1027  noobj_loss: 0.1697  bbox_loss: 0.0270  cls_loss: 0.0896  \n",
      "<<<iteration:[420/657] - total_loss: 0.4382  obj_loss: 0.1010  noobj_loss: 0.1881  bbox_loss: 0.0296  cls_loss: 0.0954  \n",
      "<<<iteration:[440/657] - total_loss: 0.4511  obj_loss: 0.0982  noobj_loss: 0.1857  bbox_loss: 0.0345  cls_loss: 0.0875  \n",
      "<<<iteration:[460/657] - total_loss: 0.4643  obj_loss: 0.1050  noobj_loss: 0.1795  bbox_loss: 0.0347  cls_loss: 0.0960  \n",
      "<<<iteration:[480/657] - total_loss: 0.4367  obj_loss: 0.0999  noobj_loss: 0.1850  bbox_loss: 0.0308  cls_loss: 0.0901  \n",
      "<<<iteration:[500/657] - total_loss: 0.4554  obj_loss: 0.0965  noobj_loss: 0.1801  bbox_loss: 0.0301  cls_loss: 0.1183  \n",
      "<<<iteration:[520/657] - total_loss: 0.4279  obj_loss: 0.1020  noobj_loss: 0.1816  bbox_loss: 0.0321  cls_loss: 0.0746  \n",
      "<<<iteration:[540/657] - total_loss: 0.4553  obj_loss: 0.0954  noobj_loss: 0.1711  bbox_loss: 0.0282  cls_loss: 0.1336  \n",
      "<<<iteration:[560/657] - total_loss: 0.4197  obj_loss: 0.0957  noobj_loss: 0.1685  bbox_loss: 0.0294  cls_loss: 0.0926  \n",
      "<<<iteration:[580/657] - total_loss: 0.4572  obj_loss: 0.1023  noobj_loss: 0.1765  bbox_loss: 0.0319  cls_loss: 0.1071  \n",
      "<<<iteration:[600/657] - total_loss: 0.4537  obj_loss: 0.0964  noobj_loss: 0.1709  bbox_loss: 0.0334  cls_loss: 0.1048  \n",
      "<<<iteration:[620/657] - total_loss: 0.4333  obj_loss: 0.0970  noobj_loss: 0.1886  bbox_loss: 0.0312  cls_loss: 0.0858  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[640/657] - total_loss: 0.4200  obj_loss: 0.0999  noobj_loss: 0.1873  bbox_loss: 0.0287  cls_loss: 0.0832  \n",
      "\n",
      "epoch:44/100 - Train Loss: 0.4552, Val Loss: 0.4474\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.5438  obj_loss: 0.0957  noobj_loss: 0.1837  bbox_loss: 0.0499  cls_loss: 0.1066  \n",
      "<<<iteration:[40/657] - total_loss: 0.4286  obj_loss: 0.0970  noobj_loss: 0.1778  bbox_loss: 0.0299  cls_loss: 0.0935  \n",
      "<<<iteration:[60/657] - total_loss: 0.4518  obj_loss: 0.1127  noobj_loss: 0.1757  bbox_loss: 0.0315  cls_loss: 0.0936  \n",
      "<<<iteration:[80/657] - total_loss: 0.4725  obj_loss: 0.0984  noobj_loss: 0.1755  bbox_loss: 0.0405  cls_loss: 0.0837  \n",
      "<<<iteration:[100/657] - total_loss: 0.4685  obj_loss: 0.1040  noobj_loss: 0.1784  bbox_loss: 0.0314  cls_loss: 0.1185  \n",
      "<<<iteration:[120/657] - total_loss: 0.4247  obj_loss: 0.0931  noobj_loss: 0.1695  bbox_loss: 0.0301  cls_loss: 0.0963  \n",
      "<<<iteration:[140/657] - total_loss: 0.3895  obj_loss: 0.0910  noobj_loss: 0.1782  bbox_loss: 0.0275  cls_loss: 0.0719  \n",
      "<<<iteration:[160/657] - total_loss: 0.4324  obj_loss: 0.1053  noobj_loss: 0.1776  bbox_loss: 0.0285  cls_loss: 0.0957  \n",
      "<<<iteration:[180/657] - total_loss: 0.4536  obj_loss: 0.0998  noobj_loss: 0.1798  bbox_loss: 0.0339  cls_loss: 0.0943  \n",
      "<<<iteration:[200/657] - total_loss: 0.4265  obj_loss: 0.0891  noobj_loss: 0.1810  bbox_loss: 0.0319  cls_loss: 0.0874  \n",
      "<<<iteration:[220/657] - total_loss: 0.4366  obj_loss: 0.1040  noobj_loss: 0.1654  bbox_loss: 0.0332  cls_loss: 0.0839  \n",
      "<<<iteration:[240/657] - total_loss: 0.4456  obj_loss: 0.0867  noobj_loss: 0.1637  bbox_loss: 0.0376  cls_loss: 0.0891  \n",
      "<<<iteration:[260/657] - total_loss: 0.4348  obj_loss: 0.0973  noobj_loss: 0.1856  bbox_loss: 0.0303  cls_loss: 0.0933  \n",
      "<<<iteration:[280/657] - total_loss: 0.4229  obj_loss: 0.1006  noobj_loss: 0.1718  bbox_loss: 0.0275  cls_loss: 0.0991  \n",
      "<<<iteration:[300/657] - total_loss: 0.4631  obj_loss: 0.0890  noobj_loss: 0.1763  bbox_loss: 0.0320  cls_loss: 0.1259  \n",
      "<<<iteration:[320/657] - total_loss: 0.4581  obj_loss: 0.0983  noobj_loss: 0.1794  bbox_loss: 0.0318  cls_loss: 0.1112  \n",
      "<<<iteration:[340/657] - total_loss: 0.4155  obj_loss: 0.1053  noobj_loss: 0.1688  bbox_loss: 0.0282  cls_loss: 0.0850  \n",
      "<<<iteration:[360/657] - total_loss: 0.4448  obj_loss: 0.1005  noobj_loss: 0.1772  bbox_loss: 0.0307  cls_loss: 0.1023  \n",
      "<<<iteration:[380/657] - total_loss: 0.4107  obj_loss: 0.1012  noobj_loss: 0.1731  bbox_loss: 0.0279  cls_loss: 0.0834  \n",
      "<<<iteration:[400/657] - total_loss: 0.4432  obj_loss: 0.1042  noobj_loss: 0.1712  bbox_loss: 0.0285  cls_loss: 0.1108  \n",
      "<<<iteration:[420/657] - total_loss: 0.4466  obj_loss: 0.1027  noobj_loss: 0.1700  bbox_loss: 0.0297  cls_loss: 0.1101  \n",
      "<<<iteration:[440/657] - total_loss: 0.4521  obj_loss: 0.1006  noobj_loss: 0.1850  bbox_loss: 0.0340  cls_loss: 0.0892  \n",
      "<<<iteration:[460/657] - total_loss: 0.4199  obj_loss: 0.1013  noobj_loss: 0.1824  bbox_loss: 0.0265  cls_loss: 0.0949  \n",
      "<<<iteration:[480/657] - total_loss: 0.4318  obj_loss: 0.0994  noobj_loss: 0.1770  bbox_loss: 0.0301  cls_loss: 0.0933  \n",
      "<<<iteration:[500/657] - total_loss: 0.4544  obj_loss: 0.0913  noobj_loss: 0.1772  bbox_loss: 0.0376  cls_loss: 0.0866  \n",
      "<<<iteration:[520/657] - total_loss: 0.4256  obj_loss: 0.1157  noobj_loss: 0.1774  bbox_loss: 0.0282  cls_loss: 0.0799  \n",
      "<<<iteration:[540/657] - total_loss: 0.4505  obj_loss: 0.0915  noobj_loss: 0.1784  bbox_loss: 0.0343  cls_loss: 0.0985  \n",
      "<<<iteration:[560/657] - total_loss: 0.4458  obj_loss: 0.0917  noobj_loss: 0.1727  bbox_loss: 0.0327  cls_loss: 0.1043  \n",
      "<<<iteration:[580/657] - total_loss: 0.4270  obj_loss: 0.0998  noobj_loss: 0.1743  bbox_loss: 0.0328  cls_loss: 0.0761  \n",
      "<<<iteration:[600/657] - total_loss: 0.4083  obj_loss: 0.0949  noobj_loss: 0.1729  bbox_loss: 0.0287  cls_loss: 0.0836  \n",
      "<<<iteration:[620/657] - total_loss: 0.4158  obj_loss: 0.0909  noobj_loss: 0.1700  bbox_loss: 0.0293  cls_loss: 0.0934  \n",
      "<<<iteration:[640/657] - total_loss: 0.4111  obj_loss: 0.0958  noobj_loss: 0.1839  bbox_loss: 0.0288  cls_loss: 0.0792  \n",
      "\n",
      "epoch:45/100 - Train Loss: 0.4380, Val Loss: 0.4488\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4765  obj_loss: 0.1091  noobj_loss: 0.1804  bbox_loss: 0.0322  cls_loss: 0.1163  \n",
      "<<<iteration:[40/657] - total_loss: 0.4089  obj_loss: 0.0947  noobj_loss: 0.1725  bbox_loss: 0.0292  cls_loss: 0.0822  \n",
      "<<<iteration:[60/657] - total_loss: 0.4049  obj_loss: 0.0945  noobj_loss: 0.1732  bbox_loss: 0.0280  cls_loss: 0.0840  \n",
      "<<<iteration:[80/657] - total_loss: 0.4435  obj_loss: 0.0970  noobj_loss: 0.1665  bbox_loss: 0.0342  cls_loss: 0.0925  \n",
      "<<<iteration:[100/657] - total_loss: 0.4426  obj_loss: 0.0823  noobj_loss: 0.1717  bbox_loss: 0.0348  cls_loss: 0.1005  \n",
      "<<<iteration:[120/657] - total_loss: 0.4667  obj_loss: 0.0954  noobj_loss: 0.1785  bbox_loss: 0.0369  cls_loss: 0.0977  \n",
      "<<<iteration:[140/657] - total_loss: 0.4601  obj_loss: 0.0978  noobj_loss: 0.1689  bbox_loss: 0.0344  cls_loss: 0.1059  \n",
      "<<<iteration:[160/657] - total_loss: 0.4349  obj_loss: 0.0946  noobj_loss: 0.1707  bbox_loss: 0.0348  cls_loss: 0.0812  \n",
      "<<<iteration:[180/657] - total_loss: 0.4108  obj_loss: 0.0938  noobj_loss: 0.1628  bbox_loss: 0.0319  cls_loss: 0.0762  \n",
      "<<<iteration:[200/657] - total_loss: 0.4387  obj_loss: 0.1046  noobj_loss: 0.1695  bbox_loss: 0.0317  cls_loss: 0.0909  \n",
      "<<<iteration:[220/657] - total_loss: 0.4301  obj_loss: 0.0978  noobj_loss: 0.1746  bbox_loss: 0.0304  cls_loss: 0.0928  \n",
      "<<<iteration:[240/657] - total_loss: 0.4398  obj_loss: 0.1044  noobj_loss: 0.1653  bbox_loss: 0.0322  cls_loss: 0.0915  \n",
      "<<<iteration:[260/657] - total_loss: 0.4073  obj_loss: 0.1035  noobj_loss: 0.1703  bbox_loss: 0.0270  cls_loss: 0.0835  \n",
      "<<<iteration:[280/657] - total_loss: 0.4450  obj_loss: 0.0939  noobj_loss: 0.1781  bbox_loss: 0.0369  cls_loss: 0.0777  \n",
      "<<<iteration:[300/657] - total_loss: 0.4203  obj_loss: 0.0987  noobj_loss: 0.1731  bbox_loss: 0.0307  cls_loss: 0.0817  \n",
      "<<<iteration:[320/657] - total_loss: 0.4302  obj_loss: 0.1003  noobj_loss: 0.1962  bbox_loss: 0.0291  cls_loss: 0.0860  \n",
      "<<<iteration:[340/657] - total_loss: 0.4162  obj_loss: 0.1067  noobj_loss: 0.1770  bbox_loss: 0.0274  cls_loss: 0.0839  \n",
      "<<<iteration:[360/657] - total_loss: 0.4199  obj_loss: 0.0995  noobj_loss: 0.1741  bbox_loss: 0.0279  cls_loss: 0.0940  \n",
      "<<<iteration:[380/657] - total_loss: 0.4228  obj_loss: 0.0894  noobj_loss: 0.1788  bbox_loss: 0.0309  cls_loss: 0.0897  \n",
      "<<<iteration:[400/657] - total_loss: 0.4308  obj_loss: 0.0997  noobj_loss: 0.1666  bbox_loss: 0.0283  cls_loss: 0.1065  \n",
      "<<<iteration:[420/657] - total_loss: 0.4026  obj_loss: 0.1001  noobj_loss: 0.1740  bbox_loss: 0.0283  cls_loss: 0.0741  \n",
      "<<<iteration:[440/657] - total_loss: 0.4312  obj_loss: 0.1023  noobj_loss: 0.1785  bbox_loss: 0.0289  cls_loss: 0.0949  \n",
      "<<<iteration:[460/657] - total_loss: 0.4344  obj_loss: 0.1036  noobj_loss: 0.1661  bbox_loss: 0.0290  cls_loss: 0.1028  \n",
      "<<<iteration:[480/657] - total_loss: 0.4213  obj_loss: 0.1112  noobj_loss: 0.1740  bbox_loss: 0.0277  cls_loss: 0.0845  \n",
      "<<<iteration:[500/657] - total_loss: 0.4136  obj_loss: 0.0937  noobj_loss: 0.1664  bbox_loss: 0.0295  cls_loss: 0.0890  \n",
      "<<<iteration:[520/657] - total_loss: 0.4343  obj_loss: 0.1094  noobj_loss: 0.1697  bbox_loss: 0.0263  cls_loss: 0.1087  \n",
      "<<<iteration:[540/657] - total_loss: 0.4109  obj_loss: 0.0882  noobj_loss: 0.1695  bbox_loss: 0.0293  cls_loss: 0.0916  \n",
      "<<<iteration:[560/657] - total_loss: 0.4338  obj_loss: 0.1088  noobj_loss: 0.1831  bbox_loss: 0.0271  cls_loss: 0.0978  \n",
      "<<<iteration:[580/657] - total_loss: 0.4601  obj_loss: 0.0979  noobj_loss: 0.1792  bbox_loss: 0.0347  cls_loss: 0.0990  \n",
      "<<<iteration:[600/657] - total_loss: 0.4277  obj_loss: 0.0937  noobj_loss: 0.1748  bbox_loss: 0.0337  cls_loss: 0.0780  \n",
      "<<<iteration:[620/657] - total_loss: 0.4378  obj_loss: 0.0985  noobj_loss: 0.1763  bbox_loss: 0.0287  cls_loss: 0.1074  \n",
      "<<<iteration:[640/657] - total_loss: 0.4181  obj_loss: 0.0945  noobj_loss: 0.1619  bbox_loss: 0.0284  cls_loss: 0.1007  \n",
      "\n",
      "epoch:46/100 - Train Loss: 0.4296, Val Loss: 0.4482\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4811  obj_loss: 0.0996  noobj_loss: 0.1785  bbox_loss: 0.0416  cls_loss: 0.0842  \n",
      "<<<iteration:[40/657] - total_loss: 0.4598  obj_loss: 0.0991  noobj_loss: 0.1811  bbox_loss: 0.0361  cls_loss: 0.0900  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[60/657] - total_loss: 0.4753  obj_loss: 0.0943  noobj_loss: 0.1710  bbox_loss: 0.0357  cls_loss: 0.1170  \n",
      "<<<iteration:[80/657] - total_loss: 0.4410  obj_loss: 0.1005  noobj_loss: 0.1700  bbox_loss: 0.0329  cls_loss: 0.0908  \n",
      "<<<iteration:[100/657] - total_loss: 0.4132  obj_loss: 0.1098  noobj_loss: 0.1690  bbox_loss: 0.0253  cls_loss: 0.0926  \n",
      "<<<iteration:[120/657] - total_loss: 0.4038  obj_loss: 0.1049  noobj_loss: 0.1673  bbox_loss: 0.0276  cls_loss: 0.0775  \n",
      "<<<iteration:[140/657] - total_loss: 0.4211  obj_loss: 0.0969  noobj_loss: 0.1709  bbox_loss: 0.0276  cls_loss: 0.1005  \n",
      "<<<iteration:[160/657] - total_loss: 0.4224  obj_loss: 0.0952  noobj_loss: 0.1743  bbox_loss: 0.0303  cls_loss: 0.0888  \n",
      "<<<iteration:[180/657] - total_loss: 0.4234  obj_loss: 0.1149  noobj_loss: 0.1836  bbox_loss: 0.0256  cls_loss: 0.0889  \n",
      "<<<iteration:[200/657] - total_loss: 0.4307  obj_loss: 0.0988  noobj_loss: 0.1641  bbox_loss: 0.0285  cls_loss: 0.1075  \n",
      "<<<iteration:[220/657] - total_loss: 0.4313  obj_loss: 0.0998  noobj_loss: 0.1632  bbox_loss: 0.0275  cls_loss: 0.1123  \n",
      "<<<iteration:[240/657] - total_loss: 0.3914  obj_loss: 0.0907  noobj_loss: 0.1676  bbox_loss: 0.0266  cls_loss: 0.0841  \n",
      "<<<iteration:[260/657] - total_loss: 0.4489  obj_loss: 0.1014  noobj_loss: 0.1753  bbox_loss: 0.0316  cls_loss: 0.1017  \n",
      "<<<iteration:[280/657] - total_loss: 0.4725  obj_loss: 0.1086  noobj_loss: 0.1659  bbox_loss: 0.0356  cls_loss: 0.1029  \n",
      "<<<iteration:[300/657] - total_loss: 0.4145  obj_loss: 0.1068  noobj_loss: 0.1708  bbox_loss: 0.0284  cls_loss: 0.0803  \n",
      "<<<iteration:[320/657] - total_loss: 0.4074  obj_loss: 0.1066  noobj_loss: 0.1692  bbox_loss: 0.0261  cls_loss: 0.0858  \n",
      "<<<iteration:[340/657] - total_loss: 0.4418  obj_loss: 0.1003  noobj_loss: 0.1886  bbox_loss: 0.0326  cls_loss: 0.0842  \n",
      "<<<iteration:[360/657] - total_loss: 0.4159  obj_loss: 0.1021  noobj_loss: 0.1721  bbox_loss: 0.0261  cls_loss: 0.0971  \n",
      "<<<iteration:[380/657] - total_loss: 0.4109  obj_loss: 0.0947  noobj_loss: 0.1762  bbox_loss: 0.0283  cls_loss: 0.0867  \n",
      "<<<iteration:[400/657] - total_loss: 0.3932  obj_loss: 0.0970  noobj_loss: 0.1643  bbox_loss: 0.0249  cls_loss: 0.0894  \n",
      "<<<iteration:[420/657] - total_loss: 0.4449  obj_loss: 0.0995  noobj_loss: 0.1762  bbox_loss: 0.0322  cls_loss: 0.0962  \n",
      "<<<iteration:[440/657] - total_loss: 0.4394  obj_loss: 0.0976  noobj_loss: 0.1791  bbox_loss: 0.0307  cls_loss: 0.0987  \n",
      "<<<iteration:[460/657] - total_loss: 0.4191  obj_loss: 0.1058  noobj_loss: 0.1748  bbox_loss: 0.0272  cls_loss: 0.0898  \n",
      "<<<iteration:[480/657] - total_loss: 0.4210  obj_loss: 0.1049  noobj_loss: 0.1713  bbox_loss: 0.0273  cls_loss: 0.0938  \n",
      "<<<iteration:[500/657] - total_loss: 0.4275  obj_loss: 0.1039  noobj_loss: 0.1681  bbox_loss: 0.0297  cls_loss: 0.0909  \n",
      "<<<iteration:[520/657] - total_loss: 0.4291  obj_loss: 0.1065  noobj_loss: 0.1703  bbox_loss: 0.0309  cls_loss: 0.0831  \n",
      "<<<iteration:[540/657] - total_loss: 0.4384  obj_loss: 0.0937  noobj_loss: 0.1745  bbox_loss: 0.0321  cls_loss: 0.0972  \n",
      "<<<iteration:[560/657] - total_loss: 0.4337  obj_loss: 0.1009  noobj_loss: 0.1624  bbox_loss: 0.0298  cls_loss: 0.1028  \n",
      "<<<iteration:[580/657] - total_loss: 0.4280  obj_loss: 0.1170  noobj_loss: 0.1658  bbox_loss: 0.0275  cls_loss: 0.0908  \n",
      "<<<iteration:[600/657] - total_loss: 0.7913  obj_loss: 0.0709  noobj_loss: 0.1699  bbox_loss: 0.1071  cls_loss: 0.1000  \n",
      "<<<iteration:[620/657] - total_loss: 0.4445  obj_loss: 0.0944  noobj_loss: 0.1656  bbox_loss: 0.0364  cls_loss: 0.0854  \n",
      "<<<iteration:[640/657] - total_loss: 0.4144  obj_loss: 0.0913  noobj_loss: 0.1676  bbox_loss: 0.0313  cls_loss: 0.0827  \n",
      "\n",
      "epoch:47/100 - Train Loss: 0.4404, Val Loss: 0.5136\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4663  obj_loss: 0.1190  noobj_loss: 0.1718  bbox_loss: 0.0349  cls_loss: 0.0871  \n",
      "<<<iteration:[40/657] - total_loss: 0.4477  obj_loss: 0.0936  noobj_loss: 0.1850  bbox_loss: 0.0325  cls_loss: 0.0993  \n",
      "<<<iteration:[60/657] - total_loss: 0.4313  obj_loss: 0.1046  noobj_loss: 0.1644  bbox_loss: 0.0290  cls_loss: 0.0996  \n",
      "<<<iteration:[80/657] - total_loss: 0.4185  obj_loss: 0.1032  noobj_loss: 0.1634  bbox_loss: 0.0280  cls_loss: 0.0937  \n",
      "<<<iteration:[100/657] - total_loss: 0.4115  obj_loss: 0.0988  noobj_loss: 0.1687  bbox_loss: 0.0292  cls_loss: 0.0823  \n",
      "<<<iteration:[120/657] - total_loss: 0.4218  obj_loss: 0.0976  noobj_loss: 0.1611  bbox_loss: 0.0293  cls_loss: 0.0971  \n",
      "<<<iteration:[140/657] - total_loss: 0.4195  obj_loss: 0.1006  noobj_loss: 0.1736  bbox_loss: 0.0282  cls_loss: 0.0913  \n",
      "<<<iteration:[160/657] - total_loss: 0.4155  obj_loss: 0.0994  noobj_loss: 0.1631  bbox_loss: 0.0293  cls_loss: 0.0878  \n",
      "<<<iteration:[180/657] - total_loss: 0.4220  obj_loss: 0.0958  noobj_loss: 0.1664  bbox_loss: 0.0324  cls_loss: 0.0808  \n",
      "<<<iteration:[200/657] - total_loss: 0.4530  obj_loss: 0.1044  noobj_loss: 0.1728  bbox_loss: 0.0313  cls_loss: 0.1059  \n",
      "<<<iteration:[220/657] - total_loss: 0.4295  obj_loss: 0.0956  noobj_loss: 0.1771  bbox_loss: 0.0307  cls_loss: 0.0917  \n",
      "<<<iteration:[240/657] - total_loss: 0.4175  obj_loss: 0.0950  noobj_loss: 0.1680  bbox_loss: 0.0291  cls_loss: 0.0930  \n",
      "<<<iteration:[260/657] - total_loss: 0.4368  obj_loss: 0.1123  noobj_loss: 0.1650  bbox_loss: 0.0293  cls_loss: 0.0955  \n",
      "<<<iteration:[280/657] - total_loss: 0.3962  obj_loss: 0.0997  noobj_loss: 0.1566  bbox_loss: 0.0262  cls_loss: 0.0873  \n",
      "<<<iteration:[300/657] - total_loss: 0.4137  obj_loss: 0.1094  noobj_loss: 0.1768  bbox_loss: 0.0274  cls_loss: 0.0790  \n",
      "<<<iteration:[320/657] - total_loss: 0.4062  obj_loss: 0.1125  noobj_loss: 0.1718  bbox_loss: 0.0268  cls_loss: 0.0739  \n",
      "<<<iteration:[340/657] - total_loss: 0.4059  obj_loss: 0.1097  noobj_loss: 0.1603  bbox_loss: 0.0269  cls_loss: 0.0815  \n",
      "<<<iteration:[360/657] - total_loss: 0.4107  obj_loss: 0.0896  noobj_loss: 0.1650  bbox_loss: 0.0324  cls_loss: 0.0765  \n",
      "<<<iteration:[380/657] - total_loss: 0.4518  obj_loss: 0.0998  noobj_loss: 0.1790  bbox_loss: 0.0351  cls_loss: 0.0869  \n",
      "<<<iteration:[400/657] - total_loss: 0.4417  obj_loss: 0.0997  noobj_loss: 0.1747  bbox_loss: 0.0295  cls_loss: 0.1069  \n",
      "<<<iteration:[420/657] - total_loss: 0.4127  obj_loss: 0.1057  noobj_loss: 0.1857  bbox_loss: 0.0269  cls_loss: 0.0797  \n",
      "<<<iteration:[440/657] - total_loss: 0.4175  obj_loss: 0.0999  noobj_loss: 0.1642  bbox_loss: 0.0293  cls_loss: 0.0892  \n",
      "<<<iteration:[460/657] - total_loss: 0.4058  obj_loss: 0.1052  noobj_loss: 0.1703  bbox_loss: 0.0275  cls_loss: 0.0781  \n",
      "<<<iteration:[480/657] - total_loss: 0.4135  obj_loss: 0.0917  noobj_loss: 0.1754  bbox_loss: 0.0280  cls_loss: 0.0943  \n",
      "<<<iteration:[500/657] - total_loss: 0.4352  obj_loss: 0.0936  noobj_loss: 0.1773  bbox_loss: 0.0295  cls_loss: 0.1056  \n",
      "<<<iteration:[520/657] - total_loss: 0.4272  obj_loss: 0.0928  noobj_loss: 0.1605  bbox_loss: 0.0284  cls_loss: 0.1121  \n",
      "<<<iteration:[540/657] - total_loss: 0.4274  obj_loss: 0.1047  noobj_loss: 0.1638  bbox_loss: 0.0294  cls_loss: 0.0937  \n",
      "<<<iteration:[560/657] - total_loss: 0.4169  obj_loss: 0.1076  noobj_loss: 0.1785  bbox_loss: 0.0252  cls_loss: 0.0938  \n",
      "<<<iteration:[580/657] - total_loss: 0.4305  obj_loss: 0.0984  noobj_loss: 0.1567  bbox_loss: 0.0314  cls_loss: 0.0969  \n",
      "<<<iteration:[600/657] - total_loss: 0.4244  obj_loss: 0.0938  noobj_loss: 0.2110  bbox_loss: 0.0262  cls_loss: 0.0940  \n",
      "<<<iteration:[620/657] - total_loss: 0.4221  obj_loss: 0.0959  noobj_loss: 0.1653  bbox_loss: 0.0331  cls_loss: 0.0780  \n",
      "<<<iteration:[640/657] - total_loss: 0.4029  obj_loss: 0.1006  noobj_loss: 0.1620  bbox_loss: 0.0283  cls_loss: 0.0796  \n",
      "\n",
      "epoch:48/100 - Train Loss: 0.4236, Val Loss: 0.5628\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4538  obj_loss: 0.1038  noobj_loss: 0.1845  bbox_loss: 0.0339  cls_loss: 0.0884  \n",
      "<<<iteration:[40/657] - total_loss: 0.4192  obj_loss: 0.1097  noobj_loss: 0.1633  bbox_loss: 0.0271  cls_loss: 0.0925  \n",
      "<<<iteration:[60/657] - total_loss: 0.4422  obj_loss: 0.0922  noobj_loss: 0.1780  bbox_loss: 0.0311  cls_loss: 0.1056  \n",
      "<<<iteration:[80/657] - total_loss: 0.4074  obj_loss: 0.1036  noobj_loss: 0.1650  bbox_loss: 0.0254  cls_loss: 0.0942  \n",
      "<<<iteration:[100/657] - total_loss: 0.4266  obj_loss: 0.0975  noobj_loss: 0.1627  bbox_loss: 0.0301  cls_loss: 0.0973  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[120/657] - total_loss: 0.4285  obj_loss: 0.0982  noobj_loss: 0.1636  bbox_loss: 0.0307  cls_loss: 0.0952  \n",
      "<<<iteration:[140/657] - total_loss: 0.4372  obj_loss: 0.1100  noobj_loss: 0.1800  bbox_loss: 0.0281  cls_loss: 0.0967  \n",
      "<<<iteration:[160/657] - total_loss: 0.4010  obj_loss: 0.0815  noobj_loss: 0.1642  bbox_loss: 0.0291  cls_loss: 0.0920  \n",
      "<<<iteration:[180/657] - total_loss: 0.4163  obj_loss: 0.1015  noobj_loss: 0.1655  bbox_loss: 0.0261  cls_loss: 0.1018  \n",
      "<<<iteration:[200/657] - total_loss: 0.4044  obj_loss: 0.1101  noobj_loss: 0.1659  bbox_loss: 0.0246  cls_loss: 0.0884  \n",
      "<<<iteration:[220/657] - total_loss: 0.4129  obj_loss: 0.1050  noobj_loss: 0.1678  bbox_loss: 0.0263  cls_loss: 0.0923  \n",
      "<<<iteration:[240/657] - total_loss: 0.4291  obj_loss: 0.1057  noobj_loss: 0.1696  bbox_loss: 0.0278  cls_loss: 0.0997  \n",
      "<<<iteration:[260/657] - total_loss: 0.4355  obj_loss: 0.0969  noobj_loss: 0.1665  bbox_loss: 0.0303  cls_loss: 0.1038  \n",
      "<<<iteration:[280/657] - total_loss: 0.3921  obj_loss: 0.0999  noobj_loss: 0.1655  bbox_loss: 0.0252  cls_loss: 0.0834  \n",
      "<<<iteration:[300/657] - total_loss: 0.4258  obj_loss: 0.1030  noobj_loss: 0.1641  bbox_loss: 0.0289  cls_loss: 0.0964  \n",
      "<<<iteration:[320/657] - total_loss: 0.4311  obj_loss: 0.0995  noobj_loss: 0.1726  bbox_loss: 0.0311  cls_loss: 0.0901  \n",
      "<<<iteration:[340/657] - total_loss: 0.4009  obj_loss: 0.0969  noobj_loss: 0.1647  bbox_loss: 0.0289  cls_loss: 0.0770  \n",
      "<<<iteration:[360/657] - total_loss: 0.4084  obj_loss: 0.1002  noobj_loss: 0.1628  bbox_loss: 0.0302  cls_loss: 0.0756  \n",
      "<<<iteration:[380/657] - total_loss: 0.4123  obj_loss: 0.1091  noobj_loss: 0.1669  bbox_loss: 0.0264  cls_loss: 0.0879  \n",
      "<<<iteration:[400/657] - total_loss: 0.4235  obj_loss: 0.0992  noobj_loss: 0.1682  bbox_loss: 0.0325  cls_loss: 0.0774  \n",
      "<<<iteration:[420/657] - total_loss: 0.4034  obj_loss: 0.0991  noobj_loss: 0.1592  bbox_loss: 0.0285  cls_loss: 0.0819  \n",
      "<<<iteration:[440/657] - total_loss: 0.3911  obj_loss: 0.1005  noobj_loss: 0.1635  bbox_loss: 0.0259  cls_loss: 0.0791  \n",
      "<<<iteration:[460/657] - total_loss: 0.4119  obj_loss: 0.1051  noobj_loss: 0.1698  bbox_loss: 0.0249  cls_loss: 0.0975  \n",
      "<<<iteration:[480/657] - total_loss: 0.4308  obj_loss: 0.1135  noobj_loss: 0.1617  bbox_loss: 0.0277  cls_loss: 0.0977  \n",
      "<<<iteration:[500/657] - total_loss: 0.4159  obj_loss: 0.1065  noobj_loss: 0.1640  bbox_loss: 0.0278  cls_loss: 0.0882  \n",
      "<<<iteration:[520/657] - total_loss: 0.4017  obj_loss: 0.1052  noobj_loss: 0.1631  bbox_loss: 0.0268  cls_loss: 0.0810  \n",
      "<<<iteration:[540/657] - total_loss: 0.3996  obj_loss: 0.0979  noobj_loss: 0.1629  bbox_loss: 0.0277  cls_loss: 0.0815  \n",
      "<<<iteration:[560/657] - total_loss: 0.4343  obj_loss: 0.1044  noobj_loss: 0.1655  bbox_loss: 0.0304  cls_loss: 0.0954  \n",
      "<<<iteration:[580/657] - total_loss: 0.4316  obj_loss: 0.1042  noobj_loss: 0.1712  bbox_loss: 0.0333  cls_loss: 0.0753  \n",
      "<<<iteration:[600/657] - total_loss: 0.4320  obj_loss: 0.0909  noobj_loss: 0.1626  bbox_loss: 0.0339  cls_loss: 0.0902  \n",
      "<<<iteration:[620/657] - total_loss: 0.4036  obj_loss: 0.0999  noobj_loss: 0.1605  bbox_loss: 0.0273  cls_loss: 0.0869  \n",
      "<<<iteration:[640/657] - total_loss: 0.4384  obj_loss: 0.1013  noobj_loss: 0.1658  bbox_loss: 0.0341  cls_loss: 0.0837  \n",
      "\n",
      "epoch:49/100 - Train Loss: 0.4177, Val Loss: 0.4429\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4573  obj_loss: 0.0979  noobj_loss: 0.1689  bbox_loss: 0.0341  cls_loss: 0.1042  \n",
      "<<<iteration:[40/657] - total_loss: 0.4156  obj_loss: 0.0992  noobj_loss: 0.1701  bbox_loss: 0.0300  cls_loss: 0.0811  \n",
      "<<<iteration:[60/657] - total_loss: 0.4374  obj_loss: 0.0950  noobj_loss: 0.1585  bbox_loss: 0.0375  cls_loss: 0.0758  \n",
      "<<<iteration:[80/657] - total_loss: 0.4177  obj_loss: 0.0952  noobj_loss: 0.1597  bbox_loss: 0.0329  cls_loss: 0.0782  \n",
      "<<<iteration:[100/657] - total_loss: 0.4305  obj_loss: 0.1000  noobj_loss: 0.1584  bbox_loss: 0.0344  cls_loss: 0.0792  \n",
      "<<<iteration:[120/657] - total_loss: 0.4118  obj_loss: 0.0935  noobj_loss: 0.1613  bbox_loss: 0.0260  cls_loss: 0.1076  \n",
      "<<<iteration:[140/657] - total_loss: 0.3792  obj_loss: 0.1123  noobj_loss: 0.1594  bbox_loss: 0.0232  cls_loss: 0.0712  \n",
      "<<<iteration:[160/657] - total_loss: 0.4022  obj_loss: 0.0980  noobj_loss: 0.1607  bbox_loss: 0.0273  cls_loss: 0.0873  \n",
      "<<<iteration:[180/657] - total_loss: 0.4025  obj_loss: 0.1010  noobj_loss: 0.1583  bbox_loss: 0.0264  cls_loss: 0.0903  \n",
      "<<<iteration:[200/657] - total_loss: 0.4200  obj_loss: 0.0990  noobj_loss: 0.1564  bbox_loss: 0.0272  cls_loss: 0.1066  \n",
      "<<<iteration:[220/657] - total_loss: 0.3925  obj_loss: 0.0924  noobj_loss: 0.1699  bbox_loss: 0.0271  cls_loss: 0.0796  \n",
      "<<<iteration:[240/657] - total_loss: 0.4271  obj_loss: 0.1116  noobj_loss: 0.1600  bbox_loss: 0.0270  cls_loss: 0.1003  \n",
      "<<<iteration:[260/657] - total_loss: 0.4211  obj_loss: 0.0971  noobj_loss: 0.1688  bbox_loss: 0.0311  cls_loss: 0.0841  \n",
      "<<<iteration:[280/657] - total_loss: 0.4277  obj_loss: 0.1096  noobj_loss: 0.1695  bbox_loss: 0.0292  cls_loss: 0.0876  \n",
      "<<<iteration:[300/657] - total_loss: 0.4293  obj_loss: 0.1006  noobj_loss: 0.1573  bbox_loss: 0.0301  cls_loss: 0.0994  \n",
      "<<<iteration:[320/657] - total_loss: 0.4055  obj_loss: 0.0991  noobj_loss: 0.1590  bbox_loss: 0.0265  cls_loss: 0.0943  \n",
      "<<<iteration:[340/657] - total_loss: 0.4049  obj_loss: 0.0933  noobj_loss: 0.1644  bbox_loss: 0.0272  cls_loss: 0.0936  \n",
      "<<<iteration:[360/657] - total_loss: 0.4113  obj_loss: 0.1038  noobj_loss: 0.1656  bbox_loss: 0.0269  cls_loss: 0.0901  \n",
      "<<<iteration:[380/657] - total_loss: 0.4157  obj_loss: 0.1044  noobj_loss: 0.1677  bbox_loss: 0.0263  cls_loss: 0.0958  \n",
      "<<<iteration:[400/657] - total_loss: 0.3936  obj_loss: 0.0967  noobj_loss: 0.1658  bbox_loss: 0.0267  cls_loss: 0.0807  \n",
      "<<<iteration:[420/657] - total_loss: 0.3914  obj_loss: 0.1009  noobj_loss: 0.1628  bbox_loss: 0.0260  cls_loss: 0.0790  \n",
      "<<<iteration:[440/657] - total_loss: 0.4272  obj_loss: 0.0986  noobj_loss: 0.1801  bbox_loss: 0.0290  cls_loss: 0.0936  \n",
      "<<<iteration:[460/657] - total_loss: 0.3867  obj_loss: 0.0883  noobj_loss: 0.1664  bbox_loss: 0.0268  cls_loss: 0.0812  \n",
      "<<<iteration:[480/657] - total_loss: 0.3939  obj_loss: 0.0924  noobj_loss: 0.1587  bbox_loss: 0.0272  cls_loss: 0.0863  \n",
      "<<<iteration:[500/657] - total_loss: 0.3997  obj_loss: 0.1123  noobj_loss: 0.1614  bbox_loss: 0.0257  cls_loss: 0.0780  \n",
      "<<<iteration:[520/657] - total_loss: 0.4255  obj_loss: 0.1017  noobj_loss: 0.1669  bbox_loss: 0.0285  cls_loss: 0.0979  \n",
      "<<<iteration:[540/657] - total_loss: 0.3924  obj_loss: 0.0970  noobj_loss: 0.1669  bbox_loss: 0.0277  cls_loss: 0.0737  \n",
      "<<<iteration:[560/657] - total_loss: 0.4067  obj_loss: 0.1083  noobj_loss: 0.1664  bbox_loss: 0.0259  cls_loss: 0.0856  \n",
      "<<<iteration:[580/657] - total_loss: 0.3979  obj_loss: 0.0982  noobj_loss: 0.1601  bbox_loss: 0.0258  cls_loss: 0.0909  \n",
      "<<<iteration:[600/657] - total_loss: 0.3945  obj_loss: 0.1019  noobj_loss: 0.1638  bbox_loss: 0.0271  cls_loss: 0.0753  \n",
      "<<<iteration:[620/657] - total_loss: 0.4191  obj_loss: 0.1095  noobj_loss: 0.1642  bbox_loss: 0.0261  cls_loss: 0.0969  \n",
      "<<<iteration:[640/657] - total_loss: 0.4164  obj_loss: 0.1112  noobj_loss: 0.1693  bbox_loss: 0.0251  cls_loss: 0.0950  \n",
      "\n",
      "epoch:50/100 - Train Loss: 0.4107, Val Loss: 0.4310\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4183  obj_loss: 0.1153  noobj_loss: 0.1728  bbox_loss: 0.0268  cls_loss: 0.0824  \n",
      "<<<iteration:[40/657] - total_loss: 0.4303  obj_loss: 0.0965  noobj_loss: 0.1867  bbox_loss: 0.0293  cls_loss: 0.0938  \n",
      "<<<iteration:[60/657] - total_loss: 0.4149  obj_loss: 0.1026  noobj_loss: 0.1675  bbox_loss: 0.0282  cls_loss: 0.0876  \n",
      "<<<iteration:[80/657] - total_loss: 0.3951  obj_loss: 0.1087  noobj_loss: 0.1571  bbox_loss: 0.0262  cls_loss: 0.0769  \n",
      "<<<iteration:[100/657] - total_loss: 0.3964  obj_loss: 0.0969  noobj_loss: 0.1527  bbox_loss: 0.0270  cls_loss: 0.0884  \n",
      "<<<iteration:[120/657] - total_loss: 0.4056  obj_loss: 0.1057  noobj_loss: 0.1604  bbox_loss: 0.0259  cls_loss: 0.0901  \n",
      "<<<iteration:[140/657] - total_loss: 0.3997  obj_loss: 0.1015  noobj_loss: 0.1596  bbox_loss: 0.0275  cls_loss: 0.0812  \n",
      "<<<iteration:[160/657] - total_loss: 0.4298  obj_loss: 0.1010  noobj_loss: 0.1720  bbox_loss: 0.0293  cls_loss: 0.0964  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[180/657] - total_loss: 0.3982  obj_loss: 0.1041  noobj_loss: 0.1646  bbox_loss: 0.0255  cls_loss: 0.0844  \n",
      "<<<iteration:[200/657] - total_loss: 0.4257  obj_loss: 0.1032  noobj_loss: 0.1690  bbox_loss: 0.0298  cls_loss: 0.0891  \n",
      "<<<iteration:[220/657] - total_loss: 0.3809  obj_loss: 0.0956  noobj_loss: 0.1626  bbox_loss: 0.0238  cls_loss: 0.0852  \n",
      "<<<iteration:[240/657] - total_loss: 0.3799  obj_loss: 0.0985  noobj_loss: 0.1644  bbox_loss: 0.0261  cls_loss: 0.0686  \n",
      "<<<iteration:[260/657] - total_loss: 0.4285  obj_loss: 0.1034  noobj_loss: 0.1727  bbox_loss: 0.0283  cls_loss: 0.0973  \n",
      "<<<iteration:[280/657] - total_loss: 0.3904  obj_loss: 0.0961  noobj_loss: 0.1595  bbox_loss: 0.0274  cls_loss: 0.0777  \n",
      "<<<iteration:[300/657] - total_loss: 0.4353  obj_loss: 0.0963  noobj_loss: 0.1615  bbox_loss: 0.0277  cls_loss: 0.1196  \n",
      "<<<iteration:[320/657] - total_loss: 0.3833  obj_loss: 0.0947  noobj_loss: 0.1594  bbox_loss: 0.0270  cls_loss: 0.0741  \n",
      "<<<iteration:[340/657] - total_loss: 0.4021  obj_loss: 0.0943  noobj_loss: 0.1584  bbox_loss: 0.0264  cls_loss: 0.0965  \n",
      "<<<iteration:[360/657] - total_loss: 0.3932  obj_loss: 0.1052  noobj_loss: 0.1559  bbox_loss: 0.0237  cls_loss: 0.0914  \n",
      "<<<iteration:[380/657] - total_loss: 0.3979  obj_loss: 0.1036  noobj_loss: 0.1574  bbox_loss: 0.0271  cls_loss: 0.0801  \n",
      "<<<iteration:[400/657] - total_loss: 0.4064  obj_loss: 0.0982  noobj_loss: 0.1582  bbox_loss: 0.0299  cls_loss: 0.0793  \n",
      "<<<iteration:[420/657] - total_loss: 0.5544  obj_loss: 0.0859  noobj_loss: 0.1596  bbox_loss: 0.0619  cls_loss: 0.0791  \n",
      "<<<iteration:[440/657] - total_loss: 0.4133  obj_loss: 0.0961  noobj_loss: 0.1547  bbox_loss: 0.0293  cls_loss: 0.0931  \n",
      "<<<iteration:[460/657] - total_loss: 0.4242  obj_loss: 0.1025  noobj_loss: 0.1577  bbox_loss: 0.0308  cls_loss: 0.0888  \n",
      "<<<iteration:[480/657] - total_loss: 0.4044  obj_loss: 0.1052  noobj_loss: 0.1555  bbox_loss: 0.0273  cls_loss: 0.0849  \n",
      "<<<iteration:[500/657] - total_loss: 0.4223  obj_loss: 0.1107  noobj_loss: 0.1598  bbox_loss: 0.0280  cls_loss: 0.0916  \n",
      "<<<iteration:[520/657] - total_loss: 0.3938  obj_loss: 0.0930  noobj_loss: 0.1578  bbox_loss: 0.0263  cls_loss: 0.0906  \n",
      "<<<iteration:[540/657] - total_loss: 0.4287  obj_loss: 0.1057  noobj_loss: 0.1776  bbox_loss: 0.0285  cls_loss: 0.0918  \n",
      "<<<iteration:[560/657] - total_loss: 0.4003  obj_loss: 0.1088  noobj_loss: 0.1544  bbox_loss: 0.0250  cls_loss: 0.0893  \n",
      "<<<iteration:[580/657] - total_loss: 0.4292  obj_loss: 0.1043  noobj_loss: 0.1598  bbox_loss: 0.0299  cls_loss: 0.0956  \n",
      "<<<iteration:[600/657] - total_loss: 0.4314  obj_loss: 0.1026  noobj_loss: 0.1621  bbox_loss: 0.0299  cls_loss: 0.0981  \n",
      "<<<iteration:[620/657] - total_loss: 0.4087  obj_loss: 0.0938  noobj_loss: 0.1571  bbox_loss: 0.0309  cls_loss: 0.0818  \n",
      "<<<iteration:[640/657] - total_loss: 0.4238  obj_loss: 0.1018  noobj_loss: 0.1550  bbox_loss: 0.0268  cls_loss: 0.1105  \n",
      "\n",
      "epoch:51/100 - Train Loss: 0.4123, Val Loss: 0.4307\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4285  obj_loss: 0.1132  noobj_loss: 0.1602  bbox_loss: 0.0288  cls_loss: 0.0913  \n",
      "<<<iteration:[40/657] - total_loss: 0.4000  obj_loss: 0.1016  noobj_loss: 0.1601  bbox_loss: 0.0263  cls_loss: 0.0870  \n",
      "<<<iteration:[60/657] - total_loss: 0.4051  obj_loss: 0.1022  noobj_loss: 0.1556  bbox_loss: 0.0292  cls_loss: 0.0788  \n",
      "<<<iteration:[80/657] - total_loss: 0.3882  obj_loss: 0.0983  noobj_loss: 0.1574  bbox_loss: 0.0238  cls_loss: 0.0920  \n",
      "<<<iteration:[100/657] - total_loss: 0.4358  obj_loss: 0.1075  noobj_loss: 0.1582  bbox_loss: 0.0283  cls_loss: 0.1077  \n",
      "<<<iteration:[120/657] - total_loss: 0.3703  obj_loss: 0.0947  noobj_loss: 0.1596  bbox_loss: 0.0234  cls_loss: 0.0787  \n",
      "<<<iteration:[140/657] - total_loss: 0.4328  obj_loss: 0.0980  noobj_loss: 0.1680  bbox_loss: 0.0317  cls_loss: 0.0921  \n",
      "<<<iteration:[160/657] - total_loss: 7.4695  obj_loss: 0.0578  noobj_loss: 0.2267  bbox_loss: 1.4366  cls_loss: 0.1157  \n",
      "<<<iteration:[180/657] - total_loss: 5.9476  obj_loss: 0.0137  noobj_loss: 0.1636  bbox_loss: 1.1524  cls_loss: 0.0902  \n",
      "<<<iteration:[200/657] - total_loss: 1.8474  obj_loss: 0.0373  noobj_loss: 0.1638  bbox_loss: 0.3273  cls_loss: 0.0914  \n",
      "<<<iteration:[220/657] - total_loss: 1.3089  obj_loss: 0.0536  noobj_loss: 0.1482  bbox_loss: 0.2175  cls_loss: 0.0939  \n",
      "<<<iteration:[240/657] - total_loss: 0.9517  obj_loss: 0.0651  noobj_loss: 0.1599  bbox_loss: 0.1444  cls_loss: 0.0847  \n",
      "<<<iteration:[260/657] - total_loss: 0.7298  obj_loss: 0.0649  noobj_loss: 0.1601  bbox_loss: 0.0986  cls_loss: 0.0920  \n",
      "<<<iteration:[280/657] - total_loss: 0.5982  obj_loss: 0.0709  noobj_loss: 0.1659  bbox_loss: 0.0728  cls_loss: 0.0803  \n",
      "<<<iteration:[300/657] - total_loss: 0.5849  obj_loss: 0.0807  noobj_loss: 0.1593  bbox_loss: 0.0653  cls_loss: 0.0982  \n",
      "<<<iteration:[320/657] - total_loss: 0.5085  obj_loss: 0.0853  noobj_loss: 0.1614  bbox_loss: 0.0520  cls_loss: 0.0823  \n",
      "<<<iteration:[340/657] - total_loss: 0.4613  obj_loss: 0.0998  noobj_loss: 0.1547  bbox_loss: 0.0395  cls_loss: 0.0866  \n",
      "<<<iteration:[360/657] - total_loss: 0.4703  obj_loss: 0.0888  noobj_loss: 0.1477  bbox_loss: 0.0448  cls_loss: 0.0836  \n",
      "<<<iteration:[380/657] - total_loss: 0.4648  obj_loss: 0.0908  noobj_loss: 0.1503  bbox_loss: 0.0424  cls_loss: 0.0869  \n",
      "<<<iteration:[400/657] - total_loss: 0.4510  obj_loss: 0.0940  noobj_loss: 0.1520  bbox_loss: 0.0367  cls_loss: 0.0976  \n",
      "<<<iteration:[420/657] - total_loss: 0.4761  obj_loss: 0.0955  noobj_loss: 0.1621  bbox_loss: 0.0405  cls_loss: 0.0971  \n",
      "<<<iteration:[440/657] - total_loss: 0.4532  obj_loss: 0.0899  noobj_loss: 0.1503  bbox_loss: 0.0378  cls_loss: 0.0991  \n",
      "<<<iteration:[460/657] - total_loss: 0.4437  obj_loss: 0.0936  noobj_loss: 0.1698  bbox_loss: 0.0365  cls_loss: 0.0829  \n",
      "<<<iteration:[480/657] - total_loss: 0.4628  obj_loss: 0.0925  noobj_loss: 0.1615  bbox_loss: 0.0379  cls_loss: 0.1002  \n",
      "<<<iteration:[500/657] - total_loss: 0.4273  obj_loss: 0.1042  noobj_loss: 0.1502  bbox_loss: 0.0361  cls_loss: 0.0675  \n",
      "<<<iteration:[520/657] - total_loss: 0.4268  obj_loss: 0.0943  noobj_loss: 0.1590  bbox_loss: 0.0332  cls_loss: 0.0872  \n",
      "<<<iteration:[540/657] - total_loss: 0.4087  obj_loss: 0.0961  noobj_loss: 0.1647  bbox_loss: 0.0307  cls_loss: 0.0766  \n",
      "<<<iteration:[560/657] - total_loss: 0.4098  obj_loss: 0.0958  noobj_loss: 0.1598  bbox_loss: 0.0274  cls_loss: 0.0974  \n",
      "<<<iteration:[580/657] - total_loss: 0.4323  obj_loss: 0.1040  noobj_loss: 0.1474  bbox_loss: 0.0316  cls_loss: 0.0967  \n",
      "<<<iteration:[600/657] - total_loss: 0.4499  obj_loss: 0.0978  noobj_loss: 0.1575  bbox_loss: 0.0380  cls_loss: 0.0831  \n",
      "<<<iteration:[620/657] - total_loss: 0.4205  obj_loss: 0.1014  noobj_loss: 0.1597  bbox_loss: 0.0300  cls_loss: 0.0890  \n",
      "<<<iteration:[640/657] - total_loss: 0.5089  obj_loss: 0.0849  noobj_loss: 0.1622  bbox_loss: 0.0521  cls_loss: 0.0824  \n",
      "\n",
      "epoch:52/100 - Train Loss: 0.9226, Val Loss: 0.4925\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4530  obj_loss: 0.1014  noobj_loss: 0.1700  bbox_loss: 0.0338  cls_loss: 0.0975  \n",
      "<<<iteration:[40/657] - total_loss: 0.4208  obj_loss: 0.1001  noobj_loss: 0.1588  bbox_loss: 0.0321  cls_loss: 0.0810  \n",
      "<<<iteration:[60/657] - total_loss: 0.4292  obj_loss: 0.1005  noobj_loss: 0.1587  bbox_loss: 0.0295  cls_loss: 0.1019  \n",
      "<<<iteration:[80/657] - total_loss: 0.3968  obj_loss: 0.1028  noobj_loss: 0.1617  bbox_loss: 0.0262  cls_loss: 0.0823  \n",
      "<<<iteration:[100/657] - total_loss: 0.3990  obj_loss: 0.1008  noobj_loss: 0.1550  bbox_loss: 0.0284  cls_loss: 0.0786  \n",
      "<<<iteration:[120/657] - total_loss: 0.3858  obj_loss: 0.1034  noobj_loss: 0.1571  bbox_loss: 0.0250  cls_loss: 0.0789  \n",
      "<<<iteration:[140/657] - total_loss: 0.4089  obj_loss: 0.1063  noobj_loss: 0.1645  bbox_loss: 0.0278  cls_loss: 0.0815  \n",
      "<<<iteration:[160/657] - total_loss: 0.4116  obj_loss: 0.0885  noobj_loss: 0.1637  bbox_loss: 0.0306  cls_loss: 0.0882  \n",
      "<<<iteration:[180/657] - total_loss: 0.4320  obj_loss: 0.1029  noobj_loss: 0.1577  bbox_loss: 0.0321  cls_loss: 0.0899  \n",
      "<<<iteration:[200/657] - total_loss: 0.4275  obj_loss: 0.1067  noobj_loss: 0.1573  bbox_loss: 0.0292  cls_loss: 0.0960  \n",
      "<<<iteration:[220/657] - total_loss: 0.4412  obj_loss: 0.0896  noobj_loss: 0.1503  bbox_loss: 0.0361  cls_loss: 0.0959  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[240/657] - total_loss: 0.4031  obj_loss: 0.1058  noobj_loss: 0.1592  bbox_loss: 0.0275  cls_loss: 0.0801  \n",
      "<<<iteration:[260/657] - total_loss: 0.3917  obj_loss: 0.1048  noobj_loss: 0.1537  bbox_loss: 0.0252  cls_loss: 0.0839  \n",
      "<<<iteration:[280/657] - total_loss: 0.4130  obj_loss: 0.1020  noobj_loss: 0.1649  bbox_loss: 0.0278  cls_loss: 0.0895  \n",
      "<<<iteration:[300/657] - total_loss: 0.3942  obj_loss: 0.0974  noobj_loss: 0.1577  bbox_loss: 0.0273  cls_loss: 0.0815  \n",
      "<<<iteration:[320/657] - total_loss: 0.3760  obj_loss: 0.0981  noobj_loss: 0.1572  bbox_loss: 0.0251  cls_loss: 0.0737  \n",
      "<<<iteration:[340/657] - total_loss: 0.4005  obj_loss: 0.1030  noobj_loss: 0.1538  bbox_loss: 0.0260  cls_loss: 0.0905  \n",
      "<<<iteration:[360/657] - total_loss: 0.3841  obj_loss: 0.1003  noobj_loss: 0.1514  bbox_loss: 0.0249  cls_loss: 0.0838  \n",
      "<<<iteration:[380/657] - total_loss: 0.4114  obj_loss: 0.1099  noobj_loss: 0.1648  bbox_loss: 0.0275  cls_loss: 0.0816  \n",
      "<<<iteration:[400/657] - total_loss: 0.4030  obj_loss: 0.1043  noobj_loss: 0.1558  bbox_loss: 0.0257  cls_loss: 0.0923  \n",
      "<<<iteration:[420/657] - total_loss: 0.4279  obj_loss: 0.1104  noobj_loss: 0.1582  bbox_loss: 0.0289  cls_loss: 0.0941  \n",
      "<<<iteration:[440/657] - total_loss: 0.4062  obj_loss: 0.1024  noobj_loss: 0.1559  bbox_loss: 0.0277  cls_loss: 0.0873  \n",
      "<<<iteration:[460/657] - total_loss: 0.3898  obj_loss: 0.1107  noobj_loss: 0.1602  bbox_loss: 0.0242  cls_loss: 0.0777  \n",
      "<<<iteration:[480/657] - total_loss: 0.4015  obj_loss: 0.1037  noobj_loss: 0.1583  bbox_loss: 0.0267  cls_loss: 0.0852  \n",
      "<<<iteration:[500/657] - total_loss: 0.4249  obj_loss: 0.1043  noobj_loss: 0.1610  bbox_loss: 0.0298  cls_loss: 0.0911  \n",
      "<<<iteration:[520/657] - total_loss: 0.4021  obj_loss: 0.0943  noobj_loss: 0.1546  bbox_loss: 0.0272  cls_loss: 0.0946  \n",
      "<<<iteration:[540/657] - total_loss: 0.3984  obj_loss: 0.1044  noobj_loss: 0.1573  bbox_loss: 0.0263  cls_loss: 0.0837  \n",
      "<<<iteration:[560/657] - total_loss: 0.4097  obj_loss: 0.1065  noobj_loss: 0.1528  bbox_loss: 0.0268  cls_loss: 0.0929  \n",
      "<<<iteration:[580/657] - total_loss: 0.4007  obj_loss: 0.1003  noobj_loss: 0.1572  bbox_loss: 0.0281  cls_loss: 0.0814  \n",
      "<<<iteration:[600/657] - total_loss: 0.4358  obj_loss: 0.1010  noobj_loss: 0.1480  bbox_loss: 0.0321  cls_loss: 0.1003  \n",
      "<<<iteration:[620/657] - total_loss: 0.4199  obj_loss: 0.1000  noobj_loss: 0.1665  bbox_loss: 0.0293  cls_loss: 0.0902  \n",
      "<<<iteration:[640/657] - total_loss: 0.4133  obj_loss: 0.1096  noobj_loss: 0.1542  bbox_loss: 0.0273  cls_loss: 0.0901  \n",
      "\n",
      "epoch:53/100 - Train Loss: 0.4087, Val Loss: 0.4331\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4474  obj_loss: 0.1109  noobj_loss: 0.1694  bbox_loss: 0.0293  cls_loss: 0.1056  \n",
      "<<<iteration:[40/657] - total_loss: 0.3778  obj_loss: 0.1043  noobj_loss: 0.1537  bbox_loss: 0.0241  cls_loss: 0.0762  \n",
      "<<<iteration:[60/657] - total_loss: 0.4350  obj_loss: 0.0998  noobj_loss: 0.1566  bbox_loss: 0.0330  cls_loss: 0.0919  \n",
      "<<<iteration:[80/657] - total_loss: 0.4040  obj_loss: 0.0975  noobj_loss: 0.1580  bbox_loss: 0.0290  cls_loss: 0.0823  \n",
      "<<<iteration:[100/657] - total_loss: 0.4044  obj_loss: 0.1026  noobj_loss: 0.1574  bbox_loss: 0.0287  cls_loss: 0.0796  \n",
      "<<<iteration:[120/657] - total_loss: 0.3871  obj_loss: 0.1054  noobj_loss: 0.1553  bbox_loss: 0.0241  cls_loss: 0.0837  \n",
      "<<<iteration:[140/657] - total_loss: 0.3790  obj_loss: 0.1043  noobj_loss: 0.1532  bbox_loss: 0.0235  cls_loss: 0.0807  \n",
      "<<<iteration:[160/657] - total_loss: 0.4021  obj_loss: 0.1096  noobj_loss: 0.1543  bbox_loss: 0.0259  cls_loss: 0.0860  \n",
      "<<<iteration:[180/657] - total_loss: 0.4062  obj_loss: 0.0992  noobj_loss: 0.1588  bbox_loss: 0.0280  cls_loss: 0.0875  \n",
      "<<<iteration:[200/657] - total_loss: 0.3994  obj_loss: 0.1008  noobj_loss: 0.1532  bbox_loss: 0.0276  cls_loss: 0.0842  \n",
      "<<<iteration:[220/657] - total_loss: 0.4286  obj_loss: 0.0943  noobj_loss: 0.1676  bbox_loss: 0.0316  cls_loss: 0.0928  \n",
      "<<<iteration:[240/657] - total_loss: 0.3974  obj_loss: 0.1144  noobj_loss: 0.1553  bbox_loss: 0.0253  cls_loss: 0.0790  \n",
      "<<<iteration:[260/657] - total_loss: 0.3894  obj_loss: 0.1051  noobj_loss: 0.1574  bbox_loss: 0.0236  cls_loss: 0.0878  \n",
      "<<<iteration:[280/657] - total_loss: 0.4141  obj_loss: 0.0992  noobj_loss: 0.1525  bbox_loss: 0.0310  cls_loss: 0.0837  \n",
      "<<<iteration:[300/657] - total_loss: 0.3851  obj_loss: 0.0985  noobj_loss: 0.1464  bbox_loss: 0.0273  cls_loss: 0.0768  \n",
      "<<<iteration:[320/657] - total_loss: 0.3784  obj_loss: 0.0999  noobj_loss: 0.1619  bbox_loss: 0.0247  cls_loss: 0.0740  \n",
      "<<<iteration:[340/657] - total_loss: 0.4256  obj_loss: 0.1064  noobj_loss: 0.1569  bbox_loss: 0.0296  cls_loss: 0.0927  \n",
      "<<<iteration:[360/657] - total_loss: 0.4052  obj_loss: 0.1101  noobj_loss: 0.1539  bbox_loss: 0.0256  cls_loss: 0.0902  \n",
      "<<<iteration:[380/657] - total_loss: 0.4058  obj_loss: 0.1034  noobj_loss: 0.1544  bbox_loss: 0.0277  cls_loss: 0.0868  \n",
      "<<<iteration:[400/657] - total_loss: 0.3998  obj_loss: 0.1035  noobj_loss: 0.1640  bbox_loss: 0.0266  cls_loss: 0.0815  \n",
      "<<<iteration:[420/657] - total_loss: 0.4198  obj_loss: 0.1112  noobj_loss: 0.1516  bbox_loss: 0.0276  cls_loss: 0.0946  \n",
      "<<<iteration:[440/657] - total_loss: 0.4006  obj_loss: 0.1005  noobj_loss: 0.1509  bbox_loss: 0.0271  cls_loss: 0.0890  \n",
      "<<<iteration:[460/657] - total_loss: 0.3997  obj_loss: 0.0999  noobj_loss: 0.1621  bbox_loss: 0.0279  cls_loss: 0.0794  \n",
      "<<<iteration:[480/657] - total_loss: 0.3991  obj_loss: 0.1041  noobj_loss: 0.1554  bbox_loss: 0.0266  cls_loss: 0.0844  \n",
      "<<<iteration:[500/657] - total_loss: 0.3743  obj_loss: 0.1040  noobj_loss: 0.1567  bbox_loss: 0.0236  cls_loss: 0.0742  \n",
      "<<<iteration:[520/657] - total_loss: 0.4142  obj_loss: 0.1084  noobj_loss: 0.1498  bbox_loss: 0.0282  cls_loss: 0.0899  \n",
      "<<<iteration:[540/657] - total_loss: 0.4029  obj_loss: 0.1038  noobj_loss: 0.1590  bbox_loss: 0.0270  cls_loss: 0.0849  \n",
      "<<<iteration:[560/657] - total_loss: 0.3742  obj_loss: 0.0825  noobj_loss: 0.1471  bbox_loss: 0.0270  cls_loss: 0.0832  \n",
      "<<<iteration:[580/657] - total_loss: 0.4020  obj_loss: 0.1070  noobj_loss: 0.1550  bbox_loss: 0.0253  cls_loss: 0.0910  \n",
      "<<<iteration:[600/657] - total_loss: 0.4105  obj_loss: 0.1085  noobj_loss: 0.1599  bbox_loss: 0.0261  cls_loss: 0.0916  \n",
      "<<<iteration:[620/657] - total_loss: 0.4171  obj_loss: 0.1004  noobj_loss: 0.1530  bbox_loss: 0.0287  cls_loss: 0.0969  \n",
      "<<<iteration:[640/657] - total_loss: 0.4070  obj_loss: 0.1137  noobj_loss: 0.1607  bbox_loss: 0.0246  cls_loss: 0.0902  \n",
      "\n",
      "epoch:54/100 - Train Loss: 0.4030, Val Loss: 0.4153\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4356  obj_loss: 0.1098  noobj_loss: 0.1584  bbox_loss: 0.0263  cls_loss: 0.1152  \n",
      "<<<iteration:[40/657] - total_loss: 0.3851  obj_loss: 0.0992  noobj_loss: 0.1556  bbox_loss: 0.0250  cls_loss: 0.0833  \n",
      "<<<iteration:[60/657] - total_loss: 0.3888  obj_loss: 0.1048  noobj_loss: 0.1606  bbox_loss: 0.0255  cls_loss: 0.0762  \n",
      "<<<iteration:[80/657] - total_loss: 0.4047  obj_loss: 0.1005  noobj_loss: 0.1533  bbox_loss: 0.0267  cls_loss: 0.0940  \n",
      "<<<iteration:[100/657] - total_loss: 0.4580  obj_loss: 0.0981  noobj_loss: 0.1609  bbox_loss: 0.0369  cls_loss: 0.0949  \n",
      "<<<iteration:[120/657] - total_loss: 0.4034  obj_loss: 0.1033  noobj_loss: 0.1594  bbox_loss: 0.0275  cls_loss: 0.0830  \n",
      "<<<iteration:[140/657] - total_loss: 0.4107  obj_loss: 0.1047  noobj_loss: 0.1543  bbox_loss: 0.0280  cls_loss: 0.0887  \n",
      "<<<iteration:[160/657] - total_loss: 0.4265  obj_loss: 0.1112  noobj_loss: 0.1511  bbox_loss: 0.0320  cls_loss: 0.0799  \n",
      "<<<iteration:[180/657] - total_loss: 0.4133  obj_loss: 0.1166  noobj_loss: 0.1566  bbox_loss: 0.0241  cls_loss: 0.0979  \n",
      "<<<iteration:[200/657] - total_loss: 0.3873  obj_loss: 0.0988  noobj_loss: 0.1513  bbox_loss: 0.0272  cls_loss: 0.0768  \n",
      "<<<iteration:[220/657] - total_loss: 0.3915  obj_loss: 0.1115  noobj_loss: 0.1541  bbox_loss: 0.0223  cls_loss: 0.0916  \n",
      "<<<iteration:[240/657] - total_loss: 0.3855  obj_loss: 0.1048  noobj_loss: 0.1562  bbox_loss: 0.0251  cls_loss: 0.0773  \n",
      "<<<iteration:[260/657] - total_loss: 0.3910  obj_loss: 0.1075  noobj_loss: 0.1607  bbox_loss: 0.0241  cls_loss: 0.0825  \n",
      "<<<iteration:[280/657] - total_loss: 0.3918  obj_loss: 0.1045  noobj_loss: 0.1527  bbox_loss: 0.0265  cls_loss: 0.0784  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[300/657] - total_loss: 0.3917  obj_loss: 0.1146  noobj_loss: 0.1539  bbox_loss: 0.0232  cls_loss: 0.0840  \n",
      "<<<iteration:[320/657] - total_loss: 0.4301  obj_loss: 0.1105  noobj_loss: 0.1631  bbox_loss: 0.0301  cls_loss: 0.0876  \n",
      "<<<iteration:[340/657] - total_loss: 0.3929  obj_loss: 0.0957  noobj_loss: 0.1561  bbox_loss: 0.0263  cls_loss: 0.0875  \n",
      "<<<iteration:[360/657] - total_loss: 0.3947  obj_loss: 0.1050  noobj_loss: 0.1543  bbox_loss: 0.0251  cls_loss: 0.0869  \n",
      "<<<iteration:[380/657] - total_loss: 0.3971  obj_loss: 0.1037  noobj_loss: 0.1501  bbox_loss: 0.0269  cls_loss: 0.0839  \n",
      "<<<iteration:[400/657] - total_loss: 0.4298  obj_loss: 0.1187  noobj_loss: 0.1538  bbox_loss: 0.0280  cls_loss: 0.0939  \n",
      "<<<iteration:[420/657] - total_loss: 0.3992  obj_loss: 0.0999  noobj_loss: 0.1520  bbox_loss: 0.0265  cls_loss: 0.0907  \n",
      "<<<iteration:[440/657] - total_loss: 0.3761  obj_loss: 0.1043  noobj_loss: 0.1509  bbox_loss: 0.0251  cls_loss: 0.0706  \n",
      "<<<iteration:[460/657] - total_loss: 0.4142  obj_loss: 0.1006  noobj_loss: 0.1607  bbox_loss: 0.0281  cls_loss: 0.0928  \n",
      "<<<iteration:[480/657] - total_loss: 0.4123  obj_loss: 0.1084  noobj_loss: 0.1512  bbox_loss: 0.0273  cls_loss: 0.0919  \n",
      "<<<iteration:[500/657] - total_loss: 0.4253  obj_loss: 0.1043  noobj_loss: 0.1483  bbox_loss: 0.0299  cls_loss: 0.0972  \n",
      "<<<iteration:[520/657] - total_loss: 0.3969  obj_loss: 0.0964  noobj_loss: 0.1568  bbox_loss: 0.0266  cls_loss: 0.0890  \n",
      "<<<iteration:[540/657] - total_loss: 0.4082  obj_loss: 0.1110  noobj_loss: 0.1547  bbox_loss: 0.0265  cls_loss: 0.0873  \n",
      "<<<iteration:[560/657] - total_loss: 0.3918  obj_loss: 0.1030  noobj_loss: 0.1491  bbox_loss: 0.0261  cls_loss: 0.0837  \n",
      "<<<iteration:[580/657] - total_loss: 0.4135  obj_loss: 0.1071  noobj_loss: 0.1661  bbox_loss: 0.0275  cls_loss: 0.0858  \n",
      "<<<iteration:[600/657] - total_loss: 0.3997  obj_loss: 0.1038  noobj_loss: 0.1524  bbox_loss: 0.0266  cls_loss: 0.0866  \n",
      "<<<iteration:[620/657] - total_loss: 0.4054  obj_loss: 0.1074  noobj_loss: 0.1473  bbox_loss: 0.0273  cls_loss: 0.0881  \n",
      "<<<iteration:[640/657] - total_loss: 0.3825  obj_loss: 0.1037  noobj_loss: 0.1516  bbox_loss: 0.0248  cls_loss: 0.0788  \n",
      "\n",
      "epoch:55/100 - Train Loss: 0.4035, Val Loss: 0.4710\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4374  obj_loss: 0.1098  noobj_loss: 0.1678  bbox_loss: 0.0292  cls_loss: 0.0978  \n",
      "<<<iteration:[40/657] - total_loss: 0.4258  obj_loss: 0.0936  noobj_loss: 0.1547  bbox_loss: 0.0345  cls_loss: 0.0823  \n",
      "<<<iteration:[60/657] - total_loss: 0.4015  obj_loss: 0.1066  noobj_loss: 0.1497  bbox_loss: 0.0284  cls_loss: 0.0779  \n",
      "<<<iteration:[80/657] - total_loss: 0.3724  obj_loss: 0.1082  noobj_loss: 0.1557  bbox_loss: 0.0241  cls_loss: 0.0659  \n",
      "<<<iteration:[100/657] - total_loss: 0.3640  obj_loss: 0.0996  noobj_loss: 0.1541  bbox_loss: 0.0230  cls_loss: 0.0724  \n",
      "<<<iteration:[120/657] - total_loss: 0.4243  obj_loss: 0.1065  noobj_loss: 0.1585  bbox_loss: 0.0286  cls_loss: 0.0952  \n",
      "<<<iteration:[140/657] - total_loss: 0.3847  obj_loss: 0.1074  noobj_loss: 0.1487  bbox_loss: 0.0262  cls_loss: 0.0722  \n",
      "<<<iteration:[160/657] - total_loss: 0.4288  obj_loss: 0.1071  noobj_loss: 0.1527  bbox_loss: 0.0309  cls_loss: 0.0911  \n",
      "<<<iteration:[180/657] - total_loss: 0.4026  obj_loss: 0.1002  noobj_loss: 0.1463  bbox_loss: 0.0278  cls_loss: 0.0904  \n",
      "<<<iteration:[200/657] - total_loss: 0.4089  obj_loss: 0.1132  noobj_loss: 0.1506  bbox_loss: 0.0255  cls_loss: 0.0928  \n",
      "<<<iteration:[220/657] - total_loss: 0.3967  obj_loss: 0.1126  noobj_loss: 0.1596  bbox_loss: 0.0250  cls_loss: 0.0795  \n",
      "<<<iteration:[240/657] - total_loss: 0.4295  obj_loss: 0.1185  noobj_loss: 0.1537  bbox_loss: 0.0276  cls_loss: 0.0961  \n",
      "<<<iteration:[260/657] - total_loss: 0.3786  obj_loss: 0.0974  noobj_loss: 0.1481  bbox_loss: 0.0264  cls_loss: 0.0752  \n",
      "<<<iteration:[280/657] - total_loss: 0.4122  obj_loss: 0.1002  noobj_loss: 0.1519  bbox_loss: 0.0271  cls_loss: 0.1007  \n",
      "<<<iteration:[300/657] - total_loss: 0.3950  obj_loss: 0.0988  noobj_loss: 0.1488  bbox_loss: 0.0282  cls_loss: 0.0809  \n",
      "<<<iteration:[320/657] - total_loss: 0.3926  obj_loss: 0.1056  noobj_loss: 0.1517  bbox_loss: 0.0239  cls_loss: 0.0919  \n",
      "<<<iteration:[340/657] - total_loss: 0.4125  obj_loss: 0.1153  noobj_loss: 0.1539  bbox_loss: 0.0252  cls_loss: 0.0943  \n",
      "<<<iteration:[360/657] - total_loss: 0.3881  obj_loss: 0.0995  noobj_loss: 0.1538  bbox_loss: 0.0256  cls_loss: 0.0838  \n",
      "<<<iteration:[380/657] - total_loss: 0.3956  obj_loss: 0.1144  noobj_loss: 0.1585  bbox_loss: 0.0259  cls_loss: 0.0724  \n",
      "<<<iteration:[400/657] - total_loss: 0.3783  obj_loss: 0.1017  noobj_loss: 0.1477  bbox_loss: 0.0248  cls_loss: 0.0786  \n",
      "<<<iteration:[420/657] - total_loss: 0.4085  obj_loss: 0.1042  noobj_loss: 0.1635  bbox_loss: 0.0283  cls_loss: 0.0809  \n",
      "<<<iteration:[440/657] - total_loss: 0.3809  obj_loss: 0.1079  noobj_loss: 0.1492  bbox_loss: 0.0237  cls_loss: 0.0802  \n",
      "<<<iteration:[460/657] - total_loss: 0.4021  obj_loss: 0.1084  noobj_loss: 0.1453  bbox_loss: 0.0304  cls_loss: 0.0692  \n",
      "<<<iteration:[480/657] - total_loss: 0.3856  obj_loss: 0.0979  noobj_loss: 0.1502  bbox_loss: 0.0263  cls_loss: 0.0813  \n",
      "<<<iteration:[500/657] - total_loss: 0.4165  obj_loss: 0.1036  noobj_loss: 0.1511  bbox_loss: 0.0281  cls_loss: 0.0966  \n",
      "<<<iteration:[520/657] - total_loss: 0.4019  obj_loss: 0.1100  noobj_loss: 0.1555  bbox_loss: 0.0265  cls_loss: 0.0815  \n",
      "<<<iteration:[540/657] - total_loss: 0.4005  obj_loss: 0.1040  noobj_loss: 0.1517  bbox_loss: 0.0251  cls_loss: 0.0953  \n",
      "<<<iteration:[560/657] - total_loss: 0.3998  obj_loss: 0.1006  noobj_loss: 0.1547  bbox_loss: 0.0284  cls_loss: 0.0799  \n",
      "<<<iteration:[580/657] - total_loss: 0.3941  obj_loss: 0.1123  noobj_loss: 0.1583  bbox_loss: 0.0255  cls_loss: 0.0751  \n",
      "<<<iteration:[600/657] - total_loss: 0.3596  obj_loss: 0.1075  noobj_loss: 0.1493  bbox_loss: 0.0208  cls_loss: 0.0734  \n",
      "<<<iteration:[620/657] - total_loss: 0.4003  obj_loss: 0.1024  noobj_loss: 0.1459  bbox_loss: 0.0267  cls_loss: 0.0914  \n",
      "<<<iteration:[640/657] - total_loss: 0.4035  obj_loss: 0.0944  noobj_loss: 0.1521  bbox_loss: 0.0279  cls_loss: 0.0934  \n",
      "\n",
      "epoch:56/100 - Train Loss: 0.3992, Val Loss: 0.4233\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4284  obj_loss: 0.1042  noobj_loss: 0.1603  bbox_loss: 0.0305  cls_loss: 0.0914  \n",
      "<<<iteration:[40/657] - total_loss: 0.4502  obj_loss: 0.1092  noobj_loss: 0.1522  bbox_loss: 0.0351  cls_loss: 0.0893  \n",
      "<<<iteration:[60/657] - total_loss: 0.3939  obj_loss: 0.0992  noobj_loss: 0.1539  bbox_loss: 0.0258  cls_loss: 0.0887  \n",
      "<<<iteration:[80/657] - total_loss: 0.3977  obj_loss: 0.1086  noobj_loss: 0.1605  bbox_loss: 0.0255  cls_loss: 0.0811  \n",
      "<<<iteration:[100/657] - total_loss: 0.3830  obj_loss: 0.1195  noobj_loss: 0.1528  bbox_loss: 0.0216  cls_loss: 0.0790  \n",
      "<<<iteration:[120/657] - total_loss: 0.4082  obj_loss: 0.0983  noobj_loss: 0.1514  bbox_loss: 0.0289  cls_loss: 0.0896  \n",
      "<<<iteration:[140/657] - total_loss: 0.4162  obj_loss: 0.0957  noobj_loss: 0.1595  bbox_loss: 0.0315  cls_loss: 0.0835  \n",
      "<<<iteration:[160/657] - total_loss: 0.4077  obj_loss: 0.1046  noobj_loss: 0.1540  bbox_loss: 0.0282  cls_loss: 0.0850  \n",
      "<<<iteration:[180/657] - total_loss: 0.3703  obj_loss: 0.1082  noobj_loss: 0.1487  bbox_loss: 0.0222  cls_loss: 0.0766  \n",
      "<<<iteration:[200/657] - total_loss: 0.3872  obj_loss: 0.1054  noobj_loss: 0.1439  bbox_loss: 0.0257  cls_loss: 0.0812  \n",
      "<<<iteration:[220/657] - total_loss: 0.3907  obj_loss: 0.1028  noobj_loss: 0.1602  bbox_loss: 0.0245  cls_loss: 0.0851  \n",
      "<<<iteration:[240/657] - total_loss: 0.4136  obj_loss: 0.1056  noobj_loss: 0.1523  bbox_loss: 0.0271  cls_loss: 0.0965  \n",
      "<<<iteration:[260/657] - total_loss: 0.3947  obj_loss: 0.1091  noobj_loss: 0.1639  bbox_loss: 0.0267  cls_loss: 0.0702  \n",
      "<<<iteration:[280/657] - total_loss: 0.3915  obj_loss: 0.0998  noobj_loss: 0.1509  bbox_loss: 0.0274  cls_loss: 0.0790  \n",
      "<<<iteration:[300/657] - total_loss: 0.3836  obj_loss: 0.0926  noobj_loss: 0.1437  bbox_loss: 0.0251  cls_loss: 0.0935  \n",
      "<<<iteration:[320/657] - total_loss: 0.4072  obj_loss: 0.1148  noobj_loss: 0.1603  bbox_loss: 0.0251  cls_loss: 0.0866  \n",
      "<<<iteration:[340/657] - total_loss: 0.3744  obj_loss: 0.0991  noobj_loss: 0.1493  bbox_loss: 0.0246  cls_loss: 0.0774  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[360/657] - total_loss: 0.3777  obj_loss: 0.1077  noobj_loss: 0.1402  bbox_loss: 0.0240  cls_loss: 0.0801  \n",
      "<<<iteration:[380/657] - total_loss: 0.3916  obj_loss: 0.1124  noobj_loss: 0.1533  bbox_loss: 0.0233  cls_loss: 0.0862  \n",
      "<<<iteration:[400/657] - total_loss: 0.3810  obj_loss: 0.1038  noobj_loss: 0.1567  bbox_loss: 0.0240  cls_loss: 0.0787  \n",
      "<<<iteration:[420/657] - total_loss: 0.3802  obj_loss: 0.1081  noobj_loss: 0.1537  bbox_loss: 0.0234  cls_loss: 0.0781  \n",
      "<<<iteration:[440/657] - total_loss: 0.4027  obj_loss: 0.1100  noobj_loss: 0.1561  bbox_loss: 0.0263  cls_loss: 0.0832  \n",
      "<<<iteration:[460/657] - total_loss: 0.4078  obj_loss: 0.1176  noobj_loss: 0.1538  bbox_loss: 0.0234  cls_loss: 0.0962  \n",
      "<<<iteration:[480/657] - total_loss: 0.4026  obj_loss: 0.1151  noobj_loss: 0.1521  bbox_loss: 0.0242  cls_loss: 0.0903  \n",
      "<<<iteration:[500/657] - total_loss: 0.3930  obj_loss: 0.1045  noobj_loss: 0.1521  bbox_loss: 0.0264  cls_loss: 0.0806  \n",
      "<<<iteration:[520/657] - total_loss: 0.4285  obj_loss: 0.0962  noobj_loss: 0.1508  bbox_loss: 0.0325  cls_loss: 0.0945  \n",
      "<<<iteration:[540/657] - total_loss: 0.3830  obj_loss: 0.0925  noobj_loss: 0.1492  bbox_loss: 0.0273  cls_loss: 0.0796  \n",
      "<<<iteration:[560/657] - total_loss: 0.4084  obj_loss: 0.0985  noobj_loss: 0.1459  bbox_loss: 0.0313  cls_loss: 0.0805  \n",
      "<<<iteration:[580/657] - total_loss: 0.4174  obj_loss: 0.1092  noobj_loss: 0.1592  bbox_loss: 0.0253  cls_loss: 0.1019  \n",
      "<<<iteration:[600/657] - total_loss: 0.3842  obj_loss: 0.0937  noobj_loss: 0.1516  bbox_loss: 0.0270  cls_loss: 0.0799  \n",
      "<<<iteration:[620/657] - total_loss: 0.3857  obj_loss: 0.1172  noobj_loss: 0.1549  bbox_loss: 0.0234  cls_loss: 0.0742  \n",
      "<<<iteration:[640/657] - total_loss: 0.3974  obj_loss: 0.0958  noobj_loss: 0.1472  bbox_loss: 0.0259  cls_loss: 0.0982  \n",
      "\n",
      "epoch:57/100 - Train Loss: 0.3975, Val Loss: 0.4163\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4103  obj_loss: 0.1119  noobj_loss: 0.1627  bbox_loss: 0.0271  cls_loss: 0.0818  \n",
      "<<<iteration:[40/657] - total_loss: 0.3866  obj_loss: 0.1128  noobj_loss: 0.1506  bbox_loss: 0.0237  cls_loss: 0.0802  \n",
      "<<<iteration:[60/657] - total_loss: 0.3886  obj_loss: 0.1073  noobj_loss: 0.1461  bbox_loss: 0.0261  cls_loss: 0.0777  \n",
      "<<<iteration:[80/657] - total_loss: 0.3979  obj_loss: 0.1029  noobj_loss: 0.1534  bbox_loss: 0.0272  cls_loss: 0.0824  \n",
      "<<<iteration:[100/657] - total_loss: 0.3964  obj_loss: 0.1068  noobj_loss: 0.1514  bbox_loss: 0.0249  cls_loss: 0.0894  \n",
      "<<<iteration:[120/657] - total_loss: 0.4033  obj_loss: 0.1184  noobj_loss: 0.1525  bbox_loss: 0.0260  cls_loss: 0.0785  \n",
      "<<<iteration:[140/657] - total_loss: 0.3687  obj_loss: 0.1099  noobj_loss: 0.1475  bbox_loss: 0.0215  cls_loss: 0.0778  \n",
      "<<<iteration:[160/657] - total_loss: 0.3897  obj_loss: 0.1154  noobj_loss: 0.1476  bbox_loss: 0.0237  cls_loss: 0.0820  \n",
      "<<<iteration:[180/657] - total_loss: 0.3745  obj_loss: 0.1107  noobj_loss: 0.1483  bbox_loss: 0.0227  cls_loss: 0.0763  \n",
      "<<<iteration:[200/657] - total_loss: 0.3891  obj_loss: 0.1127  noobj_loss: 0.1536  bbox_loss: 0.0243  cls_loss: 0.0782  \n",
      "<<<iteration:[220/657] - total_loss: 0.3820  obj_loss: 0.0923  noobj_loss: 0.1467  bbox_loss: 0.0259  cls_loss: 0.0866  \n",
      "<<<iteration:[240/657] - total_loss: 0.4171  obj_loss: 0.1093  noobj_loss: 0.1492  bbox_loss: 0.0262  cls_loss: 0.1020  \n",
      "<<<iteration:[260/657] - total_loss: 0.4239  obj_loss: 0.1134  noobj_loss: 0.1490  bbox_loss: 0.0264  cls_loss: 0.1041  \n",
      "<<<iteration:[280/657] - total_loss: 0.4138  obj_loss: 0.1138  noobj_loss: 0.1496  bbox_loss: 0.0283  cls_loss: 0.0837  \n",
      "<<<iteration:[300/657] - total_loss: 0.3853  obj_loss: 0.1036  noobj_loss: 0.1549  bbox_loss: 0.0268  cls_loss: 0.0701  \n",
      "<<<iteration:[320/657] - total_loss: 0.3865  obj_loss: 0.1084  noobj_loss: 0.1542  bbox_loss: 0.0232  cls_loss: 0.0849  \n",
      "<<<iteration:[340/657] - total_loss: 0.3650  obj_loss: 0.0939  noobj_loss: 0.1503  bbox_loss: 0.0250  cls_loss: 0.0707  \n",
      "<<<iteration:[360/657] - total_loss: 0.3839  obj_loss: 0.1224  noobj_loss: 0.1441  bbox_loss: 0.0223  cls_loss: 0.0777  \n",
      "<<<iteration:[380/657] - total_loss: 0.4142  obj_loss: 0.1053  noobj_loss: 0.1703  bbox_loss: 0.0277  cls_loss: 0.0851  \n",
      "<<<iteration:[400/657] - total_loss: 0.3772  obj_loss: 0.1134  noobj_loss: 0.1446  bbox_loss: 0.0243  cls_loss: 0.0699  \n",
      "<<<iteration:[420/657] - total_loss: 0.4126  obj_loss: 0.1011  noobj_loss: 0.1509  bbox_loss: 0.0298  cls_loss: 0.0872  \n",
      "<<<iteration:[440/657] - total_loss: 0.3978  obj_loss: 0.1026  noobj_loss: 0.1514  bbox_loss: 0.0261  cls_loss: 0.0889  \n",
      "<<<iteration:[460/657] - total_loss: 0.4023  obj_loss: 0.1006  noobj_loss: 0.1536  bbox_loss: 0.0246  cls_loss: 0.1019  \n",
      "<<<iteration:[480/657] - total_loss: 0.3922  obj_loss: 0.1115  noobj_loss: 0.1515  bbox_loss: 0.0227  cls_loss: 0.0914  \n",
      "<<<iteration:[500/657] - total_loss: 0.3956  obj_loss: 0.1119  noobj_loss: 0.1533  bbox_loss: 0.0262  cls_loss: 0.0761  \n",
      "<<<iteration:[520/657] - total_loss: 0.4102  obj_loss: 0.1104  noobj_loss: 0.1501  bbox_loss: 0.0281  cls_loss: 0.0842  \n",
      "<<<iteration:[540/657] - total_loss: 0.3798  obj_loss: 0.1103  noobj_loss: 0.1505  bbox_loss: 0.0222  cls_loss: 0.0832  \n",
      "<<<iteration:[560/657] - total_loss: 0.4051  obj_loss: 0.1140  noobj_loss: 0.1471  bbox_loss: 0.0278  cls_loss: 0.0788  \n",
      "<<<iteration:[580/657] - total_loss: 0.3928  obj_loss: 0.0986  noobj_loss: 0.1571  bbox_loss: 0.0280  cls_loss: 0.0758  \n",
      "<<<iteration:[600/657] - total_loss: 0.3915  obj_loss: 0.1033  noobj_loss: 0.1507  bbox_loss: 0.0261  cls_loss: 0.0824  \n",
      "<<<iteration:[620/657] - total_loss: 0.3921  obj_loss: 0.1174  noobj_loss: 0.1505  bbox_loss: 0.0235  cls_loss: 0.0820  \n",
      "<<<iteration:[640/657] - total_loss: 0.3883  obj_loss: 0.1008  noobj_loss: 0.1440  bbox_loss: 0.0230  cls_loss: 0.1005  \n",
      "\n",
      "epoch:58/100 - Train Loss: 0.3932, Val Loss: 0.4115\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4264  obj_loss: 0.1199  noobj_loss: 0.1523  bbox_loss: 0.0279  cls_loss: 0.0910  \n",
      "<<<iteration:[40/657] - total_loss: 0.3962  obj_loss: 0.1155  noobj_loss: 0.1525  bbox_loss: 0.0224  cls_loss: 0.0924  \n",
      "<<<iteration:[60/657] - total_loss: 0.3875  obj_loss: 0.1083  noobj_loss: 0.1471  bbox_loss: 0.0250  cls_loss: 0.0805  \n",
      "<<<iteration:[80/657] - total_loss: 0.3911  obj_loss: 0.0982  noobj_loss: 0.1567  bbox_loss: 0.0283  cls_loss: 0.0731  \n",
      "<<<iteration:[100/657] - total_loss: 0.4114  obj_loss: 0.1089  noobj_loss: 0.1559  bbox_loss: 0.0292  cls_loss: 0.0788  \n",
      "<<<iteration:[120/657] - total_loss: 0.3910  obj_loss: 0.1115  noobj_loss: 0.1489  bbox_loss: 0.0234  cls_loss: 0.0881  \n",
      "<<<iteration:[140/657] - total_loss: 0.3853  obj_loss: 0.1031  noobj_loss: 0.1472  bbox_loss: 0.0248  cls_loss: 0.0846  \n",
      "<<<iteration:[160/657] - total_loss: 0.3620  obj_loss: 0.1026  noobj_loss: 0.1539  bbox_loss: 0.0219  cls_loss: 0.0729  \n",
      "<<<iteration:[180/657] - total_loss: 0.3926  obj_loss: 0.1075  noobj_loss: 0.1548  bbox_loss: 0.0235  cls_loss: 0.0901  \n",
      "<<<iteration:[200/657] - total_loss: 0.4717  obj_loss: 0.0824  noobj_loss: 0.1495  bbox_loss: 0.0479  cls_loss: 0.0752  \n",
      "<<<iteration:[220/657] - total_loss: 0.3960  obj_loss: 0.0994  noobj_loss: 0.1498  bbox_loss: 0.0267  cls_loss: 0.0882  \n",
      "<<<iteration:[240/657] - total_loss: 0.3618  obj_loss: 0.0953  noobj_loss: 0.1442  bbox_loss: 0.0248  cls_loss: 0.0704  \n",
      "<<<iteration:[260/657] - total_loss: 0.4082  obj_loss: 0.1046  noobj_loss: 0.1511  bbox_loss: 0.0295  cls_loss: 0.0807  \n",
      "<<<iteration:[280/657] - total_loss: 0.3969  obj_loss: 0.1071  noobj_loss: 0.1514  bbox_loss: 0.0248  cls_loss: 0.0902  \n",
      "<<<iteration:[300/657] - total_loss: 0.4313  obj_loss: 0.0950  noobj_loss: 0.1572  bbox_loss: 0.0350  cls_loss: 0.0825  \n",
      "<<<iteration:[320/657] - total_loss: 0.3919  obj_loss: 0.1115  noobj_loss: 0.1561  bbox_loss: 0.0253  cls_loss: 0.0760  \n",
      "<<<iteration:[340/657] - total_loss: 0.3891  obj_loss: 0.1081  noobj_loss: 0.1481  bbox_loss: 0.0236  cls_loss: 0.0890  \n",
      "<<<iteration:[360/657] - total_loss: 0.3711  obj_loss: 0.1039  noobj_loss: 0.1476  bbox_loss: 0.0241  cls_loss: 0.0731  \n",
      "<<<iteration:[380/657] - total_loss: 0.3747  obj_loss: 0.1088  noobj_loss: 0.1512  bbox_loss: 0.0234  cls_loss: 0.0735  \n",
      "<<<iteration:[400/657] - total_loss: 0.4126  obj_loss: 0.1078  noobj_loss: 0.1523  bbox_loss: 0.0285  cls_loss: 0.0863  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[420/657] - total_loss: 0.4029  obj_loss: 0.1170  noobj_loss: 0.1470  bbox_loss: 0.0241  cls_loss: 0.0919  \n",
      "<<<iteration:[440/657] - total_loss: 0.4031  obj_loss: 0.1118  noobj_loss: 0.1531  bbox_loss: 0.0264  cls_loss: 0.0826  \n",
      "<<<iteration:[460/657] - total_loss: 0.3815  obj_loss: 0.1092  noobj_loss: 0.1468  bbox_loss: 0.0250  cls_loss: 0.0740  \n",
      "<<<iteration:[480/657] - total_loss: 0.3967  obj_loss: 0.1137  noobj_loss: 0.1537  bbox_loss: 0.0239  cls_loss: 0.0868  \n",
      "<<<iteration:[500/657] - total_loss: 0.3862  obj_loss: 0.0981  noobj_loss: 0.1419  bbox_loss: 0.0243  cls_loss: 0.0955  \n",
      "<<<iteration:[520/657] - total_loss: 0.3979  obj_loss: 0.1114  noobj_loss: 0.1529  bbox_loss: 0.0253  cls_loss: 0.0837  \n",
      "<<<iteration:[540/657] - total_loss: 0.3710  obj_loss: 0.1035  noobj_loss: 0.1464  bbox_loss: 0.0234  cls_loss: 0.0772  \n",
      "<<<iteration:[560/657] - total_loss: 0.3956  obj_loss: 0.1140  noobj_loss: 0.1583  bbox_loss: 0.0235  cls_loss: 0.0850  \n",
      "<<<iteration:[580/657] - total_loss: 0.3789  obj_loss: 0.1096  noobj_loss: 0.1462  bbox_loss: 0.0241  cls_loss: 0.0755  \n",
      "<<<iteration:[600/657] - total_loss: 0.4188  obj_loss: 0.1030  noobj_loss: 0.1770  bbox_loss: 0.0275  cls_loss: 0.0896  \n",
      "<<<iteration:[620/657] - total_loss: 0.4134  obj_loss: 0.1151  noobj_loss: 0.1516  bbox_loss: 0.0260  cls_loss: 0.0923  \n",
      "<<<iteration:[640/657] - total_loss: 0.4058  obj_loss: 0.1075  noobj_loss: 0.1748  bbox_loss: 0.0252  cls_loss: 0.0852  \n",
      "\n",
      "epoch:59/100 - Train Loss: 0.3965, Val Loss: 0.4186\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4007  obj_loss: 0.1077  noobj_loss: 0.1550  bbox_loss: 0.0250  cls_loss: 0.0903  \n",
      "<<<iteration:[40/657] - total_loss: 0.3825  obj_loss: 0.0997  noobj_loss: 0.1530  bbox_loss: 0.0254  cls_loss: 0.0795  \n",
      "<<<iteration:[60/657] - total_loss: 0.3905  obj_loss: 0.0997  noobj_loss: 0.1461  bbox_loss: 0.0262  cls_loss: 0.0865  \n",
      "<<<iteration:[80/657] - total_loss: 0.4109  obj_loss: 0.1148  noobj_loss: 0.1523  bbox_loss: 0.0237  cls_loss: 0.1014  \n",
      "<<<iteration:[100/657] - total_loss: 0.3861  obj_loss: 0.0994  noobj_loss: 0.1441  bbox_loss: 0.0260  cls_loss: 0.0844  \n",
      "<<<iteration:[120/657] - total_loss: 0.4134  obj_loss: 0.1192  noobj_loss: 0.1495  bbox_loss: 0.0261  cls_loss: 0.0888  \n",
      "<<<iteration:[140/657] - total_loss: 0.3890  obj_loss: 0.0992  noobj_loss: 0.1495  bbox_loss: 0.0263  cls_loss: 0.0835  \n",
      "<<<iteration:[160/657] - total_loss: 0.3941  obj_loss: 0.1130  noobj_loss: 0.1478  bbox_loss: 0.0249  cls_loss: 0.0825  \n",
      "<<<iteration:[180/657] - total_loss: 0.3629  obj_loss: 0.1072  noobj_loss: 0.1516  bbox_loss: 0.0241  cls_loss: 0.0596  \n",
      "<<<iteration:[200/657] - total_loss: 0.3867  obj_loss: 0.1099  noobj_loss: 0.1508  bbox_loss: 0.0245  cls_loss: 0.0789  \n",
      "<<<iteration:[220/657] - total_loss: 0.3889  obj_loss: 0.1096  noobj_loss: 0.1453  bbox_loss: 0.0266  cls_loss: 0.0737  \n",
      "<<<iteration:[240/657] - total_loss: 0.3811  obj_loss: 0.1058  noobj_loss: 0.1430  bbox_loss: 0.0264  cls_loss: 0.0717  \n",
      "<<<iteration:[260/657] - total_loss: 0.3924  obj_loss: 0.1007  noobj_loss: 0.1518  bbox_loss: 0.0269  cls_loss: 0.0816  \n",
      "<<<iteration:[280/657] - total_loss: 0.3705  obj_loss: 0.1044  noobj_loss: 0.1470  bbox_loss: 0.0236  cls_loss: 0.0747  \n",
      "<<<iteration:[300/657] - total_loss: 0.3704  obj_loss: 0.0913  noobj_loss: 0.1495  bbox_loss: 0.0283  cls_loss: 0.0631  \n",
      "<<<iteration:[320/657] - total_loss: 0.4173  obj_loss: 0.1134  noobj_loss: 0.1541  bbox_loss: 0.0277  cls_loss: 0.0885  \n",
      "<<<iteration:[340/657] - total_loss: 0.3998  obj_loss: 0.1101  noobj_loss: 0.1492  bbox_loss: 0.0228  cls_loss: 0.1009  \n",
      "<<<iteration:[360/657] - total_loss: 0.3965  obj_loss: 0.1079  noobj_loss: 0.1513  bbox_loss: 0.0257  cls_loss: 0.0844  \n",
      "<<<iteration:[380/657] - total_loss: 0.3987  obj_loss: 0.1012  noobj_loss: 0.1543  bbox_loss: 0.0273  cls_loss: 0.0837  \n",
      "<<<iteration:[400/657] - total_loss: 0.3814  obj_loss: 0.1105  noobj_loss: 0.1494  bbox_loss: 0.0232  cls_loss: 0.0802  \n",
      "<<<iteration:[420/657] - total_loss: 0.3940  obj_loss: 0.1014  noobj_loss: 0.1475  bbox_loss: 0.0276  cls_loss: 0.0807  \n",
      "<<<iteration:[440/657] - total_loss: 0.4011  obj_loss: 0.1120  noobj_loss: 0.1524  bbox_loss: 0.0253  cls_loss: 0.0861  \n",
      "<<<iteration:[460/657] - total_loss: 0.3782  obj_loss: 0.0971  noobj_loss: 0.1398  bbox_loss: 0.0240  cls_loss: 0.0910  \n",
      "<<<iteration:[480/657] - total_loss: 0.3781  obj_loss: 0.1190  noobj_loss: 0.1408  bbox_loss: 0.0216  cls_loss: 0.0806  \n",
      "<<<iteration:[500/657] - total_loss: 0.3660  obj_loss: 0.1064  noobj_loss: 0.1451  bbox_loss: 0.0218  cls_loss: 0.0781  \n",
      "<<<iteration:[520/657] - total_loss: 0.3903  obj_loss: 0.1068  noobj_loss: 0.1531  bbox_loss: 0.0246  cls_loss: 0.0837  \n",
      "<<<iteration:[540/657] - total_loss: 0.3759  obj_loss: 0.1068  noobj_loss: 0.1477  bbox_loss: 0.0250  cls_loss: 0.0701  \n",
      "<<<iteration:[560/657] - total_loss: 0.3747  obj_loss: 0.1001  noobj_loss: 0.1438  bbox_loss: 0.0230  cls_loss: 0.0880  \n",
      "<<<iteration:[580/657] - total_loss: 0.3861  obj_loss: 0.1003  noobj_loss: 0.1493  bbox_loss: 0.0260  cls_loss: 0.0812  \n",
      "<<<iteration:[600/657] - total_loss: 0.3817  obj_loss: 0.1061  noobj_loss: 0.1507  bbox_loss: 0.0236  cls_loss: 0.0822  \n",
      "<<<iteration:[620/657] - total_loss: 0.3854  obj_loss: 0.1187  noobj_loss: 0.1453  bbox_loss: 0.0227  cls_loss: 0.0807  \n",
      "<<<iteration:[640/657] - total_loss: 0.3948  obj_loss: 0.1155  noobj_loss: 0.1549  bbox_loss: 0.0252  cls_loss: 0.0760  \n",
      "\n",
      "epoch:60/100 - Train Loss: 0.3877, Val Loss: 0.4235\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4207  obj_loss: 0.1183  noobj_loss: 0.1589  bbox_loss: 0.0269  cls_loss: 0.0885  \n",
      "<<<iteration:[40/657] - total_loss: 0.4073  obj_loss: 0.1040  noobj_loss: 0.1456  bbox_loss: 0.0268  cls_loss: 0.0967  \n",
      "<<<iteration:[60/657] - total_loss: 0.3775  obj_loss: 0.1026  noobj_loss: 0.1422  bbox_loss: 0.0249  cls_loss: 0.0793  \n",
      "<<<iteration:[80/657] - total_loss: 0.3771  obj_loss: 0.1075  noobj_loss: 0.1464  bbox_loss: 0.0239  cls_loss: 0.0771  \n",
      "<<<iteration:[100/657] - total_loss: 0.3699  obj_loss: 0.1110  noobj_loss: 0.1426  bbox_loss: 0.0248  cls_loss: 0.0635  \n",
      "<<<iteration:[120/657] - total_loss: 0.4114  obj_loss: 0.0945  noobj_loss: 0.1467  bbox_loss: 0.0297  cls_loss: 0.0952  \n",
      "<<<iteration:[140/657] - total_loss: 0.3707  obj_loss: 0.1088  noobj_loss: 0.1586  bbox_loss: 0.0221  cls_loss: 0.0721  \n",
      "<<<iteration:[160/657] - total_loss: 0.3843  obj_loss: 0.1118  noobj_loss: 0.1434  bbox_loss: 0.0250  cls_loss: 0.0757  \n",
      "<<<iteration:[180/657] - total_loss: 0.3710  obj_loss: 0.0994  noobj_loss: 0.1418  bbox_loss: 0.0247  cls_loss: 0.0773  \n",
      "<<<iteration:[200/657] - total_loss: 0.3918  obj_loss: 0.1097  noobj_loss: 0.1421  bbox_loss: 0.0235  cls_loss: 0.0933  \n",
      "<<<iteration:[220/657] - total_loss: 0.3849  obj_loss: 0.1039  noobj_loss: 0.1512  bbox_loss: 0.0250  cls_loss: 0.0802  \n",
      "<<<iteration:[240/657] - total_loss: 0.3893  obj_loss: 0.1142  noobj_loss: 0.1485  bbox_loss: 0.0237  cls_loss: 0.0823  \n",
      "<<<iteration:[260/657] - total_loss: 0.3875  obj_loss: 0.1205  noobj_loss: 0.1488  bbox_loss: 0.0230  cls_loss: 0.0774  \n",
      "<<<iteration:[280/657] - total_loss: 0.3785  obj_loss: 0.1123  noobj_loss: 0.1450  bbox_loss: 0.0239  cls_loss: 0.0743  \n",
      "<<<iteration:[300/657] - total_loss: 0.3912  obj_loss: 0.1045  noobj_loss: 0.1430  bbox_loss: 0.0258  cls_loss: 0.0864  \n",
      "<<<iteration:[320/657] - total_loss: 0.3919  obj_loss: 0.1117  noobj_loss: 0.1449  bbox_loss: 0.0243  cls_loss: 0.0864  \n",
      "<<<iteration:[340/657] - total_loss: 0.3807  obj_loss: 0.1113  noobj_loss: 0.1454  bbox_loss: 0.0227  cls_loss: 0.0831  \n",
      "<<<iteration:[360/657] - total_loss: 0.3862  obj_loss: 0.1076  noobj_loss: 0.1503  bbox_loss: 0.0250  cls_loss: 0.0787  \n",
      "<<<iteration:[380/657] - total_loss: 0.3757  obj_loss: 0.1112  noobj_loss: 0.1507  bbox_loss: 0.0236  cls_loss: 0.0713  \n",
      "<<<iteration:[400/657] - total_loss: 0.3866  obj_loss: 0.1109  noobj_loss: 0.1447  bbox_loss: 0.0243  cls_loss: 0.0818  \n",
      "<<<iteration:[420/657] - total_loss: 0.4029  obj_loss: 0.1051  noobj_loss: 0.1483  bbox_loss: 0.0270  cls_loss: 0.0885  \n",
      "<<<iteration:[440/657] - total_loss: 0.3643  obj_loss: 0.1016  noobj_loss: 0.1505  bbox_loss: 0.0223  cls_loss: 0.0760  \n",
      "<<<iteration:[460/657] - total_loss: 0.3974  obj_loss: 0.1133  noobj_loss: 0.1513  bbox_loss: 0.0258  cls_loss: 0.0793  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[480/657] - total_loss: 0.3751  obj_loss: 0.1113  noobj_loss: 0.1452  bbox_loss: 0.0213  cls_loss: 0.0845  \n",
      "<<<iteration:[500/657] - total_loss: 0.3721  obj_loss: 0.0932  noobj_loss: 0.1474  bbox_loss: 0.0247  cls_loss: 0.0815  \n",
      "<<<iteration:[520/657] - total_loss: 0.3729  obj_loss: 0.1141  noobj_loss: 0.1470  bbox_loss: 0.0227  cls_loss: 0.0716  \n",
      "<<<iteration:[540/657] - total_loss: 0.3908  obj_loss: 0.1016  noobj_loss: 0.1488  bbox_loss: 0.0256  cls_loss: 0.0870  \n",
      "<<<iteration:[560/657] - total_loss: 0.3869  obj_loss: 0.1047  noobj_loss: 0.1428  bbox_loss: 0.0248  cls_loss: 0.0867  \n",
      "<<<iteration:[580/657] - total_loss: 0.3814  obj_loss: 0.1109  noobj_loss: 0.1433  bbox_loss: 0.0244  cls_loss: 0.0771  \n",
      "<<<iteration:[600/657] - total_loss: 0.3951  obj_loss: 0.1117  noobj_loss: 0.1514  bbox_loss: 0.0247  cls_loss: 0.0843  \n",
      "<<<iteration:[620/657] - total_loss: 0.3935  obj_loss: 0.1032  noobj_loss: 0.1475  bbox_loss: 0.0236  cls_loss: 0.0986  \n",
      "<<<iteration:[640/657] - total_loss: 0.4052  obj_loss: 0.0971  noobj_loss: 0.1460  bbox_loss: 0.0293  cls_loss: 0.0886  \n",
      "\n",
      "epoch:61/100 - Train Loss: 0.3864, Val Loss: 0.4079\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4010  obj_loss: 0.1249  noobj_loss: 0.1504  bbox_loss: 0.0242  cls_loss: 0.0800  \n",
      "<<<iteration:[40/657] - total_loss: 0.3936  obj_loss: 0.1114  noobj_loss: 0.1586  bbox_loss: 0.0230  cls_loss: 0.0881  \n",
      "<<<iteration:[60/657] - total_loss: 0.3812  obj_loss: 0.1169  noobj_loss: 0.1426  bbox_loss: 0.0230  cls_loss: 0.0778  \n",
      "<<<iteration:[80/657] - total_loss: 0.3805  obj_loss: 0.0997  noobj_loss: 0.1458  bbox_loss: 0.0267  cls_loss: 0.0742  \n",
      "<<<iteration:[100/657] - total_loss: 0.4028  obj_loss: 0.1120  noobj_loss: 0.1570  bbox_loss: 0.0255  cls_loss: 0.0847  \n",
      "<<<iteration:[120/657] - total_loss: 0.3740  obj_loss: 0.0966  noobj_loss: 0.1436  bbox_loss: 0.0273  cls_loss: 0.0693  \n",
      "<<<iteration:[140/657] - total_loss: 0.3654  obj_loss: 0.1079  noobj_loss: 0.1413  bbox_loss: 0.0227  cls_loss: 0.0733  \n",
      "<<<iteration:[160/657] - total_loss: 0.3797  obj_loss: 0.1151  noobj_loss: 0.1434  bbox_loss: 0.0215  cls_loss: 0.0853  \n",
      "<<<iteration:[180/657] - total_loss: 0.3639  obj_loss: 0.1037  noobj_loss: 0.1422  bbox_loss: 0.0233  cls_loss: 0.0724  \n",
      "<<<iteration:[200/657] - total_loss: 0.3866  obj_loss: 0.1027  noobj_loss: 0.1529  bbox_loss: 0.0242  cls_loss: 0.0865  \n",
      "<<<iteration:[220/657] - total_loss: 0.3808  obj_loss: 0.1142  noobj_loss: 0.1513  bbox_loss: 0.0236  cls_loss: 0.0731  \n",
      "<<<iteration:[240/657] - total_loss: 0.3944  obj_loss: 0.1016  noobj_loss: 0.1486  bbox_loss: 0.0261  cls_loss: 0.0881  \n",
      "<<<iteration:[260/657] - total_loss: 0.3661  obj_loss: 0.0958  noobj_loss: 0.1483  bbox_loss: 0.0231  cls_loss: 0.0805  \n",
      "<<<iteration:[280/657] - total_loss: 0.4017  obj_loss: 0.1130  noobj_loss: 0.1478  bbox_loss: 0.0243  cls_loss: 0.0934  \n",
      "<<<iteration:[300/657] - total_loss: 0.3999  obj_loss: 0.1164  noobj_loss: 0.1465  bbox_loss: 0.0232  cls_loss: 0.0942  \n",
      "<<<iteration:[320/657] - total_loss: 0.3647  obj_loss: 0.1018  noobj_loss: 0.1463  bbox_loss: 0.0238  cls_loss: 0.0705  \n",
      "<<<iteration:[340/657] - total_loss: 0.3794  obj_loss: 0.1183  noobj_loss: 0.1421  bbox_loss: 0.0220  cls_loss: 0.0802  \n",
      "<<<iteration:[360/657] - total_loss: 0.3729  obj_loss: 0.1099  noobj_loss: 0.1465  bbox_loss: 0.0223  cls_loss: 0.0784  \n",
      "<<<iteration:[380/657] - total_loss: 0.3752  obj_loss: 0.1037  noobj_loss: 0.1469  bbox_loss: 0.0250  cls_loss: 0.0732  \n",
      "<<<iteration:[400/657] - total_loss: 0.3873  obj_loss: 0.0998  noobj_loss: 0.1393  bbox_loss: 0.0241  cls_loss: 0.0975  \n",
      "<<<iteration:[420/657] - total_loss: 0.3833  obj_loss: 0.1201  noobj_loss: 0.1421  bbox_loss: 0.0233  cls_loss: 0.0754  \n",
      "<<<iteration:[440/657] - total_loss: 0.3871  obj_loss: 0.1181  noobj_loss: 0.1442  bbox_loss: 0.0225  cls_loss: 0.0842  \n",
      "<<<iteration:[460/657] - total_loss: 0.3625  obj_loss: 0.1086  noobj_loss: 0.1532  bbox_loss: 0.0219  cls_loss: 0.0676  \n",
      "<<<iteration:[480/657] - total_loss: 0.3791  obj_loss: 0.1008  noobj_loss: 0.1516  bbox_loss: 0.0239  cls_loss: 0.0832  \n",
      "<<<iteration:[500/657] - total_loss: 0.3818  obj_loss: 0.1051  noobj_loss: 0.1485  bbox_loss: 0.0239  cls_loss: 0.0827  \n",
      "<<<iteration:[520/657] - total_loss: 0.3820  obj_loss: 0.1110  noobj_loss: 0.1494  bbox_loss: 0.0234  cls_loss: 0.0794  \n",
      "<<<iteration:[540/657] - total_loss: 0.4199  obj_loss: 0.1092  noobj_loss: 0.1459  bbox_loss: 0.0275  cls_loss: 0.1002  \n",
      "<<<iteration:[560/657] - total_loss: 0.4040  obj_loss: 0.1143  noobj_loss: 0.1509  bbox_loss: 0.0257  cls_loss: 0.0857  \n",
      "<<<iteration:[580/657] - total_loss: 0.3921  obj_loss: 0.0948  noobj_loss: 0.1469  bbox_loss: 0.0282  cls_loss: 0.0830  \n",
      "<<<iteration:[600/657] - total_loss: 0.3670  obj_loss: 0.1158  noobj_loss: 0.1455  bbox_loss: 0.0205  cls_loss: 0.0758  \n",
      "<<<iteration:[620/657] - total_loss: 0.3687  obj_loss: 0.1119  noobj_loss: 0.1401  bbox_loss: 0.0235  cls_loss: 0.0693  \n",
      "<<<iteration:[640/657] - total_loss: 0.3975  obj_loss: 0.1172  noobj_loss: 0.1444  bbox_loss: 0.0256  cls_loss: 0.0801  \n",
      "\n",
      "epoch:62/100 - Train Loss: 0.3839, Val Loss: 0.4386\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3976  obj_loss: 0.1184  noobj_loss: 0.1486  bbox_loss: 0.0251  cls_loss: 0.0797  \n",
      "<<<iteration:[40/657] - total_loss: 0.3715  obj_loss: 0.1094  noobj_loss: 0.1436  bbox_loss: 0.0236  cls_loss: 0.0725  \n",
      "<<<iteration:[60/657] - total_loss: 0.3895  obj_loss: 0.1129  noobj_loss: 0.1465  bbox_loss: 0.0248  cls_loss: 0.0796  \n",
      "<<<iteration:[80/657] - total_loss: 0.3616  obj_loss: 0.1014  noobj_loss: 0.1425  bbox_loss: 0.0229  cls_loss: 0.0743  \n",
      "<<<iteration:[100/657] - total_loss: 0.3724  obj_loss: 0.1094  noobj_loss: 0.1423  bbox_loss: 0.0226  cls_loss: 0.0790  \n",
      "<<<iteration:[120/657] - total_loss: 0.3808  obj_loss: 0.1014  noobj_loss: 0.1543  bbox_loss: 0.0255  cls_loss: 0.0746  \n",
      "<<<iteration:[140/657] - total_loss: 0.3607  obj_loss: 0.1058  noobj_loss: 0.1453  bbox_loss: 0.0225  cls_loss: 0.0696  \n",
      "<<<iteration:[160/657] - total_loss: 0.3805  obj_loss: 0.1130  noobj_loss: 0.1484  bbox_loss: 0.0234  cls_loss: 0.0760  \n",
      "<<<iteration:[180/657] - total_loss: 0.4094  obj_loss: 0.0993  noobj_loss: 0.1429  bbox_loss: 0.0321  cls_loss: 0.0780  \n",
      "<<<iteration:[200/657] - total_loss: 0.3821  obj_loss: 0.1145  noobj_loss: 0.1467  bbox_loss: 0.0232  cls_loss: 0.0781  \n",
      "<<<iteration:[220/657] - total_loss: 0.3663  obj_loss: 0.1005  noobj_loss: 0.1416  bbox_loss: 0.0246  cls_loss: 0.0720  \n",
      "<<<iteration:[240/657] - total_loss: 0.3978  obj_loss: 0.1194  noobj_loss: 0.1496  bbox_loss: 0.0234  cls_loss: 0.0865  \n",
      "<<<iteration:[260/657] - total_loss: 0.4226  obj_loss: 0.1270  noobj_loss: 0.1542  bbox_loss: 0.0251  cls_loss: 0.0929  \n",
      "<<<iteration:[280/657] - total_loss: 0.3719  obj_loss: 0.1014  noobj_loss: 0.1439  bbox_loss: 0.0236  cls_loss: 0.0807  \n",
      "<<<iteration:[300/657] - total_loss: 0.3563  obj_loss: 0.1001  noobj_loss: 0.1397  bbox_loss: 0.0231  cls_loss: 0.0711  \n",
      "<<<iteration:[320/657] - total_loss: 0.3722  obj_loss: 0.1050  noobj_loss: 0.1435  bbox_loss: 0.0238  cls_loss: 0.0765  \n",
      "<<<iteration:[340/657] - total_loss: 0.3737  obj_loss: 0.1023  noobj_loss: 0.1423  bbox_loss: 0.0239  cls_loss: 0.0805  \n",
      "<<<iteration:[360/657] - total_loss: 0.3663  obj_loss: 0.1079  noobj_loss: 0.1356  bbox_loss: 0.0227  cls_loss: 0.0770  \n",
      "<<<iteration:[380/657] - total_loss: 0.3838  obj_loss: 0.1042  noobj_loss: 0.1542  bbox_loss: 0.0239  cls_loss: 0.0828  \n",
      "<<<iteration:[400/657] - total_loss: 0.3901  obj_loss: 0.1092  noobj_loss: 0.1430  bbox_loss: 0.0253  cls_loss: 0.0827  \n",
      "<<<iteration:[420/657] - total_loss: 0.3900  obj_loss: 0.1042  noobj_loss: 0.1389  bbox_loss: 0.0247  cls_loss: 0.0928  \n",
      "<<<iteration:[440/657] - total_loss: 0.3791  obj_loss: 0.1117  noobj_loss: 0.1386  bbox_loss: 0.0218  cls_loss: 0.0892  \n",
      "<<<iteration:[460/657] - total_loss: 0.3877  obj_loss: 0.1131  noobj_loss: 0.1450  bbox_loss: 0.0241  cls_loss: 0.0817  \n",
      "<<<iteration:[480/657] - total_loss: 0.3953  obj_loss: 0.1027  noobj_loss: 0.1441  bbox_loss: 0.0267  cls_loss: 0.0870  \n",
      "<<<iteration:[500/657] - total_loss: 0.4646  obj_loss: 0.0888  noobj_loss: 0.1399  bbox_loss: 0.0444  cls_loss: 0.0837  \n",
      "<<<iteration:[520/657] - total_loss: 0.3912  obj_loss: 0.1127  noobj_loss: 0.1375  bbox_loss: 0.0265  cls_loss: 0.0771  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[540/657] - total_loss: 0.7786  obj_loss: 0.0728  noobj_loss: 0.1379  bbox_loss: 0.1103  cls_loss: 0.0852  \n",
      "<<<iteration:[560/657] - total_loss: 0.3984  obj_loss: 0.1093  noobj_loss: 0.1454  bbox_loss: 0.0269  cls_loss: 0.0817  \n",
      "<<<iteration:[580/657] - total_loss: 0.3897  obj_loss: 0.1005  noobj_loss: 0.1411  bbox_loss: 0.0298  cls_loss: 0.0696  \n",
      "<<<iteration:[600/657] - total_loss: 0.3758  obj_loss: 0.1016  noobj_loss: 0.1404  bbox_loss: 0.0231  cls_loss: 0.0885  \n",
      "<<<iteration:[620/657] - total_loss: 0.4065  obj_loss: 0.0989  noobj_loss: 0.1390  bbox_loss: 0.0309  cls_loss: 0.0835  \n",
      "<<<iteration:[640/657] - total_loss: 0.3771  obj_loss: 0.0996  noobj_loss: 0.1426  bbox_loss: 0.0254  cls_loss: 0.0792  \n",
      "\n",
      "epoch:63/100 - Train Loss: 0.3969, Val Loss: 0.4467\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4034  obj_loss: 0.1131  noobj_loss: 0.1525  bbox_loss: 0.0266  cls_loss: 0.0812  \n",
      "<<<iteration:[40/657] - total_loss: 0.3571  obj_loss: 0.1020  noobj_loss: 0.1463  bbox_loss: 0.0226  cls_loss: 0.0691  \n",
      "<<<iteration:[60/657] - total_loss: 0.4065  obj_loss: 0.1105  noobj_loss: 0.1451  bbox_loss: 0.0273  cls_loss: 0.0872  \n",
      "<<<iteration:[80/657] - total_loss: 0.4233  obj_loss: 0.0872  noobj_loss: 0.1390  bbox_loss: 0.0394  cls_loss: 0.0698  \n",
      "<<<iteration:[100/657] - total_loss: 0.3998  obj_loss: 0.1050  noobj_loss: 0.1428  bbox_loss: 0.0267  cls_loss: 0.0899  \n",
      "<<<iteration:[120/657] - total_loss: 0.3758  obj_loss: 0.1060  noobj_loss: 0.1378  bbox_loss: 0.0224  cls_loss: 0.0888  \n",
      "<<<iteration:[140/657] - total_loss: 0.3685  obj_loss: 0.1084  noobj_loss: 0.1370  bbox_loss: 0.0232  cls_loss: 0.0755  \n",
      "<<<iteration:[160/657] - total_loss: 0.3870  obj_loss: 0.0953  noobj_loss: 0.1422  bbox_loss: 0.0272  cls_loss: 0.0843  \n",
      "<<<iteration:[180/657] - total_loss: 0.3842  obj_loss: 0.1031  noobj_loss: 0.1397  bbox_loss: 0.0269  cls_loss: 0.0766  \n",
      "<<<iteration:[200/657] - total_loss: 0.3689  obj_loss: 0.1023  noobj_loss: 0.1387  bbox_loss: 0.0227  cls_loss: 0.0839  \n",
      "<<<iteration:[220/657] - total_loss: 0.4070  obj_loss: 0.1152  noobj_loss: 0.1440  bbox_loss: 0.0251  cls_loss: 0.0945  \n",
      "<<<iteration:[240/657] - total_loss: 0.4127  obj_loss: 0.1179  noobj_loss: 0.1488  bbox_loss: 0.0246  cls_loss: 0.0975  \n",
      "<<<iteration:[260/657] - total_loss: 0.3948  obj_loss: 0.1060  noobj_loss: 0.1520  bbox_loss: 0.0253  cls_loss: 0.0863  \n",
      "<<<iteration:[280/657] - total_loss: 0.3920  obj_loss: 0.1089  noobj_loss: 0.1468  bbox_loss: 0.0249  cls_loss: 0.0852  \n",
      "<<<iteration:[300/657] - total_loss: 0.3813  obj_loss: 0.1155  noobj_loss: 0.1458  bbox_loss: 0.0240  cls_loss: 0.0728  \n",
      "<<<iteration:[320/657] - total_loss: 0.3817  obj_loss: 0.1098  noobj_loss: 0.1444  bbox_loss: 0.0248  cls_loss: 0.0759  \n",
      "<<<iteration:[340/657] - total_loss: 0.3702  obj_loss: 0.1027  noobj_loss: 0.1370  bbox_loss: 0.0250  cls_loss: 0.0739  \n",
      "<<<iteration:[360/657] - total_loss: 0.4120  obj_loss: 0.1139  noobj_loss: 0.1449  bbox_loss: 0.0286  cls_loss: 0.0825  \n",
      "<<<iteration:[380/657] - total_loss: 0.3890  obj_loss: 0.1151  noobj_loss: 0.1366  bbox_loss: 0.0259  cls_loss: 0.0763  \n",
      "<<<iteration:[400/657] - total_loss: 0.3694  obj_loss: 0.1063  noobj_loss: 0.1414  bbox_loss: 0.0243  cls_loss: 0.0710  \n",
      "<<<iteration:[420/657] - total_loss: 0.3625  obj_loss: 0.1148  noobj_loss: 0.1391  bbox_loss: 0.0219  cls_loss: 0.0687  \n",
      "<<<iteration:[440/657] - total_loss: 0.3723  obj_loss: 0.1103  noobj_loss: 0.1429  bbox_loss: 0.0228  cls_loss: 0.0766  \n",
      "<<<iteration:[460/657] - total_loss: 0.3780  obj_loss: 0.1079  noobj_loss: 0.1429  bbox_loss: 0.0238  cls_loss: 0.0799  \n",
      "<<<iteration:[480/657] - total_loss: 0.3848  obj_loss: 0.1093  noobj_loss: 0.1413  bbox_loss: 0.0230  cls_loss: 0.0900  \n",
      "<<<iteration:[500/657] - total_loss: 0.3748  obj_loss: 0.1058  noobj_loss: 0.1417  bbox_loss: 0.0244  cls_loss: 0.0764  \n",
      "<<<iteration:[520/657] - total_loss: 0.3720  obj_loss: 0.1070  noobj_loss: 0.1430  bbox_loss: 0.0224  cls_loss: 0.0814  \n",
      "<<<iteration:[540/657] - total_loss: 0.3786  obj_loss: 0.1120  noobj_loss: 0.1394  bbox_loss: 0.0246  cls_loss: 0.0739  \n",
      "<<<iteration:[560/657] - total_loss: 0.3939  obj_loss: 0.1129  noobj_loss: 0.1453  bbox_loss: 0.0245  cls_loss: 0.0860  \n",
      "<<<iteration:[580/657] - total_loss: 0.3655  obj_loss: 0.1184  noobj_loss: 0.1430  bbox_loss: 0.0199  cls_loss: 0.0763  \n",
      "<<<iteration:[600/657] - total_loss: 0.3907  obj_loss: 0.1130  noobj_loss: 0.1497  bbox_loss: 0.0250  cls_loss: 0.0779  \n",
      "<<<iteration:[620/657] - total_loss: 0.3991  obj_loss: 0.1095  noobj_loss: 0.1496  bbox_loss: 0.0281  cls_loss: 0.0744  \n",
      "<<<iteration:[640/657] - total_loss: 0.3788  obj_loss: 0.0964  noobj_loss: 0.1388  bbox_loss: 0.0261  cls_loss: 0.0824  \n",
      "\n",
      "epoch:64/100 - Train Loss: 0.3848, Val Loss: 0.4086\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3986  obj_loss: 0.1104  noobj_loss: 0.1489  bbox_loss: 0.0256  cls_loss: 0.0857  \n",
      "<<<iteration:[40/657] - total_loss: 0.3687  obj_loss: 0.1080  noobj_loss: 0.1368  bbox_loss: 0.0231  cls_loss: 0.0769  \n",
      "<<<iteration:[60/657] - total_loss: 0.3707  obj_loss: 0.1122  noobj_loss: 0.1424  bbox_loss: 0.0227  cls_loss: 0.0739  \n",
      "<<<iteration:[80/657] - total_loss: 0.3690  obj_loss: 0.1031  noobj_loss: 0.1398  bbox_loss: 0.0253  cls_loss: 0.0694  \n",
      "<<<iteration:[100/657] - total_loss: 0.3785  obj_loss: 0.1224  noobj_loss: 0.1486  bbox_loss: 0.0217  cls_loss: 0.0733  \n",
      "<<<iteration:[120/657] - total_loss: 0.3520  obj_loss: 0.1097  noobj_loss: 0.1414  bbox_loss: 0.0211  cls_loss: 0.0659  \n",
      "<<<iteration:[140/657] - total_loss: 0.3726  obj_loss: 0.1120  noobj_loss: 0.1474  bbox_loss: 0.0214  cls_loss: 0.0798  \n",
      "<<<iteration:[160/657] - total_loss: 0.4050  obj_loss: 0.0966  noobj_loss: 0.1544  bbox_loss: 0.0294  cls_loss: 0.0840  \n",
      "<<<iteration:[180/657] - total_loss: 0.3837  obj_loss: 0.1300  noobj_loss: 0.1354  bbox_loss: 0.0215  cls_loss: 0.0783  \n",
      "<<<iteration:[200/657] - total_loss: 0.3922  obj_loss: 0.1015  noobj_loss: 0.1421  bbox_loss: 0.0259  cls_loss: 0.0904  \n",
      "<<<iteration:[220/657] - total_loss: 0.3710  obj_loss: 0.1024  noobj_loss: 0.1441  bbox_loss: 0.0229  cls_loss: 0.0818  \n",
      "<<<iteration:[240/657] - total_loss: 0.3921  obj_loss: 0.1042  noobj_loss: 0.1369  bbox_loss: 0.0249  cls_loss: 0.0952  \n",
      "<<<iteration:[260/657] - total_loss: 0.3716  obj_loss: 0.1037  noobj_loss: 0.1403  bbox_loss: 0.0255  cls_loss: 0.0701  \n",
      "<<<iteration:[280/657] - total_loss: 0.3805  obj_loss: 0.1054  noobj_loss: 0.1452  bbox_loss: 0.0253  cls_loss: 0.0758  \n",
      "<<<iteration:[300/657] - total_loss: 0.3643  obj_loss: 0.1041  noobj_loss: 0.1494  bbox_loss: 0.0239  cls_loss: 0.0661  \n",
      "<<<iteration:[320/657] - total_loss: 0.3972  obj_loss: 0.1098  noobj_loss: 0.1453  bbox_loss: 0.0260  cls_loss: 0.0849  \n",
      "<<<iteration:[340/657] - total_loss: 0.4183  obj_loss: 0.1041  noobj_loss: 0.1405  bbox_loss: 0.0308  cls_loss: 0.0898  \n",
      "<<<iteration:[360/657] - total_loss: 0.4088  obj_loss: 0.1088  noobj_loss: 0.1453  bbox_loss: 0.0271  cls_loss: 0.0919  \n",
      "<<<iteration:[380/657] - total_loss: 0.3795  obj_loss: 0.1122  noobj_loss: 0.1462  bbox_loss: 0.0233  cls_loss: 0.0779  \n",
      "<<<iteration:[400/657] - total_loss: 0.4004  obj_loss: 0.1090  noobj_loss: 0.1372  bbox_loss: 0.0288  cls_loss: 0.0788  \n",
      "<<<iteration:[420/657] - total_loss: 0.3590  obj_loss: 0.0999  noobj_loss: 0.1311  bbox_loss: 0.0234  cls_loss: 0.0766  \n",
      "<<<iteration:[440/657] - total_loss: 0.3475  obj_loss: 0.1073  noobj_loss: 0.1359  bbox_loss: 0.0219  cls_loss: 0.0627  \n",
      "<<<iteration:[460/657] - total_loss: 0.3727  obj_loss: 0.1140  noobj_loss: 0.1470  bbox_loss: 0.0217  cls_loss: 0.0770  \n",
      "<<<iteration:[480/657] - total_loss: 0.3645  obj_loss: 0.1114  noobj_loss: 0.1422  bbox_loss: 0.0242  cls_loss: 0.0613  \n",
      "<<<iteration:[500/657] - total_loss: 0.3796  obj_loss: 0.1137  noobj_loss: 0.1433  bbox_loss: 0.0231  cls_loss: 0.0788  \n",
      "<<<iteration:[520/657] - total_loss: 0.3899  obj_loss: 0.1134  noobj_loss: 0.1423  bbox_loss: 0.0245  cls_loss: 0.0827  \n",
      "<<<iteration:[540/657] - total_loss: 0.3658  obj_loss: 0.1062  noobj_loss: 0.1387  bbox_loss: 0.0239  cls_loss: 0.0705  \n",
      "<<<iteration:[560/657] - total_loss: 0.4062  obj_loss: 0.1062  noobj_loss: 0.1419  bbox_loss: 0.0256  cls_loss: 0.1011  \n",
      "<<<iteration:[580/657] - total_loss: 0.3882  obj_loss: 0.1167  noobj_loss: 0.1452  bbox_loss: 0.0225  cls_loss: 0.0861  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[600/657] - total_loss: 0.3535  obj_loss: 0.1143  noobj_loss: 0.1391  bbox_loss: 0.0215  cls_loss: 0.0623  \n",
      "<<<iteration:[620/657] - total_loss: 0.3892  obj_loss: 0.1145  noobj_loss: 0.1380  bbox_loss: 0.0250  cls_loss: 0.0805  \n",
      "<<<iteration:[640/657] - total_loss: 0.3814  obj_loss: 0.1152  noobj_loss: 0.1458  bbox_loss: 0.0224  cls_loss: 0.0813  \n",
      "\n",
      "epoch:65/100 - Train Loss: 0.3802, Val Loss: 0.4113\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4214  obj_loss: 0.1253  noobj_loss: 0.1484  bbox_loss: 0.0240  cls_loss: 0.1019  \n",
      "<<<iteration:[40/657] - total_loss: 0.3533  obj_loss: 0.1004  noobj_loss: 0.1453  bbox_loss: 0.0199  cls_loss: 0.0807  \n",
      "<<<iteration:[60/657] - total_loss: 0.3767  obj_loss: 0.1062  noobj_loss: 0.1461  bbox_loss: 0.0232  cls_loss: 0.0813  \n",
      "<<<iteration:[80/657] - total_loss: 0.3795  obj_loss: 0.1076  noobj_loss: 0.1449  bbox_loss: 0.0245  cls_loss: 0.0772  \n",
      "<<<iteration:[100/657] - total_loss: 0.3861  obj_loss: 0.1102  noobj_loss: 0.1400  bbox_loss: 0.0253  cls_loss: 0.0793  \n",
      "<<<iteration:[120/657] - total_loss: 0.3745  obj_loss: 0.1135  noobj_loss: 0.1369  bbox_loss: 0.0223  cls_loss: 0.0814  \n",
      "<<<iteration:[140/657] - total_loss: 0.3993  obj_loss: 0.1137  noobj_loss: 0.1389  bbox_loss: 0.0240  cls_loss: 0.0960  \n",
      "<<<iteration:[160/657] - total_loss: 0.3653  obj_loss: 0.1032  noobj_loss: 0.1319  bbox_loss: 0.0235  cls_loss: 0.0785  \n",
      "<<<iteration:[180/657] - total_loss: 0.3909  obj_loss: 0.1076  noobj_loss: 0.1506  bbox_loss: 0.0235  cls_loss: 0.0905  \n",
      "<<<iteration:[200/657] - total_loss: 0.3761  obj_loss: 0.1184  noobj_loss: 0.1435  bbox_loss: 0.0239  cls_loss: 0.0665  \n",
      "<<<iteration:[220/657] - total_loss: 0.3602  obj_loss: 0.1053  noobj_loss: 0.1393  bbox_loss: 0.0231  cls_loss: 0.0700  \n",
      "<<<iteration:[240/657] - total_loss: 0.4010  obj_loss: 0.1131  noobj_loss: 0.1414  bbox_loss: 0.0283  cls_loss: 0.0758  \n",
      "<<<iteration:[260/657] - total_loss: 0.3853  obj_loss: 0.1099  noobj_loss: 0.1421  bbox_loss: 0.0256  cls_loss: 0.0762  \n",
      "<<<iteration:[280/657] - total_loss: 0.3791  obj_loss: 0.1091  noobj_loss: 0.1458  bbox_loss: 0.0245  cls_loss: 0.0748  \n",
      "<<<iteration:[300/657] - total_loss: 0.3578  obj_loss: 0.1083  noobj_loss: 0.1374  bbox_loss: 0.0222  cls_loss: 0.0696  \n",
      "<<<iteration:[320/657] - total_loss: 0.3830  obj_loss: 0.1053  noobj_loss: 0.1438  bbox_loss: 0.0258  cls_loss: 0.0766  \n",
      "<<<iteration:[340/657] - total_loss: 0.3735  obj_loss: 0.1098  noobj_loss: 0.1398  bbox_loss: 0.0223  cls_loss: 0.0821  \n",
      "<<<iteration:[360/657] - total_loss: 0.3418  obj_loss: 0.0992  noobj_loss: 0.1419  bbox_loss: 0.0201  cls_loss: 0.0712  \n",
      "<<<iteration:[380/657] - total_loss: 0.3673  obj_loss: 0.1091  noobj_loss: 0.1388  bbox_loss: 0.0234  cls_loss: 0.0719  \n",
      "<<<iteration:[400/657] - total_loss: 0.3720  obj_loss: 0.1105  noobj_loss: 0.1396  bbox_loss: 0.0242  cls_loss: 0.0707  \n",
      "<<<iteration:[420/657] - total_loss: 0.3535  obj_loss: 0.0996  noobj_loss: 0.1471  bbox_loss: 0.0230  cls_loss: 0.0651  \n",
      "<<<iteration:[440/657] - total_loss: 0.3743  obj_loss: 0.1097  noobj_loss: 0.1461  bbox_loss: 0.0228  cls_loss: 0.0777  \n",
      "<<<iteration:[460/657] - total_loss: 0.3826  obj_loss: 0.1209  noobj_loss: 0.1445  bbox_loss: 0.0213  cls_loss: 0.0828  \n",
      "<<<iteration:[480/657] - total_loss: 0.3753  obj_loss: 0.1211  noobj_loss: 0.1430  bbox_loss: 0.0222  cls_loss: 0.0715  \n",
      "<<<iteration:[500/657] - total_loss: 0.3650  obj_loss: 0.1089  noobj_loss: 0.1404  bbox_loss: 0.0215  cls_loss: 0.0782  \n",
      "<<<iteration:[520/657] - total_loss: 0.3774  obj_loss: 0.0989  noobj_loss: 0.1487  bbox_loss: 0.0257  cls_loss: 0.0754  \n",
      "<<<iteration:[540/657] - total_loss: 0.3774  obj_loss: 0.1088  noobj_loss: 0.1429  bbox_loss: 0.0236  cls_loss: 0.0790  \n",
      "<<<iteration:[560/657] - total_loss: 0.3835  obj_loss: 0.1083  noobj_loss: 0.1414  bbox_loss: 0.0236  cls_loss: 0.0864  \n",
      "<<<iteration:[580/657] - total_loss: 0.3870  obj_loss: 0.1108  noobj_loss: 0.1463  bbox_loss: 0.0250  cls_loss: 0.0780  \n",
      "<<<iteration:[600/657] - total_loss: 0.3509  obj_loss: 0.1098  noobj_loss: 0.1335  bbox_loss: 0.0216  cls_loss: 0.0663  \n",
      "<<<iteration:[620/657] - total_loss: 0.3986  obj_loss: 0.1202  noobj_loss: 0.1322  bbox_loss: 0.0236  cls_loss: 0.0942  \n",
      "<<<iteration:[640/657] - total_loss: 0.3845  obj_loss: 0.1024  noobj_loss: 0.1488  bbox_loss: 0.0266  cls_loss: 0.0745  \n",
      "\n",
      "epoch:66/100 - Train Loss: 0.3759, Val Loss: 0.4142\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4102  obj_loss: 0.1165  noobj_loss: 0.1464  bbox_loss: 0.0279  cls_loss: 0.0810  \n",
      "<<<iteration:[40/657] - total_loss: 0.3609  obj_loss: 0.1106  noobj_loss: 0.1438  bbox_loss: 0.0207  cls_loss: 0.0750  \n",
      "<<<iteration:[60/657] - total_loss: 0.3711  obj_loss: 0.1018  noobj_loss: 0.1461  bbox_loss: 0.0243  cls_loss: 0.0748  \n",
      "<<<iteration:[80/657] - total_loss: 0.3617  obj_loss: 0.0984  noobj_loss: 0.1368  bbox_loss: 0.0235  cls_loss: 0.0771  \n",
      "<<<iteration:[100/657] - total_loss: 0.3780  obj_loss: 0.1124  noobj_loss: 0.1385  bbox_loss: 0.0236  cls_loss: 0.0784  \n",
      "<<<iteration:[120/657] - total_loss: 0.3534  obj_loss: 0.1028  noobj_loss: 0.1363  bbox_loss: 0.0222  cls_loss: 0.0713  \n",
      "<<<iteration:[140/657] - total_loss: 0.3607  obj_loss: 0.1018  noobj_loss: 0.1360  bbox_loss: 0.0222  cls_loss: 0.0797  \n",
      "<<<iteration:[160/657] - total_loss: 0.3728  obj_loss: 0.1132  noobj_loss: 0.1363  bbox_loss: 0.0236  cls_loss: 0.0734  \n",
      "<<<iteration:[180/657] - total_loss: 0.3811  obj_loss: 0.1149  noobj_loss: 0.1411  bbox_loss: 0.0220  cls_loss: 0.0856  \n",
      "<<<iteration:[200/657] - total_loss: 0.3641  obj_loss: 0.1116  noobj_loss: 0.1421  bbox_loss: 0.0210  cls_loss: 0.0767  \n",
      "<<<iteration:[220/657] - total_loss: 0.3694  obj_loss: 0.1052  noobj_loss: 0.1388  bbox_loss: 0.0228  cls_loss: 0.0807  \n",
      "<<<iteration:[240/657] - total_loss: 0.3764  obj_loss: 0.1159  noobj_loss: 0.1390  bbox_loss: 0.0209  cls_loss: 0.0864  \n",
      "<<<iteration:[260/657] - total_loss: 0.3796  obj_loss: 0.1059  noobj_loss: 0.1481  bbox_loss: 0.0252  cls_loss: 0.0735  \n",
      "<<<iteration:[280/657] - total_loss: 0.3554  obj_loss: 0.1142  noobj_loss: 0.1421  bbox_loss: 0.0212  cls_loss: 0.0639  \n",
      "<<<iteration:[300/657] - total_loss: 0.3741  obj_loss: 0.1143  noobj_loss: 0.1368  bbox_loss: 0.0222  cls_loss: 0.0806  \n",
      "<<<iteration:[320/657] - total_loss: 0.3799  obj_loss: 0.1163  noobj_loss: 0.1487  bbox_loss: 0.0231  cls_loss: 0.0737  \n",
      "<<<iteration:[340/657] - total_loss: 0.3902  obj_loss: 0.1198  noobj_loss: 0.1388  bbox_loss: 0.0215  cls_loss: 0.0934  \n",
      "<<<iteration:[360/657] - total_loss: 0.3725  obj_loss: 0.1096  noobj_loss: 0.1395  bbox_loss: 0.0234  cls_loss: 0.0759  \n",
      "<<<iteration:[380/657] - total_loss: 0.3899  obj_loss: 0.1141  noobj_loss: 0.1400  bbox_loss: 0.0231  cls_loss: 0.0902  \n",
      "<<<iteration:[400/657] - total_loss: 0.3727  obj_loss: 0.1045  noobj_loss: 0.1384  bbox_loss: 0.0226  cls_loss: 0.0861  \n",
      "<<<iteration:[420/657] - total_loss: 0.3809  obj_loss: 0.0981  noobj_loss: 0.1329  bbox_loss: 0.0274  cls_loss: 0.0797  \n",
      "<<<iteration:[440/657] - total_loss: 0.3690  obj_loss: 0.1105  noobj_loss: 0.1403  bbox_loss: 0.0218  cls_loss: 0.0795  \n",
      "<<<iteration:[460/657] - total_loss: 0.3880  obj_loss: 0.1036  noobj_loss: 0.1459  bbox_loss: 0.0240  cls_loss: 0.0912  \n",
      "<<<iteration:[480/657] - total_loss: 0.3866  obj_loss: 0.1094  noobj_loss: 0.1445  bbox_loss: 0.0226  cls_loss: 0.0921  \n",
      "<<<iteration:[500/657] - total_loss: 0.3720  obj_loss: 0.1066  noobj_loss: 0.1481  bbox_loss: 0.0224  cls_loss: 0.0792  \n",
      "<<<iteration:[520/657] - total_loss: 0.3709  obj_loss: 0.1125  noobj_loss: 0.1464  bbox_loss: 0.0230  cls_loss: 0.0703  \n",
      "<<<iteration:[540/657] - total_loss: 0.3578  obj_loss: 0.1061  noobj_loss: 0.1410  bbox_loss: 0.0233  cls_loss: 0.0645  \n",
      "<<<iteration:[560/657] - total_loss: 0.3825  obj_loss: 0.1089  noobj_loss: 0.1401  bbox_loss: 0.0268  cls_loss: 0.0695  \n",
      "<<<iteration:[580/657] - total_loss: 0.3810  obj_loss: 0.1168  noobj_loss: 0.1441  bbox_loss: 0.0239  cls_loss: 0.0728  \n",
      "<<<iteration:[600/657] - total_loss: 0.3648  obj_loss: 0.1088  noobj_loss: 0.1415  bbox_loss: 0.0230  cls_loss: 0.0703  \n",
      "<<<iteration:[620/657] - total_loss: 0.3919  obj_loss: 0.1096  noobj_loss: 0.1414  bbox_loss: 0.0265  cls_loss: 0.0790  \n",
      "<<<iteration:[640/657] - total_loss: 0.3743  obj_loss: 0.1148  noobj_loss: 0.1419  bbox_loss: 0.0247  cls_loss: 0.0650  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:67/100 - Train Loss: 0.3752, Val Loss: 0.5118\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4334  obj_loss: 0.1119  noobj_loss: 0.1437  bbox_loss: 0.0337  cls_loss: 0.0811  \n",
      "<<<iteration:[40/657] - total_loss: 0.4055  obj_loss: 0.1048  noobj_loss: 0.1420  bbox_loss: 0.0308  cls_loss: 0.0755  \n",
      "<<<iteration:[60/657] - total_loss: 0.3929  obj_loss: 0.1130  noobj_loss: 0.1407  bbox_loss: 0.0241  cls_loss: 0.0891  \n",
      "<<<iteration:[80/657] - total_loss: 0.3842  obj_loss: 0.1063  noobj_loss: 0.1499  bbox_loss: 0.0266  cls_loss: 0.0701  \n",
      "<<<iteration:[100/657] - total_loss: 0.3643  obj_loss: 0.1188  noobj_loss: 0.1379  bbox_loss: 0.0201  cls_loss: 0.0759  \n",
      "<<<iteration:[120/657] - total_loss: 0.3819  obj_loss: 0.1106  noobj_loss: 0.1426  bbox_loss: 0.0235  cls_loss: 0.0826  \n",
      "<<<iteration:[140/657] - total_loss: 0.3791  obj_loss: 0.1128  noobj_loss: 0.1415  bbox_loss: 0.0217  cls_loss: 0.0870  \n",
      "<<<iteration:[160/657] - total_loss: 0.3535  obj_loss: 0.1029  noobj_loss: 0.1453  bbox_loss: 0.0217  cls_loss: 0.0695  \n",
      "<<<iteration:[180/657] - total_loss: 0.3744  obj_loss: 0.1229  noobj_loss: 0.1438  bbox_loss: 0.0215  cls_loss: 0.0719  \n",
      "<<<iteration:[200/657] - total_loss: 0.3871  obj_loss: 0.0990  noobj_loss: 0.1361  bbox_loss: 0.0285  cls_loss: 0.0778  \n",
      "<<<iteration:[220/657] - total_loss: 0.3483  obj_loss: 0.1071  noobj_loss: 0.1397  bbox_loss: 0.0214  cls_loss: 0.0642  \n",
      "<<<iteration:[240/657] - total_loss: 0.3536  obj_loss: 0.1130  noobj_loss: 0.1399  bbox_loss: 0.0213  cls_loss: 0.0641  \n",
      "<<<iteration:[260/657] - total_loss: 0.3743  obj_loss: 0.1215  noobj_loss: 0.1385  bbox_loss: 0.0203  cls_loss: 0.0819  \n",
      "<<<iteration:[280/657] - total_loss: 0.3803  obj_loss: 0.0988  noobj_loss: 0.1449  bbox_loss: 0.0256  cls_loss: 0.0813  \n",
      "<<<iteration:[300/657] - total_loss: 0.3694  obj_loss: 0.1161  noobj_loss: 0.1389  bbox_loss: 0.0239  cls_loss: 0.0642  \n",
      "<<<iteration:[320/657] - total_loss: 0.3626  obj_loss: 0.1058  noobj_loss: 0.1398  bbox_loss: 0.0234  cls_loss: 0.0697  \n",
      "<<<iteration:[340/657] - total_loss: 0.3794  obj_loss: 0.0960  noobj_loss: 0.1499  bbox_loss: 0.0263  cls_loss: 0.0768  \n",
      "<<<iteration:[360/657] - total_loss: 0.3620  obj_loss: 0.1045  noobj_loss: 0.1400  bbox_loss: 0.0232  cls_loss: 0.0715  \n",
      "<<<iteration:[380/657] - total_loss: 0.3920  obj_loss: 0.1255  noobj_loss: 0.1383  bbox_loss: 0.0225  cls_loss: 0.0849  \n",
      "<<<iteration:[400/657] - total_loss: 0.3576  obj_loss: 0.1048  noobj_loss: 0.1382  bbox_loss: 0.0222  cls_loss: 0.0725  \n",
      "<<<iteration:[420/657] - total_loss: 0.4139  obj_loss: 0.1085  noobj_loss: 0.1478  bbox_loss: 0.0300  cls_loss: 0.0814  \n",
      "<<<iteration:[440/657] - total_loss: 0.4050  obj_loss: 0.1059  noobj_loss: 0.1395  bbox_loss: 0.0303  cls_loss: 0.0777  \n",
      "<<<iteration:[460/657] - total_loss: 0.4118  obj_loss: 0.1085  noobj_loss: 0.1465  bbox_loss: 0.0297  cls_loss: 0.0813  \n",
      "<<<iteration:[480/657] - total_loss: 0.3854  obj_loss: 0.1137  noobj_loss: 0.1369  bbox_loss: 0.0245  cls_loss: 0.0809  \n",
      "<<<iteration:[500/657] - total_loss: 0.3879  obj_loss: 0.1081  noobj_loss: 0.1403  bbox_loss: 0.0256  cls_loss: 0.0816  \n",
      "<<<iteration:[520/657] - total_loss: 0.4047  obj_loss: 0.1045  noobj_loss: 0.1493  bbox_loss: 0.0284  cls_loss: 0.0836  \n",
      "<<<iteration:[540/657] - total_loss: 0.4132  obj_loss: 0.1249  noobj_loss: 0.1410  bbox_loss: 0.0264  cls_loss: 0.0856  \n",
      "<<<iteration:[560/657] - total_loss: 0.3803  obj_loss: 0.0998  noobj_loss: 0.1363  bbox_loss: 0.0260  cls_loss: 0.0823  \n",
      "<<<iteration:[580/657] - total_loss: 0.3829  obj_loss: 0.1049  noobj_loss: 0.1446  bbox_loss: 0.0265  cls_loss: 0.0734  \n",
      "<<<iteration:[600/657] - total_loss: 0.3825  obj_loss: 0.1080  noobj_loss: 0.1362  bbox_loss: 0.0268  cls_loss: 0.0722  \n",
      "<<<iteration:[620/657] - total_loss: 0.3640  obj_loss: 0.1215  noobj_loss: 0.1372  bbox_loss: 0.0195  cls_loss: 0.0761  \n",
      "<<<iteration:[640/657] - total_loss: 0.3934  obj_loss: 0.1082  noobj_loss: 0.1417  bbox_loss: 0.0256  cls_loss: 0.0863  \n",
      "\n",
      "epoch:68/100 - Train Loss: 0.3830, Val Loss: 0.4221\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3953  obj_loss: 0.1122  noobj_loss: 0.1480  bbox_loss: 0.0255  cls_loss: 0.0816  \n",
      "<<<iteration:[40/657] - total_loss: 0.3655  obj_loss: 0.1151  noobj_loss: 0.1402  bbox_loss: 0.0223  cls_loss: 0.0686  \n",
      "<<<iteration:[60/657] - total_loss: 0.3327  obj_loss: 0.1052  noobj_loss: 0.1380  bbox_loss: 0.0187  cls_loss: 0.0649  \n",
      "<<<iteration:[80/657] - total_loss: 0.3723  obj_loss: 0.1044  noobj_loss: 0.1377  bbox_loss: 0.0233  cls_loss: 0.0827  \n",
      "<<<iteration:[100/657] - total_loss: 0.3623  obj_loss: 0.1171  noobj_loss: 0.1415  bbox_loss: 0.0199  cls_loss: 0.0747  \n",
      "<<<iteration:[120/657] - total_loss: 0.3955  obj_loss: 0.1061  noobj_loss: 0.1416  bbox_loss: 0.0293  cls_loss: 0.0723  \n",
      "<<<iteration:[140/657] - total_loss: 0.3710  obj_loss: 0.1101  noobj_loss: 0.1364  bbox_loss: 0.0230  cls_loss: 0.0779  \n",
      "<<<iteration:[160/657] - total_loss: 0.3742  obj_loss: 0.1202  noobj_loss: 0.1414  bbox_loss: 0.0216  cls_loss: 0.0755  \n",
      "<<<iteration:[180/657] - total_loss: 0.3788  obj_loss: 0.1127  noobj_loss: 0.1422  bbox_loss: 0.0243  cls_loss: 0.0732  \n",
      "<<<iteration:[200/657] - total_loss: 0.3843  obj_loss: 0.1182  noobj_loss: 0.1419  bbox_loss: 0.0221  cls_loss: 0.0846  \n",
      "<<<iteration:[220/657] - total_loss: 0.3480  obj_loss: 0.1055  noobj_loss: 0.1336  bbox_loss: 0.0225  cls_loss: 0.0634  \n",
      "<<<iteration:[240/657] - total_loss: 0.3647  obj_loss: 0.1129  noobj_loss: 0.1386  bbox_loss: 0.0205  cls_loss: 0.0801  \n",
      "<<<iteration:[260/657] - total_loss: 0.3607  obj_loss: 0.1067  noobj_loss: 0.1357  bbox_loss: 0.0210  cls_loss: 0.0812  \n",
      "<<<iteration:[280/657] - total_loss: 0.3700  obj_loss: 0.1132  noobj_loss: 0.1344  bbox_loss: 0.0234  cls_loss: 0.0727  \n",
      "<<<iteration:[300/657] - total_loss: 0.3786  obj_loss: 0.1142  noobj_loss: 0.1405  bbox_loss: 0.0231  cls_loss: 0.0787  \n",
      "<<<iteration:[320/657] - total_loss: 0.3804  obj_loss: 0.1132  noobj_loss: 0.1439  bbox_loss: 0.0230  cls_loss: 0.0805  \n",
      "<<<iteration:[340/657] - total_loss: 0.3551  obj_loss: 0.1053  noobj_loss: 0.1354  bbox_loss: 0.0224  cls_loss: 0.0702  \n",
      "<<<iteration:[360/657] - total_loss: 0.3940  obj_loss: 0.1014  noobj_loss: 0.1419  bbox_loss: 0.0277  cls_loss: 0.0828  \n",
      "<<<iteration:[380/657] - total_loss: 0.3632  obj_loss: 0.1082  noobj_loss: 0.1451  bbox_loss: 0.0235  cls_loss: 0.0651  \n",
      "<<<iteration:[400/657] - total_loss: 0.3675  obj_loss: 0.1181  noobj_loss: 0.1386  bbox_loss: 0.0207  cls_loss: 0.0767  \n",
      "<<<iteration:[420/657] - total_loss: 0.3779  obj_loss: 0.1115  noobj_loss: 0.1332  bbox_loss: 0.0244  cls_loss: 0.0780  \n",
      "<<<iteration:[440/657] - total_loss: 0.3659  obj_loss: 0.1232  noobj_loss: 0.1405  bbox_loss: 0.0197  cls_loss: 0.0741  \n",
      "<<<iteration:[460/657] - total_loss: 0.3517  obj_loss: 0.1120  noobj_loss: 0.1419  bbox_loss: 0.0226  cls_loss: 0.0559  \n",
      "<<<iteration:[480/657] - total_loss: 0.3780  obj_loss: 0.0978  noobj_loss: 0.1415  bbox_loss: 0.0243  cls_loss: 0.0879  \n",
      "<<<iteration:[500/657] - total_loss: 0.3601  obj_loss: 0.1134  noobj_loss: 0.1322  bbox_loss: 0.0210  cls_loss: 0.0757  \n",
      "<<<iteration:[520/657] - total_loss: 0.3784  obj_loss: 0.1124  noobj_loss: 0.1419  bbox_loss: 0.0222  cls_loss: 0.0840  \n",
      "<<<iteration:[540/657] - total_loss: 0.3843  obj_loss: 0.1101  noobj_loss: 0.1342  bbox_loss: 0.0241  cls_loss: 0.0868  \n",
      "<<<iteration:[560/657] - total_loss: 0.4015  obj_loss: 0.1227  noobj_loss: 0.1465  bbox_loss: 0.0238  cls_loss: 0.0864  \n",
      "<<<iteration:[580/657] - total_loss: 0.3770  obj_loss: 0.1032  noobj_loss: 0.1370  bbox_loss: 0.0236  cls_loss: 0.0875  \n",
      "<<<iteration:[600/657] - total_loss: 0.3707  obj_loss: 0.1122  noobj_loss: 0.1468  bbox_loss: 0.0236  cls_loss: 0.0670  \n",
      "<<<iteration:[620/657] - total_loss: 0.3946  obj_loss: 0.1224  noobj_loss: 0.1412  bbox_loss: 0.0227  cls_loss: 0.0883  \n",
      "<<<iteration:[640/657] - total_loss: 0.3822  obj_loss: 0.1164  noobj_loss: 0.1438  bbox_loss: 0.0219  cls_loss: 0.0843  \n",
      "\n",
      "epoch:69/100 - Train Loss: 0.3722, Val Loss: 0.4032\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3863  obj_loss: 0.1196  noobj_loss: 0.1478  bbox_loss: 0.0219  cls_loss: 0.0832  \n",
      "<<<iteration:[40/657] - total_loss: 0.3739  obj_loss: 0.1112  noobj_loss: 0.1350  bbox_loss: 0.0229  cls_loss: 0.0810  \n",
      "<<<iteration:[60/657] - total_loss: 0.3622  obj_loss: 0.1094  noobj_loss: 0.1464  bbox_loss: 0.0214  cls_loss: 0.0725  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[80/657] - total_loss: 0.3990  obj_loss: 0.0992  noobj_loss: 0.1476  bbox_loss: 0.0273  cls_loss: 0.0895  \n",
      "<<<iteration:[100/657] - total_loss: 0.3730  obj_loss: 0.1192  noobj_loss: 0.1366  bbox_loss: 0.0221  cls_loss: 0.0752  \n",
      "<<<iteration:[120/657] - total_loss: 0.3506  obj_loss: 0.0993  noobj_loss: 0.1329  bbox_loss: 0.0220  cls_loss: 0.0748  \n",
      "<<<iteration:[140/657] - total_loss: 0.3914  obj_loss: 0.1202  noobj_loss: 0.1318  bbox_loss: 0.0231  cls_loss: 0.0900  \n",
      "<<<iteration:[160/657] - total_loss: 0.3655  obj_loss: 0.1158  noobj_loss: 0.1367  bbox_loss: 0.0213  cls_loss: 0.0748  \n",
      "<<<iteration:[180/657] - total_loss: 0.3524  obj_loss: 0.1069  noobj_loss: 0.1418  bbox_loss: 0.0207  cls_loss: 0.0713  \n",
      "<<<iteration:[200/657] - total_loss: 0.3660  obj_loss: 0.1066  noobj_loss: 0.1374  bbox_loss: 0.0219  cls_loss: 0.0813  \n",
      "<<<iteration:[220/657] - total_loss: 0.3685  obj_loss: 0.1093  noobj_loss: 0.1378  bbox_loss: 0.0201  cls_loss: 0.0897  \n",
      "<<<iteration:[240/657] - total_loss: 0.3592  obj_loss: 0.1131  noobj_loss: 0.1403  bbox_loss: 0.0212  cls_loss: 0.0699  \n",
      "<<<iteration:[260/657] - total_loss: 0.3648  obj_loss: 0.1173  noobj_loss: 0.1329  bbox_loss: 0.0233  cls_loss: 0.0644  \n",
      "<<<iteration:[280/657] - total_loss: 0.3769  obj_loss: 0.1248  noobj_loss: 0.1408  bbox_loss: 0.0230  cls_loss: 0.0669  \n",
      "<<<iteration:[300/657] - total_loss: 0.3740  obj_loss: 0.1198  noobj_loss: 0.1376  bbox_loss: 0.0217  cls_loss: 0.0767  \n",
      "<<<iteration:[320/657] - total_loss: 0.3612  obj_loss: 0.1051  noobj_loss: 0.1345  bbox_loss: 0.0252  cls_loss: 0.0630  \n",
      "<<<iteration:[340/657] - total_loss: 0.3726  obj_loss: 0.1031  noobj_loss: 0.1380  bbox_loss: 0.0242  cls_loss: 0.0797  \n",
      "<<<iteration:[360/657] - total_loss: 0.3674  obj_loss: 0.1085  noobj_loss: 0.1371  bbox_loss: 0.0217  cls_loss: 0.0817  \n",
      "<<<iteration:[380/657] - total_loss: 0.5495  obj_loss: 0.1007  noobj_loss: 0.1387  bbox_loss: 0.0631  cls_loss: 0.0638  \n",
      "<<<iteration:[400/657] - total_loss: 0.5072  obj_loss: 0.0861  noobj_loss: 0.1314  bbox_loss: 0.0561  cls_loss: 0.0750  \n",
      "<<<iteration:[420/657] - total_loss: 0.4013  obj_loss: 0.0895  noobj_loss: 0.1396  bbox_loss: 0.0380  cls_loss: 0.0519  \n",
      "<<<iteration:[440/657] - total_loss: 0.3696  obj_loss: 0.1044  noobj_loss: 0.1342  bbox_loss: 0.0246  cls_loss: 0.0751  \n",
      "<<<iteration:[460/657] - total_loss: 0.3690  obj_loss: 0.1043  noobj_loss: 0.1360  bbox_loss: 0.0249  cls_loss: 0.0721  \n",
      "<<<iteration:[480/657] - total_loss: 0.3551  obj_loss: 0.1036  noobj_loss: 0.1337  bbox_loss: 0.0229  cls_loss: 0.0702  \n",
      "<<<iteration:[500/657] - total_loss: 0.3776  obj_loss: 0.1059  noobj_loss: 0.1365  bbox_loss: 0.0268  cls_loss: 0.0696  \n",
      "<<<iteration:[520/657] - total_loss: 0.3811  obj_loss: 0.1120  noobj_loss: 0.1266  bbox_loss: 0.0259  cls_loss: 0.0761  \n",
      "<<<iteration:[540/657] - total_loss: 0.3773  obj_loss: 0.1211  noobj_loss: 0.1388  bbox_loss: 0.0227  cls_loss: 0.0730  \n",
      "<<<iteration:[560/657] - total_loss: 0.3952  obj_loss: 0.1123  noobj_loss: 0.1429  bbox_loss: 0.0254  cls_loss: 0.0844  \n",
      "<<<iteration:[580/657] - total_loss: 0.3835  obj_loss: 0.1135  noobj_loss: 0.1409  bbox_loss: 0.0234  cls_loss: 0.0825  \n",
      "<<<iteration:[600/657] - total_loss: 0.3604  obj_loss: 0.1018  noobj_loss: 0.1444  bbox_loss: 0.0230  cls_loss: 0.0716  \n",
      "<<<iteration:[620/657] - total_loss: 0.3720  obj_loss: 0.1024  noobj_loss: 0.1432  bbox_loss: 0.0224  cls_loss: 0.0859  \n",
      "<<<iteration:[640/657] - total_loss: 0.3575  obj_loss: 0.1082  noobj_loss: 0.1399  bbox_loss: 0.0228  cls_loss: 0.0652  \n",
      "\n",
      "epoch:70/100 - Train Loss: 0.3819, Val Loss: 0.4017\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3896  obj_loss: 0.1214  noobj_loss: 0.1393  bbox_loss: 0.0233  cls_loss: 0.0819  \n",
      "<<<iteration:[40/657] - total_loss: 0.3671  obj_loss: 0.1140  noobj_loss: 0.1339  bbox_loss: 0.0234  cls_loss: 0.0692  \n",
      "<<<iteration:[60/657] - total_loss: 0.3472  obj_loss: 0.1098  noobj_loss: 0.1346  bbox_loss: 0.0198  cls_loss: 0.0711  \n",
      "<<<iteration:[80/657] - total_loss: 0.3766  obj_loss: 0.1044  noobj_loss: 0.1376  bbox_loss: 0.0233  cls_loss: 0.0870  \n",
      "<<<iteration:[100/657] - total_loss: 0.3738  obj_loss: 0.1065  noobj_loss: 0.1408  bbox_loss: 0.0260  cls_loss: 0.0671  \n",
      "<<<iteration:[120/657] - total_loss: 0.3798  obj_loss: 0.1152  noobj_loss: 0.1357  bbox_loss: 0.0235  cls_loss: 0.0793  \n",
      "<<<iteration:[140/657] - total_loss: 0.3642  obj_loss: 0.1133  noobj_loss: 0.1419  bbox_loss: 0.0202  cls_loss: 0.0786  \n",
      "<<<iteration:[160/657] - total_loss: 0.3635  obj_loss: 0.1083  noobj_loss: 0.1339  bbox_loss: 0.0222  cls_loss: 0.0774  \n",
      "<<<iteration:[180/657] - total_loss: 0.3506  obj_loss: 0.1227  noobj_loss: 0.1385  bbox_loss: 0.0201  cls_loss: 0.0579  \n",
      "<<<iteration:[200/657] - total_loss: 0.3715  obj_loss: 0.1160  noobj_loss: 0.1347  bbox_loss: 0.0204  cls_loss: 0.0862  \n",
      "<<<iteration:[220/657] - total_loss: 0.3728  obj_loss: 0.1053  noobj_loss: 0.1392  bbox_loss: 0.0240  cls_loss: 0.0780  \n",
      "<<<iteration:[240/657] - total_loss: 0.3693  obj_loss: 0.1045  noobj_loss: 0.1364  bbox_loss: 0.0242  cls_loss: 0.0757  \n",
      "<<<iteration:[260/657] - total_loss: 0.3527  obj_loss: 0.0990  noobj_loss: 0.1367  bbox_loss: 0.0223  cls_loss: 0.0738  \n",
      "<<<iteration:[280/657] - total_loss: 0.3763  obj_loss: 0.1119  noobj_loss: 0.1401  bbox_loss: 0.0256  cls_loss: 0.0661  \n",
      "<<<iteration:[300/657] - total_loss: 0.3837  obj_loss: 0.1046  noobj_loss: 0.1454  bbox_loss: 0.0259  cls_loss: 0.0769  \n",
      "<<<iteration:[320/657] - total_loss: 0.3503  obj_loss: 0.1161  noobj_loss: 0.1346  bbox_loss: 0.0192  cls_loss: 0.0710  \n",
      "<<<iteration:[340/657] - total_loss: 0.3667  obj_loss: 0.1106  noobj_loss: 0.1369  bbox_loss: 0.0218  cls_loss: 0.0786  \n",
      "<<<iteration:[360/657] - total_loss: 0.3718  obj_loss: 0.1206  noobj_loss: 0.1377  bbox_loss: 0.0219  cls_loss: 0.0728  \n",
      "<<<iteration:[380/657] - total_loss: 0.3442  obj_loss: 0.1026  noobj_loss: 0.1331  bbox_loss: 0.0204  cls_loss: 0.0731  \n",
      "<<<iteration:[400/657] - total_loss: 0.3875  obj_loss: 0.1068  noobj_loss: 0.1409  bbox_loss: 0.0260  cls_loss: 0.0804  \n",
      "<<<iteration:[420/657] - total_loss: 0.3835  obj_loss: 0.1138  noobj_loss: 0.1367  bbox_loss: 0.0212  cls_loss: 0.0955  \n",
      "<<<iteration:[440/657] - total_loss: 0.3947  obj_loss: 0.1124  noobj_loss: 0.1455  bbox_loss: 0.0255  cls_loss: 0.0821  \n",
      "<<<iteration:[460/657] - total_loss: 0.3760  obj_loss: 0.1036  noobj_loss: 0.1347  bbox_loss: 0.0260  cls_loss: 0.0750  \n",
      "<<<iteration:[480/657] - total_loss: 0.3691  obj_loss: 0.1107  noobj_loss: 0.1340  bbox_loss: 0.0248  cls_loss: 0.0675  \n",
      "<<<iteration:[500/657] - total_loss: 0.3988  obj_loss: 0.1086  noobj_loss: 0.1384  bbox_loss: 0.0266  cls_loss: 0.0883  \n",
      "<<<iteration:[520/657] - total_loss: 0.3730  obj_loss: 0.1065  noobj_loss: 0.1389  bbox_loss: 0.0249  cls_loss: 0.0727  \n",
      "<<<iteration:[540/657] - total_loss: 0.3615  obj_loss: 0.1168  noobj_loss: 0.1358  bbox_loss: 0.0209  cls_loss: 0.0722  \n",
      "<<<iteration:[560/657] - total_loss: 0.3727  obj_loss: 0.1207  noobj_loss: 0.1455  bbox_loss: 0.0205  cls_loss: 0.0770  \n",
      "<<<iteration:[580/657] - total_loss: 0.3838  obj_loss: 0.1290  noobj_loss: 0.1431  bbox_loss: 0.0244  cls_loss: 0.0615  \n",
      "<<<iteration:[600/657] - total_loss: 0.3709  obj_loss: 0.1170  noobj_loss: 0.1418  bbox_loss: 0.0235  cls_loss: 0.0656  \n",
      "<<<iteration:[620/657] - total_loss: 0.3382  obj_loss: 0.1046  noobj_loss: 0.1386  bbox_loss: 0.0214  cls_loss: 0.0572  \n",
      "<<<iteration:[640/657] - total_loss: 0.3960  obj_loss: 0.1105  noobj_loss: 0.1425  bbox_loss: 0.0251  cls_loss: 0.0887  \n",
      "\n",
      "epoch:71/100 - Train Loss: 0.3706, Val Loss: 0.4009\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3964  obj_loss: 0.1135  noobj_loss: 0.1465  bbox_loss: 0.0257  cls_loss: 0.0810  \n",
      "<<<iteration:[40/657] - total_loss: 0.3506  obj_loss: 0.1089  noobj_loss: 0.1416  bbox_loss: 0.0216  cls_loss: 0.0629  \n",
      "<<<iteration:[60/657] - total_loss: 0.3616  obj_loss: 0.1239  noobj_loss: 0.1352  bbox_loss: 0.0217  cls_loss: 0.0614  \n",
      "<<<iteration:[80/657] - total_loss: 0.3692  obj_loss: 0.1121  noobj_loss: 0.1327  bbox_loss: 0.0232  cls_loss: 0.0745  \n",
      "<<<iteration:[100/657] - total_loss: 0.3965  obj_loss: 0.1198  noobj_loss: 0.1366  bbox_loss: 0.0244  cls_loss: 0.0863  \n",
      "<<<iteration:[120/657] - total_loss: 0.3606  obj_loss: 0.1049  noobj_loss: 0.1368  bbox_loss: 0.0218  cls_loss: 0.0781  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[140/657] - total_loss: 0.3682  obj_loss: 0.1120  noobj_loss: 0.1347  bbox_loss: 0.0232  cls_loss: 0.0730  \n",
      "<<<iteration:[160/657] - total_loss: 0.3941  obj_loss: 0.1098  noobj_loss: 0.1393  bbox_loss: 0.0253  cls_loss: 0.0884  \n",
      "<<<iteration:[180/657] - total_loss: 0.3796  obj_loss: 0.1101  noobj_loss: 0.1381  bbox_loss: 0.0229  cls_loss: 0.0859  \n",
      "<<<iteration:[200/657] - total_loss: 0.3907  obj_loss: 0.1096  noobj_loss: 0.1361  bbox_loss: 0.0242  cls_loss: 0.0922  \n",
      "<<<iteration:[220/657] - total_loss: 0.3441  obj_loss: 0.1055  noobj_loss: 0.1424  bbox_loss: 0.0207  cls_loss: 0.0641  \n",
      "<<<iteration:[240/657] - total_loss: 0.3811  obj_loss: 0.1130  noobj_loss: 0.1314  bbox_loss: 0.0241  cls_loss: 0.0821  \n",
      "<<<iteration:[260/657] - total_loss: 0.3901  obj_loss: 0.1162  noobj_loss: 0.1350  bbox_loss: 0.0240  cls_loss: 0.0864  \n",
      "<<<iteration:[280/657] - total_loss: 0.3653  obj_loss: 0.1058  noobj_loss: 0.1374  bbox_loss: 0.0249  cls_loss: 0.0661  \n",
      "<<<iteration:[300/657] - total_loss: 0.3624  obj_loss: 0.1115  noobj_loss: 0.1405  bbox_loss: 0.0223  cls_loss: 0.0690  \n",
      "<<<iteration:[320/657] - total_loss: 0.3741  obj_loss: 0.1178  noobj_loss: 0.1352  bbox_loss: 0.0227  cls_loss: 0.0754  \n",
      "<<<iteration:[340/657] - total_loss: 0.3655  obj_loss: 0.1052  noobj_loss: 0.1398  bbox_loss: 0.0237  cls_loss: 0.0717  \n",
      "<<<iteration:[360/657] - total_loss: 0.3743  obj_loss: 0.1015  noobj_loss: 0.1402  bbox_loss: 0.0244  cls_loss: 0.0805  \n",
      "<<<iteration:[380/657] - total_loss: 0.3761  obj_loss: 0.1136  noobj_loss: 0.1320  bbox_loss: 0.0214  cls_loss: 0.0897  \n",
      "<<<iteration:[400/657] - total_loss: 0.3668  obj_loss: 0.1205  noobj_loss: 0.1415  bbox_loss: 0.0209  cls_loss: 0.0712  \n",
      "<<<iteration:[420/657] - total_loss: 0.3784  obj_loss: 0.1011  noobj_loss: 0.1409  bbox_loss: 0.0256  cls_loss: 0.0790  \n",
      "<<<iteration:[440/657] - total_loss: 0.3685  obj_loss: 0.1208  noobj_loss: 0.1384  bbox_loss: 0.0220  cls_loss: 0.0685  \n",
      "<<<iteration:[460/657] - total_loss: 0.3756  obj_loss: 0.1109  noobj_loss: 0.1373  bbox_loss: 0.0245  cls_loss: 0.0737  \n",
      "<<<iteration:[480/657] - total_loss: 0.3608  obj_loss: 0.1046  noobj_loss: 0.1400  bbox_loss: 0.0228  cls_loss: 0.0723  \n",
      "<<<iteration:[500/657] - total_loss: 0.3431  obj_loss: 0.1021  noobj_loss: 0.1344  bbox_loss: 0.0216  cls_loss: 0.0657  \n",
      "<<<iteration:[520/657] - total_loss: 0.3409  obj_loss: 0.1178  noobj_loss: 0.1366  bbox_loss: 0.0194  cls_loss: 0.0579  \n",
      "<<<iteration:[540/657] - total_loss: 0.3785  obj_loss: 0.1233  noobj_loss: 0.1391  bbox_loss: 0.0223  cls_loss: 0.0743  \n",
      "<<<iteration:[560/657] - total_loss: 0.3690  obj_loss: 0.1034  noobj_loss: 0.1411  bbox_loss: 0.0236  cls_loss: 0.0767  \n",
      "<<<iteration:[580/657] - total_loss: 0.3730  obj_loss: 0.1148  noobj_loss: 0.1327  bbox_loss: 0.0224  cls_loss: 0.0798  \n",
      "<<<iteration:[600/657] - total_loss: 0.3761  obj_loss: 0.1087  noobj_loss: 0.1418  bbox_loss: 0.0234  cls_loss: 0.0796  \n",
      "<<<iteration:[620/657] - total_loss: 0.3797  obj_loss: 0.1137  noobj_loss: 0.1354  bbox_loss: 0.0241  cls_loss: 0.0779  \n",
      "<<<iteration:[640/657] - total_loss: 0.3411  obj_loss: 0.1109  noobj_loss: 0.1366  bbox_loss: 0.0195  cls_loss: 0.0645  \n",
      "\n",
      "epoch:72/100 - Train Loss: 0.3692, Val Loss: 0.4059\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3868  obj_loss: 0.1227  noobj_loss: 0.1425  bbox_loss: 0.0221  cls_loss: 0.0826  \n",
      "<<<iteration:[40/657] - total_loss: 0.3430  obj_loss: 0.1041  noobj_loss: 0.1398  bbox_loss: 0.0224  cls_loss: 0.0568  \n",
      "<<<iteration:[60/657] - total_loss: 0.3388  obj_loss: 0.1082  noobj_loss: 0.1352  bbox_loss: 0.0194  cls_loss: 0.0660  \n",
      "<<<iteration:[80/657] - total_loss: 0.3765  obj_loss: 0.1260  noobj_loss: 0.1360  bbox_loss: 0.0202  cls_loss: 0.0813  \n",
      "<<<iteration:[100/657] - total_loss: 0.3623  obj_loss: 0.1166  noobj_loss: 0.1411  bbox_loss: 0.0214  cls_loss: 0.0680  \n",
      "<<<iteration:[120/657] - total_loss: 0.3866  obj_loss: 0.1111  noobj_loss: 0.1386  bbox_loss: 0.0230  cls_loss: 0.0912  \n",
      "<<<iteration:[140/657] - total_loss: 0.3533  obj_loss: 0.1101  noobj_loss: 0.1391  bbox_loss: 0.0216  cls_loss: 0.0656  \n",
      "<<<iteration:[160/657] - total_loss: 0.3856  obj_loss: 0.1216  noobj_loss: 0.1309  bbox_loss: 0.0238  cls_loss: 0.0797  \n",
      "<<<iteration:[180/657] - total_loss: 0.5555  obj_loss: 0.0994  noobj_loss: 0.1464  bbox_loss: 0.0594  cls_loss: 0.0858  \n",
      "<<<iteration:[200/657] - total_loss: 0.5062  obj_loss: 0.0989  noobj_loss: 0.1361  bbox_loss: 0.0481  cls_loss: 0.0985  \n",
      "<<<iteration:[220/657] - total_loss: 0.4566  obj_loss: 0.0935  noobj_loss: 0.1386  bbox_loss: 0.0401  cls_loss: 0.0935  \n",
      "<<<iteration:[240/657] - total_loss: 0.4803  obj_loss: 0.1010  noobj_loss: 0.1376  bbox_loss: 0.0449  cls_loss: 0.0862  \n",
      "<<<iteration:[260/657] - total_loss: 0.5088  obj_loss: 0.1110  noobj_loss: 0.1411  bbox_loss: 0.0492  cls_loss: 0.0813  \n",
      "<<<iteration:[280/657] - total_loss: 0.4887  obj_loss: 0.1109  noobj_loss: 0.1377  bbox_loss: 0.0461  cls_loss: 0.0785  \n",
      "<<<iteration:[300/657] - total_loss: 0.4924  obj_loss: 0.1064  noobj_loss: 0.1305  bbox_loss: 0.0496  cls_loss: 0.0729  \n",
      "<<<iteration:[320/657] - total_loss: 0.4582  obj_loss: 0.1173  noobj_loss: 0.1355  bbox_loss: 0.0413  cls_loss: 0.0665  \n",
      "<<<iteration:[340/657] - total_loss: 0.4733  obj_loss: 0.1127  noobj_loss: 0.1367  bbox_loss: 0.0437  cls_loss: 0.0737  \n",
      "<<<iteration:[360/657] - total_loss: 0.4345  obj_loss: 0.1045  noobj_loss: 0.1356  bbox_loss: 0.0367  cls_loss: 0.0786  \n",
      "<<<iteration:[380/657] - total_loss: 0.5134  obj_loss: 0.1051  noobj_loss: 0.1333  bbox_loss: 0.0520  cls_loss: 0.0819  \n",
      "<<<iteration:[400/657] - total_loss: 0.4859  obj_loss: 0.1123  noobj_loss: 0.1367  bbox_loss: 0.0465  cls_loss: 0.0728  \n",
      "<<<iteration:[420/657] - total_loss: 0.4231  obj_loss: 0.1104  noobj_loss: 0.1418  bbox_loss: 0.0338  cls_loss: 0.0726  \n",
      "<<<iteration:[440/657] - total_loss: 0.4808  obj_loss: 0.1114  noobj_loss: 0.1397  bbox_loss: 0.0441  cls_loss: 0.0793  \n",
      "<<<iteration:[460/657] - total_loss: 0.4212  obj_loss: 0.1159  noobj_loss: 0.1406  bbox_loss: 0.0335  cls_loss: 0.0674  \n",
      "<<<iteration:[480/657] - total_loss: 0.4477  obj_loss: 0.1130  noobj_loss: 0.1438  bbox_loss: 0.0401  cls_loss: 0.0624  \n",
      "<<<iteration:[500/657] - total_loss: 0.4529  obj_loss: 0.0987  noobj_loss: 0.1434  bbox_loss: 0.0409  cls_loss: 0.0782  \n",
      "<<<iteration:[520/657] - total_loss: 0.4609  obj_loss: 0.1006  noobj_loss: 0.1337  bbox_loss: 0.0442  cls_loss: 0.0724  \n",
      "<<<iteration:[540/657] - total_loss: 0.4229  obj_loss: 0.1118  noobj_loss: 0.1347  bbox_loss: 0.0351  cls_loss: 0.0684  \n",
      "<<<iteration:[560/657] - total_loss: 0.4318  obj_loss: 0.1133  noobj_loss: 0.1334  bbox_loss: 0.0315  cls_loss: 0.0944  \n",
      "<<<iteration:[580/657] - total_loss: 0.4266  obj_loss: 0.1004  noobj_loss: 0.1432  bbox_loss: 0.0343  cls_loss: 0.0831  \n",
      "<<<iteration:[600/657] - total_loss: 0.3790  obj_loss: 0.1026  noobj_loss: 0.1332  bbox_loss: 0.0268  cls_loss: 0.0758  \n",
      "<<<iteration:[620/657] - total_loss: 0.4408  obj_loss: 0.1113  noobj_loss: 0.1427  bbox_loss: 0.0375  cls_loss: 0.0707  \n",
      "<<<iteration:[640/657] - total_loss: 0.4424  obj_loss: 0.1124  noobj_loss: 0.1386  bbox_loss: 0.0380  cls_loss: 0.0705  \n",
      "\n",
      "epoch:73/100 - Train Loss: 0.4360, Val Loss: 0.4556\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4448  obj_loss: 0.1195  noobj_loss: 0.1429  bbox_loss: 0.0341  cls_loss: 0.0834  \n",
      "<<<iteration:[40/657] - total_loss: 0.4381  obj_loss: 0.1121  noobj_loss: 0.1369  bbox_loss: 0.0341  cls_loss: 0.0868  \n",
      "<<<iteration:[60/657] - total_loss: 0.3897  obj_loss: 0.1072  noobj_loss: 0.1343  bbox_loss: 0.0268  cls_loss: 0.0812  \n",
      "<<<iteration:[80/657] - total_loss: 0.4043  obj_loss: 0.0958  noobj_loss: 0.1323  bbox_loss: 0.0335  cls_loss: 0.0748  \n",
      "<<<iteration:[100/657] - total_loss: 0.3916  obj_loss: 0.1219  noobj_loss: 0.1366  bbox_loss: 0.0270  cls_loss: 0.0666  \n",
      "<<<iteration:[120/657] - total_loss: 0.4064  obj_loss: 0.1072  noobj_loss: 0.1374  bbox_loss: 0.0283  cls_loss: 0.0890  \n",
      "<<<iteration:[140/657] - total_loss: 0.3850  obj_loss: 0.1062  noobj_loss: 0.1354  bbox_loss: 0.0276  cls_loss: 0.0729  \n",
      "<<<iteration:[160/657] - total_loss: 0.3732  obj_loss: 0.1106  noobj_loss: 0.1364  bbox_loss: 0.0244  cls_loss: 0.0725  \n",
      "<<<iteration:[180/657] - total_loss: 0.3864  obj_loss: 0.1163  noobj_loss: 0.1365  bbox_loss: 0.0261  cls_loss: 0.0715  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[200/657] - total_loss: 0.3665  obj_loss: 0.1034  noobj_loss: 0.1353  bbox_loss: 0.0240  cls_loss: 0.0755  \n",
      "<<<iteration:[220/657] - total_loss: 0.4051  obj_loss: 0.1195  noobj_loss: 0.1502  bbox_loss: 0.0282  cls_loss: 0.0694  \n",
      "<<<iteration:[240/657] - total_loss: 0.4195  obj_loss: 0.0978  noobj_loss: 0.1356  bbox_loss: 0.0356  cls_loss: 0.0760  \n",
      "<<<iteration:[260/657] - total_loss: 0.3726  obj_loss: 0.1022  noobj_loss: 0.1323  bbox_loss: 0.0284  cls_loss: 0.0622  \n",
      "<<<iteration:[280/657] - total_loss: 0.3811  obj_loss: 0.1096  noobj_loss: 0.1326  bbox_loss: 0.0254  cls_loss: 0.0780  \n",
      "<<<iteration:[300/657] - total_loss: 0.3674  obj_loss: 0.1016  noobj_loss: 0.1389  bbox_loss: 0.0271  cls_loss: 0.0607  \n",
      "<<<iteration:[320/657] - total_loss: 0.3894  obj_loss: 0.1079  noobj_loss: 0.1347  bbox_loss: 0.0291  cls_loss: 0.0688  \n",
      "<<<iteration:[340/657] - total_loss: 0.3767  obj_loss: 0.1126  noobj_loss: 0.1318  bbox_loss: 0.0263  cls_loss: 0.0666  \n",
      "<<<iteration:[360/657] - total_loss: 0.3731  obj_loss: 0.1048  noobj_loss: 0.1354  bbox_loss: 0.0247  cls_loss: 0.0770  \n",
      "<<<iteration:[380/657] - total_loss: 0.3377  obj_loss: 0.1091  noobj_loss: 0.1293  bbox_loss: 0.0193  cls_loss: 0.0672  \n",
      "<<<iteration:[400/657] - total_loss: 0.3918  obj_loss: 0.1028  noobj_loss: 0.1405  bbox_loss: 0.0272  cls_loss: 0.0828  \n",
      "<<<iteration:[420/657] - total_loss: 0.3819  obj_loss: 0.1094  noobj_loss: 0.1352  bbox_loss: 0.0263  cls_loss: 0.0735  \n",
      "<<<iteration:[440/657] - total_loss: 0.3616  obj_loss: 0.1074  noobj_loss: 0.1336  bbox_loss: 0.0235  cls_loss: 0.0701  \n",
      "<<<iteration:[460/657] - total_loss: 0.3770  obj_loss: 0.1135  noobj_loss: 0.1355  bbox_loss: 0.0250  cls_loss: 0.0709  \n",
      "<<<iteration:[480/657] - total_loss: 0.3538  obj_loss: 0.1210  noobj_loss: 0.1390  bbox_loss: 0.0220  cls_loss: 0.0533  \n",
      "<<<iteration:[500/657] - total_loss: 0.3801  obj_loss: 0.1028  noobj_loss: 0.1402  bbox_loss: 0.0263  cls_loss: 0.0757  \n",
      "<<<iteration:[520/657] - total_loss: 0.4817  obj_loss: 0.0906  noobj_loss: 0.1303  bbox_loss: 0.0484  cls_loss: 0.0838  \n",
      "<<<iteration:[540/657] - total_loss: 0.3726  obj_loss: 0.1124  noobj_loss: 0.1426  bbox_loss: 0.0235  cls_loss: 0.0713  \n",
      "<<<iteration:[560/657] - total_loss: 0.3795  obj_loss: 0.1172  noobj_loss: 0.1398  bbox_loss: 0.0237  cls_loss: 0.0740  \n",
      "<<<iteration:[580/657] - total_loss: 0.3624  obj_loss: 0.1100  noobj_loss: 0.1345  bbox_loss: 0.0222  cls_loss: 0.0740  \n",
      "<<<iteration:[600/657] - total_loss: 0.4052  obj_loss: 0.1186  noobj_loss: 0.1343  bbox_loss: 0.0261  cls_loss: 0.0891  \n",
      "<<<iteration:[620/657] - total_loss: 0.3713  obj_loss: 0.1125  noobj_loss: 0.1341  bbox_loss: 0.0229  cls_loss: 0.0773  \n",
      "<<<iteration:[640/657] - total_loss: 0.3592  obj_loss: 0.1088  noobj_loss: 0.1291  bbox_loss: 0.0218  cls_loss: 0.0767  \n",
      "\n",
      "epoch:74/100 - Train Loss: 0.3854, Val Loss: 0.3956\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3798  obj_loss: 0.1220  noobj_loss: 0.1367  bbox_loss: 0.0227  cls_loss: 0.0762  \n",
      "<<<iteration:[40/657] - total_loss: 0.4047  obj_loss: 0.0897  noobj_loss: 0.1382  bbox_loss: 0.0336  cls_loss: 0.0780  \n",
      "<<<iteration:[60/657] - total_loss: 0.3481  obj_loss: 0.1107  noobj_loss: 0.1299  bbox_loss: 0.0225  cls_loss: 0.0599  \n",
      "<<<iteration:[80/657] - total_loss: 0.3589  obj_loss: 0.1216  noobj_loss: 0.1317  bbox_loss: 0.0205  cls_loss: 0.0690  \n",
      "<<<iteration:[100/657] - total_loss: 0.3756  obj_loss: 0.1110  noobj_loss: 0.1327  bbox_loss: 0.0256  cls_loss: 0.0701  \n",
      "<<<iteration:[120/657] - total_loss: 0.3554  obj_loss: 0.1022  noobj_loss: 0.1355  bbox_loss: 0.0239  cls_loss: 0.0660  \n",
      "<<<iteration:[140/657] - total_loss: 0.3641  obj_loss: 0.1046  noobj_loss: 0.1378  bbox_loss: 0.0229  cls_loss: 0.0763  \n",
      "<<<iteration:[160/657] - total_loss: 0.3743  obj_loss: 0.1062  noobj_loss: 0.1356  bbox_loss: 0.0233  cls_loss: 0.0835  \n",
      "<<<iteration:[180/657] - total_loss: 0.3711  obj_loss: 0.1205  noobj_loss: 0.1323  bbox_loss: 0.0218  cls_loss: 0.0753  \n",
      "<<<iteration:[200/657] - total_loss: 0.3641  obj_loss: 0.1137  noobj_loss: 0.1313  bbox_loss: 0.0214  cls_loss: 0.0778  \n",
      "<<<iteration:[220/657] - total_loss: 0.3428  obj_loss: 0.1086  noobj_loss: 0.1360  bbox_loss: 0.0210  cls_loss: 0.0614  \n",
      "<<<iteration:[240/657] - total_loss: 0.3779  obj_loss: 0.1202  noobj_loss: 0.1373  bbox_loss: 0.0225  cls_loss: 0.0768  \n",
      "<<<iteration:[260/657] - total_loss: 0.3878  obj_loss: 0.1143  noobj_loss: 0.1328  bbox_loss: 0.0270  cls_loss: 0.0721  \n",
      "<<<iteration:[280/657] - total_loss: 0.3658  obj_loss: 0.1172  noobj_loss: 0.1300  bbox_loss: 0.0211  cls_loss: 0.0779  \n",
      "<<<iteration:[300/657] - total_loss: 0.4061  obj_loss: 0.1192  noobj_loss: 0.1345  bbox_loss: 0.0281  cls_loss: 0.0791  \n",
      "<<<iteration:[320/657] - total_loss: 0.3526  obj_loss: 0.1083  noobj_loss: 0.1302  bbox_loss: 0.0210  cls_loss: 0.0745  \n",
      "<<<iteration:[340/657] - total_loss: 0.3415  obj_loss: 0.1065  noobj_loss: 0.1366  bbox_loss: 0.0209  cls_loss: 0.0621  \n",
      "<<<iteration:[360/657] - total_loss: 0.3634  obj_loss: 0.1212  noobj_loss: 0.1421  bbox_loss: 0.0193  cls_loss: 0.0748  \n",
      "<<<iteration:[380/657] - total_loss: 0.3449  obj_loss: 0.1087  noobj_loss: 0.1309  bbox_loss: 0.0218  cls_loss: 0.0619  \n",
      "<<<iteration:[400/657] - total_loss: 0.3742  obj_loss: 0.1193  noobj_loss: 0.1370  bbox_loss: 0.0231  cls_loss: 0.0709  \n",
      "<<<iteration:[420/657] - total_loss: 0.3769  obj_loss: 0.1006  noobj_loss: 0.1318  bbox_loss: 0.0252  cls_loss: 0.0846  \n",
      "<<<iteration:[440/657] - total_loss: 0.3871  obj_loss: 0.1074  noobj_loss: 0.1245  bbox_loss: 0.0281  cls_loss: 0.0770  \n",
      "<<<iteration:[460/657] - total_loss: 0.3638  obj_loss: 0.1057  noobj_loss: 0.1381  bbox_loss: 0.0233  cls_loss: 0.0726  \n",
      "<<<iteration:[480/657] - total_loss: 0.3581  obj_loss: 0.1124  noobj_loss: 0.1336  bbox_loss: 0.0225  cls_loss: 0.0663  \n",
      "<<<iteration:[500/657] - total_loss: 0.3487  obj_loss: 0.1160  noobj_loss: 0.1280  bbox_loss: 0.0196  cls_loss: 0.0708  \n",
      "<<<iteration:[520/657] - total_loss: 0.3549  obj_loss: 0.1162  noobj_loss: 0.1309  bbox_loss: 0.0204  cls_loss: 0.0715  \n",
      "<<<iteration:[540/657] - total_loss: 0.3559  obj_loss: 0.1079  noobj_loss: 0.1422  bbox_loss: 0.0230  cls_loss: 0.0619  \n",
      "<<<iteration:[560/657] - total_loss: 0.3810  obj_loss: 0.1091  noobj_loss: 0.1274  bbox_loss: 0.0232  cls_loss: 0.0920  \n",
      "<<<iteration:[580/657] - total_loss: 0.3872  obj_loss: 0.1184  noobj_loss: 0.1368  bbox_loss: 0.0246  cls_loss: 0.0772  \n",
      "<<<iteration:[600/657] - total_loss: 0.3641  obj_loss: 0.1204  noobj_loss: 0.1341  bbox_loss: 0.0209  cls_loss: 0.0722  \n",
      "<<<iteration:[620/657] - total_loss: 0.3786  obj_loss: 0.1157  noobj_loss: 0.1394  bbox_loss: 0.0226  cls_loss: 0.0800  \n",
      "<<<iteration:[640/657] - total_loss: 0.3790  obj_loss: 0.1077  noobj_loss: 0.1328  bbox_loss: 0.0255  cls_loss: 0.0775  \n",
      "\n",
      "epoch:75/100 - Train Loss: 0.3683, Val Loss: 0.3983\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3988  obj_loss: 0.1055  noobj_loss: 0.1366  bbox_loss: 0.0255  cls_loss: 0.0974  \n",
      "<<<iteration:[40/657] - total_loss: 0.3683  obj_loss: 0.1138  noobj_loss: 0.1370  bbox_loss: 0.0234  cls_loss: 0.0690  \n",
      "<<<iteration:[60/657] - total_loss: 0.3774  obj_loss: 0.1284  noobj_loss: 0.1369  bbox_loss: 0.0205  cls_loss: 0.0779  \n",
      "<<<iteration:[80/657] - total_loss: 0.3954  obj_loss: 0.1085  noobj_loss: 0.1300  bbox_loss: 0.0262  cls_loss: 0.0909  \n",
      "<<<iteration:[100/657] - total_loss: 0.3592  obj_loss: 0.0922  noobj_loss: 0.1350  bbox_loss: 0.0263  cls_loss: 0.0681  \n",
      "<<<iteration:[120/657] - total_loss: 0.3498  obj_loss: 0.0999  noobj_loss: 0.1302  bbox_loss: 0.0217  cls_loss: 0.0765  \n",
      "<<<iteration:[140/657] - total_loss: 0.3713  obj_loss: 0.1062  noobj_loss: 0.1369  bbox_loss: 0.0247  cls_loss: 0.0734  \n",
      "<<<iteration:[160/657] - total_loss: 0.3443  obj_loss: 0.1069  noobj_loss: 0.1352  bbox_loss: 0.0219  cls_loss: 0.0604  \n",
      "<<<iteration:[180/657] - total_loss: 0.3593  obj_loss: 0.1074  noobj_loss: 0.1395  bbox_loss: 0.0221  cls_loss: 0.0716  \n",
      "<<<iteration:[200/657] - total_loss: 0.3630  obj_loss: 0.1090  noobj_loss: 0.1333  bbox_loss: 0.0220  cls_loss: 0.0774  \n",
      "<<<iteration:[220/657] - total_loss: 0.3649  obj_loss: 0.1173  noobj_loss: 0.1405  bbox_loss: 0.0215  cls_loss: 0.0701  \n",
      "<<<iteration:[240/657] - total_loss: 0.3581  obj_loss: 0.1083  noobj_loss: 0.1305  bbox_loss: 0.0233  cls_loss: 0.0679  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[260/657] - total_loss: 0.3610  obj_loss: 0.1136  noobj_loss: 0.1302  bbox_loss: 0.0209  cls_loss: 0.0777  \n",
      "<<<iteration:[280/657] - total_loss: 0.3762  obj_loss: 0.1076  noobj_loss: 0.1404  bbox_loss: 0.0223  cls_loss: 0.0868  \n",
      "<<<iteration:[300/657] - total_loss: 0.3482  obj_loss: 0.1104  noobj_loss: 0.1318  bbox_loss: 0.0208  cls_loss: 0.0678  \n",
      "<<<iteration:[320/657] - total_loss: 0.3378  obj_loss: 0.1084  noobj_loss: 0.1341  bbox_loss: 0.0200  cls_loss: 0.0625  \n",
      "<<<iteration:[340/657] - total_loss: 0.3852  obj_loss: 0.1100  noobj_loss: 0.1341  bbox_loss: 0.0253  cls_loss: 0.0816  \n",
      "<<<iteration:[360/657] - total_loss: 0.3621  obj_loss: 0.1182  noobj_loss: 0.1367  bbox_loss: 0.0225  cls_loss: 0.0632  \n",
      "<<<iteration:[380/657] - total_loss: 0.3778  obj_loss: 0.1301  noobj_loss: 0.1392  bbox_loss: 0.0223  cls_loss: 0.0664  \n",
      "<<<iteration:[400/657] - total_loss: 0.3407  obj_loss: 0.1198  noobj_loss: 0.1343  bbox_loss: 0.0183  cls_loss: 0.0624  \n",
      "<<<iteration:[420/657] - total_loss: 0.3494  obj_loss: 0.1156  noobj_loss: 0.1363  bbox_loss: 0.0206  cls_loss: 0.0625  \n",
      "<<<iteration:[440/657] - total_loss: 0.3600  obj_loss: 0.1064  noobj_loss: 0.1225  bbox_loss: 0.0212  cls_loss: 0.0863  \n",
      "<<<iteration:[460/657] - total_loss: 0.3717  obj_loss: 0.1086  noobj_loss: 0.1300  bbox_loss: 0.0245  cls_loss: 0.0754  \n",
      "<<<iteration:[480/657] - total_loss: 0.3718  obj_loss: 0.1108  noobj_loss: 0.1308  bbox_loss: 0.0242  cls_loss: 0.0748  \n",
      "<<<iteration:[500/657] - total_loss: 0.3599  obj_loss: 0.1146  noobj_loss: 0.1377  bbox_loss: 0.0207  cls_loss: 0.0729  \n",
      "<<<iteration:[520/657] - total_loss: 0.3783  obj_loss: 0.1181  noobj_loss: 0.1354  bbox_loss: 0.0237  cls_loss: 0.0737  \n",
      "<<<iteration:[540/657] - total_loss: 0.3567  obj_loss: 0.1077  noobj_loss: 0.1360  bbox_loss: 0.0224  cls_loss: 0.0688  \n",
      "<<<iteration:[560/657] - total_loss: 0.3560  obj_loss: 0.1260  noobj_loss: 0.1315  bbox_loss: 0.0186  cls_loss: 0.0710  \n",
      "<<<iteration:[580/657] - total_loss: 0.3732  obj_loss: 0.1183  noobj_loss: 0.1367  bbox_loss: 0.0218  cls_loss: 0.0774  \n",
      "<<<iteration:[600/657] - total_loss: 0.3433  obj_loss: 0.1183  noobj_loss: 0.1337  bbox_loss: 0.0203  cls_loss: 0.0567  \n",
      "<<<iteration:[620/657] - total_loss: 0.3739  obj_loss: 0.1019  noobj_loss: 0.1283  bbox_loss: 0.0289  cls_loss: 0.0633  \n",
      "<<<iteration:[640/657] - total_loss: 0.3769  obj_loss: 0.1110  noobj_loss: 0.1344  bbox_loss: 0.0248  cls_loss: 0.0749  \n",
      "\n",
      "epoch:76/100 - Train Loss: 0.3643, Val Loss: 0.3922\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3994  obj_loss: 0.1136  noobj_loss: 0.1446  bbox_loss: 0.0272  cls_loss: 0.0777  \n",
      "<<<iteration:[40/657] - total_loss: 0.3593  obj_loss: 0.1065  noobj_loss: 0.1432  bbox_loss: 0.0215  cls_loss: 0.0738  \n",
      "<<<iteration:[60/657] - total_loss: 0.3402  obj_loss: 0.1138  noobj_loss: 0.1343  bbox_loss: 0.0193  cls_loss: 0.0628  \n",
      "<<<iteration:[80/657] - total_loss: 0.3651  obj_loss: 0.1203  noobj_loss: 0.1326  bbox_loss: 0.0215  cls_loss: 0.0711  \n",
      "<<<iteration:[100/657] - total_loss: 0.3595  obj_loss: 0.1085  noobj_loss: 0.1302  bbox_loss: 0.0222  cls_loss: 0.0751  \n",
      "<<<iteration:[120/657] - total_loss: 0.3526  obj_loss: 0.1113  noobj_loss: 0.1305  bbox_loss: 0.0202  cls_loss: 0.0752  \n",
      "<<<iteration:[140/657] - total_loss: 0.3455  obj_loss: 0.1110  noobj_loss: 0.1353  bbox_loss: 0.0194  cls_loss: 0.0698  \n",
      "<<<iteration:[160/657] - total_loss: 0.3519  obj_loss: 0.1121  noobj_loss: 0.1295  bbox_loss: 0.0211  cls_loss: 0.0694  \n",
      "<<<iteration:[180/657] - total_loss: 0.3626  obj_loss: 0.1075  noobj_loss: 0.1337  bbox_loss: 0.0252  cls_loss: 0.0625  \n",
      "<<<iteration:[200/657] - total_loss: 0.3728  obj_loss: 0.1139  noobj_loss: 0.1265  bbox_loss: 0.0267  cls_loss: 0.0624  \n",
      "<<<iteration:[220/657] - total_loss: 0.3658  obj_loss: 0.1169  noobj_loss: 0.1372  bbox_loss: 0.0221  cls_loss: 0.0697  \n",
      "<<<iteration:[240/657] - total_loss: 0.3641  obj_loss: 0.1044  noobj_loss: 0.1304  bbox_loss: 0.0244  cls_loss: 0.0725  \n",
      "<<<iteration:[260/657] - total_loss: 0.3666  obj_loss: 0.1120  noobj_loss: 0.1336  bbox_loss: 0.0244  cls_loss: 0.0660  \n",
      "<<<iteration:[280/657] - total_loss: 0.3787  obj_loss: 0.1166  noobj_loss: 0.1349  bbox_loss: 0.0252  cls_loss: 0.0684  \n",
      "<<<iteration:[300/657] - total_loss: 0.3610  obj_loss: 0.1175  noobj_loss: 0.1344  bbox_loss: 0.0219  cls_loss: 0.0669  \n",
      "<<<iteration:[320/657] - total_loss: 0.3870  obj_loss: 0.1207  noobj_loss: 0.1308  bbox_loss: 0.0238  cls_loss: 0.0819  \n",
      "<<<iteration:[340/657] - total_loss: 0.3514  obj_loss: 0.1130  noobj_loss: 0.1349  bbox_loss: 0.0207  cls_loss: 0.0676  \n",
      "<<<iteration:[360/657] - total_loss: 0.3512  obj_loss: 0.1022  noobj_loss: 0.1307  bbox_loss: 0.0238  cls_loss: 0.0644  \n",
      "<<<iteration:[380/657] - total_loss: 0.3576  obj_loss: 0.1150  noobj_loss: 0.1304  bbox_loss: 0.0208  cls_loss: 0.0735  \n",
      "<<<iteration:[400/657] - total_loss: 0.3575  obj_loss: 0.1178  noobj_loss: 0.1320  bbox_loss: 0.0201  cls_loss: 0.0729  \n",
      "<<<iteration:[420/657] - total_loss: 0.3754  obj_loss: 0.1111  noobj_loss: 0.1367  bbox_loss: 0.0233  cls_loss: 0.0795  \n",
      "<<<iteration:[440/657] - total_loss: 0.3589  obj_loss: 0.1207  noobj_loss: 0.1339  bbox_loss: 0.0216  cls_loss: 0.0632  \n",
      "<<<iteration:[460/657] - total_loss: 0.3655  obj_loss: 0.1131  noobj_loss: 0.1319  bbox_loss: 0.0213  cls_loss: 0.0801  \n",
      "<<<iteration:[480/657] - total_loss: 0.3624  obj_loss: 0.1083  noobj_loss: 0.1305  bbox_loss: 0.0227  cls_loss: 0.0756  \n",
      "<<<iteration:[500/657] - total_loss: 0.3899  obj_loss: 0.1118  noobj_loss: 0.1340  bbox_loss: 0.0264  cls_loss: 0.0788  \n",
      "<<<iteration:[520/657] - total_loss: 0.3781  obj_loss: 0.1142  noobj_loss: 0.1402  bbox_loss: 0.0235  cls_loss: 0.0764  \n",
      "<<<iteration:[540/657] - total_loss: 0.3633  obj_loss: 0.1071  noobj_loss: 0.1324  bbox_loss: 0.0225  cls_loss: 0.0772  \n",
      "<<<iteration:[560/657] - total_loss: 0.3633  obj_loss: 0.1171  noobj_loss: 0.1307  bbox_loss: 0.0209  cls_loss: 0.0763  \n",
      "<<<iteration:[580/657] - total_loss: 0.3790  obj_loss: 0.1172  noobj_loss: 0.1344  bbox_loss: 0.0239  cls_loss: 0.0752  \n",
      "<<<iteration:[600/657] - total_loss: 0.3568  obj_loss: 0.1114  noobj_loss: 0.1330  bbox_loss: 0.0231  cls_loss: 0.0634  \n",
      "<<<iteration:[620/657] - total_loss: 0.3442  obj_loss: 0.1094  noobj_loss: 0.1346  bbox_loss: 0.0207  cls_loss: 0.0642  \n",
      "<<<iteration:[640/657] - total_loss: 0.3472  obj_loss: 0.1100  noobj_loss: 0.1335  bbox_loss: 0.0212  cls_loss: 0.0645  \n",
      "\n",
      "epoch:77/100 - Train Loss: 0.3636, Val Loss: 0.3892\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3578  obj_loss: 0.1159  noobj_loss: 0.1395  bbox_loss: 0.0218  cls_loss: 0.0634  \n",
      "<<<iteration:[40/657] - total_loss: 0.3581  obj_loss: 0.1167  noobj_loss: 0.1303  bbox_loss: 0.0201  cls_loss: 0.0759  \n",
      "<<<iteration:[60/657] - total_loss: 0.3743  obj_loss: 0.1191  noobj_loss: 0.1352  bbox_loss: 0.0214  cls_loss: 0.0804  \n",
      "<<<iteration:[80/657] - total_loss: 0.3770  obj_loss: 0.1172  noobj_loss: 0.1353  bbox_loss: 0.0234  cls_loss: 0.0752  \n",
      "<<<iteration:[100/657] - total_loss: 0.3707  obj_loss: 0.1138  noobj_loss: 0.1350  bbox_loss: 0.0223  cls_loss: 0.0778  \n",
      "<<<iteration:[120/657] - total_loss: 0.3665  obj_loss: 0.1216  noobj_loss: 0.1321  bbox_loss: 0.0209  cls_loss: 0.0742  \n",
      "<<<iteration:[140/657] - total_loss: 0.3568  obj_loss: 0.1217  noobj_loss: 0.1336  bbox_loss: 0.0217  cls_loss: 0.0598  \n",
      "<<<iteration:[160/657] - total_loss: 0.3628  obj_loss: 0.1203  noobj_loss: 0.1293  bbox_loss: 0.0212  cls_loss: 0.0716  \n",
      "<<<iteration:[180/657] - total_loss: 0.3533  obj_loss: 0.1095  noobj_loss: 0.1297  bbox_loss: 0.0234  cls_loss: 0.0620  \n",
      "<<<iteration:[200/657] - total_loss: 0.3473  obj_loss: 0.1041  noobj_loss: 0.1325  bbox_loss: 0.0213  cls_loss: 0.0706  \n",
      "<<<iteration:[220/657] - total_loss: 0.3649  obj_loss: 0.1180  noobj_loss: 0.1290  bbox_loss: 0.0210  cls_loss: 0.0776  \n",
      "<<<iteration:[240/657] - total_loss: 0.3666  obj_loss: 0.1185  noobj_loss: 0.1360  bbox_loss: 0.0230  cls_loss: 0.0650  \n",
      "<<<iteration:[260/657] - total_loss: 0.3444  obj_loss: 0.1080  noobj_loss: 0.1403  bbox_loss: 0.0212  cls_loss: 0.0602  \n",
      "<<<iteration:[280/657] - total_loss: 0.3681  obj_loss: 0.1177  noobj_loss: 0.1354  bbox_loss: 0.0228  cls_loss: 0.0687  \n",
      "<<<iteration:[300/657] - total_loss: 0.3631  obj_loss: 0.1255  noobj_loss: 0.1364  bbox_loss: 0.0213  cls_loss: 0.0628  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[320/657] - total_loss: 0.3555  obj_loss: 0.1128  noobj_loss: 0.1324  bbox_loss: 0.0213  cls_loss: 0.0701  \n",
      "<<<iteration:[340/657] - total_loss: 0.3502  obj_loss: 0.1140  noobj_loss: 0.1372  bbox_loss: 0.0212  cls_loss: 0.0617  \n",
      "<<<iteration:[360/657] - total_loss: 0.3495  obj_loss: 0.1021  noobj_loss: 0.1371  bbox_loss: 0.0214  cls_loss: 0.0719  \n",
      "<<<iteration:[380/657] - total_loss: 0.3999  obj_loss: 0.1049  noobj_loss: 0.1260  bbox_loss: 0.0310  cls_loss: 0.0773  \n",
      "<<<iteration:[400/657] - total_loss: 0.3729  obj_loss: 0.1069  noobj_loss: 0.1360  bbox_loss: 0.0263  cls_loss: 0.0666  \n",
      "<<<iteration:[420/657] - total_loss: 0.3748  obj_loss: 0.1008  noobj_loss: 0.1294  bbox_loss: 0.0281  cls_loss: 0.0689  \n",
      "<<<iteration:[440/657] - total_loss: 0.3809  obj_loss: 0.1169  noobj_loss: 0.1318  bbox_loss: 0.0224  cls_loss: 0.0861  \n",
      "<<<iteration:[460/657] - total_loss: 0.3642  obj_loss: 0.1025  noobj_loss: 0.1337  bbox_loss: 0.0235  cls_loss: 0.0774  \n",
      "<<<iteration:[480/657] - total_loss: 0.3559  obj_loss: 0.1166  noobj_loss: 0.1346  bbox_loss: 0.0214  cls_loss: 0.0650  \n",
      "<<<iteration:[500/657] - total_loss: 0.3481  obj_loss: 0.1155  noobj_loss: 0.1296  bbox_loss: 0.0207  cls_loss: 0.0641  \n",
      "<<<iteration:[520/657] - total_loss: 0.3751  obj_loss: 0.1175  noobj_loss: 0.1303  bbox_loss: 0.0220  cls_loss: 0.0825  \n",
      "<<<iteration:[540/657] - total_loss: 0.3609  obj_loss: 0.1097  noobj_loss: 0.1339  bbox_loss: 0.0230  cls_loss: 0.0692  \n",
      "<<<iteration:[560/657] - total_loss: 0.3480  obj_loss: 0.1109  noobj_loss: 0.1308  bbox_loss: 0.0204  cls_loss: 0.0696  \n",
      "<<<iteration:[580/657] - total_loss: 0.3568  obj_loss: 0.1084  noobj_loss: 0.1307  bbox_loss: 0.0215  cls_loss: 0.0758  \n",
      "<<<iteration:[600/657] - total_loss: 0.3395  obj_loss: 0.1043  noobj_loss: 0.1315  bbox_loss: 0.0207  cls_loss: 0.0661  \n",
      "<<<iteration:[620/657] - total_loss: 0.3569  obj_loss: 0.1131  noobj_loss: 0.1314  bbox_loss: 0.0209  cls_loss: 0.0737  \n",
      "<<<iteration:[640/657] - total_loss: 0.3739  obj_loss: 0.1138  noobj_loss: 0.1290  bbox_loss: 0.0231  cls_loss: 0.0800  \n",
      "\n",
      "epoch:78/100 - Train Loss: 0.3635, Val Loss: 0.3955\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3910  obj_loss: 0.1206  noobj_loss: 0.1360  bbox_loss: 0.0235  cls_loss: 0.0847  \n",
      "<<<iteration:[40/657] - total_loss: 0.3582  obj_loss: 0.1178  noobj_loss: 0.1340  bbox_loss: 0.0219  cls_loss: 0.0639  \n",
      "<<<iteration:[60/657] - total_loss: 0.3578  obj_loss: 0.1196  noobj_loss: 0.1310  bbox_loss: 0.0201  cls_loss: 0.0721  \n",
      "<<<iteration:[80/657] - total_loss: 0.3611  obj_loss: 0.1235  noobj_loss: 0.1303  bbox_loss: 0.0200  cls_loss: 0.0724  \n",
      "<<<iteration:[100/657] - total_loss: 0.3405  obj_loss: 0.1052  noobj_loss: 0.1327  bbox_loss: 0.0204  cls_loss: 0.0672  \n",
      "<<<iteration:[120/657] - total_loss: 0.3875  obj_loss: 0.1160  noobj_loss: 0.1322  bbox_loss: 0.0255  cls_loss: 0.0777  \n",
      "<<<iteration:[140/657] - total_loss: 0.3553  obj_loss: 0.1057  noobj_loss: 0.1288  bbox_loss: 0.0217  cls_loss: 0.0769  \n",
      "<<<iteration:[160/657] - total_loss: 0.3421  obj_loss: 0.1037  noobj_loss: 0.1237  bbox_loss: 0.0223  cls_loss: 0.0650  \n",
      "<<<iteration:[180/657] - total_loss: 0.3563  obj_loss: 0.1150  noobj_loss: 0.1309  bbox_loss: 0.0218  cls_loss: 0.0666  \n",
      "<<<iteration:[200/657] - total_loss: 0.3765  obj_loss: 0.1108  noobj_loss: 0.1304  bbox_loss: 0.0250  cls_loss: 0.0756  \n",
      "<<<iteration:[220/657] - total_loss: 0.3931  obj_loss: 0.1165  noobj_loss: 0.1359  bbox_loss: 0.0275  cls_loss: 0.0713  \n",
      "<<<iteration:[240/657] - total_loss: 0.3713  obj_loss: 0.1162  noobj_loss: 0.1361  bbox_loss: 0.0225  cls_loss: 0.0747  \n",
      "<<<iteration:[260/657] - total_loss: 0.3460  obj_loss: 0.1085  noobj_loss: 0.1286  bbox_loss: 0.0198  cls_loss: 0.0742  \n",
      "<<<iteration:[280/657] - total_loss: 0.3442  obj_loss: 0.1125  noobj_loss: 0.1299  bbox_loss: 0.0196  cls_loss: 0.0688  \n",
      "<<<iteration:[300/657] - total_loss: 0.3600  obj_loss: 0.1210  noobj_loss: 0.1400  bbox_loss: 0.0206  cls_loss: 0.0660  \n",
      "<<<iteration:[320/657] - total_loss: 0.3399  obj_loss: 0.1146  noobj_loss: 0.1349  bbox_loss: 0.0192  cls_loss: 0.0617  \n",
      "<<<iteration:[340/657] - total_loss: 0.3829  obj_loss: 0.1021  noobj_loss: 0.1471  bbox_loss: 0.0273  cls_loss: 0.0706  \n",
      "<<<iteration:[360/657] - total_loss: 0.3524  obj_loss: 0.1186  noobj_loss: 0.1321  bbox_loss: 0.0207  cls_loss: 0.0640  \n",
      "<<<iteration:[380/657] - total_loss: 0.3499  obj_loss: 0.1268  noobj_loss: 0.1293  bbox_loss: 0.0200  cls_loss: 0.0584  \n",
      "<<<iteration:[400/657] - total_loss: 0.3596  obj_loss: 0.1134  noobj_loss: 0.1292  bbox_loss: 0.0217  cls_loss: 0.0730  \n",
      "<<<iteration:[420/657] - total_loss: 0.3380  obj_loss: 0.1143  noobj_loss: 0.1359  bbox_loss: 0.0194  cls_loss: 0.0590  \n",
      "<<<iteration:[440/657] - total_loss: 0.4097  obj_loss: 0.1241  noobj_loss: 0.1315  bbox_loss: 0.0215  cls_loss: 0.1127  \n",
      "<<<iteration:[460/657] - total_loss: 0.3489  obj_loss: 0.1173  noobj_loss: 0.1321  bbox_loss: 0.0196  cls_loss: 0.0675  \n",
      "<<<iteration:[480/657] - total_loss: 0.3585  obj_loss: 0.1092  noobj_loss: 0.1312  bbox_loss: 0.0218  cls_loss: 0.0746  \n",
      "<<<iteration:[500/657] - total_loss: 0.3883  obj_loss: 0.1010  noobj_loss: 0.1353  bbox_loss: 0.0301  cls_loss: 0.0691  \n",
      "<<<iteration:[520/657] - total_loss: 0.3508  obj_loss: 0.1161  noobj_loss: 0.1331  bbox_loss: 0.0214  cls_loss: 0.0612  \n",
      "<<<iteration:[540/657] - total_loss: 0.3627  obj_loss: 0.1298  noobj_loss: 0.1332  bbox_loss: 0.0201  cls_loss: 0.0656  \n",
      "<<<iteration:[560/657] - total_loss: 0.3421  obj_loss: 0.1143  noobj_loss: 0.1342  bbox_loss: 0.0202  cls_loss: 0.0598  \n",
      "<<<iteration:[580/657] - total_loss: 0.3414  obj_loss: 0.1162  noobj_loss: 0.1289  bbox_loss: 0.0198  cls_loss: 0.0617  \n",
      "<<<iteration:[600/657] - total_loss: 0.3763  obj_loss: 0.1135  noobj_loss: 0.1315  bbox_loss: 0.0230  cls_loss: 0.0819  \n",
      "<<<iteration:[620/657] - total_loss: 0.3677  obj_loss: 0.1131  noobj_loss: 0.1364  bbox_loss: 0.0224  cls_loss: 0.0744  \n",
      "<<<iteration:[640/657] - total_loss: 0.3451  obj_loss: 0.1134  noobj_loss: 0.1332  bbox_loss: 0.0182  cls_loss: 0.0742  \n",
      "\n",
      "epoch:79/100 - Train Loss: 0.3609, Val Loss: 0.4010\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3527  obj_loss: 0.1204  noobj_loss: 0.1332  bbox_loss: 0.0219  cls_loss: 0.0563  \n",
      "<<<iteration:[40/657] - total_loss: 0.3427  obj_loss: 0.1143  noobj_loss: 0.1257  bbox_loss: 0.0183  cls_loss: 0.0742  \n",
      "<<<iteration:[60/657] - total_loss: 0.3537  obj_loss: 0.0979  noobj_loss: 0.1248  bbox_loss: 0.0235  cls_loss: 0.0761  \n",
      "<<<iteration:[80/657] - total_loss: 0.3453  obj_loss: 0.1190  noobj_loss: 0.1280  bbox_loss: 0.0200  cls_loss: 0.0622  \n",
      "<<<iteration:[100/657] - total_loss: 0.3543  obj_loss: 0.1022  noobj_loss: 0.1375  bbox_loss: 0.0229  cls_loss: 0.0690  \n",
      "<<<iteration:[120/657] - total_loss: 0.3491  obj_loss: 0.1083  noobj_loss: 0.1356  bbox_loss: 0.0230  cls_loss: 0.0582  \n",
      "<<<iteration:[140/657] - total_loss: 0.3331  obj_loss: 0.1130  noobj_loss: 0.1286  bbox_loss: 0.0192  cls_loss: 0.0596  \n",
      "<<<iteration:[160/657] - total_loss: 0.3571  obj_loss: 0.1151  noobj_loss: 0.1338  bbox_loss: 0.0219  cls_loss: 0.0657  \n",
      "<<<iteration:[180/657] - total_loss: 0.3572  obj_loss: 0.1093  noobj_loss: 0.1287  bbox_loss: 0.0230  cls_loss: 0.0685  \n",
      "<<<iteration:[200/657] - total_loss: 0.3599  obj_loss: 0.1118  noobj_loss: 0.1302  bbox_loss: 0.0221  cls_loss: 0.0726  \n",
      "<<<iteration:[220/657] - total_loss: 0.3517  obj_loss: 0.1065  noobj_loss: 0.1379  bbox_loss: 0.0219  cls_loss: 0.0667  \n",
      "<<<iteration:[240/657] - total_loss: 0.3657  obj_loss: 0.1189  noobj_loss: 0.1307  bbox_loss: 0.0213  cls_loss: 0.0749  \n",
      "<<<iteration:[260/657] - total_loss: 0.3586  obj_loss: 0.1145  noobj_loss: 0.1323  bbox_loss: 0.0207  cls_loss: 0.0744  \n",
      "<<<iteration:[280/657] - total_loss: 0.5742  obj_loss: 0.0777  noobj_loss: 0.1261  bbox_loss: 0.0706  cls_loss: 0.0807  \n",
      "<<<iteration:[300/657] - total_loss: 0.3672  obj_loss: 0.1104  noobj_loss: 0.1295  bbox_loss: 0.0242  cls_loss: 0.0711  \n",
      "<<<iteration:[320/657] - total_loss: 0.3649  obj_loss: 0.1187  noobj_loss: 0.1320  bbox_loss: 0.0217  cls_loss: 0.0715  \n",
      "<<<iteration:[340/657] - total_loss: 0.3455  obj_loss: 0.1053  noobj_loss: 0.1274  bbox_loss: 0.0227  cls_loss: 0.0628  \n",
      "<<<iteration:[360/657] - total_loss: 0.3999  obj_loss: 0.1230  noobj_loss: 0.1285  bbox_loss: 0.0254  cls_loss: 0.0856  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[380/657] - total_loss: 0.3586  obj_loss: 0.1127  noobj_loss: 0.1314  bbox_loss: 0.0232  cls_loss: 0.0641  \n",
      "<<<iteration:[400/657] - total_loss: 0.3645  obj_loss: 0.1253  noobj_loss: 0.1371  bbox_loss: 0.0212  cls_loss: 0.0648  \n",
      "<<<iteration:[420/657] - total_loss: 0.3652  obj_loss: 0.1164  noobj_loss: 0.1297  bbox_loss: 0.0197  cls_loss: 0.0854  \n",
      "<<<iteration:[440/657] - total_loss: 0.3438  obj_loss: 0.1091  noobj_loss: 0.1346  bbox_loss: 0.0219  cls_loss: 0.0579  \n",
      "<<<iteration:[460/657] - total_loss: 0.3725  obj_loss: 0.1167  noobj_loss: 0.1336  bbox_loss: 0.0215  cls_loss: 0.0814  \n",
      "<<<iteration:[480/657] - total_loss: 0.3771  obj_loss: 0.1162  noobj_loss: 0.1359  bbox_loss: 0.0236  cls_loss: 0.0751  \n",
      "<<<iteration:[500/657] - total_loss: 0.3840  obj_loss: 0.1010  noobj_loss: 0.1276  bbox_loss: 0.0316  cls_loss: 0.0614  \n",
      "<<<iteration:[520/657] - total_loss: 0.3690  obj_loss: 0.1185  noobj_loss: 0.1292  bbox_loss: 0.0242  cls_loss: 0.0648  \n",
      "<<<iteration:[540/657] - total_loss: 0.3421  obj_loss: 0.1061  noobj_loss: 0.1353  bbox_loss: 0.0202  cls_loss: 0.0676  \n",
      "<<<iteration:[560/657] - total_loss: 0.3522  obj_loss: 0.1147  noobj_loss: 0.1255  bbox_loss: 0.0217  cls_loss: 0.0663  \n",
      "<<<iteration:[580/657] - total_loss: 0.3405  obj_loss: 0.1093  noobj_loss: 0.1329  bbox_loss: 0.0194  cls_loss: 0.0679  \n",
      "<<<iteration:[600/657] - total_loss: 0.3603  obj_loss: 0.1010  noobj_loss: 0.1329  bbox_loss: 0.0230  cls_loss: 0.0780  \n",
      "<<<iteration:[620/657] - total_loss: 0.3558  obj_loss: 0.1068  noobj_loss: 0.1268  bbox_loss: 0.0199  cls_loss: 0.0862  \n",
      "<<<iteration:[640/657] - total_loss: 0.3623  obj_loss: 0.1146  noobj_loss: 0.1402  bbox_loss: 0.0217  cls_loss: 0.0690  \n",
      "\n",
      "epoch:80/100 - Train Loss: 0.3660, Val Loss: 0.3913\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3670  obj_loss: 0.1148  noobj_loss: 0.1396  bbox_loss: 0.0223  cls_loss: 0.0709  \n",
      "<<<iteration:[40/657] - total_loss: 0.3600  obj_loss: 0.0976  noobj_loss: 0.1320  bbox_loss: 0.0258  cls_loss: 0.0674  \n",
      "<<<iteration:[60/657] - total_loss: 0.3705  obj_loss: 0.1149  noobj_loss: 0.1343  bbox_loss: 0.0236  cls_loss: 0.0701  \n",
      "<<<iteration:[80/657] - total_loss: 0.3709  obj_loss: 0.1293  noobj_loss: 0.1294  bbox_loss: 0.0211  cls_loss: 0.0715  \n",
      "<<<iteration:[100/657] - total_loss: 0.3649  obj_loss: 0.1149  noobj_loss: 0.1267  bbox_loss: 0.0215  cls_loss: 0.0790  \n",
      "<<<iteration:[120/657] - total_loss: 0.3568  obj_loss: 0.1149  noobj_loss: 0.1314  bbox_loss: 0.0202  cls_loss: 0.0754  \n",
      "<<<iteration:[140/657] - total_loss: 0.3329  obj_loss: 0.1084  noobj_loss: 0.1231  bbox_loss: 0.0196  cls_loss: 0.0647  \n",
      "<<<iteration:[160/657] - total_loss: 0.3439  obj_loss: 0.1141  noobj_loss: 0.1334  bbox_loss: 0.0205  cls_loss: 0.0607  \n",
      "<<<iteration:[180/657] - total_loss: 0.3513  obj_loss: 0.1248  noobj_loss: 0.1256  bbox_loss: 0.0200  cls_loss: 0.0639  \n",
      "<<<iteration:[200/657] - total_loss: 0.3390  obj_loss: 0.1104  noobj_loss: 0.1300  bbox_loss: 0.0191  cls_loss: 0.0682  \n",
      "<<<iteration:[220/657] - total_loss: 0.3420  obj_loss: 0.1177  noobj_loss: 0.1342  bbox_loss: 0.0215  cls_loss: 0.0497  \n",
      "<<<iteration:[240/657] - total_loss: 0.3586  obj_loss: 0.1137  noobj_loss: 0.1298  bbox_loss: 0.0203  cls_loss: 0.0783  \n",
      "<<<iteration:[260/657] - total_loss: 0.3650  obj_loss: 0.1170  noobj_loss: 0.1352  bbox_loss: 0.0208  cls_loss: 0.0765  \n",
      "<<<iteration:[280/657] - total_loss: 0.3635  obj_loss: 0.1166  noobj_loss: 0.1360  bbox_loss: 0.0220  cls_loss: 0.0689  \n",
      "<<<iteration:[300/657] - total_loss: 0.3352  obj_loss: 0.1087  noobj_loss: 0.1331  bbox_loss: 0.0206  cls_loss: 0.0571  \n",
      "<<<iteration:[320/657] - total_loss: 0.3544  obj_loss: 0.1133  noobj_loss: 0.1280  bbox_loss: 0.0217  cls_loss: 0.0686  \n",
      "<<<iteration:[340/657] - total_loss: 0.3694  obj_loss: 0.1095  noobj_loss: 0.1364  bbox_loss: 0.0225  cls_loss: 0.0794  \n",
      "<<<iteration:[360/657] - total_loss: 0.3589  obj_loss: 0.1204  noobj_loss: 0.1304  bbox_loss: 0.0218  cls_loss: 0.0644  \n",
      "<<<iteration:[380/657] - total_loss: 0.3355  obj_loss: 0.1114  noobj_loss: 0.1273  bbox_loss: 0.0195  cls_loss: 0.0631  \n",
      "<<<iteration:[400/657] - total_loss: 0.3593  obj_loss: 0.1160  noobj_loss: 0.1328  bbox_loss: 0.0198  cls_loss: 0.0778  \n",
      "<<<iteration:[420/657] - total_loss: 0.3592  obj_loss: 0.1097  noobj_loss: 0.1314  bbox_loss: 0.0231  cls_loss: 0.0685  \n",
      "<<<iteration:[440/657] - total_loss: 0.3426  obj_loss: 0.1087  noobj_loss: 0.1269  bbox_loss: 0.0215  cls_loss: 0.0632  \n",
      "<<<iteration:[460/657] - total_loss: 0.3521  obj_loss: 0.1078  noobj_loss: 0.1345  bbox_loss: 0.0207  cls_loss: 0.0734  \n",
      "<<<iteration:[480/657] - total_loss: 0.3521  obj_loss: 0.1165  noobj_loss: 0.1288  bbox_loss: 0.0196  cls_loss: 0.0734  \n",
      "<<<iteration:[500/657] - total_loss: 0.3914  obj_loss: 0.1322  noobj_loss: 0.1280  bbox_loss: 0.0214  cls_loss: 0.0884  \n",
      "<<<iteration:[520/657] - total_loss: 0.3335  obj_loss: 0.1145  noobj_loss: 0.1331  bbox_loss: 0.0193  cls_loss: 0.0561  \n",
      "<<<iteration:[540/657] - total_loss: 0.3674  obj_loss: 0.1172  noobj_loss: 0.1311  bbox_loss: 0.0223  cls_loss: 0.0734  \n",
      "<<<iteration:[560/657] - total_loss: 0.3580  obj_loss: 0.1109  noobj_loss: 0.1258  bbox_loss: 0.0202  cls_loss: 0.0833  \n",
      "<<<iteration:[580/657] - total_loss: 0.3442  obj_loss: 0.1318  noobj_loss: 0.1300  bbox_loss: 0.0179  cls_loss: 0.0580  \n",
      "<<<iteration:[600/657] - total_loss: 0.3516  obj_loss: 0.1108  noobj_loss: 0.1383  bbox_loss: 0.0198  cls_loss: 0.0725  \n",
      "<<<iteration:[620/657] - total_loss: 0.3614  obj_loss: 0.1109  noobj_loss: 0.1337  bbox_loss: 0.0238  cls_loss: 0.0648  \n",
      "<<<iteration:[640/657] - total_loss: 0.3685  obj_loss: 0.1120  noobj_loss: 0.1358  bbox_loss: 0.0231  cls_loss: 0.0733  \n",
      "\n",
      "epoch:81/100 - Train Loss: 0.3554, Val Loss: 0.3845\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3812  obj_loss: 0.1208  noobj_loss: 0.1306  bbox_loss: 0.0224  cls_loss: 0.0832  \n",
      "<<<iteration:[40/657] - total_loss: 0.3522  obj_loss: 0.1095  noobj_loss: 0.1373  bbox_loss: 0.0188  cls_loss: 0.0803  \n",
      "<<<iteration:[60/657] - total_loss: 0.3455  obj_loss: 0.1194  noobj_loss: 0.1288  bbox_loss: 0.0204  cls_loss: 0.0596  \n",
      "<<<iteration:[80/657] - total_loss: 0.3494  obj_loss: 0.1081  noobj_loss: 0.1333  bbox_loss: 0.0213  cls_loss: 0.0682  \n",
      "<<<iteration:[100/657] - total_loss: 0.3662  obj_loss: 0.1186  noobj_loss: 0.1289  bbox_loss: 0.0226  cls_loss: 0.0699  \n",
      "<<<iteration:[120/657] - total_loss: 0.3616  obj_loss: 0.1166  noobj_loss: 0.1317  bbox_loss: 0.0224  cls_loss: 0.0670  \n",
      "<<<iteration:[140/657] - total_loss: 0.3505  obj_loss: 0.1129  noobj_loss: 0.1268  bbox_loss: 0.0206  cls_loss: 0.0714  \n",
      "<<<iteration:[160/657] - total_loss: 0.3574  obj_loss: 0.1104  noobj_loss: 0.1296  bbox_loss: 0.0218  cls_loss: 0.0731  \n",
      "<<<iteration:[180/657] - total_loss: 0.3452  obj_loss: 0.1133  noobj_loss: 0.1310  bbox_loss: 0.0198  cls_loss: 0.0673  \n",
      "<<<iteration:[200/657] - total_loss: 0.3431  obj_loss: 0.1130  noobj_loss: 0.1274  bbox_loss: 0.0199  cls_loss: 0.0670  \n",
      "<<<iteration:[220/657] - total_loss: 0.3599  obj_loss: 0.1216  noobj_loss: 0.1288  bbox_loss: 0.0203  cls_loss: 0.0725  \n",
      "<<<iteration:[240/657] - total_loss: 0.3615  obj_loss: 0.1235  noobj_loss: 0.1290  bbox_loss: 0.0204  cls_loss: 0.0717  \n",
      "<<<iteration:[260/657] - total_loss: 0.3472  obj_loss: 0.1107  noobj_loss: 0.1289  bbox_loss: 0.0215  cls_loss: 0.0647  \n",
      "<<<iteration:[280/657] - total_loss: 0.3532  obj_loss: 0.1156  noobj_loss: 0.1368  bbox_loss: 0.0212  cls_loss: 0.0635  \n",
      "<<<iteration:[300/657] - total_loss: 0.3468  obj_loss: 0.1033  noobj_loss: 0.1314  bbox_loss: 0.0212  cls_loss: 0.0718  \n",
      "<<<iteration:[320/657] - total_loss: 0.3593  obj_loss: 0.1123  noobj_loss: 0.1270  bbox_loss: 0.0223  cls_loss: 0.0719  \n",
      "<<<iteration:[340/657] - total_loss: 0.4208  obj_loss: 0.1057  noobj_loss: 0.1292  bbox_loss: 0.0358  cls_loss: 0.0715  \n",
      "<<<iteration:[360/657] - total_loss: 0.3771  obj_loss: 0.1226  noobj_loss: 0.1268  bbox_loss: 0.0250  cls_loss: 0.0662  \n",
      "<<<iteration:[380/657] - total_loss: 0.3570  obj_loss: 0.1095  noobj_loss: 0.1260  bbox_loss: 0.0216  cls_loss: 0.0763  \n",
      "<<<iteration:[400/657] - total_loss: 0.3652  obj_loss: 0.1215  noobj_loss: 0.1273  bbox_loss: 0.0209  cls_loss: 0.0756  \n",
      "<<<iteration:[420/657] - total_loss: 0.3626  obj_loss: 0.1153  noobj_loss: 0.1290  bbox_loss: 0.0223  cls_loss: 0.0715  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[440/657] - total_loss: 0.3545  obj_loss: 0.1072  noobj_loss: 0.1348  bbox_loss: 0.0242  cls_loss: 0.0589  \n",
      "<<<iteration:[460/657] - total_loss: 0.3510  obj_loss: 0.1129  noobj_loss: 0.1340  bbox_loss: 0.0208  cls_loss: 0.0668  \n",
      "<<<iteration:[480/657] - total_loss: 0.3635  obj_loss: 0.1125  noobj_loss: 0.1287  bbox_loss: 0.0218  cls_loss: 0.0776  \n",
      "<<<iteration:[500/657] - total_loss: 0.3363  obj_loss: 0.1003  noobj_loss: 0.1280  bbox_loss: 0.0208  cls_loss: 0.0681  \n",
      "<<<iteration:[520/657] - total_loss: 0.3379  obj_loss: 0.1273  noobj_loss: 0.1304  bbox_loss: 0.0168  cls_loss: 0.0616  \n",
      "<<<iteration:[540/657] - total_loss: 0.3553  obj_loss: 0.1227  noobj_loss: 0.1314  bbox_loss: 0.0205  cls_loss: 0.0646  \n",
      "<<<iteration:[560/657] - total_loss: 0.3520  obj_loss: 0.1085  noobj_loss: 0.1294  bbox_loss: 0.0227  cls_loss: 0.0653  \n",
      "<<<iteration:[580/657] - total_loss: 0.3620  obj_loss: 0.1042  noobj_loss: 0.1295  bbox_loss: 0.0248  cls_loss: 0.0691  \n",
      "<<<iteration:[600/657] - total_loss: 0.3499  obj_loss: 0.1051  noobj_loss: 0.1313  bbox_loss: 0.0226  cls_loss: 0.0660  \n",
      "<<<iteration:[620/657] - total_loss: 0.3532  obj_loss: 0.1155  noobj_loss: 0.1340  bbox_loss: 0.0212  cls_loss: 0.0649  \n",
      "<<<iteration:[640/657] - total_loss: 0.3820  obj_loss: 0.1238  noobj_loss: 0.1356  bbox_loss: 0.0227  cls_loss: 0.0771  \n",
      "\n",
      "epoch:82/100 - Train Loss: 0.3576, Val Loss: 0.3927\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3698  obj_loss: 0.1098  noobj_loss: 0.1348  bbox_loss: 0.0243  cls_loss: 0.0710  \n",
      "<<<iteration:[40/657] - total_loss: 0.3397  obj_loss: 0.1123  noobj_loss: 0.1285  bbox_loss: 0.0210  cls_loss: 0.0581  \n",
      "<<<iteration:[60/657] - total_loss: 0.3422  obj_loss: 0.1153  noobj_loss: 0.1280  bbox_loss: 0.0193  cls_loss: 0.0664  \n",
      "<<<iteration:[80/657] - total_loss: 0.3596  obj_loss: 0.1188  noobj_loss: 0.1309  bbox_loss: 0.0221  cls_loss: 0.0649  \n",
      "<<<iteration:[100/657] - total_loss: 0.3630  obj_loss: 0.0995  noobj_loss: 0.1285  bbox_loss: 0.0282  cls_loss: 0.0580  \n",
      "<<<iteration:[120/657] - total_loss: 0.3549  obj_loss: 0.1175  noobj_loss: 0.1253  bbox_loss: 0.0207  cls_loss: 0.0712  \n",
      "<<<iteration:[140/657] - total_loss: 0.3436  obj_loss: 0.1149  noobj_loss: 0.1241  bbox_loss: 0.0187  cls_loss: 0.0732  \n",
      "<<<iteration:[160/657] - total_loss: 0.3369  obj_loss: 0.1172  noobj_loss: 0.1327  bbox_loss: 0.0182  cls_loss: 0.0625  \n",
      "<<<iteration:[180/657] - total_loss: 0.3693  obj_loss: 0.1229  noobj_loss: 0.1289  bbox_loss: 0.0196  cls_loss: 0.0838  \n",
      "<<<iteration:[200/657] - total_loss: 0.3450  obj_loss: 0.1094  noobj_loss: 0.1324  bbox_loss: 0.0205  cls_loss: 0.0667  \n",
      "<<<iteration:[220/657] - total_loss: 0.3701  obj_loss: 0.1153  noobj_loss: 0.1315  bbox_loss: 0.0228  cls_loss: 0.0749  \n",
      "<<<iteration:[240/657] - total_loss: 0.3544  obj_loss: 0.1119  noobj_loss: 0.1234  bbox_loss: 0.0199  cls_loss: 0.0815  \n",
      "<<<iteration:[260/657] - total_loss: 0.3532  obj_loss: 0.1208  noobj_loss: 0.1298  bbox_loss: 0.0199  cls_loss: 0.0678  \n",
      "<<<iteration:[280/657] - total_loss: 0.3540  obj_loss: 0.1228  noobj_loss: 0.1294  bbox_loss: 0.0206  cls_loss: 0.0635  \n",
      "<<<iteration:[300/657] - total_loss: 0.3615  obj_loss: 0.1147  noobj_loss: 0.1312  bbox_loss: 0.0215  cls_loss: 0.0737  \n",
      "<<<iteration:[320/657] - total_loss: 0.3465  obj_loss: 0.1136  noobj_loss: 0.1302  bbox_loss: 0.0215  cls_loss: 0.0602  \n",
      "<<<iteration:[340/657] - total_loss: 0.3478  obj_loss: 0.1074  noobj_loss: 0.1336  bbox_loss: 0.0206  cls_loss: 0.0704  \n",
      "<<<iteration:[360/657] - total_loss: 0.3602  obj_loss: 0.1152  noobj_loss: 0.1297  bbox_loss: 0.0203  cls_loss: 0.0785  \n",
      "<<<iteration:[380/657] - total_loss: 0.3510  obj_loss: 0.1137  noobj_loss: 0.1273  bbox_loss: 0.0200  cls_loss: 0.0736  \n",
      "<<<iteration:[400/657] - total_loss: 0.3478  obj_loss: 0.1125  noobj_loss: 0.1252  bbox_loss: 0.0195  cls_loss: 0.0751  \n",
      "<<<iteration:[420/657] - total_loss: 0.3570  obj_loss: 0.1200  noobj_loss: 0.1332  bbox_loss: 0.0211  cls_loss: 0.0648  \n",
      "<<<iteration:[440/657] - total_loss: 0.3586  obj_loss: 0.1107  noobj_loss: 0.1305  bbox_loss: 0.0237  cls_loss: 0.0641  \n",
      "<<<iteration:[460/657] - total_loss: 0.3529  obj_loss: 0.1161  noobj_loss: 0.1331  bbox_loss: 0.0210  cls_loss: 0.0650  \n",
      "<<<iteration:[480/657] - total_loss: 0.3646  obj_loss: 0.1248  noobj_loss: 0.1351  bbox_loss: 0.0209  cls_loss: 0.0680  \n",
      "<<<iteration:[500/657] - total_loss: 0.3510  obj_loss: 0.1026  noobj_loss: 0.1255  bbox_loss: 0.0232  cls_loss: 0.0697  \n",
      "<<<iteration:[520/657] - total_loss: 0.3680  obj_loss: 0.1183  noobj_loss: 0.1281  bbox_loss: 0.0213  cls_loss: 0.0792  \n",
      "<<<iteration:[540/657] - total_loss: 0.3659  obj_loss: 0.1158  noobj_loss: 0.1268  bbox_loss: 0.0216  cls_loss: 0.0785  \n",
      "<<<iteration:[560/657] - total_loss: 0.3391  obj_loss: 0.1070  noobj_loss: 0.1278  bbox_loss: 0.0199  cls_loss: 0.0685  \n",
      "<<<iteration:[580/657] - total_loss: 0.3440  obj_loss: 0.1112  noobj_loss: 0.1328  bbox_loss: 0.0193  cls_loss: 0.0701  \n",
      "<<<iteration:[600/657] - total_loss: 0.3483  obj_loss: 0.1072  noobj_loss: 0.1376  bbox_loss: 0.0213  cls_loss: 0.0660  \n",
      "<<<iteration:[620/657] - total_loss: 0.3603  obj_loss: 0.1054  noobj_loss: 0.1283  bbox_loss: 0.0221  cls_loss: 0.0804  \n",
      "<<<iteration:[640/657] - total_loss: 0.3388  obj_loss: 0.1142  noobj_loss: 0.1273  bbox_loss: 0.0204  cls_loss: 0.0589  \n",
      "\n",
      "epoch:83/100 - Train Loss: 0.3536, Val Loss: 0.3872\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3632  obj_loss: 0.1188  noobj_loss: 0.1383  bbox_loss: 0.0219  cls_loss: 0.0658  \n",
      "<<<iteration:[40/657] - total_loss: 0.3605  obj_loss: 0.1119  noobj_loss: 0.1309  bbox_loss: 0.0229  cls_loss: 0.0685  \n",
      "<<<iteration:[60/657] - total_loss: 0.3578  obj_loss: 0.1090  noobj_loss: 0.1310  bbox_loss: 0.0222  cls_loss: 0.0721  \n",
      "<<<iteration:[80/657] - total_loss: 0.3288  obj_loss: 0.1138  noobj_loss: 0.1257  bbox_loss: 0.0182  cls_loss: 0.0611  \n",
      "<<<iteration:[100/657] - total_loss: 0.3446  obj_loss: 0.1133  noobj_loss: 0.1283  bbox_loss: 0.0194  cls_loss: 0.0701  \n",
      "<<<iteration:[120/657] - total_loss: 0.3434  obj_loss: 0.1173  noobj_loss: 0.1256  bbox_loss: 0.0194  cls_loss: 0.0663  \n",
      "<<<iteration:[140/657] - total_loss: 0.3638  obj_loss: 0.1186  noobj_loss: 0.1258  bbox_loss: 0.0213  cls_loss: 0.0760  \n",
      "<<<iteration:[160/657] - total_loss: 0.3589  obj_loss: 0.1097  noobj_loss: 0.1318  bbox_loss: 0.0228  cls_loss: 0.0693  \n",
      "<<<iteration:[180/657] - total_loss: 0.3480  obj_loss: 0.1192  noobj_loss: 0.1284  bbox_loss: 0.0191  cls_loss: 0.0692  \n",
      "<<<iteration:[200/657] - total_loss: 0.3542  obj_loss: 0.1099  noobj_loss: 0.1341  bbox_loss: 0.0231  cls_loss: 0.0620  \n",
      "<<<iteration:[220/657] - total_loss: 0.3488  obj_loss: 0.1132  noobj_loss: 0.1230  bbox_loss: 0.0208  cls_loss: 0.0701  \n",
      "<<<iteration:[240/657] - total_loss: 0.3439  obj_loss: 0.1110  noobj_loss: 0.1309  bbox_loss: 0.0203  cls_loss: 0.0663  \n",
      "<<<iteration:[260/657] - total_loss: 0.3739  obj_loss: 0.1120  noobj_loss: 0.1308  bbox_loss: 0.0221  cls_loss: 0.0860  \n",
      "<<<iteration:[280/657] - total_loss: 0.3339  obj_loss: 0.1246  noobj_loss: 0.1270  bbox_loss: 0.0168  cls_loss: 0.0619  \n",
      "<<<iteration:[300/657] - total_loss: 0.3465  obj_loss: 0.1194  noobj_loss: 0.1255  bbox_loss: 0.0184  cls_loss: 0.0725  \n",
      "<<<iteration:[320/657] - total_loss: 0.3618  obj_loss: 0.1217  noobj_loss: 0.1339  bbox_loss: 0.0201  cls_loss: 0.0726  \n",
      "<<<iteration:[340/657] - total_loss: 0.3369  obj_loss: 0.1064  noobj_loss: 0.1316  bbox_loss: 0.0220  cls_loss: 0.0547  \n",
      "<<<iteration:[360/657] - total_loss: 0.3552  obj_loss: 0.1083  noobj_loss: 0.1348  bbox_loss: 0.0218  cls_loss: 0.0704  \n",
      "<<<iteration:[380/657] - total_loss: 0.3456  obj_loss: 0.1116  noobj_loss: 0.1311  bbox_loss: 0.0206  cls_loss: 0.0657  \n",
      "<<<iteration:[400/657] - total_loss: 0.3426  obj_loss: 0.1142  noobj_loss: 0.1256  bbox_loss: 0.0198  cls_loss: 0.0668  \n",
      "<<<iteration:[420/657] - total_loss: 0.3587  obj_loss: 0.1195  noobj_loss: 0.1300  bbox_loss: 0.0204  cls_loss: 0.0722  \n",
      "<<<iteration:[440/657] - total_loss: 0.3756  obj_loss: 0.1137  noobj_loss: 0.1286  bbox_loss: 0.0229  cls_loss: 0.0830  \n",
      "<<<iteration:[460/657] - total_loss: 0.3895  obj_loss: 0.0998  noobj_loss: 0.1267  bbox_loss: 0.0325  cls_loss: 0.0641  \n",
      "<<<iteration:[480/657] - total_loss: 0.3582  obj_loss: 0.1112  noobj_loss: 0.1277  bbox_loss: 0.0217  cls_loss: 0.0744  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[500/657] - total_loss: 0.3414  obj_loss: 0.1111  noobj_loss: 0.1285  bbox_loss: 0.0201  cls_loss: 0.0654  \n",
      "<<<iteration:[520/657] - total_loss: 0.3384  obj_loss: 0.1219  noobj_loss: 0.1230  bbox_loss: 0.0186  cls_loss: 0.0617  \n",
      "<<<iteration:[540/657] - total_loss: 0.3551  obj_loss: 0.1232  noobj_loss: 0.1324  bbox_loss: 0.0206  cls_loss: 0.0630  \n",
      "<<<iteration:[560/657] - total_loss: 0.3442  obj_loss: 0.1099  noobj_loss: 0.1323  bbox_loss: 0.0219  cls_loss: 0.0586  \n",
      "<<<iteration:[580/657] - total_loss: 0.3633  obj_loss: 0.1188  noobj_loss: 0.1346  bbox_loss: 0.0204  cls_loss: 0.0751  \n",
      "<<<iteration:[600/657] - total_loss: 0.3784  obj_loss: 0.1101  noobj_loss: 0.1282  bbox_loss: 0.0280  cls_loss: 0.0641  \n",
      "<<<iteration:[620/657] - total_loss: 0.3364  obj_loss: 0.1045  noobj_loss: 0.1252  bbox_loss: 0.0206  cls_loss: 0.0665  \n",
      "<<<iteration:[640/657] - total_loss: 0.3482  obj_loss: 0.1106  noobj_loss: 0.1295  bbox_loss: 0.0204  cls_loss: 0.0711  \n",
      "\n",
      "epoch:84/100 - Train Loss: 0.3524, Val Loss: 0.3848\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3918  obj_loss: 0.1304  noobj_loss: 0.1410  bbox_loss: 0.0214  cls_loss: 0.0837  \n",
      "<<<iteration:[40/657] - total_loss: 0.3520  obj_loss: 0.1119  noobj_loss: 0.1299  bbox_loss: 0.0219  cls_loss: 0.0654  \n",
      "<<<iteration:[60/657] - total_loss: 0.3699  obj_loss: 0.1053  noobj_loss: 0.1405  bbox_loss: 0.0251  cls_loss: 0.0687  \n",
      "<<<iteration:[80/657] - total_loss: 0.3449  obj_loss: 0.1059  noobj_loss: 0.1309  bbox_loss: 0.0218  cls_loss: 0.0647  \n",
      "<<<iteration:[100/657] - total_loss: 0.3487  obj_loss: 0.1076  noobj_loss: 0.1278  bbox_loss: 0.0241  cls_loss: 0.0566  \n",
      "<<<iteration:[120/657] - total_loss: 0.3645  obj_loss: 0.1041  noobj_loss: 0.1322  bbox_loss: 0.0244  cls_loss: 0.0723  \n",
      "<<<iteration:[140/657] - total_loss: 0.3486  obj_loss: 0.1125  noobj_loss: 0.1293  bbox_loss: 0.0215  cls_loss: 0.0641  \n",
      "<<<iteration:[160/657] - total_loss: 0.3767  obj_loss: 0.1097  noobj_loss: 0.1227  bbox_loss: 0.0263  cls_loss: 0.0742  \n",
      "<<<iteration:[180/657] - total_loss: 0.3541  obj_loss: 0.1150  noobj_loss: 0.1260  bbox_loss: 0.0201  cls_loss: 0.0754  \n",
      "<<<iteration:[200/657] - total_loss: 0.3646  obj_loss: 0.1204  noobj_loss: 0.1254  bbox_loss: 0.0212  cls_loss: 0.0755  \n",
      "<<<iteration:[220/657] - total_loss: 0.3539  obj_loss: 0.1259  noobj_loss: 0.1309  bbox_loss: 0.0209  cls_loss: 0.0581  \n",
      "<<<iteration:[240/657] - total_loss: 0.3499  obj_loss: 0.1146  noobj_loss: 0.1276  bbox_loss: 0.0192  cls_loss: 0.0755  \n",
      "<<<iteration:[260/657] - total_loss: 0.3517  obj_loss: 0.1166  noobj_loss: 0.1323  bbox_loss: 0.0193  cls_loss: 0.0725  \n",
      "<<<iteration:[280/657] - total_loss: 0.3523  obj_loss: 0.1213  noobj_loss: 0.1295  bbox_loss: 0.0200  cls_loss: 0.0665  \n",
      "<<<iteration:[300/657] - total_loss: 0.3669  obj_loss: 0.1307  noobj_loss: 0.1266  bbox_loss: 0.0223  cls_loss: 0.0616  \n",
      "<<<iteration:[320/657] - total_loss: 0.3435  obj_loss: 0.1075  noobj_loss: 0.1299  bbox_loss: 0.0199  cls_loss: 0.0718  \n",
      "<<<iteration:[340/657] - total_loss: 0.3558  obj_loss: 0.1201  noobj_loss: 0.1345  bbox_loss: 0.0207  cls_loss: 0.0650  \n",
      "<<<iteration:[360/657] - total_loss: 0.3451  obj_loss: 0.1101  noobj_loss: 0.1286  bbox_loss: 0.0214  cls_loss: 0.0637  \n",
      "<<<iteration:[380/657] - total_loss: 0.3520  obj_loss: 0.1066  noobj_loss: 0.1330  bbox_loss: 0.0234  cls_loss: 0.0618  \n",
      "<<<iteration:[400/657] - total_loss: 0.3527  obj_loss: 0.1181  noobj_loss: 0.1288  bbox_loss: 0.0209  cls_loss: 0.0655  \n",
      "<<<iteration:[420/657] - total_loss: 0.3469  obj_loss: 0.1164  noobj_loss: 0.1274  bbox_loss: 0.0214  cls_loss: 0.0597  \n",
      "<<<iteration:[440/657] - total_loss: 0.3635  obj_loss: 0.1089  noobj_loss: 0.1276  bbox_loss: 0.0223  cls_loss: 0.0793  \n",
      "<<<iteration:[460/657] - total_loss: 0.3335  obj_loss: 0.1195  noobj_loss: 0.1256  bbox_loss: 0.0187  cls_loss: 0.0576  \n",
      "<<<iteration:[480/657] - total_loss: 0.3369  obj_loss: 0.1100  noobj_loss: 0.1268  bbox_loss: 0.0198  cls_loss: 0.0643  \n",
      "<<<iteration:[500/657] - total_loss: 0.3525  obj_loss: 0.1083  noobj_loss: 0.1334  bbox_loss: 0.0217  cls_loss: 0.0688  \n",
      "<<<iteration:[520/657] - total_loss: 0.3440  obj_loss: 0.1160  noobj_loss: 0.1269  bbox_loss: 0.0181  cls_loss: 0.0738  \n",
      "<<<iteration:[540/657] - total_loss: 0.3411  obj_loss: 0.1178  noobj_loss: 0.1311  bbox_loss: 0.0193  cls_loss: 0.0610  \n",
      "<<<iteration:[560/657] - total_loss: 0.3443  obj_loss: 0.1089  noobj_loss: 0.1333  bbox_loss: 0.0192  cls_loss: 0.0729  \n",
      "<<<iteration:[580/657] - total_loss: 0.3500  obj_loss: 0.1126  noobj_loss: 0.1255  bbox_loss: 0.0203  cls_loss: 0.0734  \n",
      "<<<iteration:[600/657] - total_loss: 0.3449  obj_loss: 0.1228  noobj_loss: 0.1336  bbox_loss: 0.0184  cls_loss: 0.0634  \n",
      "<<<iteration:[620/657] - total_loss: 0.3735  obj_loss: 0.1304  noobj_loss: 0.1293  bbox_loss: 0.0206  cls_loss: 0.0756  \n",
      "<<<iteration:[640/657] - total_loss: 0.3367  obj_loss: 0.1192  noobj_loss: 0.1300  bbox_loss: 0.0182  cls_loss: 0.0616  \n",
      "\n",
      "epoch:85/100 - Train Loss: 0.3530, Val Loss: 0.3859\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3696  obj_loss: 0.1198  noobj_loss: 0.1389  bbox_loss: 0.0226  cls_loss: 0.0673  \n",
      "<<<iteration:[40/657] - total_loss: 0.3401  obj_loss: 0.1222  noobj_loss: 0.1282  bbox_loss: 0.0185  cls_loss: 0.0615  \n",
      "<<<iteration:[60/657] - total_loss: 0.3724  obj_loss: 0.1266  noobj_loss: 0.1376  bbox_loss: 0.0198  cls_loss: 0.0780  \n",
      "<<<iteration:[80/657] - total_loss: 0.3466  obj_loss: 0.1161  noobj_loss: 0.1208  bbox_loss: 0.0200  cls_loss: 0.0700  \n",
      "<<<iteration:[100/657] - total_loss: 0.3428  obj_loss: 0.1125  noobj_loss: 0.1326  bbox_loss: 0.0193  cls_loss: 0.0676  \n",
      "<<<iteration:[120/657] - total_loss: 0.3558  obj_loss: 0.1130  noobj_loss: 0.1295  bbox_loss: 0.0199  cls_loss: 0.0785  \n",
      "<<<iteration:[140/657] - total_loss: 0.3534  obj_loss: 0.1154  noobj_loss: 0.1308  bbox_loss: 0.0203  cls_loss: 0.0710  \n",
      "<<<iteration:[160/657] - total_loss: 0.3639  obj_loss: 0.1159  noobj_loss: 0.1312  bbox_loss: 0.0217  cls_loss: 0.0741  \n",
      "<<<iteration:[180/657] - total_loss: 0.3422  obj_loss: 0.1086  noobj_loss: 0.1282  bbox_loss: 0.0209  cls_loss: 0.0651  \n",
      "<<<iteration:[200/657] - total_loss: 0.3374  obj_loss: 0.1105  noobj_loss: 0.1289  bbox_loss: 0.0212  cls_loss: 0.0562  \n",
      "<<<iteration:[220/657] - total_loss: 0.3502  obj_loss: 0.1230  noobj_loss: 0.1288  bbox_loss: 0.0183  cls_loss: 0.0713  \n",
      "<<<iteration:[240/657] - total_loss: 0.3690  obj_loss: 0.1247  noobj_loss: 0.1274  bbox_loss: 0.0198  cls_loss: 0.0816  \n",
      "<<<iteration:[260/657] - total_loss: 0.3781  obj_loss: 0.1084  noobj_loss: 0.1324  bbox_loss: 0.0271  cls_loss: 0.0680  \n",
      "<<<iteration:[280/657] - total_loss: 0.3751  obj_loss: 0.1196  noobj_loss: 0.1309  bbox_loss: 0.0215  cls_loss: 0.0825  \n",
      "<<<iteration:[300/657] - total_loss: 0.3337  obj_loss: 0.1077  noobj_loss: 0.1223  bbox_loss: 0.0201  cls_loss: 0.0640  \n",
      "<<<iteration:[320/657] - total_loss: 0.3665  obj_loss: 0.1243  noobj_loss: 0.1298  bbox_loss: 0.0208  cls_loss: 0.0732  \n",
      "<<<iteration:[340/657] - total_loss: 0.3493  obj_loss: 0.1209  noobj_loss: 0.1307  bbox_loss: 0.0193  cls_loss: 0.0666  \n",
      "<<<iteration:[360/657] - total_loss: 0.3421  obj_loss: 0.1079  noobj_loss: 0.1233  bbox_loss: 0.0219  cls_loss: 0.0631  \n",
      "<<<iteration:[380/657] - total_loss: 0.3533  obj_loss: 0.1168  noobj_loss: 0.1299  bbox_loss: 0.0207  cls_loss: 0.0680  \n",
      "<<<iteration:[400/657] - total_loss: 0.3333  obj_loss: 0.1174  noobj_loss: 0.1241  bbox_loss: 0.0188  cls_loss: 0.0597  \n",
      "<<<iteration:[420/657] - total_loss: 0.3632  obj_loss: 0.1152  noobj_loss: 0.1318  bbox_loss: 0.0230  cls_loss: 0.0669  \n",
      "<<<iteration:[440/657] - total_loss: 0.3470  obj_loss: 0.1173  noobj_loss: 0.1256  bbox_loss: 0.0178  cls_loss: 0.0779  \n",
      "<<<iteration:[460/657] - total_loss: 0.3391  obj_loss: 0.1056  noobj_loss: 0.1313  bbox_loss: 0.0224  cls_loss: 0.0558  \n",
      "<<<iteration:[480/657] - total_loss: 0.3591  obj_loss: 0.1216  noobj_loss: 0.1283  bbox_loss: 0.0214  cls_loss: 0.0661  \n",
      "<<<iteration:[500/657] - total_loss: 0.3369  obj_loss: 0.1134  noobj_loss: 0.1319  bbox_loss: 0.0202  cls_loss: 0.0568  \n",
      "<<<iteration:[520/657] - total_loss: 0.3493  obj_loss: 0.1166  noobj_loss: 0.1283  bbox_loss: 0.0205  cls_loss: 0.0661  \n",
      "<<<iteration:[540/657] - total_loss: 0.3500  obj_loss: 0.1045  noobj_loss: 0.1260  bbox_loss: 0.0225  cls_loss: 0.0699  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[560/657] - total_loss: 0.3567  obj_loss: 0.1165  noobj_loss: 0.1297  bbox_loss: 0.0209  cls_loss: 0.0710  \n",
      "<<<iteration:[580/657] - total_loss: 0.3582  obj_loss: 0.1125  noobj_loss: 0.1274  bbox_loss: 0.0202  cls_loss: 0.0808  \n",
      "<<<iteration:[600/657] - total_loss: 0.3270  obj_loss: 0.1111  noobj_loss: 0.1264  bbox_loss: 0.0194  cls_loss: 0.0558  \n",
      "<<<iteration:[620/657] - total_loss: 0.3355  obj_loss: 0.1082  noobj_loss: 0.1304  bbox_loss: 0.0201  cls_loss: 0.0616  \n",
      "<<<iteration:[640/657] - total_loss: 0.3235  obj_loss: 0.1060  noobj_loss: 0.1262  bbox_loss: 0.0197  cls_loss: 0.0557  \n",
      "\n",
      "epoch:86/100 - Train Loss: 0.3507, Val Loss: 0.3982\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3742  obj_loss: 0.1331  noobj_loss: 0.1328  bbox_loss: 0.0198  cls_loss: 0.0759  \n",
      "<<<iteration:[40/657] - total_loss: 0.3740  obj_loss: 0.1249  noobj_loss: 0.1257  bbox_loss: 0.0209  cls_loss: 0.0819  \n",
      "<<<iteration:[60/657] - total_loss: 0.3437  obj_loss: 0.1119  noobj_loss: 0.1319  bbox_loss: 0.0198  cls_loss: 0.0670  \n",
      "<<<iteration:[80/657] - total_loss: 0.3470  obj_loss: 0.1209  noobj_loss: 0.1285  bbox_loss: 0.0209  cls_loss: 0.0573  \n",
      "<<<iteration:[100/657] - total_loss: 0.3381  obj_loss: 0.1135  noobj_loss: 0.1259  bbox_loss: 0.0197  cls_loss: 0.0634  \n",
      "<<<iteration:[120/657] - total_loss: 0.3589  obj_loss: 0.1209  noobj_loss: 0.1325  bbox_loss: 0.0199  cls_loss: 0.0723  \n",
      "<<<iteration:[140/657] - total_loss: 0.3455  obj_loss: 0.1213  noobj_loss: 0.1322  bbox_loss: 0.0191  cls_loss: 0.0627  \n",
      "<<<iteration:[160/657] - total_loss: 0.3675  obj_loss: 0.1127  noobj_loss: 0.1334  bbox_loss: 0.0237  cls_loss: 0.0695  \n",
      "<<<iteration:[180/657] - total_loss: 0.3376  obj_loss: 0.1131  noobj_loss: 0.1307  bbox_loss: 0.0200  cls_loss: 0.0593  \n",
      "<<<iteration:[200/657] - total_loss: 0.3590  obj_loss: 0.1205  noobj_loss: 0.1223  bbox_loss: 0.0215  cls_loss: 0.0696  \n",
      "<<<iteration:[220/657] - total_loss: 0.3432  obj_loss: 0.1027  noobj_loss: 0.1326  bbox_loss: 0.0223  cls_loss: 0.0626  \n",
      "<<<iteration:[240/657] - total_loss: 0.3525  obj_loss: 0.1106  noobj_loss: 0.1260  bbox_loss: 0.0213  cls_loss: 0.0723  \n",
      "<<<iteration:[260/657] - total_loss: 0.3433  obj_loss: 0.1185  noobj_loss: 0.1276  bbox_loss: 0.0195  cls_loss: 0.0635  \n",
      "<<<iteration:[280/657] - total_loss: 0.3529  obj_loss: 0.1045  noobj_loss: 0.1270  bbox_loss: 0.0238  cls_loss: 0.0657  \n",
      "<<<iteration:[300/657] - total_loss: 0.3211  obj_loss: 0.1017  noobj_loss: 0.1311  bbox_loss: 0.0189  cls_loss: 0.0595  \n",
      "<<<iteration:[320/657] - total_loss: 0.3563  obj_loss: 0.1216  noobj_loss: 0.1302  bbox_loss: 0.0197  cls_loss: 0.0713  \n",
      "<<<iteration:[340/657] - total_loss: 0.3522  obj_loss: 0.1175  noobj_loss: 0.1280  bbox_loss: 0.0197  cls_loss: 0.0725  \n",
      "<<<iteration:[360/657] - total_loss: 0.3547  obj_loss: 0.1179  noobj_loss: 0.1256  bbox_loss: 0.0207  cls_loss: 0.0704  \n",
      "<<<iteration:[380/657] - total_loss: 0.3505  obj_loss: 0.1103  noobj_loss: 0.1247  bbox_loss: 0.0214  cls_loss: 0.0710  \n",
      "<<<iteration:[400/657] - total_loss: 0.3554  obj_loss: 0.1156  noobj_loss: 0.1245  bbox_loss: 0.0199  cls_loss: 0.0779  \n",
      "<<<iteration:[420/657] - total_loss: 0.3526  obj_loss: 0.1186  noobj_loss: 0.1333  bbox_loss: 0.0208  cls_loss: 0.0632  \n",
      "<<<iteration:[440/657] - total_loss: 0.3235  obj_loss: 0.1176  noobj_loss: 0.1242  bbox_loss: 0.0178  cls_loss: 0.0549  \n",
      "<<<iteration:[460/657] - total_loss: 0.3559  obj_loss: 0.1163  noobj_loss: 0.1259  bbox_loss: 0.0215  cls_loss: 0.0691  \n",
      "<<<iteration:[480/657] - total_loss: 0.3235  obj_loss: 0.1199  noobj_loss: 0.1270  bbox_loss: 0.0178  cls_loss: 0.0513  \n",
      "<<<iteration:[500/657] - total_loss: 0.3567  obj_loss: 0.1072  noobj_loss: 0.1312  bbox_loss: 0.0222  cls_loss: 0.0731  \n",
      "<<<iteration:[520/657] - total_loss: 0.3412  obj_loss: 0.1263  noobj_loss: 0.1276  bbox_loss: 0.0184  cls_loss: 0.0593  \n",
      "<<<iteration:[540/657] - total_loss: 0.3329  obj_loss: 0.0997  noobj_loss: 0.1234  bbox_loss: 0.0200  cls_loss: 0.0716  \n",
      "<<<iteration:[560/657] - total_loss: 0.3558  obj_loss: 0.1085  noobj_loss: 0.1275  bbox_loss: 0.0226  cls_loss: 0.0708  \n",
      "<<<iteration:[580/657] - total_loss: 0.3498  obj_loss: 0.1106  noobj_loss: 0.1236  bbox_loss: 0.0215  cls_loss: 0.0698  \n",
      "<<<iteration:[600/657] - total_loss: 0.3556  obj_loss: 0.1167  noobj_loss: 0.1276  bbox_loss: 0.0222  cls_loss: 0.0641  \n",
      "<<<iteration:[620/657] - total_loss: 0.3592  obj_loss: 0.1223  noobj_loss: 0.1304  bbox_loss: 0.0197  cls_loss: 0.0730  \n",
      "<<<iteration:[640/657] - total_loss: 0.3541  obj_loss: 0.1232  noobj_loss: 0.1267  bbox_loss: 0.0212  cls_loss: 0.0614  \n",
      "\n",
      "epoch:87/100 - Train Loss: 0.3491, Val Loss: 0.3825\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3601  obj_loss: 0.1071  noobj_loss: 0.1343  bbox_loss: 0.0246  cls_loss: 0.0627  \n",
      "<<<iteration:[40/657] - total_loss: 0.3320  obj_loss: 0.1191  noobj_loss: 0.1260  bbox_loss: 0.0189  cls_loss: 0.0555  \n",
      "<<<iteration:[60/657] - total_loss: 0.3355  obj_loss: 0.1182  noobj_loss: 0.1317  bbox_loss: 0.0186  cls_loss: 0.0587  \n",
      "<<<iteration:[80/657] - total_loss: 0.3438  obj_loss: 0.1326  noobj_loss: 0.1285  bbox_loss: 0.0182  cls_loss: 0.0560  \n",
      "<<<iteration:[100/657] - total_loss: 0.3707  obj_loss: 0.1098  noobj_loss: 0.1252  bbox_loss: 0.0244  cls_loss: 0.0762  \n",
      "<<<iteration:[120/657] - total_loss: 0.3638  obj_loss: 0.1196  noobj_loss: 0.1279  bbox_loss: 0.0212  cls_loss: 0.0745  \n",
      "<<<iteration:[140/657] - total_loss: 0.3255  obj_loss: 0.1129  noobj_loss: 0.1267  bbox_loss: 0.0180  cls_loss: 0.0594  \n",
      "<<<iteration:[160/657] - total_loss: 0.3235  obj_loss: 0.1116  noobj_loss: 0.1242  bbox_loss: 0.0185  cls_loss: 0.0572  \n",
      "<<<iteration:[180/657] - total_loss: 0.3587  obj_loss: 0.1195  noobj_loss: 0.1283  bbox_loss: 0.0204  cls_loss: 0.0731  \n",
      "<<<iteration:[200/657] - total_loss: 0.3285  obj_loss: 0.1135  noobj_loss: 0.1279  bbox_loss: 0.0192  cls_loss: 0.0550  \n",
      "<<<iteration:[220/657] - total_loss: 0.3355  obj_loss: 0.1166  noobj_loss: 0.1249  bbox_loss: 0.0190  cls_loss: 0.0614  \n",
      "<<<iteration:[240/657] - total_loss: 0.3708  obj_loss: 0.1171  noobj_loss: 0.1227  bbox_loss: 0.0244  cls_loss: 0.0705  \n",
      "<<<iteration:[260/657] - total_loss: 0.3658  obj_loss: 0.1137  noobj_loss: 0.1278  bbox_loss: 0.0214  cls_loss: 0.0811  \n",
      "<<<iteration:[280/657] - total_loss: 0.3426  obj_loss: 0.1140  noobj_loss: 0.1297  bbox_loss: 0.0202  cls_loss: 0.0631  \n",
      "<<<iteration:[300/657] - total_loss: 0.3545  obj_loss: 0.1197  noobj_loss: 0.1252  bbox_loss: 0.0207  cls_loss: 0.0688  \n",
      "<<<iteration:[320/657] - total_loss: 0.3396  obj_loss: 0.1008  noobj_loss: 0.1244  bbox_loss: 0.0212  cls_loss: 0.0709  \n",
      "<<<iteration:[340/657] - total_loss: 0.3485  obj_loss: 0.1195  noobj_loss: 0.1237  bbox_loss: 0.0204  cls_loss: 0.0649  \n",
      "<<<iteration:[360/657] - total_loss: 0.3325  obj_loss: 0.1190  noobj_loss: 0.1294  bbox_loss: 0.0181  cls_loss: 0.0583  \n",
      "<<<iteration:[380/657] - total_loss: 0.3705  obj_loss: 0.1160  noobj_loss: 0.1326  bbox_loss: 0.0228  cls_loss: 0.0743  \n",
      "<<<iteration:[400/657] - total_loss: 0.3488  obj_loss: 0.1220  noobj_loss: 0.1316  bbox_loss: 0.0209  cls_loss: 0.0563  \n",
      "<<<iteration:[420/657] - total_loss: 0.3521  obj_loss: 0.1168  noobj_loss: 0.1337  bbox_loss: 0.0232  cls_loss: 0.0526  \n",
      "<<<iteration:[440/657] - total_loss: 0.3583  obj_loss: 0.1201  noobj_loss: 0.1262  bbox_loss: 0.0197  cls_loss: 0.0767  \n",
      "<<<iteration:[460/657] - total_loss: 0.3498  obj_loss: 0.1214  noobj_loss: 0.1273  bbox_loss: 0.0185  cls_loss: 0.0722  \n",
      "<<<iteration:[480/657] - total_loss: 0.3561  obj_loss: 0.1137  noobj_loss: 0.1255  bbox_loss: 0.0218  cls_loss: 0.0709  \n",
      "<<<iteration:[500/657] - total_loss: 0.3610  obj_loss: 0.1190  noobj_loss: 0.1311  bbox_loss: 0.0207  cls_loss: 0.0732  \n",
      "<<<iteration:[520/657] - total_loss: 0.3683  obj_loss: 0.1077  noobj_loss: 0.1268  bbox_loss: 0.0242  cls_loss: 0.0761  \n",
      "<<<iteration:[540/657] - total_loss: 0.3618  obj_loss: 0.1231  noobj_loss: 0.1245  bbox_loss: 0.0204  cls_loss: 0.0745  \n",
      "<<<iteration:[560/657] - total_loss: 0.3537  obj_loss: 0.1192  noobj_loss: 0.1247  bbox_loss: 0.0195  cls_loss: 0.0748  \n",
      "<<<iteration:[580/657] - total_loss: 0.3489  obj_loss: 0.1124  noobj_loss: 0.1278  bbox_loss: 0.0205  cls_loss: 0.0702  \n",
      "<<<iteration:[600/657] - total_loss: 0.3708  obj_loss: 0.1188  noobj_loss: 0.1288  bbox_loss: 0.0220  cls_loss: 0.0774  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[620/657] - total_loss: 0.3263  obj_loss: 0.1160  noobj_loss: 0.1290  bbox_loss: 0.0187  cls_loss: 0.0521  \n",
      "<<<iteration:[640/657] - total_loss: 0.3627  obj_loss: 0.1067  noobj_loss: 0.1308  bbox_loss: 0.0236  cls_loss: 0.0724  \n",
      "\n",
      "epoch:88/100 - Train Loss: 0.3495, Val Loss: 0.3840\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.4032  obj_loss: 0.1171  noobj_loss: 0.1428  bbox_loss: 0.0270  cls_loss: 0.0796  \n",
      "<<<iteration:[40/657] - total_loss: 0.3568  obj_loss: 0.1177  noobj_loss: 0.1237  bbox_loss: 0.0210  cls_loss: 0.0724  \n",
      "<<<iteration:[60/657] - total_loss: 0.3590  obj_loss: 0.1317  noobj_loss: 0.1274  bbox_loss: 0.0187  cls_loss: 0.0703  \n",
      "<<<iteration:[80/657] - total_loss: 0.3473  obj_loss: 0.1203  noobj_loss: 0.1222  bbox_loss: 0.0194  cls_loss: 0.0686  \n",
      "<<<iteration:[100/657] - total_loss: 0.3334  obj_loss: 0.1189  noobj_loss: 0.1222  bbox_loss: 0.0181  cls_loss: 0.0631  \n",
      "<<<iteration:[120/657] - total_loss: 0.3342  obj_loss: 0.1117  noobj_loss: 0.1233  bbox_loss: 0.0181  cls_loss: 0.0702  \n",
      "<<<iteration:[140/657] - total_loss: 0.3567  obj_loss: 0.1100  noobj_loss: 0.1273  bbox_loss: 0.0202  cls_loss: 0.0818  \n",
      "<<<iteration:[160/657] - total_loss: 0.3487  obj_loss: 0.1152  noobj_loss: 0.1327  bbox_loss: 0.0208  cls_loss: 0.0630  \n",
      "<<<iteration:[180/657] - total_loss: 0.3785  obj_loss: 0.1269  noobj_loss: 0.1307  bbox_loss: 0.0214  cls_loss: 0.0792  \n",
      "<<<iteration:[200/657] - total_loss: 0.3547  obj_loss: 0.1112  noobj_loss: 0.1302  bbox_loss: 0.0207  cls_loss: 0.0749  \n",
      "<<<iteration:[220/657] - total_loss: 0.3540  obj_loss: 0.1150  noobj_loss: 0.1300  bbox_loss: 0.0194  cls_loss: 0.0769  \n",
      "<<<iteration:[240/657] - total_loss: 0.3358  obj_loss: 0.1232  noobj_loss: 0.1285  bbox_loss: 0.0183  cls_loss: 0.0567  \n",
      "<<<iteration:[260/657] - total_loss: 0.3593  obj_loss: 0.1189  noobj_loss: 0.1285  bbox_loss: 0.0218  cls_loss: 0.0671  \n",
      "<<<iteration:[280/657] - total_loss: 0.3561  obj_loss: 0.1145  noobj_loss: 0.1287  bbox_loss: 0.0244  cls_loss: 0.0554  \n",
      "<<<iteration:[300/657] - total_loss: 0.3580  obj_loss: 0.1206  noobj_loss: 0.1300  bbox_loss: 0.0212  cls_loss: 0.0666  \n",
      "<<<iteration:[320/657] - total_loss: 0.3299  obj_loss: 0.1170  noobj_loss: 0.1294  bbox_loss: 0.0202  cls_loss: 0.0473  \n",
      "<<<iteration:[340/657] - total_loss: 0.3770  obj_loss: 0.1023  noobj_loss: 0.1244  bbox_loss: 0.0304  cls_loss: 0.0607  \n",
      "<<<iteration:[360/657] - total_loss: 0.3739  obj_loss: 0.1069  noobj_loss: 0.1282  bbox_loss: 0.0273  cls_loss: 0.0664  \n",
      "<<<iteration:[380/657] - total_loss: 0.4522  obj_loss: 0.0897  noobj_loss: 0.1201  bbox_loss: 0.0465  cls_loss: 0.0699  \n",
      "<<<iteration:[400/657] - total_loss: 0.3914  obj_loss: 0.1011  noobj_loss: 0.1177  bbox_loss: 0.0309  cls_loss: 0.0769  \n",
      "<<<iteration:[420/657] - total_loss: 0.3376  obj_loss: 0.1091  noobj_loss: 0.1213  bbox_loss: 0.0214  cls_loss: 0.0610  \n",
      "<<<iteration:[440/657] - total_loss: 0.3238  obj_loss: 0.1145  noobj_loss: 0.1220  bbox_loss: 0.0178  cls_loss: 0.0594  \n",
      "<<<iteration:[460/657] - total_loss: 0.3607  obj_loss: 0.1186  noobj_loss: 0.1219  bbox_loss: 0.0231  cls_loss: 0.0659  \n",
      "<<<iteration:[480/657] - total_loss: 0.3485  obj_loss: 0.1200  noobj_loss: 0.1279  bbox_loss: 0.0197  cls_loss: 0.0660  \n",
      "<<<iteration:[500/657] - total_loss: 0.3431  obj_loss: 0.1193  noobj_loss: 0.1324  bbox_loss: 0.0182  cls_loss: 0.0668  \n",
      "<<<iteration:[520/657] - total_loss: 0.3606  obj_loss: 0.1196  noobj_loss: 0.1289  bbox_loss: 0.0212  cls_loss: 0.0704  \n",
      "<<<iteration:[540/657] - total_loss: 0.3342  obj_loss: 0.1157  noobj_loss: 0.1310  bbox_loss: 0.0187  cls_loss: 0.0595  \n",
      "<<<iteration:[560/657] - total_loss: 0.3529  obj_loss: 0.1108  noobj_loss: 0.1269  bbox_loss: 0.0224  cls_loss: 0.0667  \n",
      "<<<iteration:[580/657] - total_loss: 0.3296  obj_loss: 0.1143  noobj_loss: 0.1216  bbox_loss: 0.0185  cls_loss: 0.0621  \n",
      "<<<iteration:[600/657] - total_loss: 0.3422  obj_loss: 0.1081  noobj_loss: 0.1345  bbox_loss: 0.0208  cls_loss: 0.0630  \n",
      "<<<iteration:[620/657] - total_loss: 0.3474  obj_loss: 0.1132  noobj_loss: 0.1247  bbox_loss: 0.0213  cls_loss: 0.0652  \n",
      "<<<iteration:[640/657] - total_loss: 0.3540  obj_loss: 0.1142  noobj_loss: 0.1213  bbox_loss: 0.0218  cls_loss: 0.0700  \n",
      "\n",
      "epoch:89/100 - Train Loss: 0.3564, Val Loss: 0.3905\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3721  obj_loss: 0.1155  noobj_loss: 0.1362  bbox_loss: 0.0229  cls_loss: 0.0738  \n",
      "<<<iteration:[40/657] - total_loss: 0.3319  obj_loss: 0.1168  noobj_loss: 0.1216  bbox_loss: 0.0184  cls_loss: 0.0622  \n",
      "<<<iteration:[60/657] - total_loss: 0.3348  obj_loss: 0.1168  noobj_loss: 0.1288  bbox_loss: 0.0186  cls_loss: 0.0603  \n",
      "<<<iteration:[80/657] - total_loss: 0.3511  obj_loss: 0.1205  noobj_loss: 0.1248  bbox_loss: 0.0187  cls_loss: 0.0748  \n",
      "<<<iteration:[100/657] - total_loss: 0.3530  obj_loss: 0.1262  noobj_loss: 0.1257  bbox_loss: 0.0193  cls_loss: 0.0673  \n",
      "<<<iteration:[120/657] - total_loss: 0.3526  obj_loss: 0.1210  noobj_loss: 0.1279  bbox_loss: 0.0191  cls_loss: 0.0722  \n",
      "<<<iteration:[140/657] - total_loss: 0.3466  obj_loss: 0.1139  noobj_loss: 0.1327  bbox_loss: 0.0200  cls_loss: 0.0667  \n",
      "<<<iteration:[160/657] - total_loss: 0.3447  obj_loss: 0.1235  noobj_loss: 0.1268  bbox_loss: 0.0190  cls_loss: 0.0629  \n",
      "<<<iteration:[180/657] - total_loss: 0.3508  obj_loss: 0.1119  noobj_loss: 0.1291  bbox_loss: 0.0210  cls_loss: 0.0692  \n",
      "<<<iteration:[200/657] - total_loss: 0.3418  obj_loss: 0.1202  noobj_loss: 0.1281  bbox_loss: 0.0182  cls_loss: 0.0667  \n",
      "<<<iteration:[220/657] - total_loss: 0.3539  obj_loss: 0.1094  noobj_loss: 0.1233  bbox_loss: 0.0228  cls_loss: 0.0688  \n",
      "<<<iteration:[240/657] - total_loss: 0.3556  obj_loss: 0.1026  noobj_loss: 0.1181  bbox_loss: 0.0264  cls_loss: 0.0619  \n",
      "<<<iteration:[260/657] - total_loss: 0.3722  obj_loss: 0.1124  noobj_loss: 0.1276  bbox_loss: 0.0226  cls_loss: 0.0827  \n",
      "<<<iteration:[280/657] - total_loss: 0.3399  obj_loss: 0.1151  noobj_loss: 0.1237  bbox_loss: 0.0196  cls_loss: 0.0649  \n",
      "<<<iteration:[300/657] - total_loss: 0.3581  obj_loss: 0.1275  noobj_loss: 0.1236  bbox_loss: 0.0206  cls_loss: 0.0657  \n",
      "<<<iteration:[320/657] - total_loss: 0.3624  obj_loss: 0.1192  noobj_loss: 0.1316  bbox_loss: 0.0222  cls_loss: 0.0664  \n",
      "<<<iteration:[340/657] - total_loss: 0.3473  obj_loss: 0.1092  noobj_loss: 0.1274  bbox_loss: 0.0219  cls_loss: 0.0649  \n",
      "<<<iteration:[360/657] - total_loss: 0.3356  obj_loss: 0.1072  noobj_loss: 0.1297  bbox_loss: 0.0206  cls_loss: 0.0607  \n",
      "<<<iteration:[380/657] - total_loss: 0.3644  obj_loss: 0.1281  noobj_loss: 0.1285  bbox_loss: 0.0202  cls_loss: 0.0713  \n",
      "<<<iteration:[400/657] - total_loss: 0.3745  obj_loss: 0.1283  noobj_loss: 0.1232  bbox_loss: 0.0217  cls_loss: 0.0763  \n",
      "<<<iteration:[420/657] - total_loss: 0.3426  obj_loss: 0.1256  noobj_loss: 0.1257  bbox_loss: 0.0189  cls_loss: 0.0598  \n",
      "<<<iteration:[440/657] - total_loss: 0.3488  obj_loss: 0.1151  noobj_loss: 0.1261  bbox_loss: 0.0207  cls_loss: 0.0670  \n",
      "<<<iteration:[460/657] - total_loss: 0.3358  obj_loss: 0.1238  noobj_loss: 0.1290  bbox_loss: 0.0184  cls_loss: 0.0554  \n",
      "<<<iteration:[480/657] - total_loss: 0.3724  obj_loss: 0.1152  noobj_loss: 0.1327  bbox_loss: 0.0232  cls_loss: 0.0750  \n",
      "<<<iteration:[500/657] - total_loss: 0.3450  obj_loss: 0.1070  noobj_loss: 0.1254  bbox_loss: 0.0206  cls_loss: 0.0723  \n",
      "<<<iteration:[520/657] - total_loss: 0.3421  obj_loss: 0.1236  noobj_loss: 0.1263  bbox_loss: 0.0173  cls_loss: 0.0689  \n",
      "<<<iteration:[540/657] - total_loss: 0.3700  obj_loss: 0.1163  noobj_loss: 0.1317  bbox_loss: 0.0242  cls_loss: 0.0671  \n",
      "<<<iteration:[560/657] - total_loss: 0.3714  obj_loss: 0.1125  noobj_loss: 0.1271  bbox_loss: 0.0236  cls_loss: 0.0772  \n",
      "<<<iteration:[580/657] - total_loss: 0.3344  obj_loss: 0.1205  noobj_loss: 0.1274  bbox_loss: 0.0178  cls_loss: 0.0613  \n",
      "<<<iteration:[600/657] - total_loss: 0.3397  obj_loss: 0.1206  noobj_loss: 0.1242  bbox_loss: 0.0186  cls_loss: 0.0639  \n",
      "<<<iteration:[620/657] - total_loss: 0.3577  obj_loss: 0.1206  noobj_loss: 0.1311  bbox_loss: 0.0211  cls_loss: 0.0660  \n",
      "<<<iteration:[640/657] - total_loss: 0.3362  obj_loss: 0.1175  noobj_loss: 0.1301  bbox_loss: 0.0181  cls_loss: 0.0632  \n",
      "\n",
      "epoch:90/100 - Train Loss: 0.3506, Val Loss: 0.4378\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3907  obj_loss: 0.1046  noobj_loss: 0.1295  bbox_loss: 0.0300  cls_loss: 0.0713  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[40/657] - total_loss: 0.3429  obj_loss: 0.1180  noobj_loss: 0.1293  bbox_loss: 0.0193  cls_loss: 0.0635  \n",
      "<<<iteration:[60/657] - total_loss: 0.3493  obj_loss: 0.1208  noobj_loss: 0.1283  bbox_loss: 0.0196  cls_loss: 0.0666  \n",
      "<<<iteration:[80/657] - total_loss: 0.3326  obj_loss: 0.1262  noobj_loss: 0.1253  bbox_loss: 0.0186  cls_loss: 0.0506  \n",
      "<<<iteration:[100/657] - total_loss: 0.3368  obj_loss: 0.1164  noobj_loss: 0.1200  bbox_loss: 0.0181  cls_loss: 0.0701  \n",
      "<<<iteration:[120/657] - total_loss: 0.3389  obj_loss: 0.1027  noobj_loss: 0.1238  bbox_loss: 0.0216  cls_loss: 0.0662  \n",
      "<<<iteration:[140/657] - total_loss: 0.3546  obj_loss: 0.1215  noobj_loss: 0.1248  bbox_loss: 0.0192  cls_loss: 0.0745  \n",
      "<<<iteration:[160/657] - total_loss: 0.3439  obj_loss: 0.1157  noobj_loss: 0.1225  bbox_loss: 0.0184  cls_loss: 0.0750  \n",
      "<<<iteration:[180/657] - total_loss: 0.3250  obj_loss: 0.1274  noobj_loss: 0.1221  bbox_loss: 0.0163  cls_loss: 0.0551  \n",
      "<<<iteration:[200/657] - total_loss: 0.3536  obj_loss: 0.1275  noobj_loss: 0.1216  bbox_loss: 0.0192  cls_loss: 0.0695  \n",
      "<<<iteration:[220/657] - total_loss: 0.3318  obj_loss: 0.1183  noobj_loss: 0.1258  bbox_loss: 0.0197  cls_loss: 0.0522  \n",
      "<<<iteration:[240/657] - total_loss: 0.3507  obj_loss: 0.1166  noobj_loss: 0.1239  bbox_loss: 0.0197  cls_loss: 0.0738  \n",
      "<<<iteration:[260/657] - total_loss: 0.3310  obj_loss: 0.1116  noobj_loss: 0.1227  bbox_loss: 0.0185  cls_loss: 0.0654  \n",
      "<<<iteration:[280/657] - total_loss: 0.3412  obj_loss: 0.1134  noobj_loss: 0.1256  bbox_loss: 0.0201  cls_loss: 0.0644  \n",
      "<<<iteration:[300/657] - total_loss: 0.3588  obj_loss: 0.1307  noobj_loss: 0.1253  bbox_loss: 0.0197  cls_loss: 0.0668  \n",
      "<<<iteration:[320/657] - total_loss: 0.3484  obj_loss: 0.1286  noobj_loss: 0.1254  bbox_loss: 0.0185  cls_loss: 0.0645  \n",
      "<<<iteration:[340/657] - total_loss: 0.3318  obj_loss: 0.1147  noobj_loss: 0.1321  bbox_loss: 0.0204  cls_loss: 0.0488  \n",
      "<<<iteration:[360/657] - total_loss: 0.3649  obj_loss: 0.1209  noobj_loss: 0.1309  bbox_loss: 0.0215  cls_loss: 0.0709  \n",
      "<<<iteration:[380/657] - total_loss: 0.3598  obj_loss: 0.1222  noobj_loss: 0.1259  bbox_loss: 0.0195  cls_loss: 0.0770  \n",
      "<<<iteration:[400/657] - total_loss: 0.3352  obj_loss: 0.1165  noobj_loss: 0.1258  bbox_loss: 0.0201  cls_loss: 0.0551  \n",
      "<<<iteration:[420/657] - total_loss: 0.3705  obj_loss: 0.1052  noobj_loss: 0.1296  bbox_loss: 0.0247  cls_loss: 0.0769  \n",
      "<<<iteration:[440/657] - total_loss: 0.3478  obj_loss: 0.1312  noobj_loss: 0.1275  bbox_loss: 0.0190  cls_loss: 0.0580  \n",
      "<<<iteration:[460/657] - total_loss: 0.3648  obj_loss: 0.1113  noobj_loss: 0.1255  bbox_loss: 0.0238  cls_loss: 0.0716  \n",
      "<<<iteration:[480/657] - total_loss: 0.3354  obj_loss: 0.1153  noobj_loss: 0.1231  bbox_loss: 0.0193  cls_loss: 0.0620  \n",
      "<<<iteration:[500/657] - total_loss: 0.3481  obj_loss: 0.1140  noobj_loss: 0.1288  bbox_loss: 0.0221  cls_loss: 0.0592  \n",
      "<<<iteration:[520/657] - total_loss: 0.3662  obj_loss: 0.1164  noobj_loss: 0.1261  bbox_loss: 0.0212  cls_loss: 0.0809  \n",
      "<<<iteration:[540/657] - total_loss: 0.3388  obj_loss: 0.1199  noobj_loss: 0.1249  bbox_loss: 0.0185  cls_loss: 0.0638  \n",
      "<<<iteration:[560/657] - total_loss: 0.3487  obj_loss: 0.1185  noobj_loss: 0.1303  bbox_loss: 0.0203  cls_loss: 0.0634  \n",
      "<<<iteration:[580/657] - total_loss: 0.3387  obj_loss: 0.1133  noobj_loss: 0.1211  bbox_loss: 0.0205  cls_loss: 0.0626  \n",
      "<<<iteration:[600/657] - total_loss: 0.3492  obj_loss: 0.1107  noobj_loss: 0.1294  bbox_loss: 0.0217  cls_loss: 0.0652  \n",
      "<<<iteration:[620/657] - total_loss: 0.3613  obj_loss: 0.1153  noobj_loss: 0.1282  bbox_loss: 0.0211  cls_loss: 0.0764  \n",
      "<<<iteration:[640/657] - total_loss: 0.3778  obj_loss: 0.1052  noobj_loss: 0.1244  bbox_loss: 0.0295  cls_loss: 0.0629  \n",
      "\n",
      "epoch:91/100 - Train Loss: 0.3496, Val Loss: 0.3895\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3606  obj_loss: 0.1113  noobj_loss: 0.1337  bbox_loss: 0.0220  cls_loss: 0.0722  \n",
      "<<<iteration:[40/657] - total_loss: 0.3547  obj_loss: 0.1324  noobj_loss: 0.1278  bbox_loss: 0.0183  cls_loss: 0.0670  \n",
      "<<<iteration:[60/657] - total_loss: 0.3410  obj_loss: 0.1165  noobj_loss: 0.1275  bbox_loss: 0.0194  cls_loss: 0.0638  \n",
      "<<<iteration:[80/657] - total_loss: 0.3541  obj_loss: 0.1138  noobj_loss: 0.1222  bbox_loss: 0.0241  cls_loss: 0.0590  \n",
      "<<<iteration:[100/657] - total_loss: 0.3432  obj_loss: 0.1196  noobj_loss: 0.1204  bbox_loss: 0.0197  cls_loss: 0.0649  \n",
      "<<<iteration:[120/657] - total_loss: 0.3239  obj_loss: 0.1109  noobj_loss: 0.1230  bbox_loss: 0.0199  cls_loss: 0.0520  \n",
      "<<<iteration:[140/657] - total_loss: 0.3428  obj_loss: 0.1259  noobj_loss: 0.1261  bbox_loss: 0.0185  cls_loss: 0.0613  \n",
      "<<<iteration:[160/657] - total_loss: 0.3258  obj_loss: 0.1109  noobj_loss: 0.1287  bbox_loss: 0.0190  cls_loss: 0.0557  \n",
      "<<<iteration:[180/657] - total_loss: 0.3376  obj_loss: 0.1191  noobj_loss: 0.1246  bbox_loss: 0.0186  cls_loss: 0.0630  \n",
      "<<<iteration:[200/657] - total_loss: 0.3457  obj_loss: 0.1213  noobj_loss: 0.1328  bbox_loss: 0.0196  cls_loss: 0.0601  \n",
      "<<<iteration:[220/657] - total_loss: 0.3318  obj_loss: 0.1218  noobj_loss: 0.1272  bbox_loss: 0.0176  cls_loss: 0.0585  \n",
      "<<<iteration:[240/657] - total_loss: 0.3387  obj_loss: 0.1091  noobj_loss: 0.1232  bbox_loss: 0.0196  cls_loss: 0.0699  \n",
      "<<<iteration:[260/657] - total_loss: 0.3493  obj_loss: 0.1188  noobj_loss: 0.1302  bbox_loss: 0.0201  cls_loss: 0.0650  \n",
      "<<<iteration:[280/657] - total_loss: 0.3401  obj_loss: 0.1164  noobj_loss: 0.1341  bbox_loss: 0.0204  cls_loss: 0.0544  \n",
      "<<<iteration:[300/657] - total_loss: 0.3545  obj_loss: 0.1151  noobj_loss: 0.1233  bbox_loss: 0.0194  cls_loss: 0.0807  \n",
      "<<<iteration:[320/657] - total_loss: 0.3419  obj_loss: 0.1140  noobj_loss: 0.1281  bbox_loss: 0.0200  cls_loss: 0.0637  \n",
      "<<<iteration:[340/657] - total_loss: 0.3635  obj_loss: 0.1197  noobj_loss: 0.1335  bbox_loss: 0.0208  cls_loss: 0.0732  \n",
      "<<<iteration:[360/657] - total_loss: 0.3395  obj_loss: 0.1174  noobj_loss: 0.1244  bbox_loss: 0.0198  cls_loss: 0.0610  \n",
      "<<<iteration:[380/657] - total_loss: 0.3331  obj_loss: 0.1104  noobj_loss: 0.1230  bbox_loss: 0.0204  cls_loss: 0.0591  \n",
      "<<<iteration:[400/657] - total_loss: 0.3656  obj_loss: 0.1012  noobj_loss: 0.1262  bbox_loss: 0.0238  cls_loss: 0.0824  \n",
      "<<<iteration:[420/657] - total_loss: 0.3592  obj_loss: 0.1131  noobj_loss: 0.1251  bbox_loss: 0.0209  cls_loss: 0.0789  \n",
      "<<<iteration:[440/657] - total_loss: 0.3267  obj_loss: 0.1077  noobj_loss: 0.1226  bbox_loss: 0.0188  cls_loss: 0.0636  \n",
      "<<<iteration:[460/657] - total_loss: 0.3342  obj_loss: 0.1231  noobj_loss: 0.1259  bbox_loss: 0.0186  cls_loss: 0.0553  \n",
      "<<<iteration:[480/657] - total_loss: 0.3307  obj_loss: 0.1142  noobj_loss: 0.1221  bbox_loss: 0.0173  cls_loss: 0.0692  \n",
      "<<<iteration:[500/657] - total_loss: 0.3279  obj_loss: 0.1037  noobj_loss: 0.1228  bbox_loss: 0.0206  cls_loss: 0.0600  \n",
      "<<<iteration:[520/657] - total_loss: 0.3720  obj_loss: 0.1223  noobj_loss: 0.1260  bbox_loss: 0.0228  cls_loss: 0.0725  \n",
      "<<<iteration:[540/657] - total_loss: 0.3361  obj_loss: 0.1127  noobj_loss: 0.1218  bbox_loss: 0.0195  cls_loss: 0.0652  \n",
      "<<<iteration:[560/657] - total_loss: 0.3448  obj_loss: 0.1250  noobj_loss: 0.1276  bbox_loss: 0.0193  cls_loss: 0.0595  \n",
      "<<<iteration:[580/657] - total_loss: 0.3580  obj_loss: 0.1163  noobj_loss: 0.1259  bbox_loss: 0.0210  cls_loss: 0.0739  \n",
      "<<<iteration:[600/657] - total_loss: 0.3350  obj_loss: 0.1195  noobj_loss: 0.1231  bbox_loss: 0.0189  cls_loss: 0.0593  \n",
      "<<<iteration:[620/657] - total_loss: 0.3241  obj_loss: 0.1155  noobj_loss: 0.1229  bbox_loss: 0.0185  cls_loss: 0.0546  \n",
      "<<<iteration:[640/657] - total_loss: 0.3514  obj_loss: 0.1108  noobj_loss: 0.1243  bbox_loss: 0.0203  cls_loss: 0.0772  \n",
      "\n",
      "epoch:92/100 - Train Loss: 0.3431, Val Loss: 0.3919\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3685  obj_loss: 0.1272  noobj_loss: 0.1296  bbox_loss: 0.0203  cls_loss: 0.0748  \n",
      "<<<iteration:[40/657] - total_loss: 0.3490  obj_loss: 0.1220  noobj_loss: 0.1268  bbox_loss: 0.0187  cls_loss: 0.0702  \n",
      "<<<iteration:[60/657] - total_loss: 0.3307  obj_loss: 0.1230  noobj_loss: 0.1256  bbox_loss: 0.0184  cls_loss: 0.0529  \n",
      "<<<iteration:[80/657] - total_loss: 0.3297  obj_loss: 0.1114  noobj_loss: 0.1239  bbox_loss: 0.0195  cls_loss: 0.0591  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[100/657] - total_loss: 0.3555  obj_loss: 0.1217  noobj_loss: 0.1292  bbox_loss: 0.0215  cls_loss: 0.0619  \n",
      "<<<iteration:[120/657] - total_loss: 0.3488  obj_loss: 0.1186  noobj_loss: 0.1216  bbox_loss: 0.0184  cls_loss: 0.0774  \n",
      "<<<iteration:[140/657] - total_loss: 0.3521  obj_loss: 0.1267  noobj_loss: 0.1247  bbox_loss: 0.0200  cls_loss: 0.0628  \n",
      "<<<iteration:[160/657] - total_loss: 0.3303  obj_loss: 0.1070  noobj_loss: 0.1198  bbox_loss: 0.0193  cls_loss: 0.0669  \n",
      "<<<iteration:[180/657] - total_loss: 0.3570  obj_loss: 0.1282  noobj_loss: 0.1301  bbox_loss: 0.0193  cls_loss: 0.0674  \n",
      "<<<iteration:[200/657] - total_loss: 0.3340  obj_loss: 0.1105  noobj_loss: 0.1260  bbox_loss: 0.0199  cls_loss: 0.0609  \n",
      "<<<iteration:[220/657] - total_loss: 0.3744  obj_loss: 0.1207  noobj_loss: 0.1237  bbox_loss: 0.0244  cls_loss: 0.0698  \n",
      "<<<iteration:[240/657] - total_loss: 0.3469  obj_loss: 0.1180  noobj_loss: 0.1275  bbox_loss: 0.0193  cls_loss: 0.0688  \n",
      "<<<iteration:[260/657] - total_loss: 0.3340  obj_loss: 0.1151  noobj_loss: 0.1225  bbox_loss: 0.0196  cls_loss: 0.0599  \n",
      "<<<iteration:[280/657] - total_loss: 0.3371  obj_loss: 0.1163  noobj_loss: 0.1325  bbox_loss: 0.0194  cls_loss: 0.0575  \n",
      "<<<iteration:[300/657] - total_loss: 0.3283  obj_loss: 0.1156  noobj_loss: 0.1245  bbox_loss: 0.0175  cls_loss: 0.0630  \n",
      "<<<iteration:[320/657] - total_loss: 0.3393  obj_loss: 0.1217  noobj_loss: 0.1273  bbox_loss: 0.0194  cls_loss: 0.0571  \n",
      "<<<iteration:[340/657] - total_loss: 0.3399  obj_loss: 0.1249  noobj_loss: 0.1236  bbox_loss: 0.0185  cls_loss: 0.0607  \n",
      "<<<iteration:[360/657] - total_loss: 0.3392  obj_loss: 0.1231  noobj_loss: 0.1220  bbox_loss: 0.0184  cls_loss: 0.0633  \n",
      "<<<iteration:[380/657] - total_loss: 0.3378  obj_loss: 0.1163  noobj_loss: 0.1262  bbox_loss: 0.0195  cls_loss: 0.0611  \n",
      "<<<iteration:[400/657] - total_loss: 0.3442  obj_loss: 0.1234  noobj_loss: 0.1216  bbox_loss: 0.0208  cls_loss: 0.0559  \n",
      "<<<iteration:[420/657] - total_loss: 0.3536  obj_loss: 0.1237  noobj_loss: 0.1311  bbox_loss: 0.0199  cls_loss: 0.0648  \n",
      "<<<iteration:[440/657] - total_loss: 0.3504  obj_loss: 0.1339  noobj_loss: 0.1252  bbox_loss: 0.0181  cls_loss: 0.0636  \n",
      "<<<iteration:[460/657] - total_loss: 0.3596  obj_loss: 0.1171  noobj_loss: 0.1219  bbox_loss: 0.0225  cls_loss: 0.0688  \n",
      "<<<iteration:[480/657] - total_loss: 0.3276  obj_loss: 0.1154  noobj_loss: 0.1234  bbox_loss: 0.0185  cls_loss: 0.0580  \n",
      "<<<iteration:[500/657] - total_loss: 0.3468  obj_loss: 0.1055  noobj_loss: 0.1273  bbox_loss: 0.0228  cls_loss: 0.0636  \n",
      "<<<iteration:[520/657] - total_loss: 0.3579  obj_loss: 0.1014  noobj_loss: 0.1286  bbox_loss: 0.0258  cls_loss: 0.0633  \n",
      "<<<iteration:[540/657] - total_loss: 0.3584  obj_loss: 0.1217  noobj_loss: 0.1220  bbox_loss: 0.0202  cls_loss: 0.0748  \n",
      "<<<iteration:[560/657] - total_loss: 0.3520  obj_loss: 0.1254  noobj_loss: 0.1257  bbox_loss: 0.0187  cls_loss: 0.0704  \n",
      "<<<iteration:[580/657] - total_loss: 0.3682  obj_loss: 0.1239  noobj_loss: 0.1272  bbox_loss: 0.0195  cls_loss: 0.0833  \n",
      "<<<iteration:[600/657] - total_loss: 0.3594  obj_loss: 0.1207  noobj_loss: 0.1257  bbox_loss: 0.0210  cls_loss: 0.0709  \n",
      "<<<iteration:[620/657] - total_loss: 0.3493  obj_loss: 0.1158  noobj_loss: 0.1278  bbox_loss: 0.0217  cls_loss: 0.0610  \n",
      "<<<iteration:[640/657] - total_loss: 0.3398  obj_loss: 0.1215  noobj_loss: 0.1235  bbox_loss: 0.0192  cls_loss: 0.0607  \n",
      "\n",
      "epoch:93/100 - Train Loss: 0.3464, Val Loss: 0.3731\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3707  obj_loss: 0.1217  noobj_loss: 0.1316  bbox_loss: 0.0222  cls_loss: 0.0724  \n",
      "<<<iteration:[40/657] - total_loss: 0.3321  obj_loss: 0.1133  noobj_loss: 0.1248  bbox_loss: 0.0191  cls_loss: 0.0611  \n",
      "<<<iteration:[60/657] - total_loss: 0.3363  obj_loss: 0.1163  noobj_loss: 0.1273  bbox_loss: 0.0204  cls_loss: 0.0544  \n",
      "<<<iteration:[80/657] - total_loss: 0.3413  obj_loss: 0.1081  noobj_loss: 0.1209  bbox_loss: 0.0216  cls_loss: 0.0646  \n",
      "<<<iteration:[100/657] - total_loss: 0.3517  obj_loss: 0.1217  noobj_loss: 0.1195  bbox_loss: 0.0208  cls_loss: 0.0664  \n",
      "<<<iteration:[120/657] - total_loss: 0.3497  obj_loss: 0.1234  noobj_loss: 0.1285  bbox_loss: 0.0199  cls_loss: 0.0628  \n",
      "<<<iteration:[140/657] - total_loss: 0.3508  obj_loss: 0.1256  noobj_loss: 0.1321  bbox_loss: 0.0199  cls_loss: 0.0598  \n",
      "<<<iteration:[160/657] - total_loss: 0.3679  obj_loss: 0.1015  noobj_loss: 0.1260  bbox_loss: 0.0286  cls_loss: 0.0603  \n",
      "<<<iteration:[180/657] - total_loss: 0.3439  obj_loss: 0.1091  noobj_loss: 0.1274  bbox_loss: 0.0227  cls_loss: 0.0575  \n",
      "<<<iteration:[200/657] - total_loss: 0.3554  obj_loss: 0.1190  noobj_loss: 0.1214  bbox_loss: 0.0195  cls_loss: 0.0780  \n",
      "<<<iteration:[220/657] - total_loss: 0.3316  obj_loss: 0.1204  noobj_loss: 0.1318  bbox_loss: 0.0178  cls_loss: 0.0565  \n",
      "<<<iteration:[240/657] - total_loss: 0.3644  obj_loss: 0.1263  noobj_loss: 0.1261  bbox_loss: 0.0207  cls_loss: 0.0714  \n",
      "<<<iteration:[260/657] - total_loss: 0.3361  obj_loss: 0.1058  noobj_loss: 0.1245  bbox_loss: 0.0209  cls_loss: 0.0638  \n",
      "<<<iteration:[280/657] - total_loss: 0.3258  obj_loss: 0.1117  noobj_loss: 0.1258  bbox_loss: 0.0189  cls_loss: 0.0569  \n",
      "<<<iteration:[300/657] - total_loss: 0.3496  obj_loss: 0.1245  noobj_loss: 0.1246  bbox_loss: 0.0189  cls_loss: 0.0684  \n",
      "<<<iteration:[320/657] - total_loss: 0.3239  obj_loss: 0.1125  noobj_loss: 0.1231  bbox_loss: 0.0181  cls_loss: 0.0594  \n",
      "<<<iteration:[340/657] - total_loss: 0.3496  obj_loss: 0.1150  noobj_loss: 0.1269  bbox_loss: 0.0221  cls_loss: 0.0605  \n",
      "<<<iteration:[360/657] - total_loss: 0.3552  obj_loss: 0.1195  noobj_loss: 0.1260  bbox_loss: 0.0195  cls_loss: 0.0751  \n",
      "<<<iteration:[380/657] - total_loss: 0.3522  obj_loss: 0.1261  noobj_loss: 0.1227  bbox_loss: 0.0205  cls_loss: 0.0624  \n",
      "<<<iteration:[400/657] - total_loss: 0.3382  obj_loss: 0.1136  noobj_loss: 0.1223  bbox_loss: 0.0194  cls_loss: 0.0664  \n",
      "<<<iteration:[420/657] - total_loss: 0.3586  obj_loss: 0.1236  noobj_loss: 0.1284  bbox_loss: 0.0202  cls_loss: 0.0697  \n",
      "<<<iteration:[440/657] - total_loss: 0.3461  obj_loss: 0.1164  noobj_loss: 0.1295  bbox_loss: 0.0191  cls_loss: 0.0693  \n",
      "<<<iteration:[460/657] - total_loss: 0.3311  obj_loss: 0.1127  noobj_loss: 0.1312  bbox_loss: 0.0212  cls_loss: 0.0467  \n",
      "<<<iteration:[480/657] - total_loss: 0.3356  obj_loss: 0.1200  noobj_loss: 0.1270  bbox_loss: 0.0190  cls_loss: 0.0569  \n",
      "<<<iteration:[500/657] - total_loss: 0.3385  obj_loss: 0.1120  noobj_loss: 0.1190  bbox_loss: 0.0195  cls_loss: 0.0692  \n",
      "<<<iteration:[520/657] - total_loss: 0.3480  obj_loss: 0.1118  noobj_loss: 0.1216  bbox_loss: 0.0200  cls_loss: 0.0756  \n",
      "<<<iteration:[540/657] - total_loss: 0.3204  obj_loss: 0.1117  noobj_loss: 0.1194  bbox_loss: 0.0186  cls_loss: 0.0558  \n",
      "<<<iteration:[560/657] - total_loss: 0.3673  obj_loss: 0.1228  noobj_loss: 0.1190  bbox_loss: 0.0214  cls_loss: 0.0782  \n",
      "<<<iteration:[580/657] - total_loss: 0.3620  obj_loss: 0.1279  noobj_loss: 0.1231  bbox_loss: 0.0202  cls_loss: 0.0717  \n",
      "<<<iteration:[600/657] - total_loss: 0.3362  obj_loss: 0.1079  noobj_loss: 0.1246  bbox_loss: 0.0218  cls_loss: 0.0569  \n",
      "<<<iteration:[620/657] - total_loss: 0.3422  obj_loss: 0.1101  noobj_loss: 0.1231  bbox_loss: 0.0207  cls_loss: 0.0669  \n",
      "<<<iteration:[640/657] - total_loss: 0.3491  obj_loss: 0.1171  noobj_loss: 0.1218  bbox_loss: 0.0194  cls_loss: 0.0742  \n",
      "\n",
      "epoch:94/100 - Train Loss: 0.3454, Val Loss: 0.3776\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3519  obj_loss: 0.1190  noobj_loss: 0.1260  bbox_loss: 0.0213  cls_loss: 0.0637  \n",
      "<<<iteration:[40/657] - total_loss: 0.3679  obj_loss: 0.1216  noobj_loss: 0.1303  bbox_loss: 0.0213  cls_loss: 0.0747  \n",
      "<<<iteration:[60/657] - total_loss: 0.3586  obj_loss: 0.1284  noobj_loss: 0.1269  bbox_loss: 0.0207  cls_loss: 0.0635  \n",
      "<<<iteration:[80/657] - total_loss: 0.3409  obj_loss: 0.1232  noobj_loss: 0.1293  bbox_loss: 0.0185  cls_loss: 0.0605  \n",
      "<<<iteration:[100/657] - total_loss: 0.3371  obj_loss: 0.1120  noobj_loss: 0.1233  bbox_loss: 0.0183  cls_loss: 0.0718  \n",
      "<<<iteration:[120/657] - total_loss: 0.3468  obj_loss: 0.1189  noobj_loss: 0.1244  bbox_loss: 0.0208  cls_loss: 0.0617  \n",
      "<<<iteration:[140/657] - total_loss: 0.3413  obj_loss: 0.1253  noobj_loss: 0.1245  bbox_loss: 0.0175  cls_loss: 0.0661  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[160/657] - total_loss: 0.3390  obj_loss: 0.1118  noobj_loss: 0.1311  bbox_loss: 0.0200  cls_loss: 0.0615  \n",
      "<<<iteration:[180/657] - total_loss: 0.3329  obj_loss: 0.1186  noobj_loss: 0.1279  bbox_loss: 0.0186  cls_loss: 0.0574  \n",
      "<<<iteration:[200/657] - total_loss: 0.3244  obj_loss: 0.1157  noobj_loss: 0.1220  bbox_loss: 0.0184  cls_loss: 0.0555  \n",
      "<<<iteration:[220/657] - total_loss: 0.3396  obj_loss: 0.1174  noobj_loss: 0.1225  bbox_loss: 0.0182  cls_loss: 0.0699  \n",
      "<<<iteration:[240/657] - total_loss: 0.3305  obj_loss: 0.1173  noobj_loss: 0.1216  bbox_loss: 0.0181  cls_loss: 0.0616  \n",
      "<<<iteration:[260/657] - total_loss: 0.3297  obj_loss: 0.1071  noobj_loss: 0.1266  bbox_loss: 0.0193  cls_loss: 0.0630  \n",
      "<<<iteration:[280/657] - total_loss: 0.3436  obj_loss: 0.1357  noobj_loss: 0.1205  bbox_loss: 0.0190  cls_loss: 0.0523  \n",
      "<<<iteration:[300/657] - total_loss: 0.3552  obj_loss: 0.1165  noobj_loss: 0.1225  bbox_loss: 0.0239  cls_loss: 0.0581  \n",
      "<<<iteration:[320/657] - total_loss: 0.3609  obj_loss: 0.1200  noobj_loss: 0.1252  bbox_loss: 0.0219  cls_loss: 0.0688  \n",
      "<<<iteration:[340/657] - total_loss: 0.3619  obj_loss: 0.1157  noobj_loss: 0.1191  bbox_loss: 0.0228  cls_loss: 0.0725  \n",
      "<<<iteration:[360/657] - total_loss: 0.3395  obj_loss: 0.1273  noobj_loss: 0.1266  bbox_loss: 0.0192  cls_loss: 0.0529  \n",
      "<<<iteration:[380/657] - total_loss: 0.3658  obj_loss: 0.1202  noobj_loss: 0.1301  bbox_loss: 0.0206  cls_loss: 0.0773  \n",
      "<<<iteration:[400/657] - total_loss: 0.3300  obj_loss: 0.1162  noobj_loss: 0.1236  bbox_loss: 0.0196  cls_loss: 0.0539  \n",
      "<<<iteration:[420/657] - total_loss: 0.3374  obj_loss: 0.1198  noobj_loss: 0.1200  bbox_loss: 0.0194  cls_loss: 0.0605  \n",
      "<<<iteration:[440/657] - total_loss: 0.3481  obj_loss: 0.1125  noobj_loss: 0.1270  bbox_loss: 0.0210  cls_loss: 0.0672  \n",
      "<<<iteration:[460/657] - total_loss: 0.3513  obj_loss: 0.1203  noobj_loss: 0.1243  bbox_loss: 0.0199  cls_loss: 0.0694  \n",
      "<<<iteration:[480/657] - total_loss: 0.3340  obj_loss: 0.1149  noobj_loss: 0.1261  bbox_loss: 0.0203  cls_loss: 0.0546  \n",
      "<<<iteration:[500/657] - total_loss: 0.3291  obj_loss: 0.1127  noobj_loss: 0.1220  bbox_loss: 0.0186  cls_loss: 0.0623  \n",
      "<<<iteration:[520/657] - total_loss: 0.3380  obj_loss: 0.1195  noobj_loss: 0.1222  bbox_loss: 0.0171  cls_loss: 0.0719  \n",
      "<<<iteration:[540/657] - total_loss: 0.3737  obj_loss: 0.1148  noobj_loss: 0.1215  bbox_loss: 0.0265  cls_loss: 0.0655  \n",
      "<<<iteration:[560/657] - total_loss: 0.3440  obj_loss: 0.1043  noobj_loss: 0.1293  bbox_loss: 0.0197  cls_loss: 0.0764  \n",
      "<<<iteration:[580/657] - total_loss: 0.3554  obj_loss: 0.1125  noobj_loss: 0.1239  bbox_loss: 0.0234  cls_loss: 0.0641  \n",
      "<<<iteration:[600/657] - total_loss: 0.3392  obj_loss: 0.1087  noobj_loss: 0.1246  bbox_loss: 0.0202  cls_loss: 0.0671  \n",
      "<<<iteration:[620/657] - total_loss: 0.3231  obj_loss: 0.1204  noobj_loss: 0.1277  bbox_loss: 0.0182  cls_loss: 0.0478  \n",
      "<<<iteration:[640/657] - total_loss: 0.3494  obj_loss: 0.1166  noobj_loss: 0.1232  bbox_loss: 0.0193  cls_loss: 0.0749  \n",
      "\n",
      "epoch:95/100 - Train Loss: 0.3441, Val Loss: 0.3843\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3845  obj_loss: 0.1307  noobj_loss: 0.1352  bbox_loss: 0.0214  cls_loss: 0.0791  \n",
      "<<<iteration:[40/657] - total_loss: 0.3435  obj_loss: 0.1156  noobj_loss: 0.1252  bbox_loss: 0.0205  cls_loss: 0.0630  \n",
      "<<<iteration:[60/657] - total_loss: 0.3334  obj_loss: 0.1225  noobj_loss: 0.1222  bbox_loss: 0.0191  cls_loss: 0.0544  \n",
      "<<<iteration:[80/657] - total_loss: 0.3424  obj_loss: 0.1208  noobj_loss: 0.1233  bbox_loss: 0.0188  cls_loss: 0.0661  \n",
      "<<<iteration:[100/657] - total_loss: 0.3542  obj_loss: 0.1134  noobj_loss: 0.1212  bbox_loss: 0.0199  cls_loss: 0.0806  \n",
      "<<<iteration:[120/657] - total_loss: 0.3446  obj_loss: 0.1189  noobj_loss: 0.1210  bbox_loss: 0.0191  cls_loss: 0.0698  \n",
      "<<<iteration:[140/657] - total_loss: 0.3345  obj_loss: 0.1138  noobj_loss: 0.1232  bbox_loss: 0.0198  cls_loss: 0.0599  \n",
      "<<<iteration:[160/657] - total_loss: 0.3639  obj_loss: 0.1182  noobj_loss: 0.1307  bbox_loss: 0.0190  cls_loss: 0.0854  \n",
      "<<<iteration:[180/657] - total_loss: 0.3296  obj_loss: 0.1190  noobj_loss: 0.1251  bbox_loss: 0.0175  cls_loss: 0.0603  \n",
      "<<<iteration:[200/657] - total_loss: 0.3257  obj_loss: 0.1199  noobj_loss: 0.1248  bbox_loss: 0.0185  cls_loss: 0.0507  \n",
      "<<<iteration:[220/657] - total_loss: 0.3489  obj_loss: 0.1228  noobj_loss: 0.1236  bbox_loss: 0.0196  cls_loss: 0.0662  \n",
      "<<<iteration:[240/657] - total_loss: 0.3570  obj_loss: 0.1086  noobj_loss: 0.1176  bbox_loss: 0.0235  cls_loss: 0.0723  \n",
      "<<<iteration:[260/657] - total_loss: 0.3574  obj_loss: 0.1280  noobj_loss: 0.1235  bbox_loss: 0.0192  cls_loss: 0.0718  \n",
      "<<<iteration:[280/657] - total_loss: 0.3524  obj_loss: 0.1230  noobj_loss: 0.1252  bbox_loss: 0.0189  cls_loss: 0.0722  \n",
      "<<<iteration:[300/657] - total_loss: 0.3509  obj_loss: 0.1236  noobj_loss: 0.1287  bbox_loss: 0.0201  cls_loss: 0.0624  \n",
      "<<<iteration:[320/657] - total_loss: 0.3359  obj_loss: 0.1267  noobj_loss: 0.1228  bbox_loss: 0.0174  cls_loss: 0.0608  \n",
      "<<<iteration:[340/657] - total_loss: 0.3466  obj_loss: 0.1225  noobj_loss: 0.1223  bbox_loss: 0.0199  cls_loss: 0.0636  \n",
      "<<<iteration:[360/657] - total_loss: 0.3440  obj_loss: 0.1073  noobj_loss: 0.1256  bbox_loss: 0.0210  cls_loss: 0.0690  \n",
      "<<<iteration:[380/657] - total_loss: 0.3487  obj_loss: 0.1192  noobj_loss: 0.1207  bbox_loss: 0.0206  cls_loss: 0.0663  \n",
      "<<<iteration:[400/657] - total_loss: 0.3130  obj_loss: 0.1160  noobj_loss: 0.1226  bbox_loss: 0.0172  cls_loss: 0.0499  \n",
      "<<<iteration:[420/657] - total_loss: 0.3256  obj_loss: 0.1170  noobj_loss: 0.1253  bbox_loss: 0.0181  cls_loss: 0.0554  \n",
      "<<<iteration:[440/657] - total_loss: 0.3342  obj_loss: 0.1200  noobj_loss: 0.1217  bbox_loss: 0.0187  cls_loss: 0.0596  \n",
      "<<<iteration:[460/657] - total_loss: 0.3363  obj_loss: 0.1209  noobj_loss: 0.1274  bbox_loss: 0.0197  cls_loss: 0.0531  \n",
      "<<<iteration:[480/657] - total_loss: 0.3414  obj_loss: 0.1165  noobj_loss: 0.1180  bbox_loss: 0.0182  cls_loss: 0.0750  \n",
      "<<<iteration:[500/657] - total_loss: 0.3492  obj_loss: 0.1216  noobj_loss: 0.1283  bbox_loss: 0.0201  cls_loss: 0.0632  \n",
      "<<<iteration:[520/657] - total_loss: 0.3399  obj_loss: 0.1213  noobj_loss: 0.1258  bbox_loss: 0.0195  cls_loss: 0.0584  \n",
      "<<<iteration:[540/657] - total_loss: 0.3420  obj_loss: 0.1110  noobj_loss: 0.1240  bbox_loss: 0.0208  cls_loss: 0.0648  \n",
      "<<<iteration:[560/657] - total_loss: 0.3338  obj_loss: 0.1163  noobj_loss: 0.1210  bbox_loss: 0.0189  cls_loss: 0.0625  \n",
      "<<<iteration:[580/657] - total_loss: 0.3760  obj_loss: 0.1110  noobj_loss: 0.1278  bbox_loss: 0.0272  cls_loss: 0.0651  \n",
      "<<<iteration:[600/657] - total_loss: 0.3546  obj_loss: 0.1163  noobj_loss: 0.1278  bbox_loss: 0.0226  cls_loss: 0.0613  \n",
      "<<<iteration:[620/657] - total_loss: 0.3516  obj_loss: 0.1184  noobj_loss: 0.1238  bbox_loss: 0.0207  cls_loss: 0.0679  \n",
      "<<<iteration:[640/657] - total_loss: 0.3417  obj_loss: 0.1230  noobj_loss: 0.1254  bbox_loss: 0.0186  cls_loss: 0.0630  \n",
      "\n",
      "epoch:96/100 - Train Loss: 0.3444, Val Loss: 0.3865\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3600  obj_loss: 0.1213  noobj_loss: 0.1237  bbox_loss: 0.0226  cls_loss: 0.0639  \n",
      "<<<iteration:[40/657] - total_loss: 0.3314  obj_loss: 0.1157  noobj_loss: 0.1245  bbox_loss: 0.0185  cls_loss: 0.0607  \n",
      "<<<iteration:[60/657] - total_loss: 0.3625  obj_loss: 0.1107  noobj_loss: 0.1236  bbox_loss: 0.0256  cls_loss: 0.0622  \n",
      "<<<iteration:[80/657] - total_loss: 0.3284  obj_loss: 0.1163  noobj_loss: 0.1275  bbox_loss: 0.0195  cls_loss: 0.0510  \n",
      "<<<iteration:[100/657] - total_loss: 0.3389  obj_loss: 0.1115  noobj_loss: 0.1223  bbox_loss: 0.0206  cls_loss: 0.0633  \n",
      "<<<iteration:[120/657] - total_loss: 0.3370  obj_loss: 0.1232  noobj_loss: 0.1189  bbox_loss: 0.0195  cls_loss: 0.0568  \n",
      "<<<iteration:[140/657] - total_loss: 0.3592  obj_loss: 0.1257  noobj_loss: 0.1258  bbox_loss: 0.0189  cls_loss: 0.0760  \n",
      "<<<iteration:[160/657] - total_loss: 0.3586  obj_loss: 0.1131  noobj_loss: 0.1298  bbox_loss: 0.0212  cls_loss: 0.0745  \n",
      "<<<iteration:[180/657] - total_loss: 0.3263  obj_loss: 0.1133  noobj_loss: 0.1267  bbox_loss: 0.0178  cls_loss: 0.0605  \n",
      "<<<iteration:[200/657] - total_loss: 0.3335  obj_loss: 0.1234  noobj_loss: 0.1223  bbox_loss: 0.0190  cls_loss: 0.0537  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[220/657] - total_loss: 0.3466  obj_loss: 0.1227  noobj_loss: 0.1244  bbox_loss: 0.0196  cls_loss: 0.0637  \n",
      "<<<iteration:[240/657] - total_loss: 0.3298  obj_loss: 0.1178  noobj_loss: 0.1216  bbox_loss: 0.0174  cls_loss: 0.0641  \n",
      "<<<iteration:[260/657] - total_loss: 0.3361  obj_loss: 0.1242  noobj_loss: 0.1248  bbox_loss: 0.0187  cls_loss: 0.0561  \n",
      "<<<iteration:[280/657] - total_loss: 0.3274  obj_loss: 0.1135  noobj_loss: 0.1269  bbox_loss: 0.0188  cls_loss: 0.0563  \n",
      "<<<iteration:[300/657] - total_loss: 0.3381  obj_loss: 0.1217  noobj_loss: 0.1235  bbox_loss: 0.0187  cls_loss: 0.0608  \n",
      "<<<iteration:[320/657] - total_loss: 0.3585  obj_loss: 0.1111  noobj_loss: 0.1213  bbox_loss: 0.0222  cls_loss: 0.0756  \n",
      "<<<iteration:[340/657] - total_loss: 0.3134  obj_loss: 0.1185  noobj_loss: 0.1239  bbox_loss: 0.0167  cls_loss: 0.0492  \n",
      "<<<iteration:[360/657] - total_loss: 0.3312  obj_loss: 0.1068  noobj_loss: 0.1254  bbox_loss: 0.0195  cls_loss: 0.0643  \n",
      "<<<iteration:[380/657] - total_loss: 0.3563  obj_loss: 0.1206  noobj_loss: 0.1254  bbox_loss: 0.0217  cls_loss: 0.0644  \n",
      "<<<iteration:[400/657] - total_loss: 0.3376  obj_loss: 0.1232  noobj_loss: 0.1286  bbox_loss: 0.0186  cls_loss: 0.0572  \n",
      "<<<iteration:[420/657] - total_loss: 0.3293  obj_loss: 0.1180  noobj_loss: 0.1208  bbox_loss: 0.0193  cls_loss: 0.0541  \n",
      "<<<iteration:[440/657] - total_loss: 0.3430  obj_loss: 0.1250  noobj_loss: 0.1216  bbox_loss: 0.0185  cls_loss: 0.0644  \n",
      "<<<iteration:[460/657] - total_loss: 0.3519  obj_loss: 0.1191  noobj_loss: 0.1186  bbox_loss: 0.0201  cls_loss: 0.0729  \n",
      "<<<iteration:[480/657] - total_loss: 0.3383  obj_loss: 0.1175  noobj_loss: 0.1199  bbox_loss: 0.0193  cls_loss: 0.0644  \n",
      "<<<iteration:[500/657] - total_loss: 0.3396  obj_loss: 0.1250  noobj_loss: 0.1251  bbox_loss: 0.0183  cls_loss: 0.0605  \n",
      "<<<iteration:[520/657] - total_loss: 0.3309  obj_loss: 0.1228  noobj_loss: 0.1259  bbox_loss: 0.0186  cls_loss: 0.0522  \n",
      "<<<iteration:[540/657] - total_loss: 0.3426  obj_loss: 0.1178  noobj_loss: 0.1252  bbox_loss: 0.0195  cls_loss: 0.0649  \n",
      "<<<iteration:[560/657] - total_loss: 0.3465  obj_loss: 0.1262  noobj_loss: 0.1195  bbox_loss: 0.0193  cls_loss: 0.0641  \n",
      "<<<iteration:[580/657] - total_loss: 0.3094  obj_loss: 0.1117  noobj_loss: 0.1226  bbox_loss: 0.0172  cls_loss: 0.0506  \n",
      "<<<iteration:[600/657] - total_loss: 0.3479  obj_loss: 0.1187  noobj_loss: 0.1238  bbox_loss: 0.0201  cls_loss: 0.0667  \n",
      "<<<iteration:[620/657] - total_loss: 0.3462  obj_loss: 0.1144  noobj_loss: 0.1185  bbox_loss: 0.0185  cls_loss: 0.0800  \n",
      "<<<iteration:[640/657] - total_loss: 0.3485  obj_loss: 0.1284  noobj_loss: 0.1256  bbox_loss: 0.0193  cls_loss: 0.0610  \n",
      "\n",
      "epoch:97/100 - Train Loss: 0.3399, Val Loss: 0.3774\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3524  obj_loss: 0.1259  noobj_loss: 0.1308  bbox_loss: 0.0184  cls_loss: 0.0693  \n",
      "<<<iteration:[40/657] - total_loss: 0.3442  obj_loss: 0.1302  noobj_loss: 0.1227  bbox_loss: 0.0193  cls_loss: 0.0561  \n",
      "<<<iteration:[60/657] - total_loss: 0.3238  obj_loss: 0.1040  noobj_loss: 0.1226  bbox_loss: 0.0193  cls_loss: 0.0619  \n",
      "<<<iteration:[80/657] - total_loss: 0.3277  obj_loss: 0.1066  noobj_loss: 0.1228  bbox_loss: 0.0198  cls_loss: 0.0607  \n",
      "<<<iteration:[100/657] - total_loss: 0.3457  obj_loss: 0.1270  noobj_loss: 0.1277  bbox_loss: 0.0180  cls_loss: 0.0646  \n",
      "<<<iteration:[120/657] - total_loss: 0.3429  obj_loss: 0.1150  noobj_loss: 0.1280  bbox_loss: 0.0213  cls_loss: 0.0575  \n",
      "<<<iteration:[140/657] - total_loss: 0.3408  obj_loss: 0.1228  noobj_loss: 0.1182  bbox_loss: 0.0196  cls_loss: 0.0608  \n",
      "<<<iteration:[160/657] - total_loss: 0.3437  obj_loss: 0.1213  noobj_loss: 0.1275  bbox_loss: 0.0188  cls_loss: 0.0648  \n",
      "<<<iteration:[180/657] - total_loss: 0.3243  obj_loss: 0.1145  noobj_loss: 0.1220  bbox_loss: 0.0164  cls_loss: 0.0669  \n",
      "<<<iteration:[200/657] - total_loss: 0.3392  obj_loss: 0.1211  noobj_loss: 0.1264  bbox_loss: 0.0180  cls_loss: 0.0647  \n",
      "<<<iteration:[220/657] - total_loss: 0.3404  obj_loss: 0.1126  noobj_loss: 0.1237  bbox_loss: 0.0189  cls_loss: 0.0714  \n",
      "<<<iteration:[240/657] - total_loss: 0.3398  obj_loss: 0.1107  noobj_loss: 0.1220  bbox_loss: 0.0211  cls_loss: 0.0627  \n",
      "<<<iteration:[260/657] - total_loss: 0.3434  obj_loss: 0.1252  noobj_loss: 0.1218  bbox_loss: 0.0185  cls_loss: 0.0648  \n",
      "<<<iteration:[280/657] - total_loss: 0.3409  obj_loss: 0.1257  noobj_loss: 0.1261  bbox_loss: 0.0182  cls_loss: 0.0611  \n",
      "<<<iteration:[300/657] - total_loss: 0.3347  obj_loss: 0.1192  noobj_loss: 0.1247  bbox_loss: 0.0199  cls_loss: 0.0537  \n",
      "<<<iteration:[320/657] - total_loss: 0.3285  obj_loss: 0.1158  noobj_loss: 0.1210  bbox_loss: 0.0185  cls_loss: 0.0596  \n",
      "<<<iteration:[340/657] - total_loss: 0.3402  obj_loss: 0.1184  noobj_loss: 0.1235  bbox_loss: 0.0178  cls_loss: 0.0708  \n",
      "<<<iteration:[360/657] - total_loss: 0.3289  obj_loss: 0.1102  noobj_loss: 0.1262  bbox_loss: 0.0203  cls_loss: 0.0542  \n",
      "<<<iteration:[380/657] - total_loss: 0.3113  obj_loss: 0.1088  noobj_loss: 0.1209  bbox_loss: 0.0186  cls_loss: 0.0489  \n",
      "<<<iteration:[400/657] - total_loss: 0.3296  obj_loss: 0.1066  noobj_loss: 0.1182  bbox_loss: 0.0208  cls_loss: 0.0600  \n",
      "<<<iteration:[420/657] - total_loss: 0.3408  obj_loss: 0.1278  noobj_loss: 0.1248  bbox_loss: 0.0182  cls_loss: 0.0597  \n",
      "<<<iteration:[440/657] - total_loss: 0.3257  obj_loss: 0.1221  noobj_loss: 0.1243  bbox_loss: 0.0169  cls_loss: 0.0568  \n",
      "<<<iteration:[460/657] - total_loss: 0.3441  obj_loss: 0.1211  noobj_loss: 0.1216  bbox_loss: 0.0191  cls_loss: 0.0665  \n",
      "<<<iteration:[480/657] - total_loss: 0.3412  obj_loss: 0.1151  noobj_loss: 0.1228  bbox_loss: 0.0205  cls_loss: 0.0622  \n",
      "<<<iteration:[500/657] - total_loss: 0.3308  obj_loss: 0.1071  noobj_loss: 0.1304  bbox_loss: 0.0206  cls_loss: 0.0555  \n",
      "<<<iteration:[520/657] - total_loss: 0.3294  obj_loss: 0.1141  noobj_loss: 0.1231  bbox_loss: 0.0185  cls_loss: 0.0611  \n",
      "<<<iteration:[540/657] - total_loss: 0.3338  obj_loss: 0.1227  noobj_loss: 0.1208  bbox_loss: 0.0182  cls_loss: 0.0599  \n",
      "<<<iteration:[560/657] - total_loss: 0.3637  obj_loss: 0.1338  noobj_loss: 0.1311  bbox_loss: 0.0189  cls_loss: 0.0697  \n",
      "<<<iteration:[580/657] - total_loss: 0.3575  obj_loss: 0.1191  noobj_loss: 0.1217  bbox_loss: 0.0204  cls_loss: 0.0756  \n",
      "<<<iteration:[600/657] - total_loss: 0.3437  obj_loss: 0.1116  noobj_loss: 0.1224  bbox_loss: 0.0204  cls_loss: 0.0691  \n",
      "<<<iteration:[620/657] - total_loss: 0.3309  obj_loss: 0.1120  noobj_loss: 0.1214  bbox_loss: 0.0185  cls_loss: 0.0656  \n",
      "<<<iteration:[640/657] - total_loss: 0.3255  obj_loss: 0.1169  noobj_loss: 0.1215  bbox_loss: 0.0181  cls_loss: 0.0572  \n",
      "\n",
      "epoch:98/100 - Train Loss: 0.3372, Val Loss: 0.3752\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3432  obj_loss: 0.1222  noobj_loss: 0.1326  bbox_loss: 0.0192  cls_loss: 0.0587  \n",
      "<<<iteration:[40/657] - total_loss: 0.3452  obj_loss: 0.1248  noobj_loss: 0.1201  bbox_loss: 0.0178  cls_loss: 0.0713  \n",
      "<<<iteration:[60/657] - total_loss: 0.3226  obj_loss: 0.1271  noobj_loss: 0.1186  bbox_loss: 0.0165  cls_loss: 0.0535  \n",
      "<<<iteration:[80/657] - total_loss: 0.3397  obj_loss: 0.1136  noobj_loss: 0.1284  bbox_loss: 0.0202  cls_loss: 0.0611  \n",
      "<<<iteration:[100/657] - total_loss: 0.3370  obj_loss: 0.1247  noobj_loss: 0.1281  bbox_loss: 0.0186  cls_loss: 0.0553  \n",
      "<<<iteration:[120/657] - total_loss: 0.3233  obj_loss: 0.1118  noobj_loss: 0.1171  bbox_loss: 0.0187  cls_loss: 0.0593  \n",
      "<<<iteration:[140/657] - total_loss: 0.3422  obj_loss: 0.1174  noobj_loss: 0.1201  bbox_loss: 0.0203  cls_loss: 0.0632  \n",
      "<<<iteration:[160/657] - total_loss: 0.3405  obj_loss: 0.1186  noobj_loss: 0.1237  bbox_loss: 0.0188  cls_loss: 0.0661  \n",
      "<<<iteration:[180/657] - total_loss: 0.3266  obj_loss: 0.1209  noobj_loss: 0.1269  bbox_loss: 0.0177  cls_loss: 0.0538  \n",
      "<<<iteration:[200/657] - total_loss: 0.3434  obj_loss: 0.1166  noobj_loss: 0.1201  bbox_loss: 0.0200  cls_loss: 0.0668  \n",
      "<<<iteration:[220/657] - total_loss: 0.3560  obj_loss: 0.1186  noobj_loss: 0.1248  bbox_loss: 0.0210  cls_loss: 0.0698  \n",
      "<<<iteration:[240/657] - total_loss: 0.3224  obj_loss: 0.1172  noobj_loss: 0.1151  bbox_loss: 0.0185  cls_loss: 0.0552  \n",
      "<<<iteration:[260/657] - total_loss: 0.3322  obj_loss: 0.1250  noobj_loss: 0.1244  bbox_loss: 0.0173  cls_loss: 0.0584  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[280/657] - total_loss: 0.3318  obj_loss: 0.1097  noobj_loss: 0.1164  bbox_loss: 0.0200  cls_loss: 0.0641  \n",
      "<<<iteration:[300/657] - total_loss: 0.3366  obj_loss: 0.1276  noobj_loss: 0.1262  bbox_loss: 0.0191  cls_loss: 0.0506  \n",
      "<<<iteration:[320/657] - total_loss: 0.3459  obj_loss: 0.1207  noobj_loss: 0.1224  bbox_loss: 0.0198  cls_loss: 0.0651  \n",
      "<<<iteration:[340/657] - total_loss: 0.3276  obj_loss: 0.1140  noobj_loss: 0.1228  bbox_loss: 0.0191  cls_loss: 0.0569  \n",
      "<<<iteration:[360/657] - total_loss: 0.3323  obj_loss: 0.1298  noobj_loss: 0.1205  bbox_loss: 0.0174  cls_loss: 0.0555  \n",
      "<<<iteration:[380/657] - total_loss: 0.3302  obj_loss: 0.1096  noobj_loss: 0.1255  bbox_loss: 0.0198  cls_loss: 0.0590  \n",
      "<<<iteration:[400/657] - total_loss: 0.3345  obj_loss: 0.1144  noobj_loss: 0.1187  bbox_loss: 0.0213  cls_loss: 0.0544  \n",
      "<<<iteration:[420/657] - total_loss: 0.3473  obj_loss: 0.1200  noobj_loss: 0.1255  bbox_loss: 0.0190  cls_loss: 0.0694  \n",
      "<<<iteration:[440/657] - total_loss: 0.3513  obj_loss: 0.1239  noobj_loss: 0.1287  bbox_loss: 0.0194  cls_loss: 0.0659  \n",
      "<<<iteration:[460/657] - total_loss: 0.3272  obj_loss: 0.1159  noobj_loss: 0.1218  bbox_loss: 0.0189  cls_loss: 0.0557  \n",
      "<<<iteration:[480/657] - total_loss: 0.3453  obj_loss: 0.1145  noobj_loss: 0.1312  bbox_loss: 0.0200  cls_loss: 0.0652  \n",
      "<<<iteration:[500/657] - total_loss: 0.3413  obj_loss: 0.1271  noobj_loss: 0.1219  bbox_loss: 0.0169  cls_loss: 0.0686  \n",
      "<<<iteration:[520/657] - total_loss: 0.3818  obj_loss: 0.1152  noobj_loss: 0.1232  bbox_loss: 0.0254  cls_loss: 0.0780  \n",
      "<<<iteration:[540/657] - total_loss: 0.3314  obj_loss: 0.1094  noobj_loss: 0.1316  bbox_loss: 0.0213  cls_loss: 0.0497  \n",
      "<<<iteration:[560/657] - total_loss: 0.3354  obj_loss: 0.1118  noobj_loss: 0.1198  bbox_loss: 0.0194  cls_loss: 0.0666  \n",
      "<<<iteration:[580/657] - total_loss: 0.3302  obj_loss: 0.1262  noobj_loss: 0.1237  bbox_loss: 0.0178  cls_loss: 0.0534  \n",
      "<<<iteration:[600/657] - total_loss: 0.3411  obj_loss: 0.1154  noobj_loss: 0.1257  bbox_loss: 0.0185  cls_loss: 0.0701  \n",
      "<<<iteration:[620/657] - total_loss: 0.3506  obj_loss: 0.1182  noobj_loss: 0.1197  bbox_loss: 0.0191  cls_loss: 0.0773  \n",
      "<<<iteration:[640/657] - total_loss: 0.3291  obj_loss: 0.1123  noobj_loss: 0.1197  bbox_loss: 0.0184  cls_loss: 0.0649  \n",
      "\n",
      "epoch:99/100 - Train Loss: 0.3380, Val Loss: 0.3797\n",
      "\n",
      "<<<iteration:[20/657] - total_loss: 0.3522  obj_loss: 0.1299  noobj_loss: 0.1300  bbox_loss: 0.0201  cls_loss: 0.0570  \n",
      "<<<iteration:[40/657] - total_loss: 0.3401  obj_loss: 0.1158  noobj_loss: 0.1258  bbox_loss: 0.0205  cls_loss: 0.0590  \n",
      "<<<iteration:[60/657] - total_loss: 0.3355  obj_loss: 0.1159  noobj_loss: 0.1180  bbox_loss: 0.0184  cls_loss: 0.0689  \n",
      "<<<iteration:[80/657] - total_loss: 0.3254  obj_loss: 0.1103  noobj_loss: 0.1254  bbox_loss: 0.0178  cls_loss: 0.0635  \n",
      "<<<iteration:[100/657] - total_loss: 0.3269  obj_loss: 0.1211  noobj_loss: 0.1265  bbox_loss: 0.0169  cls_loss: 0.0579  \n",
      "<<<iteration:[120/657] - total_loss: 0.3288  obj_loss: 0.1203  noobj_loss: 0.1240  bbox_loss: 0.0191  cls_loss: 0.0511  \n",
      "<<<iteration:[140/657] - total_loss: 0.3505  obj_loss: 0.1258  noobj_loss: 0.1289  bbox_loss: 0.0193  cls_loss: 0.0641  \n",
      "<<<iteration:[160/657] - total_loss: 0.3265  obj_loss: 0.1233  noobj_loss: 0.1234  bbox_loss: 0.0180  cls_loss: 0.0516  \n",
      "<<<iteration:[180/657] - total_loss: 0.3103  obj_loss: 0.1147  noobj_loss: 0.1218  bbox_loss: 0.0179  cls_loss: 0.0453  \n",
      "<<<iteration:[200/657] - total_loss: 0.3196  obj_loss: 0.1057  noobj_loss: 0.1209  bbox_loss: 0.0192  cls_loss: 0.0574  \n",
      "<<<iteration:[220/657] - total_loss: 0.3413  obj_loss: 0.1214  noobj_loss: 0.1223  bbox_loss: 0.0195  cls_loss: 0.0614  \n",
      "<<<iteration:[240/657] - total_loss: 0.3568  obj_loss: 0.1308  noobj_loss: 0.1284  bbox_loss: 0.0192  cls_loss: 0.0656  \n",
      "<<<iteration:[260/657] - total_loss: 0.3358  obj_loss: 0.1193  noobj_loss: 0.1221  bbox_loss: 0.0194  cls_loss: 0.0585  \n",
      "<<<iteration:[280/657] - total_loss: 0.3567  obj_loss: 0.1287  noobj_loss: 0.1245  bbox_loss: 0.0181  cls_loss: 0.0751  \n",
      "<<<iteration:[300/657] - total_loss: 0.3502  obj_loss: 0.1243  noobj_loss: 0.1234  bbox_loss: 0.0182  cls_loss: 0.0731  \n",
      "<<<iteration:[320/657] - total_loss: 0.3445  obj_loss: 0.1128  noobj_loss: 0.1211  bbox_loss: 0.0218  cls_loss: 0.0622  \n",
      "<<<iteration:[340/657] - total_loss: 0.3374  obj_loss: 0.1176  noobj_loss: 0.1255  bbox_loss: 0.0174  cls_loss: 0.0702  \n",
      "<<<iteration:[360/657] - total_loss: 0.3156  obj_loss: 0.1156  noobj_loss: 0.1241  bbox_loss: 0.0163  cls_loss: 0.0564  \n",
      "<<<iteration:[380/657] - total_loss: 0.3480  obj_loss: 0.1154  noobj_loss: 0.1209  bbox_loss: 0.0211  cls_loss: 0.0667  \n",
      "<<<iteration:[400/657] - total_loss: 0.3232  obj_loss: 0.1114  noobj_loss: 0.1151  bbox_loss: 0.0174  cls_loss: 0.0674  \n",
      "<<<iteration:[420/657] - total_loss: 0.3281  obj_loss: 0.1239  noobj_loss: 0.1239  bbox_loss: 0.0182  cls_loss: 0.0514  \n",
      "<<<iteration:[440/657] - total_loss: 0.3401  obj_loss: 0.1134  noobj_loss: 0.1263  bbox_loss: 0.0220  cls_loss: 0.0533  \n",
      "<<<iteration:[460/657] - total_loss: 0.3318  obj_loss: 0.1207  noobj_loss: 0.1170  bbox_loss: 0.0171  cls_loss: 0.0674  \n",
      "<<<iteration:[480/657] - total_loss: 0.3409  obj_loss: 0.1201  noobj_loss: 0.1168  bbox_loss: 0.0191  cls_loss: 0.0669  \n",
      "<<<iteration:[500/657] - total_loss: 0.3421  obj_loss: 0.1139  noobj_loss: 0.1253  bbox_loss: 0.0202  cls_loss: 0.0646  \n",
      "<<<iteration:[520/657] - total_loss: 0.3337  obj_loss: 0.1168  noobj_loss: 0.1185  bbox_loss: 0.0189  cls_loss: 0.0634  \n",
      "<<<iteration:[540/657] - total_loss: 0.3301  obj_loss: 0.1073  noobj_loss: 0.1269  bbox_loss: 0.0186  cls_loss: 0.0665  \n",
      "<<<iteration:[560/657] - total_loss: 0.3305  obj_loss: 0.1213  noobj_loss: 0.1210  bbox_loss: 0.0178  cls_loss: 0.0595  \n",
      "<<<iteration:[580/657] - total_loss: 0.3474  obj_loss: 0.1133  noobj_loss: 0.1219  bbox_loss: 0.0211  cls_loss: 0.0677  \n",
      "<<<iteration:[600/657] - total_loss: 0.3793  obj_loss: 0.1121  noobj_loss: 0.1224  bbox_loss: 0.0264  cls_loss: 0.0738  \n",
      "<<<iteration:[620/657] - total_loss: 0.3354  obj_loss: 0.0983  noobj_loss: 0.1174  bbox_loss: 0.0245  cls_loss: 0.0558  \n",
      "<<<iteration:[640/657] - total_loss: 0.3572  obj_loss: 0.1333  noobj_loss: 0.1224  bbox_loss: 0.0197  cls_loss: 0.0640  \n",
      "\n",
      "epoch:100/100 - Train Loss: 0.3376, Val Loss: 0.3866\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf75ef033ca4fb285b22a106d0fb002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train bbox Loss</td><td>█▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train class Loss</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train obj Loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▄▂▂▅▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val bbox Loss</td><td>█▄▂▂█▅▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val class Loss</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val obj Loss</td><td>▂▁▂▃▂▄▃▃▄▅▃▄▆▇▆▅▆▆▅▇▆▆▆▇▇▇▆▆█▆▇█▇▇▇██▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>0.3376</td></tr><tr><td>Train bbox Loss</td><td>0.01932</td></tr><tr><td>Train class Loss</td><td>0.06206</td></tr><tr><td>Train obj Loss</td><td>0.1175</td></tr><tr><td>Val Loss</td><td>0.38656</td></tr><tr><td>Val bbox Loss</td><td>0.02824</td></tr><tr><td>Val class Loss</td><td>0.06158</td></tr><tr><td>Val obj Loss</td><td>0.12768</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-leaf-2</strong> at: <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/a7s8x4ne' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/runs/a7s8x4ne</a><br/> View job at <a href='https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDkxNzUzMA==/version_details/v0' target=\"_blank\">https://wandb.ai/urp/yolo_cbam_neck_IMAGE_PATCH/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExMDkxNzUzMA==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231029_164736-a7s8x4ne/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_epoch = 0\n",
    "best_score = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders, model, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "#     train_loss[\"obj_loss\"] += obj_loss\n",
    "#     train_loss[\"noobj_loss\"] += noobj_loss\n",
    "#     train_loss[\"bbox_loss\"] += bbox_loss\n",
    "#     train_loss[\"cls_loss\"] += cls_loss\n",
    "    wandb.log({\"Train Loss\": train_loss['total_loss'],\n",
    "               \"Train obj Loss\":train_loss[\"obj_loss\"],\n",
    "               \"Train bbox Loss\":train_loss[\"bbox_loss\"],\n",
    "               \"Train class Loss\":train_loss[\"cls_loss\"],\n",
    "               \"Val Loss\": val_loss['total_loss'],\n",
    "               \"Val obj Loss\":val_loss[\"obj_loss\"],\n",
    "               \"Val bbox Loss\":val_loss[\"bbox_loss\"],\n",
    "               \"Val class Loss\":val_loss[\"cls_loss\"],})\n",
    "    print(f\"\\nepoch:{epoch+1}/{num_epochs} - Train Loss: {train_loss['total_loss']:.4f}, Val Loss: {val_loss['total_loss']:.4f}\\n\")\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model(model.state_dict(), f'model_{epoch+1}.pth', save_dir=f\"./trained_model/{BACKBONE}_{PART}_LR{LR}_IP{PATCH_FACTOR}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637805f",
   "metadata": {},
   "source": [
    "## Test Dataset Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d867479",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, num_classes, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model = YOLO_SWIN(num_classes=num_classes)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=448\n",
    "transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path=\"./trained_model/YOLO_SWIN_T_body_LR0.0001_AUG30/model_90.pth\"\n",
    "ckpt_path=\"/workspace/Plastic_Bottle_defect_detection/trained_model/YOLO_SWIN_T_neck_LR0.0001_Image_Patch50/model_100.pth\"\n",
    "model = load_model(ckpt_path, NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "test_dataset=PET_dataset(\"neck\" ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='test', transformer=transformer, aug=None)\n",
    "test_dataloaders = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e8599",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf189c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_predict(image, model, conf_thres=0.2, iou_threshold=0.1):\n",
    "    predictions = model(image)\n",
    "    prediction = predictions.detach().cpu().squeeze(dim=0)\n",
    "    f_map=prediction\n",
    "\n",
    "#     print(prediction.shape)\n",
    "    \n",
    "    grid_size = prediction.shape[-1]\n",
    "    y_grid, x_grid = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size))\n",
    "    stride_size = IMAGE_SIZE/grid_size\n",
    "\n",
    "    conf = prediction[[0,5], ...].reshape(1, -1)\n",
    "    xc = (prediction[[1,6], ...] * IMAGE_SIZE + x_grid*stride_size).reshape(1,-1)\n",
    "    yc = (prediction[[2,7], ...] * IMAGE_SIZE + y_grid*stride_size).reshape(1,-1)\n",
    "    w = (prediction[[3,8], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    h = (prediction[[4,9], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    cls = torch.max(prediction[10:, ...].reshape(NUM_CLASSES, -1), dim=0).indices.tile(1,2)\n",
    "    \n",
    "    x_min = xc - w/2\n",
    "    y_min = yc - h/2\n",
    "    x_max = xc + w/2\n",
    "    y_max = yc + h/2\n",
    "\n",
    "    prediction_res = torch.cat([x_min, y_min, x_max, y_max, conf, cls], dim=0)\n",
    "    prediction_res = prediction_res.transpose(0,1)\n",
    "\n",
    "    # x_min과 y_min이 음수가 되지않고, x_max와 y_max가 이미지 크기를 넘지 않게 제한\n",
    "    prediction_res[:, 2].clip(min=0, max=image.shape[1]) \n",
    "    prediction_res[:, 3].clip(min=0, max=image.shape[0])\n",
    "        \n",
    "    pred_res = prediction_res[prediction_res[:, 4] > conf_thres]\n",
    "    nms_index = torchvision.ops.nms(boxes=pred_res[:, 0:4], scores=pred_res[:, 4], iou_threshold=iou_threshold)\n",
    "    pred_res_ = pred_res[nms_index].numpy()\n",
    "    \n",
    "    n_obj = pred_res_.shape[0]\n",
    "    bboxes = np.zeros(shape=(n_obj, 4), dtype=np.float32)\n",
    "    bboxes[:, 0:2] = (pred_res_[:, 0:2] + pred_res_[:, 2:4]) / 2\n",
    "    bboxes[:, 2:4] = pred_res_[:, 2:4] - pred_res_[:, 0:2]\n",
    "    scores = pred_res_[:, 4]\n",
    "    class_ids = pred_res_[:, 5]\n",
    "    \n",
    "    # 이미지 값이 들어가면 모델을 통해서, 후처리까지 포함된 yolo 포멧의 box좌표, 그 좌표에 대한 confidence score\n",
    "    # 그리고 class id를 반환\n",
    "    return bboxes, scores, class_ids,f_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b23b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_images = []\n",
    "pred_labels =[]\n",
    "feature_maps=[]\n",
    "\n",
    "for index, batch in enumerate(test_dataloaders):\n",
    "    images = batch[0].to(device)\n",
    "    bboxes, scores, class_ids, fmap = model_predict(images, model, conf_thres=0.1, iou_threshold=0.1)\n",
    "    \n",
    "    if len(bboxes) > 0:\n",
    "        prediction_yolo = np.concatenate([bboxes, scores[:, np.newaxis], class_ids[:, np.newaxis]], axis=1)\n",
    "    else:\n",
    "        prediction_yolo = np.array([])\n",
    "    \n",
    "    # 텐서형의 이미지를 다시 unnormalize를 시키고, 다시 chw를 hwc로 바꾸고 넘파이로 바꾼다.\n",
    "    np_image = make_grid(images[0], normalize=True).cpu().permute(1,2,0).numpy()\n",
    "    pred_images.append(np_image)\n",
    "    pred_labels.append(prediction_yolo)\n",
    "    feature_maps.append(fmap)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(index=(0,len(pred_images)-1))\n",
    "def show_result(index=0):\n",
    "    print(pred_labels[index])\n",
    "    if len(pred_labels[index]) > 0:\n",
    "        result = visualize(pred_images[index], pred_labels[index][:, 0:4], pred_labels[index][:, 5])\n",
    "    else:\n",
    "        result = pred_images[index]\n",
    "        \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature map에서 0,5번쨰에 해당하는 objectness 투사\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(index=(0,len(pred_images)-1))\n",
    "def show_result(index=0):\n",
    "    print(pred_labels[index])\n",
    "    if len(pred_labels[index]) > 0:\n",
    "        result = visualize(pred_images[index], pred_labels[index][:, 0:4], pred_labels[index][:, 5])\n",
    "    else:\n",
    "        result = pred_images[index]\n",
    "    \n",
    "    f_map=feature_maps[index]\n",
    "    zero_canvas=np.zeros((448,448))\n",
    "\n",
    "    cv_re1=cv2.resize(f_map[0,:,:].numpy(),(448,448))\n",
    "    cv_re2=cv2.resize(f_map[5,:,:].numpy(),(448,448))\n",
    "    zero_canvas=zero_canvas+cv_re1+cv_re2\n",
    "\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    rows = 1\n",
    "    cols = 2\n",
    "    ax1 = fig.add_subplot(rows, cols, 1)\n",
    "    ax1.imshow(result)\n",
    "    ax1.set_title('Detection')\n",
    "    ax1.axis(\"off\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(rows, cols, 2)\n",
    "    ax2.imshow(zero_canvas)\n",
    "    ax2.set_title('feature map-objectness')\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c749262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10c99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de590e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
