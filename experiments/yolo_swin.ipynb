{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c025da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d503acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee9eb3c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7fac7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'BS': 0, 'SCRATCH': 1}\n",
    "CLASS_ID_TO_NAME = {0: 'BS', 1: 'SCRATCH'}\n",
    "BOX_COLOR = {'BS':(200, 0, 0), 'SCRATCH':(0, 0, 200)}\n",
    "TEXT_COLOR = (255, 255, 255)\n",
    "\n",
    "def save_model(model_state, model_name, save_dir=\"./trained_model\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))\n",
    "\n",
    "\n",
    "def visualize_bbox(image, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    x_center, y_center, w, h = bbox\n",
    "    x_min = int(x_center - w/2)\n",
    "    y_min = int(y_center - h/2)\n",
    "    x_max = int(x_center + w/2)\n",
    "    y_max = int(y_center + h/2)\n",
    "    \n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color[class_name], thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(image, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color[class_name], -1)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "#         print('category_id: ',category_id)\n",
    "        class_name = CLASS_ID_TO_NAME[category_id.item()]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7da98",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea0166e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PET_dataset():\n",
    "    def __init__(self,part,neck_dir,body_dir,phase, transformer=None):\n",
    "        self.neck_dir=neck_dir\n",
    "        self.body_dir=body_dir\n",
    "        self.part=part\n",
    "        self.phase=phase\n",
    "        self.transformer=transformer\n",
    "        if(self.part==\"body\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.body_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.body_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        elif(self.part==\"neck\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.neck_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.neck_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "            \n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        filename, image = self.get_image(self.part, index)\n",
    "        bboxes, class_ids = self.get_label(self.part, index)\n",
    "        \n",
    "        if(self.transformer):\n",
    "            transformed_data=self.transformer(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "            image = transformed_data['image']\n",
    "            bboxes = np.array(transformed_data['bboxes'])\n",
    "            class_ids = np.array(transformed_data['class_ids'])\n",
    "            \n",
    "        \n",
    "        target = {}\n",
    "#         print(f'bboxes:{bboxes}\\nclass_ids:{class_ids}\\nlen_bboxes:{len(bboxes)}\\nlen_class_ids:{len(class_ids)}')\n",
    "#         print(f'filename: {filename}')\n",
    "        target[\"boxes\"] = torch.Tensor(bboxes).float()\n",
    "        target[\"labels\"] = torch.Tensor(class_ids).long()\n",
    "        \n",
    "        ###\n",
    "        bboxes=torch.Tensor(bboxes).float()\n",
    "        class_ids=torch.Tensor(class_ids).long()\n",
    "        target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "        ###\n",
    "        return image, target, filename\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def get_image(self, part, index): # 이미지 불러오는 함수\n",
    "        filename = self.image_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body called!-> {self.part}\")\n",
    "            image_path = self.body_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck called!-> {self.part}\")\n",
    "            image_path = self.neck_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return filename, image\n",
    "    \n",
    "    def get_label(self, part, index): # label (box좌표, class_id) 불러오는 함수\n",
    "        label_filename=self.label_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body label called!-> {self.part}\")\n",
    "            label_path = self.body_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck label called!-> {self.part}\")\n",
    "            label_path = self.neck_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        with open(label_path, 'r') as file:\n",
    "            labels = file.readlines()\n",
    "        \n",
    "        class_ids=[]\n",
    "        bboxes=[]\n",
    "        for label in labels:\n",
    "            label=label.replace(\"\\n\", \"\")\n",
    "            obj=label.split(' ')[0]\n",
    "            coor=label.split(' ')[1:]\n",
    "            obj=int(obj)\n",
    "            coor=list(map(float, coor))\n",
    "            class_ids.append(obj)\n",
    "            bboxes.append(coor)\n",
    "            \n",
    "        return bboxes, class_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "235e7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 448\n",
    "\n",
    "transformer = A.Compose([ \n",
    "        # bounding box의 변환, augmentation에서 albumentations는 Detection 학습을 할 때 굉장히 유용하다. \n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        # albumentations 라이브러리에서는 Normalization을 먼저 진행해 주고 tensor화를 진행해 주어야한다.\n",
    "    ],\n",
    "    # box 위치에 대한 transformation도 함께 진행된다. \n",
    "    bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), target_list, filename_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b245cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(part, NECK_PATH, BODY_PATH, batch_size=2):\n",
    "    transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    "    )\n",
    "    \n",
    "    dataloaders = {}\n",
    "#     train_dataset = Detection_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "    train_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer)\n",
    "    dataloaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#     val_dataset = Detection_dataset(data_dir=data_dir, phase=\"val\", transformer=transformer)\n",
    "    val_dataset=PET_dataset(part ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='valid', transformer=transformer)\n",
    "    dataloaders[\"val\"] = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6d5c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "trainset=PET_dataset(part='body',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer)\n",
    "trainset_no_trans=PET_dataset(part='body',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4849831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape:torch.Size([3, 448, 448])\n",
      "target:[[0.58172178 0.55863488 0.06113537 0.04489235 1.        ]\n",
      " [0.92389268 0.30393952 0.01996257 0.01603298 0.        ]\n",
      " [0.80255771 0.09001374 0.01996257 0.01786532 0.        ]\n",
      " [0.0246413  0.60444343 0.02557704 0.09253321 1.        ]\n",
      " [0.74422956 0.08131012 0.0205864  0.01603298 0.        ]\n",
      " [0.20586401 0.51007789 0.02495321 0.05863491 1.        ]\n",
      " [0.22181535 0.16869904 0.12494074 0.03748969 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# target 모양을 (x_cen, y_cen, w, h ,class_id)로 변경\n",
    "image, target, filename = trainset[0]\n",
    "print(f\"image.shape:{image.shape}\")\n",
    "\n",
    "print(f\"target:{target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85b8a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape:(2183, 1603, 3)\n",
      "target:[[0.58172178 0.55863488 0.06113537 0.04489235 1.        ]\n",
      " [0.92389268 0.30393952 0.01996257 0.01603298 0.        ]\n",
      " [0.80255771 0.09001374 0.01996257 0.01786532 0.        ]\n",
      " [0.0246413  0.60444343 0.02557704 0.09253321 1.        ]\n",
      " [0.74422956 0.08131012 0.0205864  0.01603298 0.        ]\n",
      " [0.20586401 0.51007789 0.02495321 0.05863491 1.        ]\n",
      " [0.22181535 0.16869904 0.12494074 0.03748969 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "image, target, filename = trainset_no_trans[0]\n",
    "print(f\"image.shape:{image.shape}\")\n",
    "# print(f\"image.type:{image.type}\") #numpy\n",
    "\n",
    "print(f\"target:{target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0fbcd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5db4ebba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8bca377c4b4fb6ac57030e5afac752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=116), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(trainset_no_trans)-1))\n",
    "\n",
    "def show_sample(index=0):\n",
    "    image, target, filename = trainset_no_trans[index]\n",
    "#     image=image.permute(1,2,0).numpy()\n",
    "    img_H, img_W, _ = image.shape\n",
    "    print(image.shape)\n",
    "\n",
    "#     bboxes = target['boxes']\n",
    "#     class_ids = target[\"labels\"]\n",
    "    \n",
    "    ###\n",
    "    bboxes = target[:, 0:4]\n",
    "    class_ids = target[:, 4]\n",
    "    ###\n",
    "    bboxes[:, [0,2]] *= img_W\n",
    "    bboxes[:, [1,3]] *= img_H\n",
    "\n",
    "    canvas = visualize(image, bboxes, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e341d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d0e88f796d47ed8304db5f48416353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=116), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(index=(0, len(trainset)-1))\n",
    "\n",
    "def show_sample(index=0):\n",
    "    image, target, filename = trainset[index]\n",
    "    image=image.permute(1,2,0).numpy()\n",
    "    img_H, img_W, _ = image.shape\n",
    "    print(image.shape)\n",
    "\n",
    "#     bboxes = target['boxes']\n",
    "#     class_ids = target[\"labels\"]\n",
    "    ###\n",
    "    bboxes = target[:, 0:4]\n",
    "    class_ids = target[:, 4]\n",
    "    ###\n",
    "    bboxes[:, [0,2]] *= img_W\n",
    "    bboxes[:, [1,3]] *= img_H\n",
    "\n",
    "    canvas = visualize(image, bboxes, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# show_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f151003",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "729f2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_SWIN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_bboxes = 2\n",
    "        self.grid_size = 7\n",
    "\n",
    "#         resnet18 = torchvision.models.resnet18(pretrained = True)\n",
    "        swin_s=torchvision.models.swin_v2_t(weights='IMAGENET1K_V1')\n",
    "        layers = [m for m in swin_s.children()] #Resnet에서 Yolo에서 가져올수 있을만한 layer만 선별적으로 가져오기 위해서\n",
    "\n",
    "        # 기존 Resnet18의 layer들중에서 맨 뒤에 두개만 제외하고 다 가져와서 Backbone으로 사용\n",
    "        self.backbone = nn.Sequential(*layers[:-3]) \n",
    "\n",
    "        # self.neck = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1, padding=0, bias=False), #Channel의 수만 변경\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False), #spatial 손실이 없게끔 padding 설정\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "        #     nn.BatchNorm2d(1024),\n",
    "        #     nn.ReLU(inplace=True)\n",
    "        #     # 여기 위에를 통과하는 동안에는 feature에 대한 정보만 바뀌고, resolution에 대한 정보는 바뀌지x\n",
    "        # )\n",
    "\n",
    "        # neck을 통과해서 feature map을 가져오고, head 부분에서 output depth와 grid size를 조절해서 뱉게해준다.\n",
    "\n",
    "        # self.head = nn.Sequential(\n",
    "        #     nn.Conv2d(in_channels=1024, out_channels=(4+1)*self.num_bboxes+num_classes, kernel_size=1, padding=0, bias=False),\n",
    "        #     nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\n",
    "        # )\n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=768, out_channels=1024, kernel_size=1, padding=0,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=1024, out_channels=(4+1)*self.num_bboxes+num_classes, kernel_size=1, padding=0, bias=False),\n",
    "                nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        # out = self.neck(out)\n",
    "        out = self.head(out) # input (batch, 3, 448, 448) -> output feature (batch, 12, 7, 7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b0a6eb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO_SWIN(\n",
       "  (backbone): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): Permute()\n",
       "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Permute()\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(768, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (13): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "model = YOLO_SWIN(num_classes=NUM_CLASSES)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "361cde55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 96, 112, 112]           4,704\n",
      "           Permute-2         [-1, 112, 112, 96]               0\n",
      "         LayerNorm-3         [-1, 112, 112, 96]             192\n",
      "            Linear-4          [-1, 15, 15, 512]           1,536\n",
      "              ReLU-5          [-1, 15, 15, 512]               0\n",
      "            Linear-6            [-1, 15, 15, 3]           1,536\n",
      "ShiftedWindowAttentionV2-7         [-1, 112, 112, 96]               0\n",
      "         LayerNorm-8         [-1, 112, 112, 96]             192\n",
      "   StochasticDepth-9         [-1, 112, 112, 96]               0\n",
      "           Linear-10        [-1, 112, 112, 384]          37,248\n",
      "             GELU-11        [-1, 112, 112, 384]               0\n",
      "          Dropout-12        [-1, 112, 112, 384]               0\n",
      "           Linear-13         [-1, 112, 112, 96]          36,960\n",
      "          Dropout-14         [-1, 112, 112, 96]               0\n",
      "        LayerNorm-15         [-1, 112, 112, 96]             192\n",
      "  StochasticDepth-16         [-1, 112, 112, 96]               0\n",
      "SwinTransformerBlockV2-17         [-1, 112, 112, 96]               0\n",
      "           Linear-18          [-1, 15, 15, 512]           1,536\n",
      "             ReLU-19          [-1, 15, 15, 512]               0\n",
      "           Linear-20            [-1, 15, 15, 3]           1,536\n",
      "ShiftedWindowAttentionV2-21         [-1, 112, 112, 96]               0\n",
      "        LayerNorm-22         [-1, 112, 112, 96]             192\n",
      "  StochasticDepth-23         [-1, 112, 112, 96]               0\n",
      "           Linear-24        [-1, 112, 112, 384]          37,248\n",
      "             GELU-25        [-1, 112, 112, 384]               0\n",
      "          Dropout-26        [-1, 112, 112, 384]               0\n",
      "           Linear-27         [-1, 112, 112, 96]          36,960\n",
      "          Dropout-28         [-1, 112, 112, 96]               0\n",
      "        LayerNorm-29         [-1, 112, 112, 96]             192\n",
      "  StochasticDepth-30         [-1, 112, 112, 96]               0\n",
      "SwinTransformerBlockV2-31         [-1, 112, 112, 96]               0\n",
      "           Linear-32          [-1, 56, 56, 192]          73,728\n",
      "        LayerNorm-33          [-1, 56, 56, 192]             384\n",
      "   PatchMergingV2-34          [-1, 56, 56, 192]               0\n",
      "           Linear-35          [-1, 15, 15, 512]           1,536\n",
      "             ReLU-36          [-1, 15, 15, 512]               0\n",
      "           Linear-37            [-1, 15, 15, 6]           3,072\n",
      "ShiftedWindowAttentionV2-38          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-39          [-1, 56, 56, 192]             384\n",
      "  StochasticDepth-40          [-1, 56, 56, 192]               0\n",
      "           Linear-41          [-1, 56, 56, 768]         148,224\n",
      "             GELU-42          [-1, 56, 56, 768]               0\n",
      "          Dropout-43          [-1, 56, 56, 768]               0\n",
      "           Linear-44          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-45          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-46          [-1, 56, 56, 192]             384\n",
      "  StochasticDepth-47          [-1, 56, 56, 192]               0\n",
      "SwinTransformerBlockV2-48          [-1, 56, 56, 192]               0\n",
      "           Linear-49          [-1, 15, 15, 512]           1,536\n",
      "             ReLU-50          [-1, 15, 15, 512]               0\n",
      "           Linear-51            [-1, 15, 15, 6]           3,072\n",
      "ShiftedWindowAttentionV2-52          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-53          [-1, 56, 56, 192]             384\n",
      "  StochasticDepth-54          [-1, 56, 56, 192]               0\n",
      "           Linear-55          [-1, 56, 56, 768]         148,224\n",
      "             GELU-56          [-1, 56, 56, 768]               0\n",
      "          Dropout-57          [-1, 56, 56, 768]               0\n",
      "           Linear-58          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-59          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-60          [-1, 56, 56, 192]             384\n",
      "  StochasticDepth-61          [-1, 56, 56, 192]               0\n",
      "SwinTransformerBlockV2-62          [-1, 56, 56, 192]               0\n",
      "           Linear-63          [-1, 28, 28, 384]         294,912\n",
      "        LayerNorm-64          [-1, 28, 28, 384]             768\n",
      "   PatchMergingV2-65          [-1, 28, 28, 384]               0\n",
      "           Linear-66          [-1, 15, 15, 512]           1,536\n",
      "             ReLU-67          [-1, 15, 15, 512]               0\n",
      "           Linear-68           [-1, 15, 15, 12]           6,144\n",
      "ShiftedWindowAttentionV2-69          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-70          [-1, 28, 28, 384]             768\n",
      "  StochasticDepth-71          [-1, 28, 28, 384]               0\n",
      "           Linear-72         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-73         [-1, 28, 28, 1536]               0\n",
      "          Dropout-74         [-1, 28, 28, 1536]               0\n",
      "           Linear-75          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-76          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-77          [-1, 28, 28, 384]             768\n",
      "  StochasticDepth-78          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlockV2-79          [-1, 28, 28, 384]               0\n",
      "           Linear-80          [-1, 15, 15, 512]           1,536\n",
      "             ReLU-81          [-1, 15, 15, 512]               0\n",
      "           Linear-82           [-1, 15, 15, 12]           6,144\n",
      "ShiftedWindowAttentionV2-83          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-84          [-1, 28, 28, 384]             768\n",
      "  StochasticDepth-85          [-1, 28, 28, 384]               0\n",
      "           Linear-86         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-87         [-1, 28, 28, 1536]               0\n",
      "          Dropout-88         [-1, 28, 28, 1536]               0\n",
      "           Linear-89          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-90          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-91          [-1, 28, 28, 384]             768\n",
      "  StochasticDepth-92          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlockV2-93          [-1, 28, 28, 384]               0\n",
      "           Linear-94          [-1, 15, 15, 512]           1,536\n",
      "             ReLU-95          [-1, 15, 15, 512]               0\n",
      "           Linear-96           [-1, 15, 15, 12]           6,144\n",
      "ShiftedWindowAttentionV2-97          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-98          [-1, 28, 28, 384]             768\n",
      "  StochasticDepth-99          [-1, 28, 28, 384]               0\n",
      "          Linear-100         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-101         [-1, 28, 28, 1536]               0\n",
      "         Dropout-102         [-1, 28, 28, 1536]               0\n",
      "          Linear-103          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-104          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-105          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-106          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlockV2-107          [-1, 28, 28, 384]               0\n",
      "          Linear-108          [-1, 15, 15, 512]           1,536\n",
      "            ReLU-109          [-1, 15, 15, 512]               0\n",
      "          Linear-110           [-1, 15, 15, 12]           6,144\n",
      "ShiftedWindowAttentionV2-111          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-112          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-113          [-1, 28, 28, 384]               0\n",
      "          Linear-114         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-115         [-1, 28, 28, 1536]               0\n",
      "         Dropout-116         [-1, 28, 28, 1536]               0\n",
      "          Linear-117          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-118          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-119          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-120          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlockV2-121          [-1, 28, 28, 384]               0\n",
      "          Linear-122          [-1, 15, 15, 512]           1,536\n",
      "            ReLU-123          [-1, 15, 15, 512]               0\n",
      "          Linear-124           [-1, 15, 15, 12]           6,144\n",
      "ShiftedWindowAttentionV2-125          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-126          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-127          [-1, 28, 28, 384]               0\n",
      "          Linear-128         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-129         [-1, 28, 28, 1536]               0\n",
      "         Dropout-130         [-1, 28, 28, 1536]               0\n",
      "          Linear-131          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-132          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-133          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-134          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlockV2-135          [-1, 28, 28, 384]               0\n",
      "          Linear-136          [-1, 15, 15, 512]           1,536\n",
      "            ReLU-137          [-1, 15, 15, 512]               0\n",
      "          Linear-138           [-1, 15, 15, 12]           6,144\n",
      "ShiftedWindowAttentionV2-139          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-140          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-141          [-1, 28, 28, 384]               0\n",
      "          Linear-142         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-143         [-1, 28, 28, 1536]               0\n",
      "         Dropout-144         [-1, 28, 28, 1536]               0\n",
      "          Linear-145          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-146          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-147          [-1, 28, 28, 384]             768\n",
      " StochasticDepth-148          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlockV2-149          [-1, 28, 28, 384]               0\n",
      "          Linear-150          [-1, 14, 14, 768]       1,179,648\n",
      "       LayerNorm-151          [-1, 14, 14, 768]           1,536\n",
      "  PatchMergingV2-152          [-1, 14, 14, 768]               0\n",
      "          Linear-153          [-1, 15, 15, 512]           1,536\n",
      "            ReLU-154          [-1, 15, 15, 512]               0\n",
      "          Linear-155           [-1, 15, 15, 24]          12,288\n",
      "ShiftedWindowAttentionV2-156          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-157          [-1, 14, 14, 768]           1,536\n",
      " StochasticDepth-158          [-1, 14, 14, 768]               0\n",
      "          Linear-159         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-160         [-1, 14, 14, 3072]               0\n",
      "         Dropout-161         [-1, 14, 14, 3072]               0\n",
      "          Linear-162          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-163          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-164          [-1, 14, 14, 768]           1,536\n",
      " StochasticDepth-165          [-1, 14, 14, 768]               0\n",
      "SwinTransformerBlockV2-166          [-1, 14, 14, 768]               0\n",
      "          Linear-167          [-1, 15, 15, 512]           1,536\n",
      "            ReLU-168          [-1, 15, 15, 512]               0\n",
      "          Linear-169           [-1, 15, 15, 24]          12,288\n",
      "ShiftedWindowAttentionV2-170          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-171          [-1, 14, 14, 768]           1,536\n",
      " StochasticDepth-172          [-1, 14, 14, 768]               0\n",
      "          Linear-173         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-174         [-1, 14, 14, 3072]               0\n",
      "         Dropout-175         [-1, 14, 14, 3072]               0\n",
      "          Linear-176          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-177          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-178          [-1, 14, 14, 768]           1,536\n",
      " StochasticDepth-179          [-1, 14, 14, 768]               0\n",
      "SwinTransformerBlockV2-180          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-181          [-1, 14, 14, 768]           1,536\n",
      "         Permute-182          [-1, 768, 14, 14]               0\n",
      "          Conv2d-183         [-1, 1024, 14, 14]         786,432\n",
      "     BatchNorm2d-184         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-185         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-186         [-1, 1024, 14, 14]       9,437,184\n",
      "     BatchNorm2d-187         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-188         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-189         [-1, 1024, 14, 14]       9,437,184\n",
      "     BatchNorm2d-190         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-191         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-192         [-1, 1024, 14, 14]       9,437,184\n",
      "     BatchNorm2d-193         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-194         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-195           [-1, 12, 14, 14]          12,288\n",
      "AdaptiveAvgPool2d-196             [-1, 12, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 48,057,056\n",
      "Trainable params: 48,057,056\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 966.52\n",
      "Params size (MB): 183.32\n",
      "Estimated Total Size (MB): 1152.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4cf3af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 448, 448).to(device)\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4c05720",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# data_dir = \"/content/drive/MyDrive/fastCamMedicalProj/DATASET/DATASET/Detection/\"\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# trainset = Detection_dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "trainset=PET_dataset(part='body',neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='train', transformer=transformer)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = YOLO_SWIN(num_classes=NUM_CLASSES)\n",
    "\n",
    "for index, batch in enumerate(trainloader):\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    filenames = batch[2]\n",
    "    \n",
    "    predictions = model(images)\n",
    "    print(f\"filename:{filenames}, target:{targets}\")\n",
    "#     print(f\"{index}--input shape:{images.shape} -> output shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da970d7",
   "metadata": {},
   "source": [
    "# Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c66945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_LOSS():\n",
    "    def __init__(self, num_classes, device, lambda_coord=5., lambda_noobj=0.5):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.grid_size = 7\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def __call__(self, predictions, targets):\n",
    "        self.batch_size, _, _, _ = predictions.shape\n",
    "        groundtruths = self.build_batch_target_grid(targets)\n",
    "        groundtruths = groundtruths.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iou1 = self.get_IoU(predictions[:, 1:5, ...], groundtruths[:, 1:5, ...])\n",
    "            iou2 = self.get_IoU(predictions[:, 6:10, ...], groundtruths[:, 1:5, ...])\n",
    "\n",
    "        ious = torch.stack([iou1, iou2], dim=1)\n",
    "        max_iou, best_box = ious.max(dim=1, keepdim=True)\n",
    "        max_iou = torch.cat([max_iou, max_iou], dim=1)\n",
    "        best_box = torch.cat([best_box.eq(0), best_box.eq(1)], dim=1)\n",
    "\n",
    "        predictions_ = predictions[:, :5*2, ...].reshape(self.batch_size, 2, 5, self.grid_size, self.grid_size)\n",
    "        obj_pred = predictions_[:, :, 0, ...]\n",
    "        xy_pred = predictions_[:, :, 1:3, ...]\n",
    "        wh_pred = predictions_[:, :, 3:5, ...]\n",
    "        cls_pred = predictions[:, 5*2:, ...]\n",
    "\n",
    "        groundtruths_ = groundtruths[:, :5, ...].reshape(self.batch_size, 1, 5, self.grid_size, self.grid_size)\n",
    "        obj_target = groundtruths_[:, :, 0, ...]\n",
    "        xy_target = groundtruths_[:, :, 1:3, ...]\n",
    "        wh_target= groundtruths_[:, :, 3:5, ...]\n",
    "        cls_target = groundtruths[:, 5:, ...]\n",
    "        \n",
    "        positive = obj_target * best_box\n",
    "\n",
    "        obj_loss = self.mse_loss(positive * obj_pred, positive * ious)\n",
    "        noobj_loss = self.mse_loss((1 - positive) * obj_pred, ious*0)\n",
    "        xy_loss = self.mse_loss(positive.unsqueeze(dim=2) * xy_pred, positive.unsqueeze(dim=2) * xy_target)\n",
    "        wh_loss = self.mse_loss(positive.unsqueeze(dim=2) * (wh_pred.sign() * (wh_pred.abs() + 1e-8).sqrt()),\n",
    "                           positive.unsqueeze(dim=2) * (wh_target + 1e-8).sqrt())\n",
    "        cls_loss = self.mse_loss(obj_target * cls_pred, cls_target)\n",
    "        \n",
    "        obj_loss /= self.batch_size\n",
    "        noobj_loss /= self.batch_size\n",
    "        bbox_loss = (xy_loss+wh_loss) / self.batch_size\n",
    "        cls_loss /= self.batch_size\n",
    "        \n",
    "        total_loss = obj_loss + self.lambda_noobj*noobj_loss + self.lambda_coord*bbox_loss + cls_loss\n",
    "        return total_loss, (obj_loss.item(), noobj_loss.item(), bbox_loss.item(), cls_loss.item())\n",
    "    \n",
    "    def build_target_grid(self, target):\n",
    "        target_grid = torch.zeros((1+4+self.num_classes, self.grid_size, self.grid_size), device=self.device)\n",
    "\n",
    "        for gt in target:\n",
    "            xc, yc, w, h, cls_id = gt\n",
    "            xn = (xc % (1/self.grid_size))\n",
    "            yn = (yc % (1/self.grid_size))\n",
    "            cls_id = int(cls_id)\n",
    "\n",
    "            i_grid = int(xc * self.grid_size)\n",
    "            j_grid = int(yc * self.grid_size)\n",
    "            target_grid[0, j_grid, i_grid] = 1\n",
    "            target_grid[1:5, j_grid, i_grid] = torch.Tensor([xn,yn,w,h])\n",
    "#             print(5+cls_id, j_grid, i_grid)\n",
    "            target_grid[5+cls_id, j_grid, i_grid] = 1\n",
    "\n",
    "        return target_grid\n",
    "    \n",
    "    def build_batch_target_grid(self, targets):\n",
    "        target_grid_batch = torch.stack([self.build_target_grid(target) for target in targets], dim=0)\n",
    "        return target_grid_batch\n",
    "    \n",
    "    def get_IoU(self, cbox1, cbox2):\n",
    "        box1 = self.xywh_to_xyxy(cbox1)\n",
    "        box2 = self.xywh_to_xyxy(cbox2)\n",
    "\n",
    "        x1 = torch.max(box1[:, 0, ...], box2[:, 0, ...])\n",
    "        y1 = torch.max(box1[:, 1, ...], box2[:, 1, ...])\n",
    "        x2 = torch.min(box1[:, 2, ...], box2[:, 2, ...])\n",
    "        y2 = torch.min(box1[:, 3, ...], box2[:, 3, ...])\n",
    "\n",
    "        intersection = (x2-x1).clamp(min=0) * (y2-y1).clamp(min=0)\n",
    "        union = abs(cbox1[:, 2, ...]*cbox1[:, 3, ...]) + \\\n",
    "                abs(cbox2[:, 2, ...]*cbox2[:, 3, ...]) - intersection\n",
    "\n",
    "        intersection[intersection.gt(0)] = intersection[intersection.gt(0)] / union[intersection.gt(0)]\n",
    "        return intersection\n",
    "    \n",
    "    def generate_xy_normed_grid(self):\n",
    "        y_offset, x_offset = torch.meshgrid(torch.arange(self.grid_size), torch.arange(self.grid_size))\n",
    "        xy_grid = torch.stack([x_offset, y_offset], dim=0)\n",
    "        xy_normed_grid = xy_grid / self.grid_size\n",
    "        return xy_normed_grid.to(self.device)\n",
    "\n",
    "    def xywh_to_xyxy(self, bboxes):\n",
    "        xy_normed_grid = self.generate_xy_normed_grid()\n",
    "        xcyc = bboxes[:,0:2,...] + xy_normed_grid.tile(self.batch_size, 1,1,1)\n",
    "        wh = bboxes[:,2:4,...]\n",
    "        x1y1 = xcyc - (wh/2)\n",
    "        x2y2 = xcyc + (wh/2)\n",
    "        return torch.cat([x1y1, x2y2], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46ad931",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1729df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, criterion, optimizer, device):\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "    \n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        running_loss = defaultdict(float)\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0].to(device)\n",
    "            targets = batch[1]\n",
    "            filenames = batch[2]\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"): # phase가 train 일때만 gradient 추적기능을 킨다.\n",
    "                predictions = model(images) #prediction shape=> B,12,7,7\n",
    "#             print(f\"predictions:{predictions}, \\ntargets: {targets}\\n\")\n",
    "            loss, (obj_loss, noobj_loss, bbox_loss, cls_loss) = criterion(predictions, targets)\n",
    "#             print(f\"loss:{loss}, obj_loss:{obj_loss}, noobj_loss:{noobj_loss}\\nbbox_loss:{bbox_loss}, cls_loss:{cls_loss}\\n--------------\\n\")\n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 현재 epoch단계에서 loss가 얼마인지 running loss 가출력\n",
    "                running_loss[\"total_loss\"] += loss.item()\n",
    "                running_loss[\"obj_loss\"] += obj_loss\n",
    "                running_loss[\"noobj_loss\"] += noobj_loss\n",
    "                running_loss[\"bbox_loss\"] += bbox_loss\n",
    "                running_loss[\"cls_loss\"] += cls_loss\n",
    "                \n",
    "                train_loss[\"total_loss\"] += loss.item()\n",
    "                train_loss[\"obj_loss\"] += obj_loss\n",
    "                train_loss[\"noobj_loss\"] += noobj_loss\n",
    "                train_loss[\"bbox_loss\"] += bbox_loss\n",
    "                train_loss[\"cls_loss\"] += cls_loss\n",
    "                \n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                    text = f\"<<<iteration:[{index}/{len(dataloaders[phase])}] - \"\n",
    "                    for k, v in running_loss.items():\n",
    "                        text += f\"{k}: {v/VERBOSE_FREQ:.4f}  \"\n",
    "                        running_loss[k] = 0.\n",
    "                    print(text)\n",
    "            else:\n",
    "                val_loss[\"total_loss\"] += loss.item()\n",
    "                val_loss[\"obj_loss\"] += obj_loss\n",
    "                val_loss[\"noobj_loss\"] += noobj_loss\n",
    "                val_loss[\"bbox_loss\"] += bbox_loss\n",
    "                val_loss[\"cls_loss\"] += cls_loss\n",
    "\n",
    "    for k in train_loss.keys():\n",
    "        train_loss[k] /= len(dataloaders[\"train\"])\n",
    "        val_loss[k] /= len(dataloaders[\"val\"])\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2771b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/content/drive/MyDrive/fastCamMedicalProj/DATASET/DATASET/Detection/\"\n",
    "NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 1\n",
    "VERBOSE_FREQ = 20\n",
    "LR=0.0001\n",
    "BACKBONE=\"YOLO_SWIN_T\"\n",
    "PART=\"body\"\n",
    "num_epochs = 100\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "\n",
    "dataloaders = build_dataloader(part=PART,NECK_PATH=NECK_PATH,BODY_PATH=BODY_PATH,batch_size=BATCH_SIZE)\n",
    "model = YOLO_SWIN(num_classes=NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "criterion = YOLO_LOSS(num_classes=NUM_CLASSES, device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "060a24e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgomduribo\u001b[0m (\u001b[33murp\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/yb/wandb/run-20231006_051152-5822c9gs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/urp/yolo_swin/runs/5822c9gs' target=\"_blank\">proud-wildflower-2</a></strong> to <a href='https://wandb.ai/urp/yolo_swin' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/urp/yolo_swin' target=\"_blank\">https://wandb.ai/urp/yolo_swin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/urp/yolo_swin/runs/5822c9gs' target=\"_blank\">https://wandb.ai/urp/yolo_swin/runs/5822c9gs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/urp/yolo_swin/runs/5822c9gs?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9f4cde2d60>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"yolo_swin\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"architecture\": BACKBONE,\n",
    "    \"dataset\": \"BODY\",\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ebab5dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[20/117] - total_loss: 24.2522  obj_loss: 0.3188  noobj_loss: 4.2417  bbox_loss: 3.7197  cls_loss: 3.2139  \n",
      "<<<iteration:[40/117] - total_loss: 50.3951  obj_loss: 0.1571  noobj_loss: 4.2987  bbox_loss: 8.8497  cls_loss: 3.8402  \n",
      "<<<iteration:[60/117] - total_loss: 36.9247  obj_loss: 1.4886  noobj_loss: 15.4979  bbox_loss: 4.9622  cls_loss: 2.8763  \n",
      "<<<iteration:[80/117] - total_loss: 16.9519  obj_loss: 0.0231  noobj_loss: 1.2643  bbox_loss: 2.7769  cls_loss: 2.4122  \n",
      "<<<iteration:[100/117] - total_loss: 12.9996  obj_loss: 0.0337  noobj_loss: 0.7079  bbox_loss: 2.0449  cls_loss: 2.3876  \n",
      "\n",
      "epoch:1/100 - Train Loss: 26.0890, Val Loss: 8.0821\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 9.5633  obj_loss: 0.0345  noobj_loss: 0.4513  bbox_loss: 1.3280  cls_loss: 2.6632  \n",
      "<<<iteration:[40/117] - total_loss: 14.9126  obj_loss: 0.0258  noobj_loss: 0.4205  bbox_loss: 2.3190  cls_loss: 3.0814  \n",
      "<<<iteration:[60/117] - total_loss: 10.8487  obj_loss: 0.0182  noobj_loss: 0.3880  bbox_loss: 1.6579  cls_loss: 2.3473  \n",
      "<<<iteration:[80/117] - total_loss: 8.5844  obj_loss: 0.0145  noobj_loss: 0.2834  bbox_loss: 1.2232  cls_loss: 2.3121  \n",
      "<<<iteration:[100/117] - total_loss: 11.5170  obj_loss: 0.0153  noobj_loss: 0.2064  bbox_loss: 1.7547  cls_loss: 2.6252  \n",
      "\n",
      "epoch:2/100 - Train Loss: 10.9644, Val Loss: 11.8569\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 24.8209  obj_loss: 0.0241  noobj_loss: 0.3951  bbox_loss: 4.3924  cls_loss: 2.6371  \n",
      "<<<iteration:[40/117] - total_loss: 8.6364  obj_loss: 0.0563  noobj_loss: 0.2948  bbox_loss: 1.1799  cls_loss: 2.5333  \n",
      "<<<iteration:[60/117] - total_loss: 11.1311  obj_loss: 0.0377  noobj_loss: 0.2070  bbox_loss: 1.5801  cls_loss: 3.0893  \n",
      "<<<iteration:[80/117] - total_loss: 11.6691  obj_loss: 0.0269  noobj_loss: 0.2445  bbox_loss: 1.8978  cls_loss: 2.0307  \n",
      "<<<iteration:[100/117] - total_loss: 7.1350  obj_loss: 0.0288  noobj_loss: 0.1756  bbox_loss: 0.9693  cls_loss: 2.1718  \n",
      "\n",
      "epoch:3/100 - Train Loss: 12.3269, Val Loss: 9.7794\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 8.9787  obj_loss: 0.0392  noobj_loss: 0.2443  bbox_loss: 1.2787  cls_loss: 2.4241  \n",
      "<<<iteration:[40/117] - total_loss: 8.6947  obj_loss: 0.0157  noobj_loss: 0.1105  bbox_loss: 1.2564  cls_loss: 2.3416  \n",
      "<<<iteration:[60/117] - total_loss: 6.5667  obj_loss: 0.0350  noobj_loss: 0.1391  bbox_loss: 0.8210  cls_loss: 2.3571  \n",
      "<<<iteration:[80/117] - total_loss: 8.7514  obj_loss: 0.0170  noobj_loss: 0.0973  bbox_loss: 1.2410  cls_loss: 2.4809  \n",
      "<<<iteration:[100/117] - total_loss: 19.8946  obj_loss: 0.0189  noobj_loss: 0.2765  bbox_loss: 3.4028  cls_loss: 2.7236  \n",
      "\n",
      "epoch:4/100 - Train Loss: 10.5565, Val Loss: 16.3189\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 14.6946  obj_loss: 0.0120  noobj_loss: 0.1711  bbox_loss: 2.4362  cls_loss: 2.4160  \n",
      "<<<iteration:[40/117] - total_loss: 11.6480  obj_loss: 0.0397  noobj_loss: 0.1364  bbox_loss: 1.6934  cls_loss: 3.0731  \n",
      "<<<iteration:[60/117] - total_loss: 7.4086  obj_loss: 0.0637  noobj_loss: 0.1764  bbox_loss: 0.9029  cls_loss: 2.7423  \n",
      "<<<iteration:[80/117] - total_loss: 6.1570  obj_loss: 0.0201  noobj_loss: 0.1703  bbox_loss: 0.8133  cls_loss: 1.9854  \n",
      "<<<iteration:[100/117] - total_loss: 5.9251  obj_loss: 0.0321  noobj_loss: 0.2496  bbox_loss: 0.6763  cls_loss: 2.3867  \n",
      "\n",
      "epoch:5/100 - Train Loss: 10.4115, Val Loss: 4.6045\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 8.5503  obj_loss: 0.0175  noobj_loss: 0.1204  bbox_loss: 1.1918  cls_loss: 2.5134  \n",
      "<<<iteration:[40/117] - total_loss: 51.0330  obj_loss: 0.0430  noobj_loss: 0.2296  bbox_loss: 9.5625  cls_loss: 3.0626  \n",
      "<<<iteration:[60/117] - total_loss: 5.2126  obj_loss: 0.0157  noobj_loss: 0.1025  bbox_loss: 0.6135  cls_loss: 2.0781  \n",
      "<<<iteration:[80/117] - total_loss: 108.7960  obj_loss: 0.0315  noobj_loss: 0.0735  bbox_loss: 21.2316  cls_loss: 2.5699  \n",
      "<<<iteration:[100/117] - total_loss: 15.4503  obj_loss: 0.0081  noobj_loss: 0.1352  bbox_loss: 2.5871  cls_loss: 2.4393  \n",
      "\n",
      "epoch:6/100 - Train Loss: 43.1238, Val Loss: 7.8270\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 18.0620  obj_loss: 0.0202  noobj_loss: 0.1474  bbox_loss: 2.9777  cls_loss: 3.0794  \n",
      "<<<iteration:[40/117] - total_loss: 6.0499  obj_loss: 0.0322  noobj_loss: 0.1077  bbox_loss: 0.6780  cls_loss: 2.5739  \n",
      "<<<iteration:[60/117] - total_loss: 20.3022  obj_loss: 0.0316  noobj_loss: 0.1464  bbox_loss: 3.4503  cls_loss: 2.9461  \n",
      "<<<iteration:[80/117] - total_loss: 8.6462  obj_loss: 0.0172  noobj_loss: 0.0768  bbox_loss: 1.2982  cls_loss: 2.0998  \n",
      "<<<iteration:[100/117] - total_loss: 8.9709  obj_loss: 0.0319  noobj_loss: 0.0845  bbox_loss: 1.3071  cls_loss: 2.3613  \n",
      "\n",
      "epoch:7/100 - Train Loss: 11.1918, Val Loss: 3.1755\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 8.4422  obj_loss: 0.0456  noobj_loss: 0.0573  bbox_loss: 1.0817  cls_loss: 2.9593  \n",
      "<<<iteration:[40/117] - total_loss: 16.3173  obj_loss: 0.0132  noobj_loss: 0.0581  bbox_loss: 2.8010  cls_loss: 2.2699  \n",
      "<<<iteration:[60/117] - total_loss: 5.2065  obj_loss: 0.0541  noobj_loss: 0.0673  bbox_loss: 0.5916  cls_loss: 2.1610  \n",
      "<<<iteration:[80/117] - total_loss: 5.8862  obj_loss: 0.0339  noobj_loss: 0.0448  bbox_loss: 0.6775  cls_loss: 2.4423  \n",
      "<<<iteration:[100/117] - total_loss: 5.3159  obj_loss: 0.0359  noobj_loss: 0.0459  bbox_loss: 0.5744  cls_loss: 2.3849  \n",
      "\n",
      "epoch:8/100 - Train Loss: 7.9033, Val Loss: 6.2983\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 5.6186  obj_loss: 0.0868  noobj_loss: 0.0642  bbox_loss: 0.6666  cls_loss: 2.1666  \n",
      "<<<iteration:[40/117] - total_loss: 6.0277  obj_loss: 0.0193  noobj_loss: 0.0420  bbox_loss: 0.7128  cls_loss: 2.4232  \n",
      "<<<iteration:[60/117] - total_loss: 5.0438  obj_loss: 0.0173  noobj_loss: 0.0483  bbox_loss: 0.5205  cls_loss: 2.3998  \n",
      "<<<iteration:[80/117] - total_loss: 7.4621  obj_loss: 0.0604  noobj_loss: 0.0389  bbox_loss: 0.9165  cls_loss: 2.7996  \n",
      "<<<iteration:[100/117] - total_loss: 4.7774  obj_loss: 0.0209  noobj_loss: 0.0363  bbox_loss: 0.4713  cls_loss: 2.3818  \n",
      "\n",
      "epoch:9/100 - Train Loss: 10.7686, Val Loss: 19.3227\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 9.0817  obj_loss: 0.0285  noobj_loss: 0.0678  bbox_loss: 1.2441  cls_loss: 2.7989  \n",
      "<<<iteration:[40/117] - total_loss: 5.1440  obj_loss: 0.0459  noobj_loss: 0.0490  bbox_loss: 0.4741  cls_loss: 2.7032  \n",
      "<<<iteration:[60/117] - total_loss: 6.2775  obj_loss: 0.0652  noobj_loss: 0.0385  bbox_loss: 0.6952  cls_loss: 2.7169  \n",
      "<<<iteration:[80/117] - total_loss: 4.2030  obj_loss: 0.0780  noobj_loss: 0.0491  bbox_loss: 0.3460  cls_loss: 2.3701  \n",
      "<<<iteration:[100/117] - total_loss: 5.9437  obj_loss: 0.0501  noobj_loss: 0.0380  bbox_loss: 0.7477  cls_loss: 2.1361  \n",
      "\n",
      "epoch:10/100 - Train Loss: 5.8881, Val Loss: 3.2956\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.4215  obj_loss: 0.0407  noobj_loss: 0.0306  bbox_loss: 0.3515  cls_loss: 2.6082  \n",
      "<<<iteration:[40/117] - total_loss: 2.9385  obj_loss: 0.0705  noobj_loss: 0.0471  bbox_loss: 0.1529  cls_loss: 2.0799  \n",
      "<<<iteration:[60/117] - total_loss: 6.1586  obj_loss: 0.0426  noobj_loss: 0.0309  bbox_loss: 0.6201  cls_loss: 3.0000  \n",
      "<<<iteration:[80/117] - total_loss: 3.1758  obj_loss: 0.0535  noobj_loss: 0.0356  bbox_loss: 0.1693  cls_loss: 2.2579  \n",
      "<<<iteration:[100/117] - total_loss: 3.6489  obj_loss: 0.0741  noobj_loss: 0.0368  bbox_loss: 0.2449  cls_loss: 2.3319  \n",
      "\n",
      "epoch:11/100 - Train Loss: 4.0451, Val Loss: 4.3468\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.3578  obj_loss: 0.0449  noobj_loss: 0.0316  bbox_loss: 0.1792  cls_loss: 2.4010  \n",
      "<<<iteration:[40/117] - total_loss: 3.2021  obj_loss: 0.0593  noobj_loss: 0.0255  bbox_loss: 0.1679  cls_loss: 2.2907  \n",
      "<<<iteration:[60/117] - total_loss: 3.5870  obj_loss: 0.0334  noobj_loss: 0.0260  bbox_loss: 0.2749  cls_loss: 2.1661  \n",
      "<<<iteration:[80/117] - total_loss: 5.0831  obj_loss: 0.0460  noobj_loss: 0.0248  bbox_loss: 0.4731  cls_loss: 2.6593  \n",
      "<<<iteration:[100/117] - total_loss: 4.5731  obj_loss: 0.0628  noobj_loss: 0.0318  bbox_loss: 0.3236  cls_loss: 2.8764  \n",
      "\n",
      "epoch:12/100 - Train Loss: 3.9004, Val Loss: 3.2020\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1309  obj_loss: 0.0260  noobj_loss: 0.0400  bbox_loss: 0.1697  cls_loss: 2.2364  \n",
      "<<<iteration:[40/117] - total_loss: 4.0167  obj_loss: 0.0360  noobj_loss: 0.0223  bbox_loss: 0.3073  cls_loss: 2.4330  \n",
      "<<<iteration:[60/117] - total_loss: 3.1873  obj_loss: 0.0473  noobj_loss: 0.0342  bbox_loss: 0.1985  cls_loss: 2.1304  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[80/117] - total_loss: 4.2970  obj_loss: 0.0268  noobj_loss: 0.0296  bbox_loss: 0.3479  cls_loss: 2.5160  \n",
      "<<<iteration:[100/117] - total_loss: 3.6144  obj_loss: 0.0600  noobj_loss: 0.0296  bbox_loss: 0.1584  cls_loss: 2.7479  \n",
      "\n",
      "epoch:13/100 - Train Loss: 3.6866, Val Loss: 3.1932\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.2148  obj_loss: 0.0414  noobj_loss: 0.0282  bbox_loss: 0.3376  cls_loss: 2.4713  \n",
      "<<<iteration:[40/117] - total_loss: 3.8532  obj_loss: 0.0897  noobj_loss: 0.0269  bbox_loss: 0.2900  cls_loss: 2.3002  \n",
      "<<<iteration:[60/117] - total_loss: 2.8435  obj_loss: 0.0566  noobj_loss: 0.0219  bbox_loss: 0.1441  cls_loss: 2.0554  \n",
      "<<<iteration:[80/117] - total_loss: 3.9249  obj_loss: 0.0449  noobj_loss: 0.0280  bbox_loss: 0.3559  cls_loss: 2.0863  \n",
      "<<<iteration:[100/117] - total_loss: 6.4896  obj_loss: 0.0483  noobj_loss: 0.0285  bbox_loss: 0.6991  cls_loss: 2.9316  \n",
      "\n",
      "epoch:14/100 - Train Loss: 4.2246, Val Loss: 2.8350\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.6304  obj_loss: 0.0475  noobj_loss: 0.0316  bbox_loss: 0.4655  cls_loss: 2.2395  \n",
      "<<<iteration:[40/117] - total_loss: 4.0542  obj_loss: 0.0612  noobj_loss: 0.0299  bbox_loss: 0.3179  cls_loss: 2.3887  \n",
      "<<<iteration:[60/117] - total_loss: 5.2518  obj_loss: 0.0172  noobj_loss: 0.0240  bbox_loss: 0.4760  cls_loss: 2.8427  \n",
      "<<<iteration:[80/117] - total_loss: 3.1803  obj_loss: 0.0675  noobj_loss: 0.0206  bbox_loss: 0.1924  cls_loss: 2.1403  \n",
      "<<<iteration:[100/117] - total_loss: 2.7975  obj_loss: 0.0234  noobj_loss: 0.0265  bbox_loss: 0.1229  cls_loss: 2.1464  \n",
      "\n",
      "epoch:15/100 - Train Loss: 4.3098, Val Loss: 2.9408\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 6.6257  obj_loss: 0.0475  noobj_loss: 0.0210  bbox_loss: 0.8107  cls_loss: 2.5140  \n",
      "<<<iteration:[40/117] - total_loss: 4.7288  obj_loss: 0.0367  noobj_loss: 0.0419  bbox_loss: 0.4008  cls_loss: 2.6674  \n",
      "<<<iteration:[60/117] - total_loss: 3.1465  obj_loss: 0.0325  noobj_loss: 0.0173  bbox_loss: 0.1375  cls_loss: 2.4176  \n",
      "<<<iteration:[80/117] - total_loss: 3.8947  obj_loss: 0.0536  noobj_loss: 0.0259  bbox_loss: 0.2900  cls_loss: 2.3781  \n",
      "<<<iteration:[100/117] - total_loss: 5.4570  obj_loss: 0.0259  noobj_loss: 0.0429  bbox_loss: 0.6663  cls_loss: 2.0782  \n",
      "\n",
      "epoch:16/100 - Train Loss: 4.4633, Val Loss: 2.9859\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9326  obj_loss: 0.0408  noobj_loss: 0.0166  bbox_loss: 0.1230  cls_loss: 2.2684  \n",
      "<<<iteration:[40/117] - total_loss: 4.8087  obj_loss: 0.0295  noobj_loss: 0.0208  bbox_loss: 0.4877  cls_loss: 2.3300  \n",
      "<<<iteration:[60/117] - total_loss: 3.9952  obj_loss: 0.0312  noobj_loss: 0.0233  bbox_loss: 0.2623  cls_loss: 2.6408  \n",
      "<<<iteration:[80/117] - total_loss: 8.3670  obj_loss: 0.0361  noobj_loss: 0.0284  bbox_loss: 1.2044  cls_loss: 2.2948  \n",
      "<<<iteration:[100/117] - total_loss: 2.6296  obj_loss: 0.0347  noobj_loss: 0.0198  bbox_loss: 0.1155  cls_loss: 2.0073  \n",
      "\n",
      "epoch:17/100 - Train Loss: 6.2190, Val Loss: 3.5335\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.5822  obj_loss: 0.0247  noobj_loss: 0.0237  bbox_loss: 0.2457  cls_loss: 2.3171  \n",
      "<<<iteration:[40/117] - total_loss: 3.1985  obj_loss: 0.0605  noobj_loss: 0.0244  bbox_loss: 0.1462  cls_loss: 2.3948  \n",
      "<<<iteration:[60/117] - total_loss: 3.5703  obj_loss: 0.0377  noobj_loss: 0.0288  bbox_loss: 0.1723  cls_loss: 2.6566  \n",
      "<<<iteration:[80/117] - total_loss: 6.7382  obj_loss: 0.0267  noobj_loss: 0.0337  bbox_loss: 0.9401  cls_loss: 1.9939  \n",
      "<<<iteration:[100/117] - total_loss: 2.6508  obj_loss: 0.0470  noobj_loss: 0.0246  bbox_loss: 0.0946  cls_loss: 2.1185  \n",
      "\n",
      "epoch:18/100 - Train Loss: 4.6002, Val Loss: 3.4070\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 5.0739  obj_loss: 0.0384  noobj_loss: 0.0348  bbox_loss: 0.4585  cls_loss: 2.7254  \n",
      "<<<iteration:[40/117] - total_loss: 4.1771  obj_loss: 0.0529  noobj_loss: 0.0299  bbox_loss: 0.3267  cls_loss: 2.4758  \n",
      "<<<iteration:[60/117] - total_loss: 2.9617  obj_loss: 0.0667  noobj_loss: 0.0395  bbox_loss: 0.1280  cls_loss: 2.2353  \n",
      "<<<iteration:[80/117] - total_loss: 3.0722  obj_loss: 0.0347  noobj_loss: 0.0261  bbox_loss: 0.1509  cls_loss: 2.2697  \n",
      "<<<iteration:[100/117] - total_loss: 3.0817  obj_loss: 0.0299  noobj_loss: 0.0243  bbox_loss: 0.1438  cls_loss: 2.3207  \n",
      "\n",
      "epoch:19/100 - Train Loss: 3.5621, Val Loss: 3.2520\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.4217  obj_loss: 0.0310  noobj_loss: 0.0235  bbox_loss: 0.2861  cls_loss: 1.9485  \n",
      "<<<iteration:[40/117] - total_loss: 4.9123  obj_loss: 0.0468  noobj_loss: 0.0242  bbox_loss: 0.3687  cls_loss: 3.0101  \n",
      "<<<iteration:[60/117] - total_loss: 3.0963  obj_loss: 0.0376  noobj_loss: 0.0295  bbox_loss: 0.1257  cls_loss: 2.4155  \n",
      "<<<iteration:[80/117] - total_loss: 3.1865  obj_loss: 0.0522  noobj_loss: 0.0181  bbox_loss: 0.1960  cls_loss: 2.1453  \n",
      "<<<iteration:[100/117] - total_loss: 2.8160  obj_loss: 0.0418  noobj_loss: 0.0173  bbox_loss: 0.1232  cls_loss: 2.1497  \n",
      "\n",
      "epoch:20/100 - Train Loss: 3.3975, Val Loss: 2.8486\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7432  obj_loss: 0.0910  noobj_loss: 0.0313  bbox_loss: 0.0996  cls_loss: 2.1385  \n",
      "<<<iteration:[40/117] - total_loss: 3.8854  obj_loss: 0.0563  noobj_loss: 0.0225  bbox_loss: 0.2007  cls_loss: 2.8143  \n",
      "<<<iteration:[60/117] - total_loss: 3.8086  obj_loss: 0.0370  noobj_loss: 0.0197  bbox_loss: 0.2694  cls_loss: 2.4147  \n",
      "<<<iteration:[80/117] - total_loss: 3.1244  obj_loss: 0.0497  noobj_loss: 0.0233  bbox_loss: 0.1372  cls_loss: 2.3768  \n",
      "<<<iteration:[100/117] - total_loss: 3.0346  obj_loss: 0.0554  noobj_loss: 0.0222  bbox_loss: 0.0969  cls_loss: 2.4836  \n",
      "\n",
      "epoch:21/100 - Train Loss: 3.2468, Val Loss: 5.2598\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.8539  obj_loss: 0.0621  noobj_loss: 0.0239  bbox_loss: 0.2431  cls_loss: 2.5642  \n",
      "<<<iteration:[40/117] - total_loss: 3.1671  obj_loss: 0.0405  noobj_loss: 0.0202  bbox_loss: 0.1543  cls_loss: 2.3450  \n",
      "<<<iteration:[60/117] - total_loss: 3.1953  obj_loss: 0.0571  noobj_loss: 0.0224  bbox_loss: 0.1832  cls_loss: 2.2108  \n",
      "<<<iteration:[80/117] - total_loss: 3.5247  obj_loss: 0.0483  noobj_loss: 0.0300  bbox_loss: 0.2101  cls_loss: 2.4111  \n",
      "<<<iteration:[100/117] - total_loss: 2.6669  obj_loss: 0.0318  noobj_loss: 0.0196  bbox_loss: 0.0931  cls_loss: 2.1599  \n",
      "\n",
      "epoch:22/100 - Train Loss: 3.2992, Val Loss: 4.6762\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7949  obj_loss: 0.0318  noobj_loss: 0.0168  bbox_loss: 0.1448  cls_loss: 2.0307  \n",
      "<<<iteration:[40/117] - total_loss: 3.0708  obj_loss: 0.0351  noobj_loss: 0.0244  bbox_loss: 0.1646  cls_loss: 2.2005  \n",
      "<<<iteration:[60/117] - total_loss: 3.2859  obj_loss: 0.0333  noobj_loss: 0.0259  bbox_loss: 0.1672  cls_loss: 2.4036  \n",
      "<<<iteration:[80/117] - total_loss: 3.5887  obj_loss: 0.0642  noobj_loss: 0.0215  bbox_loss: 0.1932  cls_loss: 2.5480  \n",
      "<<<iteration:[100/117] - total_loss: 3.3482  obj_loss: 0.0293  noobj_loss: 0.0285  bbox_loss: 0.1887  cls_loss: 2.3614  \n",
      "\n",
      "epoch:23/100 - Train Loss: 3.2047, Val Loss: 3.2251\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1849  obj_loss: 0.0716  noobj_loss: 0.0242  bbox_loss: 0.0984  cls_loss: 2.6090  \n",
      "<<<iteration:[40/117] - total_loss: 2.7723  obj_loss: 0.0363  noobj_loss: 0.0198  bbox_loss: 0.0966  cls_loss: 2.2431  \n",
      "<<<iteration:[60/117] - total_loss: 2.1129  obj_loss: 0.0344  noobj_loss: 0.0188  bbox_loss: 0.0585  cls_loss: 1.7765  \n",
      "<<<iteration:[80/117] - total_loss: 16.6224  obj_loss: 0.0341  noobj_loss: 0.0430  bbox_loss: 2.7892  cls_loss: 2.6208  \n",
      "<<<iteration:[100/117] - total_loss: 8.8615  obj_loss: 0.0242  noobj_loss: 0.0175  bbox_loss: 1.2663  cls_loss: 2.4970  \n",
      "\n",
      "epoch:24/100 - Train Loss: 6.1811, Val Loss: 3.3445\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0843  obj_loss: 0.0290  noobj_loss: 0.0327  bbox_loss: 0.1679  cls_loss: 2.1992  \n",
      "<<<iteration:[40/117] - total_loss: 3.7522  obj_loss: 0.0564  noobj_loss: 0.0210  bbox_loss: 0.2009  cls_loss: 2.6808  \n",
      "<<<iteration:[60/117] - total_loss: 2.5832  obj_loss: 0.0419  noobj_loss: 0.0203  bbox_loss: 0.0989  cls_loss: 2.0368  \n",
      "<<<iteration:[80/117] - total_loss: 3.1486  obj_loss: 0.0372  noobj_loss: 0.0226  bbox_loss: 0.1111  cls_loss: 2.5446  \n",
      "<<<iteration:[100/117] - total_loss: 2.9510  obj_loss: 0.0925  noobj_loss: 0.0223  bbox_loss: 0.0988  cls_loss: 2.3534  \n",
      "\n",
      "epoch:25/100 - Train Loss: 3.0684, Val Loss: 3.1289\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7439  obj_loss: 0.0531  noobj_loss: 0.0213  bbox_loss: 0.1049  cls_loss: 2.1558  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[40/117] - total_loss: 4.6165  obj_loss: 0.0688  noobj_loss: 0.0261  bbox_loss: 0.3518  cls_loss: 2.7757  \n",
      "<<<iteration:[60/117] - total_loss: 3.3568  obj_loss: 0.0505  noobj_loss: 0.0192  bbox_loss: 0.1737  cls_loss: 2.4282  \n",
      "<<<iteration:[80/117] - total_loss: 2.3584  obj_loss: 0.0413  noobj_loss: 0.0219  bbox_loss: 0.1185  cls_loss: 1.7135  \n",
      "<<<iteration:[100/117] - total_loss: 4.3806  obj_loss: 0.0565  noobj_loss: 0.0172  bbox_loss: 0.3520  cls_loss: 2.5554  \n",
      "\n",
      "epoch:26/100 - Train Loss: 3.5284, Val Loss: 5.6974\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8523  obj_loss: 0.0355  noobj_loss: 0.0246  bbox_loss: 0.1117  cls_loss: 2.2462  \n",
      "<<<iteration:[40/117] - total_loss: 3.0272  obj_loss: 0.0371  noobj_loss: 0.0177  bbox_loss: 0.1172  cls_loss: 2.3955  \n",
      "<<<iteration:[60/117] - total_loss: 2.8808  obj_loss: 0.0590  noobj_loss: 0.0204  bbox_loss: 0.0873  cls_loss: 2.3750  \n",
      "<<<iteration:[80/117] - total_loss: 2.7042  obj_loss: 0.0345  noobj_loss: 0.0152  bbox_loss: 0.0888  cls_loss: 2.2182  \n",
      "<<<iteration:[100/117] - total_loss: 3.1366  obj_loss: 0.0412  noobj_loss: 0.0211  bbox_loss: 0.1582  cls_loss: 2.2937  \n",
      "\n",
      "epoch:27/100 - Train Loss: 2.9311, Val Loss: 2.8926\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.2012  obj_loss: 0.0401  noobj_loss: 0.0223  bbox_loss: 0.1176  cls_loss: 2.5622  \n",
      "<<<iteration:[40/117] - total_loss: 2.9822  obj_loss: 0.0752  noobj_loss: 0.0183  bbox_loss: 0.1484  cls_loss: 2.1557  \n",
      "<<<iteration:[60/117] - total_loss: 2.6452  obj_loss: 0.0278  noobj_loss: 0.0195  bbox_loss: 0.0820  cls_loss: 2.1978  \n",
      "<<<iteration:[80/117] - total_loss: 2.7120  obj_loss: 0.0844  noobj_loss: 0.0231  bbox_loss: 0.0821  cls_loss: 2.2056  \n",
      "<<<iteration:[100/117] - total_loss: 3.2749  obj_loss: 0.0455  noobj_loss: 0.0173  bbox_loss: 0.1053  cls_loss: 2.6941  \n",
      "\n",
      "epoch:28/100 - Train Loss: 2.9585, Val Loss: 3.4548\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.2790  obj_loss: 0.0752  noobj_loss: 0.0175  bbox_loss: 0.1128  cls_loss: 2.6309  \n",
      "<<<iteration:[40/117] - total_loss: 3.1716  obj_loss: 0.0466  noobj_loss: 0.0207  bbox_loss: 0.1691  cls_loss: 2.2691  \n",
      "<<<iteration:[60/117] - total_loss: 2.6464  obj_loss: 0.0531  noobj_loss: 0.0211  bbox_loss: 0.0767  cls_loss: 2.1993  \n",
      "<<<iteration:[80/117] - total_loss: 3.1784  obj_loss: 0.0330  noobj_loss: 0.0223  bbox_loss: 0.1499  cls_loss: 2.3848  \n",
      "<<<iteration:[100/117] - total_loss: 2.5638  obj_loss: 0.0506  noobj_loss: 0.0147  bbox_loss: 0.0757  cls_loss: 2.1274  \n",
      "\n",
      "epoch:29/100 - Train Loss: 2.9448, Val Loss: 3.0590\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.7496  obj_loss: 0.0837  noobj_loss: 0.0219  bbox_loss: 0.2081  cls_loss: 2.6143  \n",
      "<<<iteration:[40/117] - total_loss: 3.0536  obj_loss: 0.0383  noobj_loss: 0.0178  bbox_loss: 0.1489  cls_loss: 2.2617  \n",
      "<<<iteration:[60/117] - total_loss: 2.7563  obj_loss: 0.0441  noobj_loss: 0.0163  bbox_loss: 0.1017  cls_loss: 2.1955  \n",
      "<<<iteration:[80/117] - total_loss: 2.5329  obj_loss: 0.0493  noobj_loss: 0.0219  bbox_loss: 0.0812  cls_loss: 2.0665  \n",
      "<<<iteration:[100/117] - total_loss: 3.1223  obj_loss: 0.0524  noobj_loss: 0.0162  bbox_loss: 0.0974  cls_loss: 2.5749  \n",
      "\n",
      "epoch:30/100 - Train Loss: 2.9728, Val Loss: 2.7979\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7780  obj_loss: 0.0626  noobj_loss: 0.0217  bbox_loss: 0.0860  cls_loss: 2.2745  \n",
      "<<<iteration:[40/117] - total_loss: 3.0546  obj_loss: 0.0479  noobj_loss: 0.0190  bbox_loss: 0.1074  cls_loss: 2.4603  \n",
      "<<<iteration:[60/117] - total_loss: 2.8847  obj_loss: 0.0578  noobj_loss: 0.0163  bbox_loss: 0.0947  cls_loss: 2.3454  \n",
      "<<<iteration:[80/117] - total_loss: 3.5240  obj_loss: 0.0521  noobj_loss: 0.0193  bbox_loss: 0.2011  cls_loss: 2.4567  \n",
      "<<<iteration:[100/117] - total_loss: 3.1243  obj_loss: 0.0376  noobj_loss: 0.0168  bbox_loss: 0.1204  cls_loss: 2.4763  \n",
      "\n",
      "epoch:31/100 - Train Loss: 2.9293, Val Loss: 3.0610\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.3761  obj_loss: 0.0415  noobj_loss: 0.0197  bbox_loss: 0.2724  cls_loss: 2.9627  \n",
      "<<<iteration:[40/117] - total_loss: 2.9864  obj_loss: 0.0443  noobj_loss: 0.0181  bbox_loss: 0.1306  cls_loss: 2.2800  \n",
      "<<<iteration:[60/117] - total_loss: 2.4318  obj_loss: 0.0862  noobj_loss: 0.0217  bbox_loss: 0.0789  cls_loss: 1.9403  \n",
      "<<<iteration:[80/117] - total_loss: 3.2142  obj_loss: 0.0855  noobj_loss: 0.0276  bbox_loss: 0.1615  cls_loss: 2.3074  \n",
      "<<<iteration:[100/117] - total_loss: 3.0415  obj_loss: 0.0719  noobj_loss: 0.0170  bbox_loss: 0.0870  cls_loss: 2.5259  \n",
      "\n",
      "epoch:32/100 - Train Loss: 3.0398, Val Loss: 2.7675\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.6074  obj_loss: 0.0464  noobj_loss: 0.0154  bbox_loss: 0.1349  cls_loss: 2.8790  \n",
      "<<<iteration:[40/117] - total_loss: 2.2444  obj_loss: 0.0555  noobj_loss: 0.0185  bbox_loss: 0.0624  cls_loss: 1.8675  \n",
      "<<<iteration:[60/117] - total_loss: 3.1820  obj_loss: 0.0752  noobj_loss: 0.0227  bbox_loss: 0.1051  cls_loss: 2.5697  \n",
      "<<<iteration:[80/117] - total_loss: 2.6989  obj_loss: 0.0528  noobj_loss: 0.0207  bbox_loss: 0.1187  cls_loss: 2.0424  \n",
      "<<<iteration:[100/117] - total_loss: 3.1032  obj_loss: 0.0434  noobj_loss: 0.0142  bbox_loss: 0.1083  cls_loss: 2.5114  \n",
      "\n",
      "epoch:33/100 - Train Loss: 2.9565, Val Loss: 4.3164\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.6849  obj_loss: 0.0559  noobj_loss: 0.0145  bbox_loss: 0.2365  cls_loss: 2.4392  \n",
      "<<<iteration:[40/117] - total_loss: 2.6272  obj_loss: 0.0402  noobj_loss: 0.0278  bbox_loss: 0.1048  cls_loss: 2.0492  \n",
      "<<<iteration:[60/117] - total_loss: 2.4770  obj_loss: 0.0432  noobj_loss: 0.0246  bbox_loss: 0.0970  cls_loss: 1.9364  \n",
      "<<<iteration:[80/117] - total_loss: 3.1531  obj_loss: 0.0558  noobj_loss: 0.0170  bbox_loss: 0.0894  cls_loss: 2.6417  \n",
      "<<<iteration:[100/117] - total_loss: 2.8624  obj_loss: 0.0659  noobj_loss: 0.0190  bbox_loss: 0.1019  cls_loss: 2.2775  \n",
      "\n",
      "epoch:34/100 - Train Loss: 2.9638, Val Loss: 3.0709\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0131  obj_loss: 0.0478  noobj_loss: 0.0170  bbox_loss: 0.0997  cls_loss: 2.4584  \n",
      "<<<iteration:[40/117] - total_loss: 2.9260  obj_loss: 0.0950  noobj_loss: 0.0225  bbox_loss: 0.0935  cls_loss: 2.3522  \n",
      "<<<iteration:[60/117] - total_loss: 2.5810  obj_loss: 0.0433  noobj_loss: 0.0153  bbox_loss: 0.0749  cls_loss: 2.1555  \n",
      "<<<iteration:[80/117] - total_loss: 2.5041  obj_loss: 0.0196  noobj_loss: 0.0183  bbox_loss: 0.0724  cls_loss: 2.1131  \n",
      "<<<iteration:[100/117] - total_loss: 3.0662  obj_loss: 0.0538  noobj_loss: 0.0239  bbox_loss: 0.1457  cls_loss: 2.2720  \n",
      "\n",
      "epoch:35/100 - Train Loss: 2.9214, Val Loss: 2.8807\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.0144  obj_loss: 0.0486  noobj_loss: 0.0196  bbox_loss: 0.2388  cls_loss: 2.7620  \n",
      "<<<iteration:[40/117] - total_loss: 3.5566  obj_loss: 0.0185  noobj_loss: 0.0160  bbox_loss: 0.2101  cls_loss: 2.4796  \n",
      "<<<iteration:[60/117] - total_loss: 2.7434  obj_loss: 0.0490  noobj_loss: 0.0150  bbox_loss: 0.0861  cls_loss: 2.2562  \n",
      "<<<iteration:[80/117] - total_loss: 2.9681  obj_loss: 0.0445  noobj_loss: 0.0179  bbox_loss: 0.0974  cls_loss: 2.4279  \n",
      "<<<iteration:[100/117] - total_loss: 2.5469  obj_loss: 0.0792  noobj_loss: 0.0176  bbox_loss: 0.0877  cls_loss: 2.0205  \n",
      "\n",
      "epoch:36/100 - Train Loss: 2.9773, Val Loss: 2.8172\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.2104  obj_loss: 0.0550  noobj_loss: 0.0150  bbox_loss: 0.1078  cls_loss: 2.6089  \n",
      "<<<iteration:[40/117] - total_loss: 2.4005  obj_loss: 0.0234  noobj_loss: 0.0170  bbox_loss: 0.0979  cls_loss: 1.8790  \n",
      "<<<iteration:[60/117] - total_loss: 2.6900  obj_loss: 0.0494  noobj_loss: 0.0220  bbox_loss: 0.0852  cls_loss: 2.2035  \n",
      "<<<iteration:[80/117] - total_loss: 2.8509  obj_loss: 0.0472  noobj_loss: 0.0201  bbox_loss: 0.0822  cls_loss: 2.3827  \n",
      "<<<iteration:[100/117] - total_loss: 3.6552  obj_loss: 0.0554  noobj_loss: 0.0175  bbox_loss: 0.2105  cls_loss: 2.5383  \n",
      "\n",
      "epoch:37/100 - Train Loss: 3.1730, Val Loss: 3.4848\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0162  obj_loss: 0.0631  noobj_loss: 0.0184  bbox_loss: 0.1197  cls_loss: 2.3455  \n",
      "<<<iteration:[40/117] - total_loss: 2.8499  obj_loss: 0.0385  noobj_loss: 0.0208  bbox_loss: 0.1038  cls_loss: 2.2821  \n",
      "<<<iteration:[60/117] - total_loss: 4.4706  obj_loss: 0.0821  noobj_loss: 0.0222  bbox_loss: 0.3868  cls_loss: 2.4434  \n",
      "<<<iteration:[80/117] - total_loss: 2.8603  obj_loss: 0.0345  noobj_loss: 0.0184  bbox_loss: 0.0950  cls_loss: 2.3415  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[100/117] - total_loss: 2.8980  obj_loss: 0.0605  noobj_loss: 0.0182  bbox_loss: 0.1003  cls_loss: 2.3270  \n",
      "\n",
      "epoch:38/100 - Train Loss: 3.2640, Val Loss: 3.1508\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7185  obj_loss: 0.0550  noobj_loss: 0.0183  bbox_loss: 0.1034  cls_loss: 2.1375  \n",
      "<<<iteration:[40/117] - total_loss: 2.9162  obj_loss: 0.0571  noobj_loss: 0.0190  bbox_loss: 0.1041  cls_loss: 2.3291  \n",
      "<<<iteration:[60/117] - total_loss: 2.7210  obj_loss: 0.0328  noobj_loss: 0.0176  bbox_loss: 0.0865  cls_loss: 2.2468  \n",
      "<<<iteration:[80/117] - total_loss: 4.6603  obj_loss: 0.0444  noobj_loss: 0.0180  bbox_loss: 0.4442  cls_loss: 2.3857  \n",
      "<<<iteration:[100/117] - total_loss: 6.0289  obj_loss: 0.0544  noobj_loss: 0.0174  bbox_loss: 0.7459  cls_loss: 2.2361  \n",
      "\n",
      "epoch:39/100 - Train Loss: 3.6605, Val Loss: 3.4128\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 5.9227  obj_loss: 0.0420  noobj_loss: 0.0171  bbox_loss: 0.7249  cls_loss: 2.2479  \n",
      "<<<iteration:[40/117] - total_loss: 2.5492  obj_loss: 0.0407  noobj_loss: 0.0208  bbox_loss: 0.1418  cls_loss: 1.7891  \n",
      "<<<iteration:[60/117] - total_loss: 3.6274  obj_loss: 0.0377  noobj_loss: 0.0137  bbox_loss: 0.1213  cls_loss: 2.9762  \n",
      "<<<iteration:[80/117] - total_loss: 6.0743  obj_loss: 0.0508  noobj_loss: 0.0168  bbox_loss: 0.6927  cls_loss: 2.5514  \n",
      "<<<iteration:[100/117] - total_loss: 2.2977  obj_loss: 0.0152  noobj_loss: 0.0229  bbox_loss: 0.0682  cls_loss: 1.9302  \n",
      "\n",
      "epoch:40/100 - Train Loss: 3.8557, Val Loss: 2.8454\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.2129  obj_loss: 0.0585  noobj_loss: 0.0220  bbox_loss: 0.0978  cls_loss: 2.6545  \n",
      "<<<iteration:[40/117] - total_loss: 2.9582  obj_loss: 0.0683  noobj_loss: 0.0247  bbox_loss: 0.1141  cls_loss: 2.3070  \n",
      "<<<iteration:[60/117] - total_loss: 2.3456  obj_loss: 0.0627  noobj_loss: 0.0186  bbox_loss: 0.0861  cls_loss: 1.8434  \n",
      "<<<iteration:[80/117] - total_loss: 3.0634  obj_loss: 0.0382  noobj_loss: 0.0202  bbox_loss: 0.1376  cls_loss: 2.3274  \n",
      "<<<iteration:[100/117] - total_loss: 2.1488  obj_loss: 0.0398  noobj_loss: 0.0162  bbox_loss: 0.0677  cls_loss: 1.7623  \n",
      "\n",
      "epoch:41/100 - Train Loss: 2.8211, Val Loss: 2.7738\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9652  obj_loss: 0.0765  noobj_loss: 0.0256  bbox_loss: 0.1566  cls_loss: 2.0927  \n",
      "<<<iteration:[40/117] - total_loss: 3.0910  obj_loss: 0.0778  noobj_loss: 0.0222  bbox_loss: 0.1115  cls_loss: 2.4447  \n",
      "<<<iteration:[60/117] - total_loss: 2.5191  obj_loss: 0.0659  noobj_loss: 0.0194  bbox_loss: 0.0686  cls_loss: 2.1004  \n",
      "<<<iteration:[80/117] - total_loss: 3.5131  obj_loss: 0.0374  noobj_loss: 0.0152  bbox_loss: 0.1333  cls_loss: 2.8018  \n",
      "<<<iteration:[100/117] - total_loss: 2.3110  obj_loss: 0.0477  noobj_loss: 0.0160  bbox_loss: 0.0772  cls_loss: 1.8691  \n",
      "\n",
      "epoch:42/100 - Train Loss: 2.8422, Val Loss: 2.9315\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.1251  obj_loss: 0.0451  noobj_loss: 0.0197  bbox_loss: 0.3667  cls_loss: 2.2364  \n",
      "<<<iteration:[40/117] - total_loss: 3.0216  obj_loss: 0.0360  noobj_loss: 0.0148  bbox_loss: 0.1325  cls_loss: 2.3158  \n",
      "<<<iteration:[60/117] - total_loss: 3.3638  obj_loss: 0.0775  noobj_loss: 0.0242  bbox_loss: 0.1545  cls_loss: 2.5017  \n",
      "<<<iteration:[80/117] - total_loss: 3.4913  obj_loss: 0.0559  noobj_loss: 0.0162  bbox_loss: 0.1177  cls_loss: 2.8387  \n",
      "<<<iteration:[100/117] - total_loss: 2.2636  obj_loss: 0.0414  noobj_loss: 0.0181  bbox_loss: 0.0935  cls_loss: 1.7458  \n",
      "\n",
      "epoch:43/100 - Train Loss: 3.3146, Val Loss: 2.8282\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9277  obj_loss: 0.0575  noobj_loss: 0.0191  bbox_loss: 0.0836  cls_loss: 2.4426  \n",
      "<<<iteration:[40/117] - total_loss: 2.9494  obj_loss: 0.0414  noobj_loss: 0.0164  bbox_loss: 0.1074  cls_loss: 2.3626  \n",
      "<<<iteration:[60/117] - total_loss: 2.2130  obj_loss: 0.0409  noobj_loss: 0.0196  bbox_loss: 0.0726  cls_loss: 1.7992  \n",
      "<<<iteration:[80/117] - total_loss: 3.6226  obj_loss: 0.0342  noobj_loss: 0.0174  bbox_loss: 0.2111  cls_loss: 2.5243  \n",
      "<<<iteration:[100/117] - total_loss: 2.7222  obj_loss: 0.0502  noobj_loss: 0.0204  bbox_loss: 0.0834  cls_loss: 2.2447  \n",
      "\n",
      "epoch:44/100 - Train Loss: 2.9637, Val Loss: 3.6550\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.5936  obj_loss: 0.0728  noobj_loss: 0.0230  bbox_loss: 0.0805  cls_loss: 2.1068  \n",
      "<<<iteration:[40/117] - total_loss: 3.8232  obj_loss: 0.0189  noobj_loss: 0.0237  bbox_loss: 0.3063  cls_loss: 2.2612  \n",
      "<<<iteration:[60/117] - total_loss: 3.5414  obj_loss: 0.0483  noobj_loss: 0.0144  bbox_loss: 0.1145  cls_loss: 2.9132  \n",
      "<<<iteration:[80/117] - total_loss: 2.8328  obj_loss: 0.0567  noobj_loss: 0.0192  bbox_loss: 0.1022  cls_loss: 2.2554  \n",
      "<<<iteration:[100/117] - total_loss: 2.1308  obj_loss: 0.0429  noobj_loss: 0.0158  bbox_loss: 0.0648  cls_loss: 1.7559  \n",
      "\n",
      "epoch:45/100 - Train Loss: 2.9323, Val Loss: 2.8126\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9919  obj_loss: 0.0385  noobj_loss: 0.0218  bbox_loss: 0.1361  cls_loss: 2.2620  \n",
      "<<<iteration:[40/117] - total_loss: 3.2652  obj_loss: 0.0365  noobj_loss: 0.0154  bbox_loss: 0.2162  cls_loss: 2.1403  \n",
      "<<<iteration:[60/117] - total_loss: 2.4659  obj_loss: 0.0302  noobj_loss: 0.0128  bbox_loss: 0.0821  cls_loss: 2.0189  \n",
      "<<<iteration:[80/117] - total_loss: 2.8413  obj_loss: 0.0739  noobj_loss: 0.0272  bbox_loss: 0.0807  cls_loss: 2.3505  \n",
      "<<<iteration:[100/117] - total_loss: 2.8209  obj_loss: 0.0498  noobj_loss: 0.0221  bbox_loss: 0.0966  cls_loss: 2.2770  \n",
      "\n",
      "epoch:46/100 - Train Loss: 2.8677, Val Loss: 2.9408\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.6288  obj_loss: 0.0657  noobj_loss: 0.0188  bbox_loss: 0.0718  cls_loss: 2.1948  \n",
      "<<<iteration:[40/117] - total_loss: 3.3155  obj_loss: 0.0523  noobj_loss: 0.0168  bbox_loss: 0.1224  cls_loss: 2.6427  \n",
      "<<<iteration:[60/117] - total_loss: 2.8518  obj_loss: 0.0395  noobj_loss: 0.0162  bbox_loss: 0.0765  cls_loss: 2.4216  \n",
      "<<<iteration:[80/117] - total_loss: 2.8531  obj_loss: 0.0811  noobj_loss: 0.0170  bbox_loss: 0.0834  cls_loss: 2.3465  \n",
      "<<<iteration:[100/117] - total_loss: 2.2549  obj_loss: 0.0837  noobj_loss: 0.0211  bbox_loss: 0.0646  cls_loss: 1.8377  \n",
      "\n",
      "epoch:47/100 - Train Loss: 2.7832, Val Loss: 3.4567\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8160  obj_loss: 0.0592  noobj_loss: 0.0291  bbox_loss: 0.1264  cls_loss: 2.1100  \n",
      "<<<iteration:[40/117] - total_loss: 2.5201  obj_loss: 0.0202  noobj_loss: 0.0180  bbox_loss: 0.0742  cls_loss: 2.1199  \n",
      "<<<iteration:[60/117] - total_loss: 3.1425  obj_loss: 0.1008  noobj_loss: 0.0246  bbox_loss: 0.1026  cls_loss: 2.5164  \n",
      "<<<iteration:[80/117] - total_loss: 2.8272  obj_loss: 0.0579  noobj_loss: 0.0195  bbox_loss: 0.0808  cls_loss: 2.3554  \n",
      "<<<iteration:[100/117] - total_loss: 3.0594  obj_loss: 0.0256  noobj_loss: 0.0157  bbox_loss: 0.1372  cls_loss: 2.3403  \n",
      "\n",
      "epoch:48/100 - Train Loss: 2.7665, Val Loss: 2.9718\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.2436  obj_loss: 0.0415  noobj_loss: 0.0273  bbox_loss: 0.0814  cls_loss: 1.7814  \n",
      "<<<iteration:[40/117] - total_loss: 2.9415  obj_loss: 0.0666  noobj_loss: 0.0168  bbox_loss: 0.1133  cls_loss: 2.2999  \n",
      "<<<iteration:[60/117] - total_loss: 2.6760  obj_loss: 0.0500  noobj_loss: 0.0189  bbox_loss: 0.1095  cls_loss: 2.0692  \n",
      "<<<iteration:[80/117] - total_loss: 2.7415  obj_loss: 0.0572  noobj_loss: 0.0199  bbox_loss: 0.1054  cls_loss: 2.1473  \n",
      "<<<iteration:[100/117] - total_loss: 2.7013  obj_loss: 0.0588  noobj_loss: 0.0167  bbox_loss: 0.0586  cls_loss: 2.3414  \n",
      "\n",
      "epoch:49/100 - Train Loss: 2.7462, Val Loss: 3.4770\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.3695  obj_loss: 0.0391  noobj_loss: 0.0205  bbox_loss: 0.0637  cls_loss: 2.0016  \n",
      "<<<iteration:[40/117] - total_loss: 2.4880  obj_loss: 0.0560  noobj_loss: 0.0141  bbox_loss: 0.0791  cls_loss: 2.0293  \n",
      "<<<iteration:[60/117] - total_loss: 2.3988  obj_loss: 0.0605  noobj_loss: 0.0206  bbox_loss: 0.0793  cls_loss: 1.9315  \n",
      "<<<iteration:[80/117] - total_loss: 3.4257  obj_loss: 0.0560  noobj_loss: 0.0257  bbox_loss: 0.1646  cls_loss: 2.5340  \n",
      "<<<iteration:[100/117] - total_loss: 3.0916  obj_loss: 0.0446  noobj_loss: 0.0172  bbox_loss: 0.1646  cls_loss: 2.2153  \n",
      "\n",
      "epoch:50/100 - Train Loss: 2.8069, Val Loss: 2.9813\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.2151  obj_loss: 0.0445  noobj_loss: 0.0147  bbox_loss: 0.0838  cls_loss: 1.7443  \n",
      "<<<iteration:[40/117] - total_loss: 2.7696  obj_loss: 0.0626  noobj_loss: 0.0245  bbox_loss: 0.1270  cls_loss: 2.0600  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[60/117] - total_loss: 3.0233  obj_loss: 0.0632  noobj_loss: 0.0208  bbox_loss: 0.0789  cls_loss: 2.5554  \n",
      "<<<iteration:[80/117] - total_loss: 2.7194  obj_loss: 0.0451  noobj_loss: 0.0212  bbox_loss: 0.0918  cls_loss: 2.2046  \n",
      "<<<iteration:[100/117] - total_loss: 3.2279  obj_loss: 0.0275  noobj_loss: 0.0179  bbox_loss: 0.0894  cls_loss: 2.7447  \n",
      "\n",
      "epoch:51/100 - Train Loss: 2.7606, Val Loss: 3.0774\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9690  obj_loss: 0.0573  noobj_loss: 0.0169  bbox_loss: 0.1067  cls_loss: 2.3698  \n",
      "<<<iteration:[40/117] - total_loss: 2.6636  obj_loss: 0.0725  noobj_loss: 0.0148  bbox_loss: 0.1514  cls_loss: 1.8266  \n",
      "<<<iteration:[60/117] - total_loss: 2.8389  obj_loss: 0.0453  noobj_loss: 0.0234  bbox_loss: 0.0670  cls_loss: 2.4469  \n",
      "<<<iteration:[80/117] - total_loss: 2.3365  obj_loss: 0.0562  noobj_loss: 0.0161  bbox_loss: 0.0816  cls_loss: 1.8644  \n",
      "<<<iteration:[100/117] - total_loss: 3.0238  obj_loss: 0.0287  noobj_loss: 0.0219  bbox_loss: 0.1463  cls_loss: 2.2528  \n",
      "\n",
      "epoch:52/100 - Train Loss: 2.7934, Val Loss: 3.4658\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.0240  obj_loss: 0.0518  noobj_loss: 0.0215  bbox_loss: 0.3409  cls_loss: 2.2570  \n",
      "<<<iteration:[40/117] - total_loss: 2.6406  obj_loss: 0.0284  noobj_loss: 0.0154  bbox_loss: 0.0831  cls_loss: 2.1890  \n",
      "<<<iteration:[60/117] - total_loss: 2.6936  obj_loss: 0.0756  noobj_loss: 0.0199  bbox_loss: 0.0785  cls_loss: 2.2154  \n",
      "<<<iteration:[80/117] - total_loss: 3.0735  obj_loss: 0.0802  noobj_loss: 0.0233  bbox_loss: 0.0977  cls_loss: 2.4930  \n",
      "<<<iteration:[100/117] - total_loss: 2.9012  obj_loss: 0.0563  noobj_loss: 0.0233  bbox_loss: 0.1141  cls_loss: 2.2629  \n",
      "\n",
      "epoch:53/100 - Train Loss: 3.0240, Val Loss: 3.0932\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.4275  obj_loss: 0.0705  noobj_loss: 0.0184  bbox_loss: 0.1850  cls_loss: 2.4228  \n",
      "<<<iteration:[40/117] - total_loss: 2.2267  obj_loss: 0.0493  noobj_loss: 0.0259  bbox_loss: 0.0881  cls_loss: 1.7238  \n",
      "<<<iteration:[60/117] - total_loss: 3.0667  obj_loss: 0.0501  noobj_loss: 0.0167  bbox_loss: 0.0901  cls_loss: 2.5577  \n",
      "<<<iteration:[80/117] - total_loss: 2.9319  obj_loss: 0.0385  noobj_loss: 0.0201  bbox_loss: 0.0914  cls_loss: 2.4264  \n",
      "<<<iteration:[100/117] - total_loss: 2.6179  obj_loss: 0.0581  noobj_loss: 0.0142  bbox_loss: 0.0614  cls_loss: 2.2456  \n",
      "\n",
      "epoch:54/100 - Train Loss: 2.7577, Val Loss: 2.9509\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.6896  obj_loss: 0.0336  noobj_loss: 0.0209  bbox_loss: 0.0939  cls_loss: 2.1762  \n",
      "<<<iteration:[40/117] - total_loss: 2.6811  obj_loss: 0.0737  noobj_loss: 0.0175  bbox_loss: 0.0801  cls_loss: 2.1983  \n",
      "<<<iteration:[60/117] - total_loss: 4.5345  obj_loss: 0.0281  noobj_loss: 0.0246  bbox_loss: 0.4342  cls_loss: 2.3232  \n",
      "<<<iteration:[80/117] - total_loss: 2.7277  obj_loss: 0.0807  noobj_loss: 0.0200  bbox_loss: 0.1564  cls_loss: 1.8551  \n",
      "<<<iteration:[100/117] - total_loss: 5.2168  obj_loss: 0.0384  noobj_loss: 0.0229  bbox_loss: 0.5439  cls_loss: 2.4476  \n",
      "\n",
      "epoch:55/100 - Train Loss: 3.6305, Val Loss: 3.4372\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.3335  obj_loss: 0.0472  noobj_loss: 0.0228  bbox_loss: 0.1928  cls_loss: 2.3109  \n",
      "<<<iteration:[40/117] - total_loss: 3.1513  obj_loss: 0.0412  noobj_loss: 0.0136  bbox_loss: 0.1144  cls_loss: 2.5314  \n",
      "<<<iteration:[60/117] - total_loss: 2.4452  obj_loss: 0.0578  noobj_loss: 0.0182  bbox_loss: 0.0733  cls_loss: 2.0116  \n",
      "<<<iteration:[80/117] - total_loss: 3.2836  obj_loss: 0.0423  noobj_loss: 0.0223  bbox_loss: 0.1131  cls_loss: 2.6647  \n",
      "<<<iteration:[100/117] - total_loss: 3.4396  obj_loss: 0.0367  noobj_loss: 0.0180  bbox_loss: 0.2859  cls_loss: 1.9645  \n",
      "\n",
      "epoch:56/100 - Train Loss: 3.1038, Val Loss: 2.7753\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.7329  obj_loss: 0.0622  noobj_loss: 0.0242  bbox_loss: 0.2051  cls_loss: 2.6331  \n",
      "<<<iteration:[40/117] - total_loss: 2.9741  obj_loss: 0.0765  noobj_loss: 0.0167  bbox_loss: 0.0914  cls_loss: 2.4320  \n",
      "<<<iteration:[60/117] - total_loss: 2.9985  obj_loss: 0.0495  noobj_loss: 0.0207  bbox_loss: 0.1909  cls_loss: 1.9840  \n",
      "<<<iteration:[80/117] - total_loss: 3.0116  obj_loss: 0.0430  noobj_loss: 0.0166  bbox_loss: 0.0950  cls_loss: 2.4854  \n",
      "<<<iteration:[100/117] - total_loss: 2.4525  obj_loss: 0.0421  noobj_loss: 0.0155  bbox_loss: 0.0885  cls_loss: 1.9603  \n",
      "\n",
      "epoch:57/100 - Train Loss: 2.9390, Val Loss: 3.2233\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9434  obj_loss: 0.0526  noobj_loss: 0.0153  bbox_loss: 0.0823  cls_loss: 2.4715  \n",
      "<<<iteration:[40/117] - total_loss: 3.0366  obj_loss: 0.0950  noobj_loss: 0.0221  bbox_loss: 0.1232  cls_loss: 2.3147  \n",
      "<<<iteration:[60/117] - total_loss: 3.2247  obj_loss: 0.0443  noobj_loss: 0.0203  bbox_loss: 0.0956  cls_loss: 2.6922  \n",
      "<<<iteration:[80/117] - total_loss: 2.7093  obj_loss: 0.0339  noobj_loss: 0.0166  bbox_loss: 0.0955  cls_loss: 2.1897  \n",
      "<<<iteration:[100/117] - total_loss: 2.0288  obj_loss: 0.0247  noobj_loss: 0.0148  bbox_loss: 0.0565  cls_loss: 1.7142  \n",
      "\n",
      "epoch:58/100 - Train Loss: 2.7568, Val Loss: 3.5128\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1050  obj_loss: 0.0494  noobj_loss: 0.0171  bbox_loss: 0.1850  cls_loss: 2.1220  \n",
      "<<<iteration:[40/117] - total_loss: 2.4861  obj_loss: 0.0806  noobj_loss: 0.0180  bbox_loss: 0.0889  cls_loss: 1.9522  \n",
      "<<<iteration:[60/117] - total_loss: 3.5526  obj_loss: 0.0522  noobj_loss: 0.0200  bbox_loss: 0.0981  cls_loss: 3.0001  \n",
      "<<<iteration:[80/117] - total_loss: 2.2920  obj_loss: 0.0492  noobj_loss: 0.0191  bbox_loss: 0.0652  cls_loss: 1.9071  \n",
      "<<<iteration:[100/117] - total_loss: 2.6457  obj_loss: 0.0351  noobj_loss: 0.0205  bbox_loss: 0.1198  cls_loss: 2.0012  \n",
      "\n",
      "epoch:59/100 - Train Loss: 2.7763, Val Loss: 2.9071\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8630  obj_loss: 0.0588  noobj_loss: 0.0205  bbox_loss: 0.1433  cls_loss: 2.0775  \n",
      "<<<iteration:[40/117] - total_loss: 3.0326  obj_loss: 0.0391  noobj_loss: 0.0177  bbox_loss: 0.1719  cls_loss: 2.1249  \n",
      "<<<iteration:[60/117] - total_loss: 2.4796  obj_loss: 0.0644  noobj_loss: 0.0154  bbox_loss: 0.0655  cls_loss: 2.0798  \n",
      "<<<iteration:[80/117] - total_loss: 3.0596  obj_loss: 0.0764  noobj_loss: 0.0199  bbox_loss: 0.1407  cls_loss: 2.2700  \n",
      "<<<iteration:[100/117] - total_loss: 2.3415  obj_loss: 0.0205  noobj_loss: 0.0190  bbox_loss: 0.0546  cls_loss: 2.0387  \n",
      "\n",
      "epoch:60/100 - Train Loss: 2.7945, Val Loss: 3.0080\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.4431  obj_loss: 0.0310  noobj_loss: 0.0184  bbox_loss: 0.1147  cls_loss: 2.8295  \n",
      "<<<iteration:[40/117] - total_loss: 3.0365  obj_loss: 0.0721  noobj_loss: 0.0138  bbox_loss: 0.1004  cls_loss: 2.4553  \n",
      "<<<iteration:[60/117] - total_loss: 2.7502  obj_loss: 0.0814  noobj_loss: 0.0184  bbox_loss: 0.0904  cls_loss: 2.2078  \n",
      "<<<iteration:[80/117] - total_loss: 2.3125  obj_loss: 0.0916  noobj_loss: 0.0226  bbox_loss: 0.0856  cls_loss: 1.7817  \n",
      "<<<iteration:[100/117] - total_loss: 2.3802  obj_loss: 0.0315  noobj_loss: 0.0236  bbox_loss: 0.0769  cls_loss: 1.9523  \n",
      "\n",
      "epoch:61/100 - Train Loss: 2.7042, Val Loss: 3.0061\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1717  obj_loss: 0.0659  noobj_loss: 0.0165  bbox_loss: 0.0931  cls_loss: 2.6319  \n",
      "<<<iteration:[40/117] - total_loss: 3.3522  obj_loss: 0.0564  noobj_loss: 0.0210  bbox_loss: 0.2097  cls_loss: 2.2369  \n",
      "<<<iteration:[60/117] - total_loss: 2.5869  obj_loss: 0.0578  noobj_loss: 0.0175  bbox_loss: 0.0836  cls_loss: 2.1027  \n",
      "<<<iteration:[80/117] - total_loss: 2.5516  obj_loss: 0.0298  noobj_loss: 0.0134  bbox_loss: 0.0680  cls_loss: 2.1751  \n",
      "<<<iteration:[100/117] - total_loss: 2.5780  obj_loss: 0.0340  noobj_loss: 0.0192  bbox_loss: 0.0769  cls_loss: 2.1498  \n",
      "\n",
      "epoch:62/100 - Train Loss: 2.7548, Val Loss: 2.9421\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7460  obj_loss: 0.0639  noobj_loss: 0.0198  bbox_loss: 0.0923  cls_loss: 2.2106  \n",
      "<<<iteration:[40/117] - total_loss: 2.6507  obj_loss: 0.0533  noobj_loss: 0.0179  bbox_loss: 0.0753  cls_loss: 2.2122  \n",
      "<<<iteration:[60/117] - total_loss: 3.2258  obj_loss: 0.0680  noobj_loss: 0.0190  bbox_loss: 0.1494  cls_loss: 2.4011  \n",
      "<<<iteration:[80/117] - total_loss: 2.6916  obj_loss: 0.0321  noobj_loss: 0.0235  bbox_loss: 0.1130  cls_loss: 2.0825  \n",
      "<<<iteration:[100/117] - total_loss: 2.0520  obj_loss: 0.0590  noobj_loss: 0.0223  bbox_loss: 0.0647  cls_loss: 1.6582  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch:63/100 - Train Loss: 2.6927, Val Loss: 2.8187\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.3863  obj_loss: 0.0988  noobj_loss: 0.0190  bbox_loss: 0.1643  cls_loss: 2.4566  \n",
      "<<<iteration:[40/117] - total_loss: 2.1267  obj_loss: 0.0466  noobj_loss: 0.0223  bbox_loss: 0.0679  cls_loss: 1.7294  \n",
      "<<<iteration:[60/117] - total_loss: 2.6702  obj_loss: 0.0230  noobj_loss: 0.0147  bbox_loss: 0.0652  cls_loss: 2.3139  \n",
      "<<<iteration:[80/117] - total_loss: 2.9018  obj_loss: 0.0803  noobj_loss: 0.0178  bbox_loss: 0.1251  cls_loss: 2.1869  \n",
      "<<<iteration:[100/117] - total_loss: 2.8251  obj_loss: 0.0719  noobj_loss: 0.0249  bbox_loss: 0.0877  cls_loss: 2.3022  \n",
      "\n",
      "epoch:64/100 - Train Loss: 2.7202, Val Loss: 2.9436\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.3048  obj_loss: 0.0497  noobj_loss: 0.0172  bbox_loss: 0.1409  cls_loss: 2.5420  \n",
      "<<<iteration:[40/117] - total_loss: 2.5271  obj_loss: 0.0327  noobj_loss: 0.0187  bbox_loss: 0.0835  cls_loss: 2.0676  \n",
      "<<<iteration:[60/117] - total_loss: 2.5160  obj_loss: 0.0454  noobj_loss: 0.0253  bbox_loss: 0.0686  cls_loss: 2.1152  \n",
      "<<<iteration:[80/117] - total_loss: 2.3078  obj_loss: 0.1078  noobj_loss: 0.0251  bbox_loss: 0.0934  cls_loss: 1.7202  \n",
      "<<<iteration:[100/117] - total_loss: 2.4065  obj_loss: 0.0241  noobj_loss: 0.0160  bbox_loss: 0.0690  cls_loss: 2.0294  \n",
      "\n",
      "epoch:65/100 - Train Loss: 2.7074, Val Loss: 2.9644\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0766  obj_loss: 0.0815  noobj_loss: 0.0276  bbox_loss: 0.1730  cls_loss: 2.1164  \n",
      "<<<iteration:[40/117] - total_loss: 3.1213  obj_loss: 0.0613  noobj_loss: 0.0201  bbox_loss: 0.1094  cls_loss: 2.5031  \n",
      "<<<iteration:[60/117] - total_loss: 2.4703  obj_loss: 0.0689  noobj_loss: 0.0241  bbox_loss: 0.0765  cls_loss: 2.0071  \n",
      "<<<iteration:[80/117] - total_loss: 2.4695  obj_loss: 0.0289  noobj_loss: 0.0153  bbox_loss: 0.0644  cls_loss: 2.1111  \n",
      "<<<iteration:[100/117] - total_loss: 2.4078  obj_loss: 0.0350  noobj_loss: 0.0178  bbox_loss: 0.0601  cls_loss: 2.0633  \n",
      "\n",
      "epoch:66/100 - Train Loss: 2.7129, Val Loss: 2.8902\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.9662  obj_loss: 0.0457  noobj_loss: 0.0201  bbox_loss: 0.1038  cls_loss: 2.3915  \n",
      "<<<iteration:[40/117] - total_loss: 2.5764  obj_loss: 0.1109  noobj_loss: 0.0290  bbox_loss: 0.0891  cls_loss: 2.0052  \n",
      "<<<iteration:[60/117] - total_loss: 3.0068  obj_loss: 0.0392  noobj_loss: 0.0202  bbox_loss: 0.0903  cls_loss: 2.5058  \n",
      "<<<iteration:[80/117] - total_loss: 3.0136  obj_loss: 0.0464  noobj_loss: 0.0204  bbox_loss: 0.1408  cls_loss: 2.2530  \n",
      "<<<iteration:[100/117] - total_loss: 2.5937  obj_loss: 0.0525  noobj_loss: 0.0200  bbox_loss: 0.0650  cls_loss: 2.2065  \n",
      "\n",
      "epoch:67/100 - Train Loss: 2.7371, Val Loss: 3.9757\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8836  obj_loss: 0.0475  noobj_loss: 0.0222  bbox_loss: 0.1009  cls_loss: 2.3204  \n",
      "<<<iteration:[40/117] - total_loss: 2.7033  obj_loss: 0.0714  noobj_loss: 0.0197  bbox_loss: 0.0829  cls_loss: 2.2075  \n",
      "<<<iteration:[60/117] - total_loss: 2.3421  obj_loss: 0.0797  noobj_loss: 0.0231  bbox_loss: 0.0870  cls_loss: 1.8157  \n",
      "<<<iteration:[80/117] - total_loss: 2.5339  obj_loss: 0.0426  noobj_loss: 0.0233  bbox_loss: 0.0923  cls_loss: 2.0182  \n",
      "<<<iteration:[100/117] - total_loss: 3.3664  obj_loss: 0.0447  noobj_loss: 0.0223  bbox_loss: 0.1637  cls_loss: 2.4918  \n",
      "\n",
      "epoch:68/100 - Train Loss: 2.7374, Val Loss: 3.0347\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1798  obj_loss: 0.0380  noobj_loss: 0.0227  bbox_loss: 0.1158  cls_loss: 2.5514  \n",
      "<<<iteration:[40/117] - total_loss: 2.1495  obj_loss: 0.0532  noobj_loss: 0.0183  bbox_loss: 0.0691  cls_loss: 1.7417  \n",
      "<<<iteration:[60/117] - total_loss: 2.6763  obj_loss: 0.0450  noobj_loss: 0.0188  bbox_loss: 0.0755  cls_loss: 2.2445  \n",
      "<<<iteration:[80/117] - total_loss: 3.8207  obj_loss: 0.0577  noobj_loss: 0.0177  bbox_loss: 0.2281  cls_loss: 2.6134  \n",
      "<<<iteration:[100/117] - total_loss: 2.5822  obj_loss: 0.0495  noobj_loss: 0.0556  bbox_loss: 0.0899  cls_loss: 2.0556  \n",
      "\n",
      "epoch:69/100 - Train Loss: 2.7643, Val Loss: 2.8955\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.5568  obj_loss: 0.1054  noobj_loss: 0.0305  bbox_loss: 0.0636  cls_loss: 2.1183  \n",
      "<<<iteration:[40/117] - total_loss: 2.9842  obj_loss: 0.0656  noobj_loss: 0.0188  bbox_loss: 0.0935  cls_loss: 2.4418  \n",
      "<<<iteration:[60/117] - total_loss: 2.4427  obj_loss: 0.0689  noobj_loss: 0.0200  bbox_loss: 0.0835  cls_loss: 1.9462  \n",
      "<<<iteration:[80/117] - total_loss: 3.0287  obj_loss: 0.0798  noobj_loss: 0.0262  bbox_loss: 0.1515  cls_loss: 2.1784  \n",
      "<<<iteration:[100/117] - total_loss: 3.1435  obj_loss: 0.0576  noobj_loss: 0.0228  bbox_loss: 0.1159  cls_loss: 2.4952  \n",
      "\n",
      "epoch:70/100 - Train Loss: 2.7136, Val Loss: 3.2981\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.6244  obj_loss: 0.0347  noobj_loss: 0.0128  bbox_loss: 0.1017  cls_loss: 2.0746  \n",
      "<<<iteration:[40/117] - total_loss: 2.6765  obj_loss: 0.0685  noobj_loss: 0.0163  bbox_loss: 0.0710  cls_loss: 2.2448  \n",
      "<<<iteration:[60/117] - total_loss: 2.9459  obj_loss: 0.0519  noobj_loss: 0.0163  bbox_loss: 0.1569  cls_loss: 2.1014  \n",
      "<<<iteration:[80/117] - total_loss: 2.2445  obj_loss: 0.0494  noobj_loss: 0.0262  bbox_loss: 0.0746  cls_loss: 1.8091  \n",
      "<<<iteration:[100/117] - total_loss: 3.2131  obj_loss: 0.1027  noobj_loss: 0.0248  bbox_loss: 0.1101  cls_loss: 2.5472  \n",
      "\n",
      "epoch:71/100 - Train Loss: 2.7157, Val Loss: 2.7517\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1812  obj_loss: 0.0352  noobj_loss: 0.0206  bbox_loss: 0.1430  cls_loss: 2.4207  \n",
      "<<<iteration:[40/117] - total_loss: 3.0702  obj_loss: 0.0654  noobj_loss: 0.0204  bbox_loss: 0.1303  cls_loss: 2.3430  \n",
      "<<<iteration:[60/117] - total_loss: 2.0174  obj_loss: 0.0368  noobj_loss: 0.0257  bbox_loss: 0.0546  cls_loss: 1.6946  \n",
      "<<<iteration:[80/117] - total_loss: 2.2947  obj_loss: 0.0490  noobj_loss: 0.0166  bbox_loss: 0.0830  cls_loss: 1.8224  \n",
      "<<<iteration:[100/117] - total_loss: 2.7221  obj_loss: 0.0456  noobj_loss: 0.0190  bbox_loss: 0.0922  cls_loss: 2.2057  \n",
      "\n",
      "epoch:72/100 - Train Loss: 2.6927, Val Loss: 2.8052\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8047  obj_loss: 0.0431  noobj_loss: 0.0215  bbox_loss: 0.0771  cls_loss: 2.3654  \n",
      "<<<iteration:[40/117] - total_loss: 2.7660  obj_loss: 0.0514  noobj_loss: 0.0190  bbox_loss: 0.1273  cls_loss: 2.0688  \n",
      "<<<iteration:[60/117] - total_loss: 2.5193  obj_loss: 0.0657  noobj_loss: 0.0207  bbox_loss: 0.0768  cls_loss: 2.0594  \n",
      "<<<iteration:[80/117] - total_loss: 2.1805  obj_loss: 0.0924  noobj_loss: 0.0211  bbox_loss: 0.0842  cls_loss: 1.6568  \n",
      "<<<iteration:[100/117] - total_loss: 3.4498  obj_loss: 0.0713  noobj_loss: 0.0238  bbox_loss: 0.1159  cls_loss: 2.7873  \n",
      "\n",
      "epoch:73/100 - Train Loss: 2.6385, Val Loss: 2.9205\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.6308  obj_loss: 0.0414  noobj_loss: 0.0248  bbox_loss: 0.0862  cls_loss: 2.1461  \n",
      "<<<iteration:[40/117] - total_loss: 3.3934  obj_loss: 0.0446  noobj_loss: 0.0184  bbox_loss: 0.1119  cls_loss: 2.7799  \n",
      "<<<iteration:[60/117] - total_loss: 2.3420  obj_loss: 0.0619  noobj_loss: 0.0255  bbox_loss: 0.0623  cls_loss: 1.9561  \n",
      "<<<iteration:[80/117] - total_loss: 2.5219  obj_loss: 0.0918  noobj_loss: 0.0210  bbox_loss: 0.1283  cls_loss: 1.7782  \n",
      "<<<iteration:[100/117] - total_loss: 2.4748  obj_loss: 0.0983  noobj_loss: 0.0300  bbox_loss: 0.0653  cls_loss: 2.0347  \n",
      "\n",
      "epoch:74/100 - Train Loss: 2.5878, Val Loss: 3.3417\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7223  obj_loss: 0.0483  noobj_loss: 0.0229  bbox_loss: 0.0662  cls_loss: 2.3314  \n",
      "<<<iteration:[40/117] - total_loss: 2.4528  obj_loss: 0.0474  noobj_loss: 0.0184  bbox_loss: 0.0748  cls_loss: 2.0221  \n",
      "<<<iteration:[60/117] - total_loss: 2.4974  obj_loss: 0.0317  noobj_loss: 0.0245  bbox_loss: 0.1066  cls_loss: 1.9205  \n",
      "<<<iteration:[80/117] - total_loss: 3.0332  obj_loss: 0.0404  noobj_loss: 0.0167  bbox_loss: 0.0893  cls_loss: 2.5379  \n",
      "<<<iteration:[100/117] - total_loss: 2.6238  obj_loss: 0.0653  noobj_loss: 0.0206  bbox_loss: 0.1067  cls_loss: 2.0146  \n",
      "\n",
      "epoch:75/100 - Train Loss: 2.6653, Val Loss: 3.1382\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.2736  obj_loss: 0.0775  noobj_loss: 0.0299  bbox_loss: 0.0630  cls_loss: 1.8659  \n",
      "<<<iteration:[40/117] - total_loss: 2.5769  obj_loss: 0.0628  noobj_loss: 0.0249  bbox_loss: 0.0881  cls_loss: 2.0612  \n",
      "<<<iteration:[60/117] - total_loss: 3.1941  obj_loss: 0.0669  noobj_loss: 0.0224  bbox_loss: 0.0836  cls_loss: 2.6981  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[80/117] - total_loss: 3.2362  obj_loss: 0.0612  noobj_loss: 0.0222  bbox_loss: 0.1669  cls_loss: 2.3294  \n",
      "<<<iteration:[100/117] - total_loss: 2.7340  obj_loss: 0.0611  noobj_loss: 0.0202  bbox_loss: 0.0816  cls_loss: 2.2546  \n",
      "\n",
      "epoch:76/100 - Train Loss: 2.6064, Val Loss: 3.0505\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.1120  obj_loss: 0.0521  noobj_loss: 0.0241  bbox_loss: 0.0659  cls_loss: 1.7182  \n",
      "<<<iteration:[40/117] - total_loss: 2.7303  obj_loss: 0.0897  noobj_loss: 0.0270  bbox_loss: 0.0773  cls_loss: 2.2405  \n",
      "<<<iteration:[60/117] - total_loss: 2.6587  obj_loss: 0.0572  noobj_loss: 0.0214  bbox_loss: 0.1073  cls_loss: 2.0540  \n",
      "<<<iteration:[80/117] - total_loss: 2.8480  obj_loss: 0.0496  noobj_loss: 0.0261  bbox_loss: 0.1370  cls_loss: 2.1001  \n",
      "<<<iteration:[100/117] - total_loss: 2.7800  obj_loss: 0.0555  noobj_loss: 0.0184  bbox_loss: 0.0986  cls_loss: 2.2225  \n",
      "\n",
      "epoch:77/100 - Train Loss: 2.6570, Val Loss: 3.4851\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0703  obj_loss: 0.0592  noobj_loss: 0.0205  bbox_loss: 0.1149  cls_loss: 2.4261  \n",
      "<<<iteration:[40/117] - total_loss: 2.4779  obj_loss: 0.0466  noobj_loss: 0.0233  bbox_loss: 0.0770  cls_loss: 2.0348  \n",
      "<<<iteration:[60/117] - total_loss: 2.4124  obj_loss: 0.0354  noobj_loss: 0.0210  bbox_loss: 0.0842  cls_loss: 1.9456  \n",
      "<<<iteration:[80/117] - total_loss: 2.8740  obj_loss: 0.0707  noobj_loss: 0.0211  bbox_loss: 0.0949  cls_loss: 2.3185  \n",
      "<<<iteration:[100/117] - total_loss: 2.7278  obj_loss: 0.0505  noobj_loss: 0.0238  bbox_loss: 0.1120  cls_loss: 2.1051  \n",
      "\n",
      "epoch:78/100 - Train Loss: 2.5936, Val Loss: 3.5558\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7737  obj_loss: 0.0621  noobj_loss: 0.0193  bbox_loss: 0.0696  cls_loss: 2.3539  \n",
      "<<<iteration:[40/117] - total_loss: 2.7594  obj_loss: 0.0352  noobj_loss: 0.0188  bbox_loss: 0.1638  cls_loss: 1.8956  \n",
      "<<<iteration:[60/117] - total_loss: 2.2544  obj_loss: 0.0415  noobj_loss: 0.0266  bbox_loss: 0.0750  cls_loss: 1.8245  \n",
      "<<<iteration:[80/117] - total_loss: 3.6446  obj_loss: 0.0472  noobj_loss: 0.0235  bbox_loss: 0.1836  cls_loss: 2.6677  \n",
      "<<<iteration:[100/117] - total_loss: 2.4727  obj_loss: 0.0548  noobj_loss: 0.0174  bbox_loss: 0.0804  cls_loss: 2.0070  \n",
      "\n",
      "epoch:79/100 - Train Loss: 2.6931, Val Loss: 2.7716\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.2917  obj_loss: 0.0413  noobj_loss: 0.0195  bbox_loss: 0.0669  cls_loss: 1.9061  \n",
      "<<<iteration:[40/117] - total_loss: 2.9483  obj_loss: 0.0607  noobj_loss: 0.0203  bbox_loss: 0.0829  cls_loss: 2.4629  \n",
      "<<<iteration:[60/117] - total_loss: 2.9175  obj_loss: 0.0493  noobj_loss: 0.0215  bbox_loss: 0.1378  cls_loss: 2.1683  \n",
      "<<<iteration:[80/117] - total_loss: 2.2560  obj_loss: 0.0368  noobj_loss: 0.0217  bbox_loss: 0.0856  cls_loss: 1.7803  \n",
      "<<<iteration:[100/117] - total_loss: 2.5957  obj_loss: 0.0529  noobj_loss: 0.0217  bbox_loss: 0.0797  cls_loss: 2.1333  \n",
      "\n",
      "epoch:80/100 - Train Loss: 2.5897, Val Loss: 2.9507\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.0369  obj_loss: 0.0496  noobj_loss: 0.0253  bbox_loss: 0.3525  cls_loss: 2.2120  \n",
      "<<<iteration:[40/117] - total_loss: 3.7750  obj_loss: 0.0834  noobj_loss: 0.0266  bbox_loss: 0.2825  cls_loss: 2.2655  \n",
      "<<<iteration:[60/117] - total_loss: 3.2411  obj_loss: 0.0514  noobj_loss: 0.0207  bbox_loss: 0.1371  cls_loss: 2.4941  \n",
      "<<<iteration:[80/117] - total_loss: 2.1161  obj_loss: 0.0485  noobj_loss: 0.0262  bbox_loss: 0.0716  cls_loss: 1.6963  \n",
      "<<<iteration:[100/117] - total_loss: 2.4855  obj_loss: 0.0257  noobj_loss: 0.0138  bbox_loss: 0.0659  cls_loss: 2.1233  \n",
      "\n",
      "epoch:81/100 - Train Loss: 3.0164, Val Loss: 3.2185\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.6709  obj_loss: 0.0560  noobj_loss: 0.0221  bbox_loss: 0.0962  cls_loss: 2.1228  \n",
      "<<<iteration:[40/117] - total_loss: 1.9713  obj_loss: 0.0593  noobj_loss: 0.0225  bbox_loss: 0.0605  cls_loss: 1.5984  \n",
      "<<<iteration:[60/117] - total_loss: 3.1755  obj_loss: 0.0516  noobj_loss: 0.0173  bbox_loss: 0.1488  cls_loss: 2.3713  \n",
      "<<<iteration:[80/117] - total_loss: 2.3010  obj_loss: 0.0748  noobj_loss: 0.0213  bbox_loss: 0.0532  cls_loss: 1.9496  \n",
      "<<<iteration:[100/117] - total_loss: 3.3972  obj_loss: 0.0870  noobj_loss: 0.0252  bbox_loss: 0.1398  cls_loss: 2.5984  \n",
      "\n",
      "epoch:82/100 - Train Loss: 2.6296, Val Loss: 3.6504\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.1341  obj_loss: 0.0471  noobj_loss: 0.0201  bbox_loss: 0.0758  cls_loss: 1.6982  \n",
      "<<<iteration:[40/117] - total_loss: 2.2794  obj_loss: 0.0499  noobj_loss: 0.0167  bbox_loss: 0.0819  cls_loss: 1.8116  \n",
      "<<<iteration:[60/117] - total_loss: 2.1793  obj_loss: 0.0423  noobj_loss: 0.0227  bbox_loss: 0.0533  cls_loss: 1.8592  \n",
      "<<<iteration:[80/117] - total_loss: 2.7982  obj_loss: 0.1072  noobj_loss: 0.0256  bbox_loss: 0.1201  cls_loss: 2.0775  \n",
      "<<<iteration:[100/117] - total_loss: 2.9754  obj_loss: 0.0520  noobj_loss: 0.0236  bbox_loss: 0.0831  cls_loss: 2.4961  \n",
      "\n",
      "epoch:83/100 - Train Loss: 2.5380, Val Loss: 3.0175\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7853  obj_loss: 0.0583  noobj_loss: 0.0265  bbox_loss: 0.0727  cls_loss: 2.3502  \n",
      "<<<iteration:[40/117] - total_loss: 2.5514  obj_loss: 0.0456  noobj_loss: 0.0226  bbox_loss: 0.0907  cls_loss: 2.0412  \n",
      "<<<iteration:[60/117] - total_loss: 2.5172  obj_loss: 0.0915  noobj_loss: 0.0204  bbox_loss: 0.0688  cls_loss: 2.0713  \n",
      "<<<iteration:[80/117] - total_loss: 2.6861  obj_loss: 0.0559  noobj_loss: 0.0176  bbox_loss: 0.0709  cls_loss: 2.2670  \n",
      "<<<iteration:[100/117] - total_loss: 2.6514  obj_loss: 0.0395  noobj_loss: 0.0302  bbox_loss: 0.1423  cls_loss: 1.8850  \n",
      "\n",
      "epoch:84/100 - Train Loss: 2.6185, Val Loss: 4.7556\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8111  obj_loss: 0.0655  noobj_loss: 0.0207  bbox_loss: 0.0928  cls_loss: 2.2713  \n",
      "<<<iteration:[40/117] - total_loss: 16.9471  obj_loss: 0.0069  noobj_loss: 0.0350  bbox_loss: 2.9161  cls_loss: 2.3420  \n",
      "<<<iteration:[60/117] - total_loss: 6.0917  obj_loss: 0.0215  noobj_loss: 0.0214  bbox_loss: 0.7575  cls_loss: 2.2722  \n",
      "<<<iteration:[80/117] - total_loss: 3.1150  obj_loss: 0.0229  noobj_loss: 0.0296  bbox_loss: 0.2014  cls_loss: 2.0704  \n",
      "<<<iteration:[100/117] - total_loss: 6.7904  obj_loss: 0.0430  noobj_loss: 0.0215  bbox_loss: 0.9020  cls_loss: 2.2267  \n",
      "\n",
      "epoch:85/100 - Train Loss: 6.5541, Val Loss: 3.2713\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.3716  obj_loss: 0.0797  noobj_loss: 0.0197  bbox_loss: 0.3740  cls_loss: 2.4121  \n",
      "<<<iteration:[40/117] - total_loss: 3.4920  obj_loss: 0.0435  noobj_loss: 0.0192  bbox_loss: 0.1680  cls_loss: 2.5988  \n",
      "<<<iteration:[60/117] - total_loss: 4.3394  obj_loss: 0.0513  noobj_loss: 0.0251  bbox_loss: 0.4444  cls_loss: 2.0534  \n",
      "<<<iteration:[80/117] - total_loss: 2.1600  obj_loss: 0.0540  noobj_loss: 0.0161  bbox_loss: 0.0661  cls_loss: 1.7676  \n",
      "<<<iteration:[100/117] - total_loss: 4.3521  obj_loss: 0.0316  noobj_loss: 0.0204  bbox_loss: 0.3972  cls_loss: 2.3242  \n",
      "\n",
      "epoch:86/100 - Train Loss: 3.6563, Val Loss: 3.6858\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 4.7521  obj_loss: 0.0301  noobj_loss: 0.0185  bbox_loss: 0.4831  cls_loss: 2.2970  \n",
      "<<<iteration:[40/117] - total_loss: 2.5142  obj_loss: 0.0732  noobj_loss: 0.0212  bbox_loss: 0.1042  cls_loss: 1.9092  \n",
      "<<<iteration:[60/117] - total_loss: 2.4412  obj_loss: 0.0696  noobj_loss: 0.0164  bbox_loss: 0.0875  cls_loss: 1.9259  \n",
      "<<<iteration:[80/117] - total_loss: 2.9004  obj_loss: 0.0363  noobj_loss: 0.0157  bbox_loss: 0.1081  cls_loss: 2.3159  \n",
      "<<<iteration:[100/117] - total_loss: 2.7197  obj_loss: 0.0911  noobj_loss: 0.0169  bbox_loss: 0.0905  cls_loss: 2.1675  \n",
      "\n",
      "epoch:87/100 - Train Loss: 3.2844, Val Loss: 2.8939\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.5100  obj_loss: 0.0496  noobj_loss: 0.0157  bbox_loss: 0.1104  cls_loss: 1.9004  \n",
      "<<<iteration:[40/117] - total_loss: 2.3844  obj_loss: 0.0566  noobj_loss: 0.0160  bbox_loss: 0.0815  cls_loss: 1.9123  \n",
      "<<<iteration:[60/117] - total_loss: 3.0365  obj_loss: 0.0596  noobj_loss: 0.0211  bbox_loss: 0.0993  cls_loss: 2.4700  \n",
      "<<<iteration:[80/117] - total_loss: 2.8880  obj_loss: 0.0667  noobj_loss: 0.0254  bbox_loss: 0.0906  cls_loss: 2.3558  \n",
      "<<<iteration:[100/117] - total_loss: 2.4280  obj_loss: 0.0804  noobj_loss: 0.0264  bbox_loss: 0.1109  cls_loss: 1.7798  \n",
      "\n",
      "epoch:88/100 - Train Loss: 2.6491, Val Loss: 5.4194\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.1090  obj_loss: 0.0559  noobj_loss: 0.0147  bbox_loss: 0.1047  cls_loss: 2.5221  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<iteration:[40/117] - total_loss: 2.4953  obj_loss: 0.0542  noobj_loss: 0.0166  bbox_loss: 0.0801  cls_loss: 2.0325  \n",
      "<<<iteration:[60/117] - total_loss: 3.0976  obj_loss: 0.0481  noobj_loss: 0.0213  bbox_loss: 0.1821  cls_loss: 2.1286  \n",
      "<<<iteration:[80/117] - total_loss: 2.0816  obj_loss: 0.0602  noobj_loss: 0.0264  bbox_loss: 0.0507  cls_loss: 1.7545  \n",
      "<<<iteration:[100/117] - total_loss: 2.5406  obj_loss: 0.0531  noobj_loss: 0.0166  bbox_loss: 0.1102  cls_loss: 1.9280  \n",
      "\n",
      "epoch:89/100 - Train Loss: 2.7122, Val Loss: 3.8466\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7086  obj_loss: 0.0691  noobj_loss: 0.0264  bbox_loss: 0.1456  cls_loss: 1.8984  \n",
      "<<<iteration:[40/117] - total_loss: 3.0871  obj_loss: 0.0565  noobj_loss: 0.0185  bbox_loss: 0.0919  cls_loss: 2.5619  \n",
      "<<<iteration:[60/117] - total_loss: 2.7795  obj_loss: 0.0652  noobj_loss: 0.0190  bbox_loss: 0.1175  cls_loss: 2.1173  \n",
      "<<<iteration:[80/117] - total_loss: 3.0779  obj_loss: 0.0539  noobj_loss: 0.0179  bbox_loss: 0.1050  cls_loss: 2.4902  \n",
      "<<<iteration:[100/117] - total_loss: 2.2849  obj_loss: 0.0496  noobj_loss: 0.0154  bbox_loss: 0.0563  cls_loss: 1.9460  \n",
      "\n",
      "epoch:90/100 - Train Loss: 2.7032, Val Loss: 2.9393\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.4819  obj_loss: 0.1070  noobj_loss: 0.0247  bbox_loss: 0.1293  cls_loss: 2.7161  \n",
      "<<<iteration:[40/117] - total_loss: 2.3111  obj_loss: 0.0544  noobj_loss: 0.0187  bbox_loss: 0.0719  cls_loss: 1.8876  \n",
      "<<<iteration:[60/117] - total_loss: 2.7341  obj_loss: 0.0303  noobj_loss: 0.0157  bbox_loss: 0.1617  cls_loss: 1.8875  \n",
      "<<<iteration:[80/117] - total_loss: 2.2020  obj_loss: 0.0534  noobj_loss: 0.0137  bbox_loss: 0.0671  cls_loss: 1.8066  \n",
      "<<<iteration:[100/117] - total_loss: 2.7048  obj_loss: 0.0772  noobj_loss: 0.0187  bbox_loss: 0.0782  cls_loss: 2.2274  \n",
      "\n",
      "epoch:91/100 - Train Loss: 2.6259, Val Loss: 2.7674\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7105  obj_loss: 0.0582  noobj_loss: 0.0170  bbox_loss: 0.0772  cls_loss: 2.2578  \n",
      "<<<iteration:[40/117] - total_loss: 1.9423  obj_loss: 0.0517  noobj_loss: 0.0169  bbox_loss: 0.0643  cls_loss: 1.5607  \n",
      "<<<iteration:[60/117] - total_loss: 2.5993  obj_loss: 0.0697  noobj_loss: 0.0182  bbox_loss: 0.0816  cls_loss: 2.1125  \n",
      "<<<iteration:[80/117] - total_loss: 3.1345  obj_loss: 0.0709  noobj_loss: 0.0152  bbox_loss: 0.1052  cls_loss: 2.5299  \n",
      "<<<iteration:[100/117] - total_loss: 2.7479  obj_loss: 0.0418  noobj_loss: 0.0186  bbox_loss: 0.1668  cls_loss: 1.8630  \n",
      "\n",
      "epoch:92/100 - Train Loss: 2.6590, Val Loss: 2.9021\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0659  obj_loss: 0.0626  noobj_loss: 0.0194  bbox_loss: 0.1945  cls_loss: 2.0211  \n",
      "<<<iteration:[40/117] - total_loss: 2.2711  obj_loss: 0.0273  noobj_loss: 0.0180  bbox_loss: 0.0846  cls_loss: 1.8120  \n",
      "<<<iteration:[60/117] - total_loss: 2.5738  obj_loss: 0.0728  noobj_loss: 0.0166  bbox_loss: 0.0749  cls_loss: 2.1180  \n",
      "<<<iteration:[80/117] - total_loss: 2.3816  obj_loss: 0.0727  noobj_loss: 0.0364  bbox_loss: 0.0781  cls_loss: 1.9002  \n",
      "<<<iteration:[100/117] - total_loss: 2.6831  obj_loss: 0.0444  noobj_loss: 0.0230  bbox_loss: 0.0835  cls_loss: 2.2097  \n",
      "\n",
      "epoch:93/100 - Train Loss: 2.6204, Val Loss: 2.7592\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.7847  obj_loss: 0.0686  noobj_loss: 0.0176  bbox_loss: 0.1439  cls_loss: 1.9880  \n",
      "<<<iteration:[40/117] - total_loss: 2.4690  obj_loss: 0.0542  noobj_loss: 0.0182  bbox_loss: 0.1260  cls_loss: 1.7760  \n",
      "<<<iteration:[60/117] - total_loss: 2.5212  obj_loss: 0.0246  noobj_loss: 0.0166  bbox_loss: 0.0957  cls_loss: 2.0098  \n",
      "<<<iteration:[80/117] - total_loss: 2.9254  obj_loss: 0.1007  noobj_loss: 0.0191  bbox_loss: 0.0997  cls_loss: 2.3165  \n",
      "<<<iteration:[100/117] - total_loss: 2.3679  obj_loss: 0.0400  noobj_loss: 0.0274  bbox_loss: 0.0747  cls_loss: 1.9406  \n",
      "\n",
      "epoch:94/100 - Train Loss: 2.6487, Val Loss: 3.0735\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.8698  obj_loss: 0.0645  noobj_loss: 0.0175  bbox_loss: 0.0863  cls_loss: 2.3651  \n",
      "<<<iteration:[40/117] - total_loss: 2.6371  obj_loss: 0.0609  noobj_loss: 0.0208  bbox_loss: 0.0952  cls_loss: 2.0898  \n",
      "<<<iteration:[60/117] - total_loss: 2.3753  obj_loss: 0.0537  noobj_loss: 0.0231  bbox_loss: 0.0692  cls_loss: 1.9641  \n",
      "<<<iteration:[80/117] - total_loss: 3.2879  obj_loss: 0.0656  noobj_loss: 0.0205  bbox_loss: 0.1690  cls_loss: 2.3672  \n",
      "<<<iteration:[100/117] - total_loss: 1.9830  obj_loss: 0.0686  noobj_loss: 0.0300  bbox_loss: 0.0679  cls_loss: 1.5600  \n",
      "\n",
      "epoch:95/100 - Train Loss: 2.6248, Val Loss: 2.9990\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.2882  obj_loss: 0.0592  noobj_loss: 0.0161  bbox_loss: 0.1541  cls_loss: 2.4506  \n",
      "<<<iteration:[40/117] - total_loss: 2.2462  obj_loss: 0.0347  noobj_loss: 0.0172  bbox_loss: 0.0763  cls_loss: 1.8214  \n",
      "<<<iteration:[60/117] - total_loss: 3.0059  obj_loss: 0.0754  noobj_loss: 0.0202  bbox_loss: 0.0788  cls_loss: 2.5263  \n",
      "<<<iteration:[80/117] - total_loss: 2.2317  obj_loss: 0.0507  noobj_loss: 0.0194  bbox_loss: 0.0747  cls_loss: 1.7977  \n",
      "<<<iteration:[100/117] - total_loss: 2.5479  obj_loss: 0.0569  noobj_loss: 0.0194  bbox_loss: 0.0797  cls_loss: 2.0829  \n",
      "\n",
      "epoch:96/100 - Train Loss: 2.5417, Val Loss: 3.3535\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.4485  obj_loss: 0.0521  noobj_loss: 0.0223  bbox_loss: 0.0966  cls_loss: 1.9020  \n",
      "<<<iteration:[40/117] - total_loss: 3.1329  obj_loss: 0.0493  noobj_loss: 0.0169  bbox_loss: 0.2087  cls_loss: 2.0315  \n",
      "<<<iteration:[60/117] - total_loss: 2.3735  obj_loss: 0.0354  noobj_loss: 0.0207  bbox_loss: 0.0685  cls_loss: 1.9851  \n",
      "<<<iteration:[80/117] - total_loss: 2.9348  obj_loss: 0.0723  noobj_loss: 0.0186  bbox_loss: 0.0903  cls_loss: 2.4015  \n",
      "<<<iteration:[100/117] - total_loss: 2.6603  obj_loss: 0.0679  noobj_loss: 0.0248  bbox_loss: 0.0663  cls_loss: 2.2483  \n",
      "\n",
      "epoch:97/100 - Train Loss: 2.6284, Val Loss: 2.9570\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.2724  obj_loss: 0.0604  noobj_loss: 0.0246  bbox_loss: 0.0668  cls_loss: 1.8659  \n",
      "<<<iteration:[40/117] - total_loss: 2.8388  obj_loss: 0.0359  noobj_loss: 0.0188  bbox_loss: 0.0864  cls_loss: 2.3613  \n",
      "<<<iteration:[60/117] - total_loss: 2.7360  obj_loss: 0.0427  noobj_loss: 0.0165  bbox_loss: 0.0889  cls_loss: 2.2405  \n",
      "<<<iteration:[80/117] - total_loss: 2.6875  obj_loss: 0.0455  noobj_loss: 0.0221  bbox_loss: 0.0867  cls_loss: 2.1974  \n",
      "<<<iteration:[100/117] - total_loss: 2.4971  obj_loss: 0.0681  noobj_loss: 0.0204  bbox_loss: 0.0956  cls_loss: 1.9407  \n",
      "\n",
      "epoch:98/100 - Train Loss: 2.4652, Val Loss: 3.0161\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 3.0693  obj_loss: 0.0582  noobj_loss: 0.0228  bbox_loss: 0.1407  cls_loss: 2.2959  \n",
      "<<<iteration:[40/117] - total_loss: 2.1508  obj_loss: 0.0629  noobj_loss: 0.0231  bbox_loss: 0.0581  cls_loss: 1.7860  \n",
      "<<<iteration:[60/117] - total_loss: 2.5854  obj_loss: 0.0554  noobj_loss: 0.0206  bbox_loss: 0.1351  cls_loss: 1.8439  \n",
      "<<<iteration:[80/117] - total_loss: 2.8352  obj_loss: 0.0655  noobj_loss: 0.0186  bbox_loss: 0.1067  cls_loss: 2.2269  \n",
      "<<<iteration:[100/117] - total_loss: 2.8411  obj_loss: 0.0847  noobj_loss: 0.0231  bbox_loss: 0.0933  cls_loss: 2.2783  \n",
      "\n",
      "epoch:99/100 - Train Loss: 2.6080, Val Loss: 3.2115\n",
      "\n",
      "<<<iteration:[20/117] - total_loss: 2.5800  obj_loss: 0.0488  noobj_loss: 0.0192  bbox_loss: 0.0993  cls_loss: 2.0252  \n",
      "<<<iteration:[40/117] - total_loss: 2.5202  obj_loss: 0.0610  noobj_loss: 0.0207  bbox_loss: 0.1015  cls_loss: 1.9411  \n",
      "<<<iteration:[60/117] - total_loss: 2.4061  obj_loss: 0.0440  noobj_loss: 0.0220  bbox_loss: 0.0705  cls_loss: 1.9986  \n",
      "<<<iteration:[80/117] - total_loss: 2.6892  obj_loss: 0.0760  noobj_loss: 0.0232  bbox_loss: 0.1110  cls_loss: 2.0465  \n",
      "<<<iteration:[100/117] - total_loss: 2.1258  obj_loss: 0.0387  noobj_loss: 0.0192  bbox_loss: 0.0770  cls_loss: 1.6927  \n",
      "\n",
      "epoch:100/100 - Train Loss: 2.4461, Val Loss: 3.1965\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14aee511509c4aef9ad45912ef0c8dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>▅▃█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train bbox Loss</td><td>▅▃█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train class Loss</td><td>█▅▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▁</td></tr><tr><td>Train obj Loss</td><td>█▁▁▁▂▁▁▂▂▁▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Val Loss</td><td>▆█▆▅▃▁▁▂▃▁▄▂▁▁▁▂▁▂▁▂▁▁▁▁▁▁▂▁▁▂▂▁▂▃▁▂▁▁▁▁</td></tr><tr><td>Val bbox Loss</td><td>▆█▆▄▃▁▁▁▄▁▃▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▃▁▂▁▁▁▁</td></tr><tr><td>Val class Loss</td><td>▄▂▃▄▂▂▂▃▁▂█▂▂▂▁▄▁▂▂▄▂▁▁▂▂▂▅▁▁▄▃▁▆▅▁▄▁▃▂▃</td></tr><tr><td>Val obj Loss</td><td>▃▃▂▁▃▇▆▁▂▆▄▆▇▆▆▁▄▁▃▃▄█▃▅▅▂▂▇▄▄▂▅▃▁▆▁▇▇▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>2.44607</td></tr><tr><td>Train bbox Loss</td><td>0.08766</td></tr><tr><td>Train class Loss</td><td>1.93767</td></tr><tr><td>Train obj Loss</td><td>0.05947</td></tr><tr><td>Val Loss</td><td>3.19651</td></tr><tr><td>Val bbox Loss</td><td>0.08837</td></tr><tr><td>Val class Loss</td><td>2.6582</td></tr><tr><td>Val obj Loss</td><td>0.07039</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-wildflower-2</strong> at: <a href='https://wandb.ai/urp/yolo_swin/runs/5822c9gs' target=\"_blank\">https://wandb.ai/urp/yolo_swin/runs/5822c9gs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231006_051152-5822c9gs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "best_epoch = 0\n",
    "best_score = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders, model, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "#     train_loss[\"obj_loss\"] += obj_loss\n",
    "#     train_loss[\"noobj_loss\"] += noobj_loss\n",
    "#     train_loss[\"bbox_loss\"] += bbox_loss\n",
    "#     train_loss[\"cls_loss\"] += cls_loss\n",
    "    wandb.log({\"Train Loss\": train_loss['total_loss'],\n",
    "               \"Train obj Loss\":train_loss[\"obj_loss\"],\n",
    "               \"Train bbox Loss\":train_loss[\"bbox_loss\"],\n",
    "               \"Train class Loss\":train_loss[\"cls_loss\"],\n",
    "               \"Val Loss\": val_loss['total_loss'],\n",
    "               \"Val obj Loss\":val_loss[\"obj_loss\"],\n",
    "               \"Val bbox Loss\":val_loss[\"bbox_loss\"],\n",
    "               \"Val class Loss\":val_loss[\"cls_loss\"],})\n",
    "    print(f\"\\nepoch:{epoch+1}/{num_epochs} - Train Loss: {train_loss['total_loss']:.4f}, Val Loss: {val_loss['total_loss']:.4f}\\n\")\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model(model.state_dict(), f'model_{epoch+1}.pth', save_dir=f\"./trained_model/{BACKBONE}_{PART}_LR{LR}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7fe95",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b71f9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63f8dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64dd5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, num_classes, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model = YOLO_SWIN(num_classes=num_classes)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d80869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=448\n",
    "transformer = A.Compose([\n",
    "    A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "76bcd114",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path=\"./trained_model/YOLO_SWIN_T_body_LR0.0001/model_90.pth\"\n",
    "model = load_model(ckpt_path, NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3709c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_predict(image, model, conf_thres=0.2, iou_threshold=0.1):\n",
    "    predictions = model(image)\n",
    "    prediction = predictions.detach().cpu().squeeze(dim=0)\n",
    "    \n",
    "    grid_size = prediction.shape[-1]\n",
    "    y_grid, x_grid = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size))\n",
    "    stride_size = IMAGE_SIZE/grid_size\n",
    "\n",
    "    conf = prediction[[0,5], ...].reshape(1, -1)\n",
    "    xc = (prediction[[1,6], ...] * IMAGE_SIZE + x_grid*stride_size).reshape(1,-1)\n",
    "    yc = (prediction[[2,7], ...] * IMAGE_SIZE + y_grid*stride_size).reshape(1,-1)\n",
    "    w = (prediction[[3,8], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    h = (prediction[[4,9], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    cls = torch.max(prediction[10:, ...].reshape(NUM_CLASSES, -1), dim=0).indices.tile(1,2)\n",
    "    \n",
    "    x_min = xc - w/2\n",
    "    y_min = yc - h/2\n",
    "    x_max = xc + w/2\n",
    "    y_max = yc + h/2\n",
    "\n",
    "    prediction_res = torch.cat([x_min, y_min, x_max, y_max, conf, cls], dim=0)\n",
    "    prediction_res = prediction_res.transpose(0,1)\n",
    "\n",
    "    # x_min과 y_min이 음수가 되지않고, x_max와 y_max가 이미지 크기를 넘지 않게 제한\n",
    "    prediction_res[:, 2].clip(min=0, max=image.shape[1]) \n",
    "    prediction_res[:, 3].clip(min=0, max=image.shape[0])\n",
    "        \n",
    "    pred_res = prediction_res[prediction_res[:, 4] > conf_thres]\n",
    "    nms_index = torchvision.ops.nms(boxes=pred_res[:, 0:4], scores=pred_res[:, 4], iou_threshold=iou_threshold)\n",
    "    pred_res_ = pred_res[nms_index].numpy()\n",
    "    \n",
    "    n_obj = pred_res_.shape[0]\n",
    "    bboxes = np.zeros(shape=(n_obj, 4), dtype=np.float32)\n",
    "    bboxes[:, 0:2] = (pred_res_[:, 0:2] + pred_res_[:, 2:4]) / 2\n",
    "    bboxes[:, 2:4] = pred_res_[:, 2:4] - pred_res_[:, 0:2]\n",
    "    scores = pred_res_[:, 4]\n",
    "    class_ids = pred_res_[:, 5]\n",
    "    \n",
    "    # 이미지 값이 들어가면 모델을 통해서, 후처리까지 포함된 yolo 포멧의 box좌표, 그 좌표에 대한 confidence score\n",
    "    # 그리고 class id를 반환\n",
    "    return bboxes, scores, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "10dddcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_images = []\n",
    "pred_labels =[]\n",
    "\n",
    "for index, batch in enumerate(dataloaders[\"val\"]):\n",
    "    images = batch[0].to(device)\n",
    "    bboxes, scores, class_ids = model_predict(images, model, conf_thres=0.1, iou_threshold=0.1)\n",
    "    \n",
    "    if len(bboxes) > 0:\n",
    "        prediction_yolo = np.concatenate([bboxes, scores[:, np.newaxis], class_ids[:, np.newaxis]], axis=1)\n",
    "    else:\n",
    "        prediction_yolo = np.array([])\n",
    "    \n",
    "    # 텐서형의 이미지를 다시 unnormalize를 시키고, 다시 chw를 hwc로 바꾸고 넘파이로 바꾼다.\n",
    "    np_image = make_grid(images[0], normalize=True).cpu().permute(1,2,0).numpy()\n",
    "    pred_images.append(np_image)\n",
    "    pred_labels.append(prediction_yolo)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b07fa545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370b29c6fc9b40699db7c0ad39541334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=19), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(index=(0,len(pred_images)-1))\n",
    "def show_result(index=0):\n",
    "    print(pred_labels[index])\n",
    "    if len(pred_labels[index]) > 0:\n",
    "        result = visualize(pred_images[index], pred_labels[index][:, 0:4], pred_labels[index][:, 5])\n",
    "    else:\n",
    "        result = pred_images[index]\n",
    "        \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5bf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7e158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742eb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789df476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba0798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
