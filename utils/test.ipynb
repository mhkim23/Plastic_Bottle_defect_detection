{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151bf3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6289e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca66ada8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.013043478260869565, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): PatchMerging(\n",
       "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02608695652173913, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03913043478260869, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): PatchMerging(\n",
       "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05217391304347826, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07826086956521738, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09130434782608696, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10434782608695652, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11739130434782608, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14347826086956522, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15652173913043477, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16956521739130434, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.20869565217391303, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2217391304347826, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.23478260869565215, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.24782608695652175, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.27391304347826084, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): PatchMerging(\n",
       "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.28695652173913044, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.3, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (permute): Permute()\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=torchvision.models.swin_s()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d146b1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 96, 112, 112]           4,704\n",
      "           Permute-2         [-1, 112, 112, 96]               0\n",
      "         LayerNorm-3         [-1, 112, 112, 96]             192\n",
      "         LayerNorm-4         [-1, 112, 112, 96]             192\n",
      "ShiftedWindowAttention-5         [-1, 112, 112, 96]               0\n",
      "   StochasticDepth-6         [-1, 112, 112, 96]               0\n",
      "         LayerNorm-7         [-1, 112, 112, 96]             192\n",
      "            Linear-8        [-1, 112, 112, 384]          37,248\n",
      "              GELU-9        [-1, 112, 112, 384]               0\n",
      "          Dropout-10        [-1, 112, 112, 384]               0\n",
      "           Linear-11         [-1, 112, 112, 96]          36,960\n",
      "          Dropout-12         [-1, 112, 112, 96]               0\n",
      "  StochasticDepth-13         [-1, 112, 112, 96]               0\n",
      "SwinTransformerBlock-14         [-1, 112, 112, 96]               0\n",
      "        LayerNorm-15         [-1, 112, 112, 96]             192\n",
      "ShiftedWindowAttention-16         [-1, 112, 112, 96]               0\n",
      "  StochasticDepth-17         [-1, 112, 112, 96]               0\n",
      "        LayerNorm-18         [-1, 112, 112, 96]             192\n",
      "           Linear-19        [-1, 112, 112, 384]          37,248\n",
      "             GELU-20        [-1, 112, 112, 384]               0\n",
      "          Dropout-21        [-1, 112, 112, 384]               0\n",
      "           Linear-22         [-1, 112, 112, 96]          36,960\n",
      "          Dropout-23         [-1, 112, 112, 96]               0\n",
      "  StochasticDepth-24         [-1, 112, 112, 96]               0\n",
      "SwinTransformerBlock-25         [-1, 112, 112, 96]               0\n",
      "        LayerNorm-26          [-1, 56, 56, 384]             768\n",
      "           Linear-27          [-1, 56, 56, 192]          73,728\n",
      "     PatchMerging-28          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-29          [-1, 56, 56, 192]             384\n",
      "ShiftedWindowAttention-30          [-1, 56, 56, 192]               0\n",
      "  StochasticDepth-31          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-32          [-1, 56, 56, 192]             384\n",
      "           Linear-33          [-1, 56, 56, 768]         148,224\n",
      "             GELU-34          [-1, 56, 56, 768]               0\n",
      "          Dropout-35          [-1, 56, 56, 768]               0\n",
      "           Linear-36          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-37          [-1, 56, 56, 192]               0\n",
      "  StochasticDepth-38          [-1, 56, 56, 192]               0\n",
      "SwinTransformerBlock-39          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-40          [-1, 56, 56, 192]             384\n",
      "ShiftedWindowAttention-41          [-1, 56, 56, 192]               0\n",
      "  StochasticDepth-42          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-43          [-1, 56, 56, 192]             384\n",
      "           Linear-44          [-1, 56, 56, 768]         148,224\n",
      "             GELU-45          [-1, 56, 56, 768]               0\n",
      "          Dropout-46          [-1, 56, 56, 768]               0\n",
      "           Linear-47          [-1, 56, 56, 192]         147,648\n",
      "          Dropout-48          [-1, 56, 56, 192]               0\n",
      "  StochasticDepth-49          [-1, 56, 56, 192]               0\n",
      "SwinTransformerBlock-50          [-1, 56, 56, 192]               0\n",
      "        LayerNorm-51          [-1, 28, 28, 768]           1,536\n",
      "           Linear-52          [-1, 28, 28, 384]         294,912\n",
      "     PatchMerging-53          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-54          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-55          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-56          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-57          [-1, 28, 28, 384]             768\n",
      "           Linear-58         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-59         [-1, 28, 28, 1536]               0\n",
      "          Dropout-60         [-1, 28, 28, 1536]               0\n",
      "           Linear-61          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-62          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-63          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-64          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-65          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-66          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-67          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-68          [-1, 28, 28, 384]             768\n",
      "           Linear-69         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-70         [-1, 28, 28, 1536]               0\n",
      "          Dropout-71         [-1, 28, 28, 1536]               0\n",
      "           Linear-72          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-73          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-74          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-75          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-76          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-77          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-78          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-79          [-1, 28, 28, 384]             768\n",
      "           Linear-80         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-81         [-1, 28, 28, 1536]               0\n",
      "          Dropout-82         [-1, 28, 28, 1536]               0\n",
      "           Linear-83          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-84          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-85          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-86          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-87          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-88          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-89          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-90          [-1, 28, 28, 384]             768\n",
      "           Linear-91         [-1, 28, 28, 1536]         591,360\n",
      "             GELU-92         [-1, 28, 28, 1536]               0\n",
      "          Dropout-93         [-1, 28, 28, 1536]               0\n",
      "           Linear-94          [-1, 28, 28, 384]         590,208\n",
      "          Dropout-95          [-1, 28, 28, 384]               0\n",
      "  StochasticDepth-96          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-97          [-1, 28, 28, 384]               0\n",
      "        LayerNorm-98          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-99          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-100          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-101          [-1, 28, 28, 384]             768\n",
      "          Linear-102         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-103         [-1, 28, 28, 1536]               0\n",
      "         Dropout-104         [-1, 28, 28, 1536]               0\n",
      "          Linear-105          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-106          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-107          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-108          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-109          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-110          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-111          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-112          [-1, 28, 28, 384]             768\n",
      "          Linear-113         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-114         [-1, 28, 28, 1536]               0\n",
      "         Dropout-115         [-1, 28, 28, 1536]               0\n",
      "          Linear-116          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-117          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-118          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-119          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-120          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-121          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-122          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-123          [-1, 28, 28, 384]             768\n",
      "          Linear-124         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-125         [-1, 28, 28, 1536]               0\n",
      "         Dropout-126         [-1, 28, 28, 1536]               0\n",
      "          Linear-127          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-128          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-129          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-130          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-131          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-132          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-133          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-134          [-1, 28, 28, 384]             768\n",
      "          Linear-135         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-136         [-1, 28, 28, 1536]               0\n",
      "         Dropout-137         [-1, 28, 28, 1536]               0\n",
      "          Linear-138          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-139          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-140          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-141          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-142          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-143          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-144          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-145          [-1, 28, 28, 384]             768\n",
      "          Linear-146         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-147         [-1, 28, 28, 1536]               0\n",
      "         Dropout-148         [-1, 28, 28, 1536]               0\n",
      "          Linear-149          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-150          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-151          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-152          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-153          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-154          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-155          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-156          [-1, 28, 28, 384]             768\n",
      "          Linear-157         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-158         [-1, 28, 28, 1536]               0\n",
      "         Dropout-159         [-1, 28, 28, 1536]               0\n",
      "          Linear-160          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-161          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-162          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-163          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-164          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-165          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-166          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-167          [-1, 28, 28, 384]             768\n",
      "          Linear-168         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-169         [-1, 28, 28, 1536]               0\n",
      "         Dropout-170         [-1, 28, 28, 1536]               0\n",
      "          Linear-171          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-172          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-173          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-174          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-175          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-176          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-177          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-178          [-1, 28, 28, 384]             768\n",
      "          Linear-179         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-180         [-1, 28, 28, 1536]               0\n",
      "         Dropout-181         [-1, 28, 28, 1536]               0\n",
      "          Linear-182          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-183          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-184          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-185          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-186          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-187          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-188          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-189          [-1, 28, 28, 384]             768\n",
      "          Linear-190         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-191         [-1, 28, 28, 1536]               0\n",
      "         Dropout-192         [-1, 28, 28, 1536]               0\n",
      "          Linear-193          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-194          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-195          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-196          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-197          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-198          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-199          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-200          [-1, 28, 28, 384]             768\n",
      "          Linear-201         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-202         [-1, 28, 28, 1536]               0\n",
      "         Dropout-203         [-1, 28, 28, 1536]               0\n",
      "          Linear-204          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-205          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-206          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-207          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-208          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-209          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-210          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-211          [-1, 28, 28, 384]             768\n",
      "          Linear-212         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-213         [-1, 28, 28, 1536]               0\n",
      "         Dropout-214         [-1, 28, 28, 1536]               0\n",
      "          Linear-215          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-216          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-217          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-218          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-219          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-220          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-221          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-222          [-1, 28, 28, 384]             768\n",
      "          Linear-223         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-224         [-1, 28, 28, 1536]               0\n",
      "         Dropout-225         [-1, 28, 28, 1536]               0\n",
      "          Linear-226          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-227          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-228          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-229          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-230          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-231          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-232          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-233          [-1, 28, 28, 384]             768\n",
      "          Linear-234         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-235         [-1, 28, 28, 1536]               0\n",
      "         Dropout-236         [-1, 28, 28, 1536]               0\n",
      "          Linear-237          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-238          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-239          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-240          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-241          [-1, 28, 28, 384]             768\n",
      "ShiftedWindowAttention-242          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-243          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-244          [-1, 28, 28, 384]             768\n",
      "          Linear-245         [-1, 28, 28, 1536]         591,360\n",
      "            GELU-246         [-1, 28, 28, 1536]               0\n",
      "         Dropout-247         [-1, 28, 28, 1536]               0\n",
      "          Linear-248          [-1, 28, 28, 384]         590,208\n",
      "         Dropout-249          [-1, 28, 28, 384]               0\n",
      " StochasticDepth-250          [-1, 28, 28, 384]               0\n",
      "SwinTransformerBlock-251          [-1, 28, 28, 384]               0\n",
      "       LayerNorm-252         [-1, 14, 14, 1536]           3,072\n",
      "          Linear-253          [-1, 14, 14, 768]       1,179,648\n",
      "    PatchMerging-254          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-255          [-1, 14, 14, 768]           1,536\n",
      "ShiftedWindowAttention-256          [-1, 14, 14, 768]               0\n",
      " StochasticDepth-257          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-258          [-1, 14, 14, 768]           1,536\n",
      "          Linear-259         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-260         [-1, 14, 14, 3072]               0\n",
      "         Dropout-261         [-1, 14, 14, 3072]               0\n",
      "          Linear-262          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-263          [-1, 14, 14, 768]               0\n",
      " StochasticDepth-264          [-1, 14, 14, 768]               0\n",
      "SwinTransformerBlock-265          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-266          [-1, 14, 14, 768]           1,536\n",
      "ShiftedWindowAttention-267          [-1, 14, 14, 768]               0\n",
      " StochasticDepth-268          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-269          [-1, 14, 14, 768]           1,536\n",
      "          Linear-270         [-1, 14, 14, 3072]       2,362,368\n",
      "            GELU-271         [-1, 14, 14, 3072]               0\n",
      "         Dropout-272         [-1, 14, 14, 3072]               0\n",
      "          Linear-273          [-1, 14, 14, 768]       2,360,064\n",
      "         Dropout-274          [-1, 14, 14, 768]               0\n",
      " StochasticDepth-275          [-1, 14, 14, 768]               0\n",
      "SwinTransformerBlock-276          [-1, 14, 14, 768]               0\n",
      "       LayerNorm-277          [-1, 14, 14, 768]           1,536\n",
      "         Permute-278          [-1, 768, 14, 14]               0\n",
      "AdaptiveAvgPool2d-279            [-1, 768, 1, 1]               0\n",
      "         Flatten-280                  [-1, 768]               0\n",
      "          Linear-281                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 33,818,440\n",
      "Trainable params: 33,818,440\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 1486.10\n",
      "Params size (MB): 129.01\n",
      "Estimated Total Size (MB): 1617.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(model, (3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f2d10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = torchvision.models.resnet18()\n",
    "resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8bdb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-11         [-1, 64, 112, 112]               0\n",
      "           Conv2d-12         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "             ReLU-14         [-1, 64, 112, 112]               0\n",
      "           Conv2d-15         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-16         [-1, 64, 112, 112]             128\n",
      "             ReLU-17         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-18         [-1, 64, 112, 112]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "             ReLU-21          [-1, 128, 56, 56]               0\n",
      "           Conv2d-22          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 56, 56]             256\n",
      "           Conv2d-24          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 56, 56]             256\n",
      "             ReLU-26          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-34          [-1, 128, 56, 56]               0\n",
      "           Conv2d-35          [-1, 256, 28, 28]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 28, 28]             512\n",
      "             ReLU-37          [-1, 256, 28, 28]               0\n",
      "           Conv2d-38          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 28, 28]             512\n",
      "           Conv2d-40          [-1, 256, 28, 28]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 28, 28]             512\n",
      "             ReLU-42          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-43          [-1, 256, 28, 28]               0\n",
      "           Conv2d-44          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 28, 28]             512\n",
      "             ReLU-46          [-1, 256, 28, 28]               0\n",
      "           Conv2d-47          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 28, 28]             512\n",
      "             ReLU-49          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-50          [-1, 256, 28, 28]               0\n",
      "           Conv2d-51          [-1, 512, 14, 14]       1,179,648\n",
      "      BatchNorm2d-52          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-53          [-1, 512, 14, 14]               0\n",
      "           Conv2d-54          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-55          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-56          [-1, 512, 14, 14]         131,072\n",
      "      BatchNorm2d-57          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-58          [-1, 512, 14, 14]               0\n",
      "       BasicBlock-59          [-1, 512, 14, 14]               0\n",
      "           Conv2d-60          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-61          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-62          [-1, 512, 14, 14]               0\n",
      "           Conv2d-63          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-64          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-65          [-1, 512, 14, 14]               0\n",
      "       BasicBlock-66          [-1, 512, 14, 14]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 11,689,512\n",
      "Trainable params: 11,689,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 251.14\n",
      "Params size (MB): 44.59\n",
      "Estimated Total Size (MB): 298.03\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(resnet18, (3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4458a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [m for m in resnet18.children()] #Resnet에서 Yolo에서 가져올수 있을만한 layer만 선별적으로 가져오기 위해서\n",
    "backbone = nn.Sequential(*layers[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bbba029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-4         [-1, 64, 112, 112]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 112, 112]             128\n",
      "              ReLU-7         [-1, 64, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-11         [-1, 64, 112, 112]               0\n",
      "           Conv2d-12         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 112, 112]             128\n",
      "             ReLU-14         [-1, 64, 112, 112]               0\n",
      "           Conv2d-15         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-16         [-1, 64, 112, 112]             128\n",
      "             ReLU-17         [-1, 64, 112, 112]               0\n",
      "       BasicBlock-18         [-1, 64, 112, 112]               0\n",
      "           Conv2d-19          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
      "             ReLU-21          [-1, 128, 56, 56]               0\n",
      "           Conv2d-22          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 56, 56]             256\n",
      "           Conv2d-24          [-1, 128, 56, 56]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 56, 56]             256\n",
      "             ReLU-26          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-27          [-1, 128, 56, 56]               0\n",
      "           Conv2d-28          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 56, 56]             256\n",
      "             ReLU-30          [-1, 128, 56, 56]               0\n",
      "           Conv2d-31          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "       BasicBlock-34          [-1, 128, 56, 56]               0\n",
      "           Conv2d-35          [-1, 256, 28, 28]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 28, 28]             512\n",
      "             ReLU-37          [-1, 256, 28, 28]               0\n",
      "           Conv2d-38          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 28, 28]             512\n",
      "           Conv2d-40          [-1, 256, 28, 28]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 28, 28]             512\n",
      "             ReLU-42          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-43          [-1, 256, 28, 28]               0\n",
      "           Conv2d-44          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 28, 28]             512\n",
      "             ReLU-46          [-1, 256, 28, 28]               0\n",
      "           Conv2d-47          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 28, 28]             512\n",
      "             ReLU-49          [-1, 256, 28, 28]               0\n",
      "       BasicBlock-50          [-1, 256, 28, 28]               0\n",
      "           Conv2d-51          [-1, 512, 14, 14]       1,179,648\n",
      "      BatchNorm2d-52          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-53          [-1, 512, 14, 14]               0\n",
      "           Conv2d-54          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-55          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-56          [-1, 512, 14, 14]         131,072\n",
      "      BatchNorm2d-57          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-58          [-1, 512, 14, 14]               0\n",
      "       BasicBlock-59          [-1, 512, 14, 14]               0\n",
      "           Conv2d-60          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-61          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-62          [-1, 512, 14, 14]               0\n",
      "           Conv2d-63          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-64          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-65          [-1, 512, 14, 14]               0\n",
      "       BasicBlock-66          [-1, 512, 14, 14]               0\n",
      "================================================================\n",
      "Total params: 11,176,512\n",
      "Trainable params: 11,176,512\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 251.12\n",
      "Params size (MB): 42.64\n",
      "Estimated Total Size (MB): 296.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torchsummary.summary(backbone, (3,448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f03693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
