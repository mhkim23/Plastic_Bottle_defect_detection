{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2fa1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torchsummary\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.image as mpimg\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ede415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d22aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'Unformed': 0, 'Burr': 1}\n",
    "CLASS_ID_TO_NAME = {0: 'Unformed', 1: 'Burr'}\n",
    "BOX_COLOR = {'Unformed':(200, 0, 0), 'Burr':(0, 0, 200)}\n",
    "TEXT_COLOR = (255, 255, 255)\n",
    "\n",
    "def save_model(model_state, model_name, save_dir=\"./trained_model\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir, model_name))\n",
    "\n",
    "\n",
    "def visualize_bbox(image, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    x_center, y_center, w, h = bbox\n",
    "    x_min = int(x_center - w/2)\n",
    "    y_min = int(y_center - h/2)\n",
    "    x_max = int(x_center + w/2)\n",
    "    y_max = int(y_center + h/2)\n",
    "    \n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color[class_name], thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(image, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color[class_name], -1)\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return image\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "#         print('category_id: ',category_id)\n",
    "        class_name = CLASS_ID_TO_NAME[category_id.item()]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc9e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "    \n",
    "    for a,b,c in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "\n",
    "    return torch.stack(image_list, dim=0), target_list, filename_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40be7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PET_dataset():\n",
    "    def __init__(self,part,neck_dir,body_dir,phase, transformer=None, aug=None, aug_factor=0):\n",
    "        self.neck_dir=neck_dir\n",
    "        self.body_dir=body_dir\n",
    "        self.part=part\n",
    "        self.phase=phase\n",
    "        self.transformer=transformer\n",
    "        self.aug=aug\n",
    "        self.aug_factor=aug_factor\n",
    "        if(self.part==\"body\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.body_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.body_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        elif(self.part==\"neck\"):\n",
    "            self.image_files = sorted([fn for fn in os.listdir(self.neck_dir+\"/\"+self.phase+\"/image\") if fn.endswith(\"jpg\")])\n",
    "            self.label_files= sorted([lab for lab in os.listdir(self.neck_dir+\"/\"+self.phase+\"/label\") if lab.endswith(\"txt\")])\n",
    "        \n",
    "        self.auged_img_list, self.auged_label_list=self.make_aug_list(self.image_files, self.label_files)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        if(self.aug==None):\n",
    "            filename, image = self.get_image(self.part, index)\n",
    "            bboxes, class_ids = self.get_label(self.part, index)\n",
    "\n",
    "            if(self.transformer):\n",
    "                transformed_data=self.transformer(image=image, bboxes=bboxes, class_ids=class_ids)\n",
    "                image = transformed_data['image']\n",
    "                bboxes = np.array(transformed_data['bboxes'])\n",
    "                class_ids = np.array(transformed_data['class_ids'])\n",
    "\n",
    "\n",
    "            target = {}\n",
    "    #         print(f'bboxes:{bboxes}\\nclass_ids:{class_ids}\\nlen_bboxes:{len(bboxes)}\\nlen_class_ids:{len(class_ids)}')\n",
    "    #         print(f'filename: {filename}')\n",
    "            target[\"boxes\"] = torch.Tensor(bboxes).float()\n",
    "            target[\"labels\"] = torch.Tensor(class_ids).long()\n",
    "\n",
    "            ###\n",
    "            bboxes=torch.Tensor(bboxes).float()\n",
    "            class_ids=torch.Tensor(class_ids).long()\n",
    "            target = np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1)\n",
    "            ###\n",
    "        else:\n",
    "            image=self.auged_img_list[index][1]\n",
    "            target=self.auged_label_list[index]\n",
    "            filename=self.auged_img_list[index][0]\n",
    "        return image, target, filename\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        length=0\n",
    "        if(self.aug==None):\n",
    "            length=len(self.image_files)\n",
    "        else:\n",
    "            length=len(self.auged_img_list)\n",
    "        return length\n",
    "    \n",
    "    def make_aug_list(self,ori_image_list,ori_label_files):\n",
    "        aug_image_list=[]\n",
    "        aug_label_list=[]\n",
    "        \n",
    "        print(f\"start making augmented images-- augmented factor:{self.aug_factor}\")\n",
    "        for i in range(len(ori_image_list)):\n",
    "            filename, ori_image = self.get_image(self.part, i)\n",
    "            ori_bboxes, ori_class_ids = self.get_label(self.part, i)\n",
    "            for j in range(self.aug_factor):\n",
    "                auged_data=self.aug(image=ori_image, bboxes=ori_bboxes, class_ids=ori_class_ids)\n",
    "                image = auged_data['image']\n",
    "                bboxes = np.array(auged_data['bboxes'])\n",
    "                class_ids = np.array(auged_data['class_ids'])\n",
    "                \n",
    "                bboxes=torch.Tensor(bboxes).float()\n",
    "                class_ids=torch.Tensor(class_ids).long()\n",
    "                \n",
    "                aug_image_list.append((filename, image))\n",
    "                aug_label_list.append(np.concatenate((bboxes, class_ids[:, np.newaxis]), axis=1))\n",
    "        \n",
    "        print(f\"total length of augmented images: {len(aug_image_list)}\")\n",
    "        \n",
    "        return aug_image_list, aug_label_list\n",
    "        \n",
    "    \n",
    "    def get_image(self, part, index): # 이미지 불러오는 함수\n",
    "        filename = self.image_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body called!-> {self.part}\")\n",
    "            image_path = self.body_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck called!-> {self.part}\")\n",
    "            image_path = self.neck_dir+\"/\"+self.phase+\"/image/\"+filename\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return filename, image\n",
    "    \n",
    "    def get_label(self, part, index): # label (box좌표, class_id) 불러오는 함수\n",
    "        label_filename=self.label_files[index]\n",
    "        if(part==\"body\"):\n",
    "#             print(f\"body label called!-> {self.part}\")\n",
    "            label_path = self.body_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        elif(part==\"neck\"):\n",
    "#             print(f\"neck label called!-> {self.part}\")\n",
    "            label_path = self.neck_dir+\"/\"+self.phase+\"/label/\"+label_filename\n",
    "        with open(label_path, 'r') as file:\n",
    "            labels = file.readlines()\n",
    "        \n",
    "        class_ids=[]\n",
    "        bboxes=[]\n",
    "        for label in labels:\n",
    "            label=label.replace(\"\\n\", \"\")\n",
    "            obj=label.split(' ')[0]\n",
    "            coor=label.split(' ')[1:]\n",
    "            obj=int(obj)\n",
    "            coor=list(map(float, coor))\n",
    "            class_ids.append(obj)\n",
    "            bboxes.append(coor)\n",
    "            \n",
    "        return bboxes, class_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e82db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO_SWIN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_bboxes = 2\n",
    "        self.grid_size = 7\n",
    "\n",
    "#         resnet18 = torchvision.models.resnet18(pretrained = True)\n",
    "        swin=torchvision.models.swin_v2_t(weights='IMAGENET1K_V1')\n",
    "        layers = [m for m in swin.children()] #Resnet에서 Yolo에서 가져올수 있을만한 layer만 선별적으로 가져오기 위해서\n",
    "\n",
    "        # 기존 Resnet18의 layer들중에서 맨 뒤에 두개만 제외하고 다 가져와서 Backbone으로 사용\n",
    "        self.backbone = nn.Sequential(*layers[:-3]) \n",
    "        self.head = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=768, out_channels=1024, kernel_size=1, padding=0,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1,bias=False),\n",
    "                nn.BatchNorm2d(1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "\n",
    "                nn.Conv2d(in_channels=1024, out_channels=(4+1)*self.num_bboxes+num_classes, kernel_size=1, padding=0, bias=False),\n",
    "                nn.AdaptiveAvgPool2d(output_size=(self.grid_size, self.grid_size))\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        # out = self.neck(out)\n",
    "        out = self.head(out) # input (batch, 3, 448, 448) -> output feature (batch, 12, 7, 7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9631387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YOLO_SWIN(\n",
       "  (backbone): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): Permute()\n",
       "        (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): PatchMergingV2(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerBlockV2(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): ShiftedWindowAttentionV2(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): Permute()\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Conv2d(768, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (13): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 2\n",
    "model = YOLO_SWIN(num_classes=NUM_CLASSES)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bfefc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, num_classes, device):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model = YOLO_SWIN(num_classes=num_classes)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "963ebf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=448\n",
    "transformer = A.Compose([\n",
    "            A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='yolo', label_fields=['class_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08ac3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path=\"./trained_model/YOLO_SWIN_T_body_LR0.0001_AUG30/model_90.pth\"\n",
    "ckpt_path=\"/workspace/Plastic_Bottle_defect_detection/trained_model/YOLO_SWIN_T_neck_LR0.0001_AUG20/model_100.pth\"\n",
    "model = load_model(ckpt_path, NUM_CLASSES, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64ada759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start making augmented images-- augmented factor:0\n",
      "total length of augmented images: 0\n"
     ]
    }
   ],
   "source": [
    "NECK_PATH = '/home/host_data/PET_data/Neck'\n",
    "BODY_PATH = '/home/host_data/PET_data/Body'\n",
    "test_dataset=PET_dataset(\"neck\" ,neck_dir=NECK_PATH,body_dir=BODY_PATH,phase='test', transformer=transformer, aug=None)\n",
    "test_dataloaders = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc7fdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_predict(image, model, conf_thres=0.2, iou_threshold=0.1):\n",
    "    predictions = model(image)\n",
    "    prediction = predictions.detach().cpu().squeeze(dim=0)\n",
    "#     print(prediction.shape)\n",
    "    \n",
    "    grid_size = prediction.shape[-1]\n",
    "    y_grid, x_grid = torch.meshgrid(torch.arange(grid_size), torch.arange(grid_size))\n",
    "    stride_size = IMAGE_SIZE/grid_size\n",
    "\n",
    "    conf = prediction[[0,5], ...].reshape(1, -1)\n",
    "    xc = (prediction[[1,6], ...] * IMAGE_SIZE + x_grid*stride_size).reshape(1,-1)\n",
    "    yc = (prediction[[2,7], ...] * IMAGE_SIZE + y_grid*stride_size).reshape(1,-1)\n",
    "    w = (prediction[[3,8], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    h = (prediction[[4,9], ...] * IMAGE_SIZE).reshape(1,-1)\n",
    "    cls = torch.max(prediction[10:, ...].reshape(NUM_CLASSES, -1), dim=0).indices.tile(1,2)\n",
    "    \n",
    "    x_min = xc - w/2\n",
    "    y_min = yc - h/2\n",
    "    x_max = xc + w/2\n",
    "    y_max = yc + h/2\n",
    "\n",
    "    prediction_res = torch.cat([x_min, y_min, x_max, y_max, conf, cls], dim=0)\n",
    "    prediction_res = prediction_res.transpose(0,1)\n",
    "\n",
    "    # x_min과 y_min이 음수가 되지않고, x_max와 y_max가 이미지 크기를 넘지 않게 제한\n",
    "    prediction_res[:, 2].clip(min=0, max=image.shape[1]) \n",
    "    prediction_res[:, 3].clip(min=0, max=image.shape[0])\n",
    "        \n",
    "    pred_res = prediction_res[prediction_res[:, 4] > conf_thres]\n",
    "    nms_index = torchvision.ops.nms(boxes=pred_res[:, 0:4], scores=pred_res[:, 4], iou_threshold=iou_threshold)\n",
    "    pred_res_ = pred_res[nms_index].numpy()\n",
    "    \n",
    "    n_obj = pred_res_.shape[0]\n",
    "    bboxes = np.zeros(shape=(n_obj, 4), dtype=np.float32)\n",
    "    bboxes[:, 0:2] = (pred_res_[:, 0:2] + pred_res_[:, 2:4]) / 2\n",
    "    bboxes[:, 2:4] = pred_res_[:, 2:4] - pred_res_[:, 0:2]\n",
    "    scores = pred_res_[:, 4]\n",
    "    class_ids = pred_res_[:, 5]\n",
    "    \n",
    "    # 이미지 값이 들어가면 모델을 통해서, 후처리까지 포함된 yolo 포멧의 box좌표, 그 좌표에 대한 confidence score\n",
    "    # 그리고 class id를 반환\n",
    "    return bboxes, scores, class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "560115c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToAbsoluteValues(size, box):\n",
    "    \n",
    "#     xIn = round(((2 * float(box[0]) - float(box[2])) * size[0] / 2))\n",
    "#     yIn = round(((2 * float(box[1]) - float(box[3])) * size[1] / 2))\n",
    "#     xEnd = xIn + round(float(box[2]) * size[0])\n",
    "#     yEnd = yIn + round(float(box[3]) * size[1])\n",
    "    xIn = round(((2 * float(box[0]) - float(box[2])) / 2))\n",
    "    yIn = round(((2 * float(box[1]) - float(box[3])) / 2))\n",
    "    xEnd = xIn + round(float(box[2]))\n",
    "    yEnd = yIn + round(float(box[3]))\n",
    "    \n",
    "    if xIn < 0:\n",
    "        xIn = 0\n",
    "    if yIn < 0:\n",
    "        yIn = 0\n",
    "    if xEnd >= size[0]:\n",
    "        xEnd = size[0] - 1\n",
    "    if yEnd >= size[1]:\n",
    "        yEnd = size[1] - 1\n",
    "        \n",
    "    return (xIn/size[0], yIn/size[1], xEnd/size[0], yEnd/size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e07cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_121.jpg'], prediction_yolo:[[1.33760300e+02 1.59974808e+02 3.07294235e+01 7.18687134e+01\n",
      "  1.90533102e-01 0.00000000e+00]\n",
      " [7.53538132e+01 3.10220947e+02 2.22676697e+01 6.38666992e+01\n",
      "  1.04858294e-01 0.00000000e+00]]\n",
      "\n",
      "(0.26339285714285715, 0.2767857142857143, 0.3325892857142857, 0.4375)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_148.jpg'], prediction_yolo:[[2.76689392e+02 1.11695084e+02 6.18981781e+01 7.95036850e+01\n",
      "  2.81707406e-01 0.00000000e+00]\n",
      " [3.78543976e+02 3.05685944e+02 1.33870850e+01 4.86723022e+01\n",
      "  1.27031922e-01 0.00000000e+00]\n",
      " [3.86717621e+02 3.49901123e+02 1.55027466e+01 5.24704590e+01\n",
      "  1.02531694e-01 0.00000000e+00]]\n",
      "\n",
      "(0.5491071428571429, 0.16071428571428573, 0.6875, 0.3392857142857143)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_153.jpg'], prediction_yolo:[[7.77617798e+01 1.55436447e+02 5.37175140e+01 9.30534286e+01\n",
      "  4.63311315e-01 0.00000000e+00]\n",
      " [2.35064865e+02 2.61697998e+02 1.79275208e+01 5.05058746e+01\n",
      "  1.46031603e-01 0.00000000e+00]\n",
      " [2.13288025e+02 2.60950134e+02 1.67450867e+01 5.12068634e+01\n",
      "  1.06552385e-01 0.00000000e+00]]\n",
      "\n",
      "(0.11383928571428571, 0.24330357142857142, 0.234375, 0.45089285714285715)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_48.jpg'], prediction_yolo:[[3.9762079e+02 3.4503366e+02 2.1577271e+01 5.7795349e+01 1.9334322e-01\n",
      "  0.0000000e+00]\n",
      " [3.3241848e+01 2.2462218e+02 1.8215017e+01 6.6723724e+01 1.6383314e-01\n",
      "  0.0000000e+00]\n",
      " [1.6092831e+02 4.2490353e+02 6.0406281e+01 9.7747314e+01 1.3971950e-01\n",
      "  0.0000000e+00]\n",
      " [5.7399406e+01 2.1050005e+02 2.1886887e+01 6.8234558e+01 1.2700090e-01\n",
      "  0.0000000e+00]]\n",
      "\n",
      "(0.8638392857142857, 0.7053571428571429, 0.9129464285714286, 0.8348214285714286)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_65.jpg'], prediction_yolo:[[2.2673999e+02 1.0493486e+02 4.8668121e+01 6.9066978e+01 5.5917531e-01\n",
      "  0.0000000e+00]\n",
      " [1.6534534e+02 2.6919440e+02 2.2178406e+01 6.1457504e+01 1.8328507e-01\n",
      "  0.0000000e+00]\n",
      " [1.4022455e+02 2.6272238e+02 2.9369019e+01 6.6685791e+01 1.4266156e-01\n",
      "  0.0000000e+00]]\n",
      "\n",
      "(0.45089285714285715, 0.15625, 0.5602678571428571, 0.31026785714285715)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_80.jpg'], prediction_yolo:[[3.96503487e+01 1.81630249e+02 3.78105469e+01 1.10860809e+02\n",
      "  1.13615617e-01 0.00000000e+00]\n",
      " [3.79048218e+02 2.88375366e+02 1.87557983e+01 4.92277222e+01\n",
      "  1.13239795e-01 0.00000000e+00]]\n",
      "\n",
      "(0.046875, 0.28125, 0.13169642857142858, 0.5290178571428571)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape1_99.jpg'], prediction_yolo:[[2.3984219e+02 2.6879800e+02 1.8577179e+01 4.8030151e+01 1.2548268e-01\n",
      "  0.0000000e+00]]\n",
      "\n",
      "(0.515625, 0.546875, 0.5580357142857143, 0.6540178571428571)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_105.jpg'], prediction_yolo:[[257.71182    155.11395     22.009705    99.496346     0.47426635\n",
      "    0.        ]]\n",
      "\n",
      "(0.5513392857142857, 0.234375, 0.6004464285714286, 0.45535714285714285)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_127.jpg'], prediction_yolo:[[106.99987   168.18636    27.753265   71.96292     0.3672395   0.       ]]\n",
      "\n",
      "(0.20758928571428573, 0.29464285714285715, 0.2700892857142857, 0.45535714285714285)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_135.jpg'], prediction_yolo:[[3.0689130e+02 1.8985950e+02 1.2729492e+01 2.4590321e+02 3.3664733e-01\n",
      "  1.0000000e+00]\n",
      " [2.9294229e+02 1.9956883e+02 1.1402893e+01 2.2722031e+02 1.7968482e-01\n",
      "  1.0000000e+00]]\n",
      "\n",
      "(0.671875, 0.14955357142857142, 0.7008928571428571, 0.6986607142857143)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_14.jpg'], prediction_yolo:[[3.4271463e+02 1.7123155e+02 3.2210510e+01 8.1390228e+01 3.7882715e-01\n",
      "  0.0000000e+00]\n",
      " [4.8610744e+01 1.4753671e+02 1.2800774e+01 2.6296393e+02 3.1676844e-01\n",
      "  1.0000000e+00]]\n",
      "\n",
      "(0.7299107142857143, 0.2924107142857143, 0.8013392857142857, 0.4732142857142857)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_151.jpg'], prediction_yolo:[[149.41777   154.78436    28.200043   80.05768     0.7387523   0.       ]]\n",
      "\n",
      "(0.3013392857142857, 0.25669642857142855, 0.3638392857142857, 0.43526785714285715)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_53.jpg'], prediction_yolo:[[4.13245605e+02 2.05801056e+02 1.64397583e+01 1.58640076e+02\n",
      "  1.95844531e-01 1.00000000e+00]\n",
      " [6.00300903e+01 1.24702866e+02 2.27353058e+01 1.13235199e+02\n",
      "  1.69637159e-01 0.00000000e+00]\n",
      " [3.46770592e+01 1.23737122e+02 2.62232361e+01 8.27130051e+01\n",
      "  1.39641836e-01 0.00000000e+00]]\n",
      "\n",
      "(0.9040178571428571, 0.28125, 0.9397321428571429, 0.6361607142857143)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_55.jpg'], prediction_yolo:[[172.97205    139.05542     30.88858     95.38962      0.61420447\n",
      "    0.        ]]\n",
      "\n",
      "(0.35267857142857145, 0.203125, 0.421875, 0.41517857142857145)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_70.jpg'], prediction_yolo:[[3.9875208e+02 2.2054062e+02 1.2157288e+01 2.0029805e+02 2.4828728e-01\n",
      "  1.0000000e+00]\n",
      " [3.6547649e+01 1.3159227e+02 2.9585793e+01 7.2079018e+01 1.9994752e-01\n",
      "  0.0000000e+00]\n",
      " [6.3013794e+01 1.3143173e+02 2.2003658e+01 8.1698608e+01 1.7291486e-01\n",
      "  0.0000000e+00]\n",
      " [4.1349396e+02 1.7420204e+02 1.1327087e+01 2.0794894e+02 1.7021373e-01\n",
      "  1.0000000e+00]]\n",
      "\n",
      "(0.8772321428571429, 0.26785714285714285, 0.9040178571428571, 0.7142857142857143)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_75.jpg'], prediction_yolo:[[113.188324  145.4989     17.717072   77.37043     0.4298257   0.       ]]\n",
      "\n",
      "(0.23214285714285715, 0.23883928571428573, 0.27232142857142855, 0.4107142857142857)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_79.jpg'], prediction_yolo:[[1.6476866e+02 1.4626970e+02 3.0063293e+01 9.0264565e+01 5.4833877e-01\n",
      "  0.0000000e+00]\n",
      " [2.7200882e+02 2.2720131e+02 3.1868835e+01 7.5045013e+01 1.3600412e-01\n",
      "  0.0000000e+00]]\n",
      "\n",
      "(0.33482142857142855, 0.22544642857142858, 0.4017857142857143, 0.4263392857142857)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape2_81.jpg'], prediction_yolo:[[277.9171   167.90057   37.47711   85.81448    0.568482   0.      ]]\n",
      "\n",
      "(0.578125, 0.27901785714285715, 0.6607142857142857, 0.47098214285714285)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape3_12.jpg'], prediction_yolo:[[ 97.06024    185.19043     35.64061     97.860535     0.47391164\n",
      "    0.        ]]\n",
      "\n",
      "(0.17633928571428573, 0.30357142857142855, 0.25669642857142855, 0.5223214285714286)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape3_14.jpg'], prediction_yolo:[[227.97722    191.02766     36.04254     98.216644     0.39746368\n",
      "    0.        ]]\n",
      "\n",
      "(0.46875, 0.3169642857142857, 0.5491071428571429, 0.5357142857142857)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape3_15.jpg'], prediction_yolo:[[ 75.93058   197.64313    29.404129   87.94061     0.4679949   0.       ]]\n",
      "\n",
      "(0.13616071428571427, 0.34375, 0.20089285714285715, 0.5401785714285714)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape3_44.jpg'], prediction_yolo:[[3.70486298e+02 1.66521072e+02 2.01489868e+01 1.00513855e+02\n",
      "  3.18020374e-01 0.00000000e+00]]\n",
      "\n",
      "(0.8035714285714286, 0.25892857142857145, 0.8482142857142857, 0.484375)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape4_24.jpg'], prediction_yolo:[[ 62.218327   142.91249     34.76499     73.432465     0.18313886\n",
      "    0.        ]]\n",
      "\n",
      "(0.10044642857142858, 0.23660714285714285, 0.17857142857142858, 0.39955357142857145)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape5_34.jpg'], prediction_yolo:[[3.0830621e+02 2.2596643e+02 3.4918640e+01 4.1150970e+01 2.6518250e-01\n",
      "  0.0000000e+00]]\n",
      "\n",
      "(0.6495535714285714, 0.4575892857142857, 0.7276785714285714, 0.5491071428571429)\n",
      "torch.Size([1, 3, 448, 448])\n",
      "filename:['shape6_34.jpg'], prediction_yolo:[[9.4935104e+01 1.7336078e+02 2.0750351e+01 5.4464294e+01 1.5851685e-01\n",
      "  0.0000000e+00]\n",
      " [1.0844884e+02 2.1183742e+02 1.8441147e+01 4.4299500e+01 1.3342102e-01\n",
      "  0.0000000e+00]]\n",
      "\n",
      "(0.18973214285714285, 0.32589285714285715, 0.23660714285714285, 0.44642857142857145)\n"
     ]
    }
   ],
   "source": [
    "pred_images = []\n",
    "pred_labels =[]\n",
    "\n",
    "for index, batch in enumerate(test_dataloaders):\n",
    "    images = batch[0].to(device)\n",
    "    filename=batch[2]\n",
    "    print(images.shape)\n",
    "    \n",
    "    bboxes, scores, class_ids = model_predict(images, model, conf_thres=0.1, iou_threshold=0.1)\n",
    "    \n",
    "    if len(bboxes) > 0:\n",
    "        prediction_yolo = np.concatenate([bboxes, scores[:, np.newaxis], class_ids[:, np.newaxis]], axis=1)\n",
    "    else:\n",
    "        prediction_yolo = np.array([])\n",
    "    \n",
    "    # 텐서형의 이미지를 다시 unnormalize를 시키고, 다시 chw를 hwc로 바꾸고 넘파이로 바꾼다.\n",
    "    np_image = make_grid(images[0], normalize=True).cpu().permute(1,2,0).numpy()\n",
    "    pred_images.append(np_image)\n",
    "    pred_labels.append(prediction_yolo)\n",
    "    print(f\"filename:{filename}, prediction_yolo:{prediction_yolo}\\n\")\n",
    "    print(f\"{convertToAbsoluteValues(size=(images.shape[2],images.shape[3]),box=prediction_yolo[0][0:4])}\")\n",
    "    txt_name=filename[0].split(\".\")[0]+\".txt\"\n",
    "    f = open(\"/home/host_data/PET2/Neck/test/label/detection/\"+txt_name, 'w')\n",
    "    for i in range(len(prediction_yolo)):\n",
    "        c=convertToAbsoluteValues(size=(images.shape[2],images.shape[3]),box=prediction_yolo[i][0:4])\n",
    "        text=f\"{int(prediction_yolo[i][5])} {prediction_yolo[i][4]} {c[0]} {c[1]} {c[2]} {c[3]}\\n\"\n",
    "        f.write(text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be20571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2cf7ff167f4c869a2818f992524f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='index', max=24), Output()), _dom_classes=('widget-intera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(index=(0,len(pred_images)-1))\n",
    "def show_result(index=0):\n",
    "    print(pred_labels[index])\n",
    "    if len(pred_labels[index]) > 0:\n",
    "        result = visualize(pred_images[index], pred_labels[index][:, 0:4], pred_labels[index][:, 5])\n",
    "    else:\n",
    "        result = pred_images[index]\n",
    "        \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b35b1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToAbsoluteValues_size(size, box):\n",
    "    \n",
    "    xIn = round(((2 * float(box[0]) - float(box[2])) * size[0] / 2))\n",
    "    yIn = round(((2 * float(box[1]) - float(box[3])) * size[1] / 2))\n",
    "    xEnd = xIn + round(float(box[2]) * size[0])\n",
    "    yEnd = yIn + round(float(box[3]) * size[1])\n",
    "    \n",
    "    if xIn < 0:\n",
    "        xIn = 0\n",
    "    if yIn < 0:\n",
    "        yIn = 0\n",
    "    if xEnd >= size[0]:\n",
    "        xEnd = size[0] - 1\n",
    "    if yEnd >= size[1]:\n",
    "        yEnd = size[1] - 1\n",
    "        \n",
    "    return (xIn/size[0], yIn/size[1], xEnd/size[0], yEnd/size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56945fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.14337599, 0.68225801, 0.055735  , 0.145161  , 0.        ]])]\n",
      "[array([[0.58001602, 0.21947201, 0.164094  , 0.20132001, 0.        ]])]\n",
      "[array([[0.21932   , 0.32191801, 0.16169199, 0.171233  , 0.        ],\n",
      "       [0.50787699, 0.57362998, 0.052239  , 0.14726   , 0.        ]])]\n",
      "[array([[0.90198499, 0.72297299, 0.040529  , 0.128378  , 0.        ]])]\n",
      "[array([[0.50946498, 0.227273  , 0.13662601, 0.22558901, 0.        ],\n",
      "       [0.35061699, 0.59427601, 0.046091  , 0.124579  , 0.        ]])]\n",
      "[array([[0.109323  , 0.41396099, 0.15924101, 0.27597401, 0.        ],\n",
      "       [0.83745903, 0.63960999, 0.042904  , 0.123377  , 0.        ]])]\n",
      "[array([[0.51067299, 0.59515601, 0.054187  , 0.16609   , 0.        ]])]\n",
      "[array([[0.55207503, 0.340996  , 0.051261  , 0.19923399, 0.        ]])]\n",
      "[array([[0.23926499, 0.38813499, 0.0642437 , 0.181031  , 0.        ]])]\n",
      "[array([[0.66097599, 0.43010801, 0.030894  , 0.58781397, 1.        ]])]\n",
      "[array([[0.111286  , 0.33790001, 0.0360801 , 0.57134902, 1.        ],\n",
      "       [0.77128798, 0.368891  , 0.0506322 , 0.196611  , 0.        ]])]\n",
      "[array([[0.343725  , 0.34905699, 0.073684  , 0.192453  , 0.        ]])]\n",
      "[array([[0.13187601, 0.27845901, 0.0446565 , 0.18458   , 0.        ]])]\n",
      "[array([[0.373469  , 0.31872499, 0.07102   , 0.22310799, 0.        ]])]\n",
      "[array([[0.11615  , 0.273662 , 0.0409005, 0.174068 , 0.       ]])]\n",
      "[array([[0.24308901, 0.33794501, 0.055285  , 0.19367599, 0.        ]])]\n",
      "[array([[0.363821, 0.346899, 0.060976, 0.236434, 0.      ]])]\n",
      "[array([[0.64314699, 0.38927299, 0.076237  , 0.19723199, 0.        ]])]\n",
      "[array([[0.23838601, 0.41774899, 0.072535  , 0.220779  , 0.        ]])]\n",
      "[array([[0.52074897, 0.42060101, 0.082994  , 0.18025801, 0.        ]])]\n",
      "[array([[0.190322 , 0.443174 , 0.0560288, 0.212317 , 0.       ]])]\n",
      "[array([[0.83401501, 0.35281399, 0.044154  , 0.22943699, 0.        ]])]\n",
      "[array([[0.161149  , 0.33522701, 0.059396  , 0.14204501, 0.        ]])]\n",
      "[array([[0.68150699, 0.50456601, 0.078767  , 0.09589   , 0.        ]])]\n",
      "[array([[0.271341  , 0.41538501, 0.040941  , 0.13538501, 0.        ]])]\n"
     ]
    }
   ],
   "source": [
    "# annotation만들기\n",
    "for index, batch in enumerate(test_dataloaders):\n",
    "    images = batch[0].to(device)\n",
    "    target=batch[1]\n",
    "    filename=batch[2]\n",
    "    print(target)\n",
    "    \n",
    "    txt_name=filename[0].split(\".\")[0]+\".txt\"\n",
    "    f = open(\"/home/host_data/PET2/Neck/test/label/groundtruth/\"+txt_name, 'w')\n",
    "    for i in range(len(target)):\n",
    "        c=convertToAbsoluteValues_size(size=(images.shape[2],images.shape[3]),box=target[0][i][0:4])\n",
    "        text=f\"{int(prediction_yolo[i][4])} {1} {c[0]} {c[1]} {c[2]} {c[3]}\\n\"\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19802fc",
   "metadata": {},
   "source": [
    "# 탐지성능검증을 위한 지표 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5f7b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: 'Unformed', 1: 'Burr'\n",
    "num2class = {\"0.0\" : \"Unformed\", \"1.0\" : \"Burr\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "870451b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box : (centerX, centerY, width, height)\n",
    "def convertToAbsoluteValues(size, box):\n",
    "    \n",
    "    xIn = round(((2 * float(box[0]) - float(box[2])) * size[0] / 2))\n",
    "    yIn = round(((2 * float(box[1]) - float(box[3])) * size[1] / 2))\n",
    "    xEnd = xIn + round(float(box[2]) * size[0])\n",
    "    yEnd = yIn + round(float(box[3]) * size[1])\n",
    "    \n",
    "    if xIn < 0:\n",
    "        xIn = 0\n",
    "    if yIn < 0:\n",
    "        yIn = 0\n",
    "    if xEnd >= size[0]:\n",
    "        xEnd = size[0] - 1\n",
    "    if yEnd >= size[1]:\n",
    "        yEnd = size[1] - 1\n",
    "    return (xIn, yIn, xEnd, yEnd)\n",
    "\n",
    "# def convertToRelativeValues(size, box):\n",
    "#     dw = 1. / (size[0])\n",
    "#     dh = 1. / (size[1])\n",
    "#     cx = (box[1] + box[0]) / 2.0\n",
    "#     cy = (box[3] + box[2]) / 2.0\n",
    "#     w = box[1] - box[0]\n",
    "#     h = box[3] - box[2]\n",
    "#     x = cx * dw\n",
    "#     y = cy * dh\n",
    "#     w = w * dw\n",
    "#     h = h * dh\n",
    "#     # x,y => (bounding_box_center)/width_of_the_image\n",
    "#     # w => bounding_box_width / width_of_the_image\n",
    "#     # h => bounding_box_height / height_of_the_image\n",
    "#     return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "01b78948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/host_data/PET2/Neck/test/image/shape1_48.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_151.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_81.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_75.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_127.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_53.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_79.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_55.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_148.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_14.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_121.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_15.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_80.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_44.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_105.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_70.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_153.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_135.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape5_34.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape6_34.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape4_24.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_65.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_14.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_12.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_99.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_48.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_151.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_81.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_75.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_127.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_53.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_79.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_55.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_148.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_14.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_121.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_15.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_80.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_44.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_105.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_70.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_153.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape2_135.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape5_34.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape6_34.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape4_24.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_65.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_14.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape3_12.jpg\n",
      "/home/host_data/PET2/Neck/test/image/shape1_99.jpg\n",
      "detections:[['shape1_48', 0.0, 0.19334322214126587, (182, 129, 447, 447)], ['shape1_48', 0.0, 0.1638331413269043, (3, 62, 45, 320)], ['shape1_48', 0.0, 0.13971950113773346, (36, 153, 227, 447)], ['shape1_48', 0.0, 0.12700089812278748, (12, 54, 80, 298)], ['shape2_151', 0.0, 0.7387523055076599, (54, 17, 217, 212)], ['shape2_81', 0.0, 0.568481981754303, (111, 20, 407, 231)], ['shape2_75', 0.0, 0.42982569336891174, (43, 15, 165, 199)], ['shape2_127', 0.0, 0.36723950505256653, (33, 30, 154, 234)], ['shape2_53', 1.0, 0.19584453105926514, (194, 0, 447, 268)], ['shape2_53', 0.0, 0.16963715851306915, (13, 0, 85, 158)], ['shape2_53', 0.0, 0.13964183628559113, (0, 0, 46, 164)], ['shape2_79', 0.0, 0.548338770866394, (60, 6, 240, 197)], ['shape2_79', 0.0, 0.13600412011146545, (112, 58, 400, 323)], ['shape2_55', 0.0, 0.614204466342926, (64, 0, 253, 184)], ['shape1_148', 0.0, 0.28170740604400635, (92, 0, 400, 148)], ['shape1_148', 0.0, 0.12703192234039307, (180, 116, 447, 446)], ['shape1_148', 0.0, 0.10253169387578964, (182, 136, 447, 447)], ['shape2_14', 0.0, 0.37882715463638306, (148, 25, 447, 237)], ['shape2_14', 1.0, 0.31676843762397766, (15, 0, 70, 155)], ['shape1_121', 0.0, 0.1905331015586853, (44, 26, 193, 222)], ['shape1_121', 0.0, 0.10485829412937164, (21, 107, 107, 447)], ['shape3_15', 0.0, 0.46799489855766296, (16, 33, 106, 275)], ['shape1_80', 0.0, 0.11361561715602875, (0, 8, 51, 245)], ['shape1_80', 0.0, 0.11323979496955872, (176, 108, 447, 421)], ['shape3_44', 0.0, 0.31802037358283997, (170, 8, 447, 225)], ['shape2_105', 0.0, 0.4742663502693176, (112, 3, 381, 207)], ['shape2_70', 1.0, 0.24828727543354034, (191, 0, 447, 280)], ['shape2_70', 0.0, 0.1999475210905075, (0, 12, 48, 180)], ['shape2_70', 0.0, 0.17291486263275146, (15, 4, 89, 177)], ['shape2_70', 1.0, 0.1702137291431427, (198, 0, 447, 209)], ['shape1_153', 0.0, 0.4633113145828247, (0, 8, 103, 210)], ['shape1_153', 0.0, 0.14603160321712494, (104, 92, 348, 379)], ['shape1_153', 0.0, 0.10655238479375839, (94, 92, 316, 378)], ['shape2_135', 1.0, 0.3366473317146301, (144, 0, 447, 223)], ['shape2_135', 1.0, 0.1796848177909851, (138, 0, 436, 243)], ['shape5_34', 0.0, 0.2651824951171875, (128, 82, 447, 328)], ['shape6_34', 0.0, 0.15851685404777527, (32, 46, 138, 246)], ['shape6_34', 0.0, 0.13342101871967316, (40, 73, 157, 307)], ['shape4_24', 0.0, 0.1831388622522354, (5, 16, 85, 195)], ['shape1_65', 0.0, 0.5591753125190735, (77, 0, 328, 139)], ['shape1_65', 0.0, 0.18328507244586945, (66, 88, 242, 387)], ['shape1_65', 0.0, 0.14266155660152435, (48, 81, 203, 377)], ['shape3_14', 0.0, 0.39746367931365967, (87, 22, 333, 262)], ['shape3_12', 0.0, 0.4739116430282593, (22, 19, 137, 253)], ['shape1_99', 0.0, 0.1254826784133911, (106, 99, 356, 392)]]\n",
      "\n",
      "\n",
      "groundtruths:[['shape1_48', 0.0, 1.0, (188, 119, 447, 447)], ['shape2_151', 0.0, 1.0, (52, 14, 222, 213)], ['shape2_81', 0.0, 1.0, (119, 21, 424, 239)], ['shape2_75', 0.0, 1.0, (36, 10, 158, 205)], ['shape2_127', 0.0, 1.0, (32, 26, 154, 240)], ['shape2_53', 0.0, 1.0, (14, 0, 83, 166)], ['shape2_79', 0.0, 1.0, (61, 0, 237, 206)], ['shape2_55', 0.0, 1.0, (60, 0, 243, 189)], ['shape1_148', 0.0, 1.0, (74, 0, 371, 125)], ['shape2_14', 0.0, 1.0, (13, 0, 71, 163)], ['shape1_121', 0.0, 1.0, (14, 104, 91, 442)], ['shape3_15', 0.0, 1.0, (24, 28, 122, 274)], ['shape1_80', 0.0, 1.0, (0, 0, 55, 248)], ['shape3_44', 0.0, 1.0, (172, 2, 447, 212)], ['shape2_105', 0.0, 1.0, (106, 10, 365, 207)], ['shape2_70', 0.0, 1.0, (13, 3, 74, 165)], ['shape1_153', 0.0, 1.0, (0, 14, 129, 197)], ['shape2_135', 0.0, 1.0, (138, 0, 441, 223)], ['shape5_34', 0.0, 1.0, (127, 81, 447, 329)], ['shape4_24', 0.0, 1.0, (16, 27, 102, 209)], ['shape1_65', 0.0, 1.0, (68, 0, 327, 127)], ['shape3_14', 0.0, 1.0, (89, 33, 341, 262)], ['shape3_12', 0.0, 1.0, (29, 20, 152, 257)], ['shape1_99', 0.0, 1.0, (96, 78, 337, 381)]]\n",
      "\n",
      "\n",
      "classes:[0.0, 1.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def boundingBoxes(labelPath, imagePath):\n",
    "    \n",
    "    detections, groundtruths, classes = [], [], []\n",
    "    \n",
    "    for boxtype in os.listdir(labelPath):\n",
    "\n",
    "        boxtypeDir = os.path.join(labelPath,boxtype)\n",
    "\n",
    "        for labelfile in os.listdir(boxtypeDir):\n",
    "            if labelfile ==\".ipynb_checkpoints\":\n",
    "                continue\n",
    "            filename = os.path.splitext(labelfile)[0]\n",
    "            with open(os.path.join(boxtypeDir, labelfile)) as f:\n",
    "                labelinfos = f.readlines()\n",
    "\n",
    "            imgfilepath = os.path.join(imagePath, filename + \".jpg\")\n",
    "            print(imgfilepath )\n",
    "            img = cv2.imread(imgfilepath)\n",
    "            h, w, _ = img.shape\n",
    "\n",
    "            for labelinfo in labelinfos:\n",
    "                label, conf, rx1, ry1, rx2, ry2 = map(float, labelinfo.strip().split())\n",
    "                x1, y1, x2, y2 = convertToAbsoluteValues((448, 448), (rx1, ry1, rx2, ry2))\n",
    "                boxinfo = [filename, label, conf, (x1, y1, x2, y2)]\n",
    "                \n",
    "                if label not in classes:\n",
    "                    classes.append(label)\n",
    "                \n",
    "                if boxtype == \"detection\":\n",
    "                    detections.append(boxinfo)\n",
    "                else:\n",
    "                    groundtruths.append(boxinfo)\n",
    "                    \n",
    "    classes = sorted(classes)\n",
    "                \n",
    "    return detections, groundtruths, classes\n",
    "\n",
    "detections, groundtruths, classes = boundingBoxes(\"/home/host_data/PET2/Neck/test/label/\", \"/home/host_data/PET2/Neck/test/image/\")\n",
    "print(f\"detections:{detections}\\n\\n\")\n",
    "print(f\"groundtruths:{groundtruths}\\n\\n\")\n",
    "print(f\"classes:{classes}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eb5c91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = A.Compose([ \n",
    "        # bounding box의 변환, augmentation에서 albumentations는 Detection 학습을 할 때 굉장히 유용하다. \n",
    "        A.Resize(height=IMAGE_SIZE, width=IMAGE_SIZE),\n",
    "#         A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "        # albumentations 라이브러리에서는 Normalization을 먼저 진행해 주고 tensor화를 진행해 주어야한다.\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3466c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxPlot(boxlist, imagePath, savePath):\n",
    "    labelfiles = sorted(list(set([filename for filename, _, _, _ in boxlist])))\n",
    "    \n",
    "    for labelfile in labelfiles:\n",
    "    \n",
    "        rectinfos = []\n",
    "        imgfilePath = os.path.join(imagePath, labelfile + \".jpg\")\n",
    "        img = cv2.imread(imgfilePath)\n",
    "        img_t=transformer(image=img)\n",
    "        img=img_t['image'].permute(1,2,0).numpy()\n",
    "\n",
    "        for filename, _, conf, (x1, y1, x2, y2) in boxlist:\n",
    "            if labelfile == filename:\n",
    "                rectinfos.append((x1, y1, x2, y2, conf))\n",
    "                \n",
    "        for x1, y1, x2, y2, conf in rectinfos:\n",
    "            \n",
    "            if conf == 1.0:\n",
    "                rectcolor = (0, 255, 0)\n",
    "            else:\n",
    "                rectcolor = (0, 0, 255)\n",
    "                \n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), rectcolor, 4)\n",
    "#         cv2.imwrite(f\"{savePath}/{labelfile}.jpg\", img)\n",
    "\n",
    "#         img = mpimg.imread(f\"{savePath}/{labelfile}.jpg\")\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "        \n",
    "# boxPlot(detections, \"image\", savePath=\"boxed_images/detection\")\n",
    "# boxPlot(groundtruths, \"image\", savePath=\"boxed_images/groundtruth\")\n",
    "boxPlot(detections + groundtruths, \"/home/host_data/PET2/Neck/test/image/\", savePath=\"./boxed_images/both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78347abb",
   "metadata": {},
   "source": [
    "## IoU(Intersection over Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d29868ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArea(box):\n",
    "    return (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "\n",
    "\n",
    "def getUnionAreas(boxA, boxB, interArea=None):\n",
    "    area_A = getArea(boxA)\n",
    "    area_B = getArea(boxB)\n",
    "    \n",
    "    if interArea is None:\n",
    "        interArea = getIntersectionArea(boxA, boxB)\n",
    "        \n",
    "    return float(area_A + area_B - interArea)\n",
    "\n",
    "def getIntersectionArea(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    # intersection area\n",
    "    return (xB - xA + 1) * (yB - yA + 1)\n",
    "\n",
    "# boxA = (Ax1,Ay1,Ax2,Ay2)\n",
    "# boxB = (Bx1,By1,Bx2,By2)\n",
    "def boxesIntersect(boxA, boxB):\n",
    "    if boxA[0] > boxB[2]:\n",
    "        return False  # boxA is right of boxB\n",
    "    if boxB[0] > boxA[2]:\n",
    "        return False  # boxA is left of boxB\n",
    "    if boxA[3] < boxB[1]:\n",
    "        return False  # boxA is above boxB\n",
    "    if boxA[1] > boxB[3]:\n",
    "        return False  # boxA is below boxB\n",
    "    return True\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # if boxes dont intersect\n",
    "    if boxesIntersect(boxA, boxB) is False:\n",
    "        return 0\n",
    "    interArea = getIntersectionArea(boxA, boxB)\n",
    "    union = getUnionAreas(boxA, boxB, interArea=interArea)\n",
    "    \n",
    "    # intersection over union\n",
    "    result = interArea / union\n",
    "    assert result >= 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "356a5198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boxA coordinates : (106, 99, 356, 392)\n",
      "boxA area : 73794\n",
      "boxB coordinates : (96, 78, 337, 381)\n",
      "boxB area : 73568\n",
      "Union area of boxA and boxB : 81706.0\n",
      "Does boxes Intersect? : True\n",
      "Intersection area of boxA and boxB : 65656\n",
      "IoU of boxA and boxB : 0.8035639977480235\n"
     ]
    }
   ],
   "source": [
    "boxA = detections[-1][-1]\n",
    "boxB = groundtruths[-1][-1]\n",
    "\n",
    "print(f\"boxA coordinates : {(boxA)}\")\n",
    "print(f\"boxA area : {getArea(boxA)}\")\n",
    "print(f\"boxB coordinates : {(boxB)}\")\n",
    "print(f\"boxB area : {getArea(boxB)}\")\n",
    "\n",
    "print(f\"Union area of boxA and boxB : {getUnionAreas(boxA, boxB)}\")\n",
    "\n",
    "print(f\"Does boxes Intersect? : {boxesIntersect(boxA, boxB)}\")\n",
    "\n",
    "print(f\"Intersection area of boxA and boxB : {getIntersectionArea(boxA, boxB)}\")\n",
    "\n",
    "print(f\"IoU of boxA and boxB : {iou(boxA, boxB)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc1f6e",
   "metadata": {},
   "source": [
    "## AP(Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2e99feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAveragePrecision(rec, prec):\n",
    "    \n",
    "    mrec = [0] + [e for e in rec] + [1]\n",
    "    mpre = [0] + [e for e in prec] + [0]\n",
    "\n",
    "    for i in range(len(mpre)-1, 0, -1):\n",
    "        mpre[i-1] = max(mpre[i-1], mpre[i])\n",
    "\n",
    "    ii = []\n",
    "\n",
    "    for i in range(len(mrec)-1):\n",
    "        if mrec[1:][i] != mrec[0:-1][i]:\n",
    "            ii.append(i+1)\n",
    "\n",
    "    ap = 0\n",
    "    for i in ii:\n",
    "        ap = ap + np.sum((mrec[i] - mrec[i-1]) * mpre[i])\n",
    "    \n",
    "    return [ap, mpre[0:len(mpre)-1], mrec[0:len(mpre)-1], ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8ac728e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElevenPointInterpolatedAP(rec, prec):\n",
    "\n",
    "    mrec = [e for e in rec]\n",
    "    mpre = [e for e in prec]\n",
    "\n",
    "    recallValues = np.linspace(0, 1, 11)\n",
    "    recallValues = list(recallValues[::-1])\n",
    "    rhoInterp, recallValid = [], []\n",
    "\n",
    "    for r in recallValues:\n",
    "        argGreaterRecalls = np.argwhere(mrec[:] >= r)\n",
    "        pmax = 0\n",
    "\n",
    "        if argGreaterRecalls.size != 0:\n",
    "            pmax = max(mpre[argGreaterRecalls.min():])\n",
    "\n",
    "        recallValid.append(r)\n",
    "        rhoInterp.append(pmax)\n",
    "\n",
    "    ap = sum(rhoInterp) / 11\n",
    "\n",
    "    return [ap, rhoInterp, recallValues, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c22b9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'class': 0.0, 'precision': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 0.91666667, 0.92307692, 0.92857143, 0.93333333,\n",
      "       0.9375    , 0.94117647, 0.94444444, 0.89473684, 0.85      ,\n",
      "       0.85714286, 0.81818182, 0.82608696, 0.79166667, 0.76      ,\n",
      "       0.73076923, 0.7037037 , 0.67857143, 0.65517241, 0.63333333,\n",
      "       0.61290323, 0.59375   , 0.57575758, 0.58823529, 0.6       ,\n",
      "       0.58333333, 0.56756757, 0.57894737, 0.56410256]), 'recall': array([0.04166667, 0.08333333, 0.125     , 0.16666667, 0.20833333,\n",
      "       0.25      , 0.29166667, 0.33333333, 0.375     , 0.41666667,\n",
      "       0.45833333, 0.45833333, 0.5       , 0.54166667, 0.58333333,\n",
      "       0.625     , 0.66666667, 0.70833333, 0.70833333, 0.70833333,\n",
      "       0.75      , 0.75      , 0.79166667, 0.79166667, 0.79166667,\n",
      "       0.79166667, 0.79166667, 0.79166667, 0.79166667, 0.79166667,\n",
      "       0.79166667, 0.79166667, 0.79166667, 0.83333333, 0.875     ,\n",
      "       0.875     , 0.875     , 0.91666667, 0.91666667]), 'AP': 0.8387018270313467, 'interpolated precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.9444444444444444, 0.8947368421052632, 0.8571428571428571, 0.8571428571428571, 0.8260869565217391, 0.8260869565217391, 0.7916666666666666, 0.76, 0.7307692307692307, 0.7037037037037037, 0.6785714285714286, 0.6551724137931034, 0.6333333333333333, 0.6129032258064516, 0.6, 0.6, 0.6, 0.6, 0.5833333333333334, 0.5789473684210527, 0.5789473684210527, 0.5641025641025641], 'interpolated recall': [0, 0.041666666666666664, 0.08333333333333333, 0.125, 0.16666666666666666, 0.20833333333333334, 0.25, 0.2916666666666667, 0.3333333333333333, 0.375, 0.4166666666666667, 0.4583333333333333, 0.4583333333333333, 0.5, 0.5416666666666666, 0.5833333333333334, 0.625, 0.6666666666666666, 0.7083333333333334, 0.7083333333333334, 0.7083333333333334, 0.75, 0.75, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.7916666666666666, 0.8333333333333334, 0.875, 0.875, 0.875, 0.9166666666666666, 0.9166666666666666], 'total positives': 24, 'total TP': 22.0, 'total FP': 17.0}, {'class': 1.0, 'precision': array([0., 0., 0., 0., 0., 0.]), 'recall': array([nan, nan, nan, nan, nan, nan]), 'AP': nan, 'interpolated precision': [0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'interpolated recall': [0, nan, nan, nan, nan, nan, nan], 'total positives': 0, 'total TP': 0.0, 'total FP': 6.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1560/2535020560.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rec = acc_TP / npos\n"
     ]
    }
   ],
   "source": [
    "def AP(detections, groundtruths, classes, IOUThreshold = 0.3, method = 'AP'):\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for c in classes:\n",
    "\n",
    "        dects = [d for d in detections if d[1] == c]\n",
    "        gts = [g for g in groundtruths if g[1] == c]\n",
    "\n",
    "        npos = len(gts)\n",
    "\n",
    "        dects = sorted(dects, key = lambda conf : conf[2], reverse=True)\n",
    "\n",
    "        TP = np.zeros(len(dects))\n",
    "        FP = np.zeros(len(dects))\n",
    "\n",
    "        det = Counter(cc[0] for cc in gts)\n",
    "\n",
    "        # 각 이미지별 ground truth box의 수\n",
    "        # {99 : 2, 380 : 4, ....}\n",
    "        # {99 : [0, 0], 380 : [0, 0, 0, 0], ...}\n",
    "        for key, val in det.items():\n",
    "            det[key] = np.zeros(val)\n",
    "\n",
    "\n",
    "        for d in range(len(dects)):\n",
    "\n",
    "\n",
    "            gt = [gt for gt in gts if gt[0] == dects[d][0]]\n",
    "\n",
    "            iouMax = 0\n",
    "\n",
    "            for j in range(len(gt)):\n",
    "                iou1 = iou(dects[d][3], gt[j][3])\n",
    "                if iou1 > iouMax:\n",
    "                    iouMax = iou1\n",
    "                    jmax = j\n",
    "\n",
    "            if iouMax >= IOUThreshold:\n",
    "                if det[dects[d][0]][jmax] == 0:\n",
    "                    TP[d] = 1\n",
    "                    det[dects[d][0]][jmax] = 1\n",
    "                else:\n",
    "                    FP[d] = 1\n",
    "            else:\n",
    "                FP[d] = 1\n",
    "\n",
    "        acc_FP = np.cumsum(FP)\n",
    "        acc_TP = np.cumsum(TP)\n",
    "        rec = acc_TP / npos\n",
    "        prec = np.divide(acc_TP, (acc_FP + acc_TP))\n",
    "\n",
    "        if method == \"AP\":\n",
    "            [ap, mpre, mrec, ii] = calculateAveragePrecision(rec, prec)\n",
    "        else:\n",
    "            [ap, mpre, mrec, _] = ElevenPointInterpolatedAP(rec, prec)\n",
    "\n",
    "        r = {\n",
    "            'class' : c,\n",
    "            'precision' : prec,\n",
    "            'recall' : rec,\n",
    "            'AP' : ap,\n",
    "            'interpolated precision' : mpre,\n",
    "            'interpolated recall' : mrec,\n",
    "            'total positives' : npos,\n",
    "            'total TP' : np.sum(TP),\n",
    "            'total FP' : np.sum(FP)\n",
    "        }\n",
    "\n",
    "        result.append(r)\n",
    "\n",
    "    return result\n",
    "\n",
    "result = AP(detections, groundtruths, classes)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e15eef",
   "metadata": {},
   "source": [
    "## mAP(mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5369aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mAP(result):\n",
    "    ap = 0\n",
    "    for r in result:\n",
    "        ap += r['AP']\n",
    "    mAP = ap / len(result)\n",
    "    \n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7b03569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unformed AP : 0.8387018270313467\n",
      "  Burr   AP : nan\n",
      "---------------------------\n",
      "mAP : nan\n"
     ]
    }
   ],
   "source": [
    "for r in result:\n",
    "    print(\"{:^8} AP : {}\".format(num2class[str(r['class'])], r['AP']))\n",
    "print(\"---------------------------\")\n",
    "print(f\"mAP : {mAP(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6c0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
